{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "38a14c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "import logging\n",
    "from bs4 import BeautifulSoup\n",
    "import html2text\n",
    "import httpx\n",
    "import yaml\n",
    "import json\n",
    "from pydantic import Field, BaseModel\n",
    "from langgraph.graph import MessagesState, StateGraph, START, END\n",
    "from typing import List\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage, ToolMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.types import Send\n",
    "import operator\n",
    "from typing import Annotated\n",
    "from typing import NamedTuple\n",
    "import composio_langchain\n",
    "\n",
    "# Set up the logger\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,  # Set to DEBUG for detailed logs\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    "    handlers=[\n",
    "        # logging.FileHandler(\"scraper.log\"),  # Log to a file\n",
    "        logging.StreamHandler()  # Log to console\n",
    "    ]\n",
    ")\n",
    "\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "5df1b6cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_documents_with_links_html(url: str) -> tuple[str, list[tuple[str, str]]]:\n",
    "    \"\"\"Fetch a document from a URL, return the markdownified text with links as markdown, and extract links with their titles.\n",
    "\n",
    "    Args:\n",
    "        url (str): The URL of the document to fetch.\n",
    "\n",
    "    Returns:\n",
    "        tuple[str, list[tuple[str, str]]]: A tuple containing the markdownified text of the document with links, and a list of (link, title) tuples.\n",
    "    \"\"\"\n",
    "    httpx_client = httpx.Client(follow_redirects=True, timeout=10)\n",
    "    links = []\n",
    "\n",
    "    try:\n",
    "        response = httpx_client.get(url, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        html_content = response.text\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "        target_div = soup.find('div', class_= \"theme-doc-markdown markdown\") #langchain\n",
    "\n",
    "        if not target_div:\n",
    "            target_div = soup.find('article') #langraph\n",
    "\n",
    "        if not target_div:\n",
    "            return \"\", links # Return empty text but still the links\n",
    "\n",
    "        # Extract links *before* converting to markdown\n",
    "        a_tags = target_div.find_all('a')\n",
    "        for a_tag in a_tags:\n",
    "            link = a_tag.get('href')\n",
    "            title = a_tag.get_text(strip=True)\n",
    "            if link:\n",
    "                links.append((link, title))\n",
    "\n",
    "        markdown_converter = html2text.HTML2Text()\n",
    "        markdown_converter.body_width = 0  # Disable line wrapping for links to stay on one line\n",
    "        markdown_text = markdown_converter.handle(str(target_div))\n",
    "\n",
    "        return markdown_text, links\n",
    "    except (httpx.HTTPStatusError, httpx.RequestError) as e:\n",
    "        return f\"Encountered an HTTP error: {str(e)}\", [] # Return error message and empty links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "d8e9e421",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ads4gpts', 'agentql', 'ainetwork', 'alpha_vantage', 'amadeus', 'apify_actors', 'arxiv', 'asknews', 'awslambda', 'azure_ai_services', 'azure_cognitive_services', 'azure_dynamic_sessions', 'bash', 'bearly', 'bing_search', 'brave_search', 'cassandra_database', 'cdp_agentkit', 'chatgpt_plugins', 'clickup', 'cogniswitch', 'connery', 'dalle_image_generator', 'dappier', 'databricks', 'dataforseo', 'dataherald', 'ddg', 'discord', 'e2b_data_analysis', 'edenai_tools', 'eleven_labs_tts', 'exa_search', 'filesystem', 'financial_datasets', 'fmp-data', 'github', 'gitlab', 'gmail', 'goat', 'golden_query', 'google_books', 'google_calendar', 'google_cloud_texttospeech', 'google_drive', 'google_finance', 'google_imagen', 'google_jobs', 'google_lens', 'google_places', 'google_scholar', 'google_search', 'google_serper', 'google_trends', 'gradio_tools', 'graphql', 'huggingface_tools', 'human_tools', 'hyperbrowser_browser_agent_tools', 'hyperbrowser_web_scraping_tools', 'ibm_watsonx', 'ifttt', 'infobip', 'ionic_shopping', 'jenkins', 'jina_search', 'jira', 'json', 'lemonai', 'linkup_search', 'memgraph', 'memorize', 'mojeek_search', 'multion', 'nasa', 'naver_search', 'nuclia', 'nvidia_riva', 'office365', 'openapi', 'openapi_nla', 'opengradient_toolkit', 'openweathermap', 'oracleai', 'oxylabs', 'pandas', 'passio_nutrition_ai', 'payman-tool', 'permit', 'playwright', 'polygon', 'powerbi', 'prolog_tool', 'pubmed', 'python', 'reddit_search', 'requests', 'riza', 'robocorp', 'salesforce', 'sceneXplain', 'scrapegraph', 'searchapi', 'searx_search', 'semanticscholar', 'serpapi', 'slack', 'spark_sql', 'sql_database', 'stackexchange', 'steam', 'stripe', 'tableau', 'taiga', 'tavily_extract', 'tavily_search', 'tilores', 'twilio', 'upstage_groundedness_check', 'valthera', 'valyu_context', 'wikidata', 'wikipedia', 'wolfram_alpha', 'writer', 'yahoo_finance_news', 'you', 'youtube', 'zapier', 'zenguard']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Your input text\n",
    "text = \"\"\"\n",
    "[ADS4GPTs](/docs/integrations/tools/ads4gpts)| Integrate AI native advertising into your Agentic application.  \n",
    "[AgentQL](/docs/integrations/tools/agentql)| AgentQL tools provides web interaction and structured data extraction...  \n",
    "[AINetwork Toolkit](/docs/integrations/tools/ainetwork)| AI Network is a layer 1 blockchain designed to accommodate large-scal...  \n",
    "[Alpha Vantage](/docs/integrations/tools/alpha_vantage)| Alpha Vantage Alpha Vantage provides realtime and historical financia...  \n",
    "[Amadeus Toolkit](/docs/integrations/tools/amadeus)| This notebook walks you through connecting LangChain to the Amadeus t...  \n",
    "[Apify Actor](/docs/integrations/tools/apify_actors)| Apify Actors are cloud programs designed for a wide range of web scra...  \n",
    "[ArXiv](/docs/integrations/tools/arxiv)| This notebook goes over how to use the arxiv tool with an agent.  \n",
    "[AskNews](/docs/integrations/tools/asknews)| AskNews infuses any LLM with the latest global news (or historical ne...  \n",
    "[AWS Lambda](/docs/integrations/tools/awslambda)| Amazon AWS Lambda is a serverless computing service provided by Amazo...  \n",
    "[Azure AI Services Toolkit](/docs/integrations/tools/azure_ai_services)| This toolkit is used to interact with the Azure AI Services API to ac...  \n",
    "[Azure Cognitive Services Toolkit](/docs/integrations/tools/azure_cognitive_services)| This toolkit is used to interact with the Azure Cognitive Services AP...  \n",
    "[Azure Container Apps dynamic sessions](/docs/integrations/tools/azure_dynamic_sessions)| Azure Container Apps dynamic sessions provides a secure and scalable ...  \n",
    "[Shell (bash)](/docs/integrations/tools/bash)| Giving agents access to the shell is powerful (though risky outside a...  \n",
    "[Bearly Code Interpreter](/docs/integrations/tools/bearly)| Bearly Code Interpreter allows for remote execution of code. This mak...  \n",
    "[Bing Search](/docs/integrations/tools/bing_search)| Bing Search is an Azure service and enables safe, ad-free, location-a...  \n",
    "[Brave Search](/docs/integrations/tools/brave_search)| This notebook goes over how to use the Brave Search tool.  \n",
    "[Cassandra Database Toolkit](/docs/integrations/tools/cassandra_database)| Apache Cassandra® is a widely used database for storing transactional...  \n",
    "[CDP](/docs/integrations/tools/cdp_agentkit)| The CDP Agentkit toolkit contains tools that enable an LLM agent to i...  \n",
    "[ChatGPT Plugins](/docs/integrations/tools/chatgpt_plugins)| OpenAI has deprecated plugins.  \n",
    "[ClickUp Toolkit](/docs/integrations/tools/clickup)| ClickUp is an all-in-one productivity platform that provides small an...  \n",
    "[Cogniswitch Toolkit](/docs/integrations/tools/cogniswitch)| CogniSwitch is used to build production ready applications that can c...  \n",
    "[Connery Toolkit and Tools](/docs/integrations/tools/connery)| Using the Connery toolkit and tools, you can integrate Connery Action...  \n",
    "[Dall-E Image Generator](/docs/integrations/tools/dalle_image_generator)| OpenAI Dall-E are text-to-image models developed by OpenAI using deep...  \n",
    "[Dappier](/docs/integrations/tools/dappier)| Dappier connects any LLM or your Agentic AI to real-time, rights-clea...  \n",
    "[Databricks Unity Catalog (UC)](/docs/integrations/tools/databricks)| This notebook shows how to use UC functions as LangChain tools, with ...  \n",
    "[DataForSEO](/docs/integrations/tools/dataforseo)| DataForSeo provides comprehensive SEO and digital marketing data solu...  \n",
    "[Dataherald](/docs/integrations/tools/dataherald)| This notebook goes over how to use the dataherald component.  \n",
    "[DuckDuckGo Search](/docs/integrations/tools/ddg)| This guide shows over how to use the DuckDuckGo search component.  \n",
    "[Discord](/docs/integrations/tools/discord)| This notebook provides a quick overview for getting started with Disc...  \n",
    "[E2B Data Analysis](/docs/integrations/tools/e2b_data_analysis)| E2B's cloud environments are great runtime sandboxes for LLMs.  \n",
    "[Eden AI](/docs/integrations/tools/edenai_tools)| This Jupyter Notebook demonstrates how to use Eden AI tools with an A...  \n",
    "[ElevenLabs Text2Speech](/docs/integrations/tools/eleven_labs_tts)| This notebook shows how to interact with the ElevenLabs API to achiev...  \n",
    "[Exa Search](/docs/integrations/tools/exa_search)| Exa is a search engine fully designed for use by LLMs. Search for doc...  \n",
    "[File System](/docs/integrations/tools/filesystem)| LangChain provides tools for interacting with a local file system out...  \n",
    "[FinancialDatasets Toolkit](/docs/integrations/tools/financial_datasets)| The financial datasets stock market API provides REST endpoints that ...  \n",
    "[FMP Data](/docs/integrations/tools/fmp-data)| Access financial market data through natural language queries.  \n",
    "[Github Toolkit](/docs/integrations/tools/github)| The Github toolkit contains tools that enable an LLM agent to interac...  \n",
    "[Gitlab Toolkit](/docs/integrations/tools/gitlab)| The Gitlab toolkit contains tools that enable an LLM agent to interac...  \n",
    "[Gmail Toolkit](/docs/integrations/tools/gmail)| This will help you getting started with the GMail toolkit. This toolk...  \n",
    "[GOAT](/docs/integrations/tools/goat)| GOAT is the finance toolkit for AI agents.  \n",
    "[Golden Query](/docs/integrations/tools/golden_query)| Golden provides a set of natural language APIs for querying and enric...  \n",
    "[Google Books](/docs/integrations/tools/google_books)| Overview  \n",
    "[Google Calendar Toolkit](/docs/integrations/tools/google_calendar)| Google Calendar is a product of Google Workspace that allows users to...  \n",
    "[Google Cloud Text-to-Speech](/docs/integrations/tools/google_cloud_texttospeech)| Google Cloud Text-to-Speech enables developers to synthesize natural-...  \n",
    "[Google Drive](/docs/integrations/tools/google_drive)| This notebook walks through connecting a LangChain to the Google Driv...  \n",
    "[Google Finance](/docs/integrations/tools/google_finance)| This notebook goes over how to use the Google Finance Tool to get inf...  \n",
    "[Google Imagen](/docs/integrations/tools/google_imagen)| Imagen on Vertex AI brings Google's state of the art image generative...  \n",
    "[Google Jobs](/docs/integrations/tools/google_jobs)| This notebook goes over how to use the Google Jobs Tool to fetch curr...  \n",
    "[Google Lens](/docs/integrations/tools/google_lens)| This notebook goes over how to use the Google Lens Tool to fetch info...  \n",
    "[Google Places](/docs/integrations/tools/google_places)| This notebook goes through how to use Google Places API  \n",
    "[Google Scholar](/docs/integrations/tools/google_scholar)| This notebook goes through how to use Google Scholar Tool  \n",
    "[Google Search](/docs/integrations/tools/google_search)| This notebook goes over how to use the google search component.  \n",
    "[Google Serper](/docs/integrations/tools/google_serper)| This notebook goes over how to use the Google Serper component to sea...  \n",
    "[Google Trends](/docs/integrations/tools/google_trends)| This notebook goes over how to use the Google Trends Tool to fetch tr...  \n",
    "[Gradio](/docs/integrations/tools/gradio_tools)| There are many 1000s of Gradio apps on Hugging Face Spaces. This libr...  \n",
    "[GraphQL](/docs/integrations/tools/graphql)| GraphQL is a query language for APIs and a runtime for executing thos...  \n",
    "[HuggingFace Hub Tools](/docs/integrations/tools/huggingface_tools)| Huggingface Tools that supporting text I/O can be  \n",
    "[Human as a tool](/docs/integrations/tools/human_tools)| Human are AGI so they can certainly be used as a tool to help out AI ...  \n",
    "[Hyperbrowser Browser Agent Tools](/docs/integrations/tools/hyperbrowser_browser_agent_tools)| Hyperbrowser is a platform for running, running browser agents, and s...  \n",
    "[Hyperbrowser Web Scraping Tools](/docs/integrations/tools/hyperbrowser_web_scraping_tools)| Hyperbrowser is a platform for running and scaling headless browsers....  \n",
    "[IBM watsonx.ai](/docs/integrations/tools/ibm_watsonx)| WatsonxToolkit is a wrapper for IBM watsonx.ai Toolkit.  \n",
    "[IFTTT WebHooks](/docs/integrations/tools/ifttt)| This notebook shows how to use IFTTT Webhooks.  \n",
    "[Infobip](/docs/integrations/tools/infobip)| This notebook that shows how to use Infobip API wrapper to send SMS m...  \n",
    "[Ionic Shopping Tool](/docs/integrations/tools/ionic_shopping)| Ionic is a plug and play ecommerce marketplace for AI Assistants. By ...  \n",
    "[Jenkins](/docs/integrations/tools/jenkins)| Tools for interacting with Jenkins.  \n",
    "[Jina Search](/docs/integrations/tools/jina_search)| This notebook provides a quick overview for getting started with Jina...  \n",
    "[Jira Toolkit](/docs/integrations/tools/jira)| This notebook goes over how to use the Jira toolkit.  \n",
    "[JSON Toolkit](/docs/integrations/tools/json)| This notebook showcases an agent interacting with large JSON/dict obj...  \n",
    "[Lemon Agent](/docs/integrations/tools/lemonai)| Lemon Agent helps you build powerful AI assistants in minutes and aut...  \n",
    "[LinkupSearchTool](/docs/integrations/tools/linkup_search)| Linkup provides an API to connect LLMs to the web and the Linkup Prem...  \n",
    "[Memgraph](/docs/integrations/tools/memgraph)| Overview  \n",
    "[Memorize](/docs/integrations/tools/memorize)| Fine-tuning LLM itself to memorize information using unsupervised lea...  \n",
    "[Mojeek Search](/docs/integrations/tools/mojeek_search)| The following notebook will explain how to get results using Mojeek S...  \n",
    "[MultiOn Toolkit](/docs/integrations/tools/multion)| MultiON has built an AI Agent that can interact with a broad array of...  \n",
    "[NASA Toolkit](/docs/integrations/tools/nasa)| This notebook shows how to use agents to interact with the NASA toolk...  \n",
    "[Naver Search](/docs/integrations/tools/naver_search)| Overview  \n",
    "[Nuclia Understanding](/docs/integrations/tools/nuclia)| Nuclia automatically indexes your unstructured data from any internal...  \n",
    "[NVIDIA Riva: ASR and TTS](/docs/integrations/tools/nvidia_riva)| NVIDIA Riva  \n",
    "[Office365 Toolkit](/docs/integrations/tools/office365)| Microsoft 365 is a product family of productivity software, collabora...  \n",
    "[OpenAPI Toolkit](/docs/integrations/tools/openapi)| We can construct agents to consume arbitrary APIs, here APIs conforma...  \n",
    "[Natural Language API Toolkits](/docs/integrations/tools/openapi_nla)| Natural Language API Toolkits (NLAToolkits) permit LangChain Agents t...  \n",
    "[OpenGradient](/docs/integrations/tools/opengradient_toolkit)| This notebook shows how to build tools using the OpenGradient toolkit...  \n",
    "[OpenWeatherMap](/docs/integrations/tools/openweathermap)| This notebook goes over how to use the OpenWeatherMap component to fe...  \n",
    "[Oracle AI Vector Search: Generate Summary](/docs/integrations/tools/oracleai)| Oracle AI Vector Search is designed for Artificial Intelligence (AI) ...  \n",
    "[Oxylabs](/docs/integrations/tools/oxylabs)| Oxylabs is a market-leading web intelligence collection platform, dri...  \n",
    "[Pandas Dataframe](/docs/integrations/tools/pandas)| This notebook shows how to use agents to interact with a Pandas DataF...  \n",
    "[Passio NutritionAI](/docs/integrations/tools/passio_nutrition_ai)| To best understand how NutritionAI can give your agents super food-nu...  \n",
    "[PaymanAI](/docs/integrations/tools/payman-tool)| PaymanAI provides functionality to send and receive payments (fiat an...  \n",
    "[Permit](/docs/integrations/tools/permit)| Permit is an access control platform that provides fine-grained, real...  \n",
    "[PlayWright Browser Toolkit](/docs/integrations/tools/playwright)| Playwright is an open-source automation tool developed by Microsoft t...  \n",
    "[Polygon IO Toolkit and Tools](/docs/integrations/tools/polygon)| This notebook shows how to use agents to interact with the Polygon IO...  \n",
    "[PowerBI Toolkit](/docs/integrations/tools/powerbi)| This notebook showcases an agent interacting with a Power BI Dataset....  \n",
    "[Prolog](/docs/integrations/tools/prolog_tool)| LangChain tools that use Prolog rules to generate answers.  \n",
    "[PubMed](/docs/integrations/tools/pubmed)| PubMed® comprises more than 35 million citations for biomedical liter...  \n",
    "[Python REPL](/docs/integrations/tools/python)| Sometimes, for complex calculations, rather than have an LLM generate...  \n",
    "[Reddit Search](/docs/integrations/tools/reddit_search)| In this notebook, we learn how the Reddit search tool works.  \n",
    "[Requests Toolkit](/docs/integrations/tools/requests)| We can use the Requests toolkit to construct agents that generate HTT...  \n",
    "[Riza Code Interpreter](/docs/integrations/tools/riza)| The Riza Code Interpreter is a WASM-based isolated environment for ru...  \n",
    "[Robocorp Toolkit](/docs/integrations/tools/robocorp)| This notebook covers how to get started with Robocorp Action Server a...  \n",
    "[Salesforce](/docs/integrations/tools/salesforce)| Tools for interacting with Salesforce.  \n",
    "[SceneXplain](/docs/integrations/tools/sceneXplain)| SceneXplain is an ImageCaptioning service accessible through the Scen...  \n",
    "[ScrapeGraph](/docs/integrations/tools/scrapegraph)| This notebook provides a quick overview for getting started with Scra...  \n",
    "[SearchApi](/docs/integrations/tools/searchapi)| This notebook shows examples of how to use SearchApi to search the we...  \n",
    "[SearxNG Search](/docs/integrations/tools/searx_search)| This notebook goes over how to use a self hosted SearxNG search API t...  \n",
    "[Semantic Scholar API Tool](/docs/integrations/tools/semanticscholar)| This notebook demos how to use the semantic scholar tool with an agen...  \n",
    "[SerpAPI](/docs/integrations/tools/serpapi)| This notebook goes over how to use the SerpAPI component to search th...  \n",
    "[Slack Toolkit](/docs/integrations/tools/slack)| This will help you getting started with the Slack toolkit. For detail...  \n",
    "[Spark SQL Toolkit](/docs/integrations/tools/spark_sql)| This notebook shows how to use agents to interact with Spark SQL. Sim...  \n",
    "[SQLDatabase Toolkit](/docs/integrations/tools/sql_database)| This will help you getting started with the SQL Database toolkit. For...  \n",
    "[StackExchange](/docs/integrations/tools/stackexchange)| Stack Exchange is a network of question-and-answer (Q&A) websites on ...  \n",
    "[Steam Toolkit](/docs/integrations/tools/steam)| Steam (Wikipedia)) is a video game digital distribution service and s...  \n",
    "[Stripe](/docs/integrations/tools/stripe)| This notebook provides a quick overview for getting started with Stri...  \n",
    "[Tableau](/docs/integrations/tools/tableau)| This notebook provides a quick overview for getting started with Tabl...  \n",
    "[Taiga](/docs/integrations/tools/taiga)| This notebook provides a quick overview for getting started with Taig...  \n",
    "[Tavily Extract](/docs/integrations/tools/tavily_extract)| Tavily is a search engine built specifically for AI agents (LLMs), de...  \n",
    "[Tavily Search](/docs/integrations/tools/tavily_search)| Tavily's Search API is a search engine built specifically for AI agen...  \n",
    "[Tilores](/docs/integrations/tools/tilores)| This notebook covers how to get started with the Tilores tools.  \n",
    "[Twilio](/docs/integrations/tools/twilio)| This notebook goes over how to use the Twilio API wrapper to send a m...  \n",
    "[Upstage](/docs/integrations/tools/upstage_groundedness_check)| This notebook covers how to get started with Upstage groundedness che...  \n",
    "[Valthera](/docs/integrations/tools/valthera)| Enable AI agents to engage users when they're most likely to respond.  \n",
    "[ValyuContext](/docs/integrations/tools/valyu_context)| Valyu allows AI applications and agents to search the internet and pr...  \n",
    "[Wikidata](/docs/integrations/tools/wikidata)| Wikidata is a free and open knowledge base that can be read and edite...  \n",
    "[Wikipedia](/docs/integrations/tools/wikipedia)| Wikipedia is a multilingual free online encyclopedia written and main...  \n",
    "[Wolfram Alpha](/docs/integrations/tools/wolfram_alpha)| This notebook goes over how to use the wolfram alpha component.  \n",
    "[Writer Tools](/docs/integrations/tools/writer)| This notebook provides a quick overview for getting started with Writ...  \n",
    "[Yahoo Finance News](/docs/integrations/tools/yahoo_finance_news)| This notebook goes over how to use the yahoofinancenews tool with an ...  \n",
    "[You.com Search](/docs/integrations/tools/you)| The you.com API is a suite of tools designed to help developers groun...  \n",
    "[YouTube](/docs/integrations/tools/youtube)| YouTube Search package searches YouTube videos avoiding using their h...  \n",
    "[Zapier Natural Language Actions](/docs/integrations/tools/zapier)| Deprecated This API will be sunset on 2023-11-17//nla.zapier.com/star...  \n",
    "[ZenGuard AI](/docs/integrations/tools/zenguard)| This tool lets you quickly set up ZenGuard AI in your Langchain-power...\n",
    "\"\"\"\n",
    "\n",
    "# Base URL for formatting\n",
    "base_url = \"https://python.langchain.com/docs/integrations/tools/\"\n",
    "\n",
    "# Regex to find and extract text after \"/docs/integrations/tools/\" stopping at \")\"\n",
    "pattern = r\"/docs/integrations/tools/([^)\\s]+)\"\n",
    "matches = re.findall(pattern, text)\n",
    "\n",
    "\n",
    "# Format matches into full links\n",
    "links_tools = [base_url + match for match in matches]\n",
    "name_tools = [match for match in matches]\n",
    "\n",
    "# Print the extracted links\n",
    "print(name_tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "99a87aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import html2text\n",
    "import httpx\n",
    "\n",
    "def fetch_documents(url: str) -> str:\n",
    "    \"\"\"Fetch a document from a URL and return the markdownified text.\n",
    "\n",
    "    Args:\n",
    "        url (str): The URL of the document to fetch.\n",
    "\n",
    "    Returns:\n",
    "        str: The markdownified text of the document.\n",
    "    \"\"\"\n",
    "    httpx_client = httpx.Client(follow_redirects=True, timeout=10)\n",
    "\n",
    "    try:\n",
    "        response = httpx_client.get(url, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        html_content = response\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    \n",
    "        img_tags = soup.find_all('img')\n",
    "        for img_tag in img_tags:\n",
    "            img_tag.decompose()\n",
    "\n",
    "        target_div = soup.find('div', class_= \"theme-doc-markdown markdown\") #langchain\n",
    "        \n",
    "        if not target_div:\n",
    "            target_div = soup.find('article') #langraph\n",
    "\n",
    "        if not target_div:\n",
    "            target_div = soup.find('html') #langraph\n",
    "\n",
    "        if not target_div:\n",
    "            return html2text.html2text(str(soup))\n",
    "        \n",
    "        return html2text.html2text(str(target_div))\n",
    "    except (httpx.HTTPStatusError, httpx.RequestError) as e:\n",
    "        return f\"Encountered an HTTP error: {str(e)}\"\n",
    "\n",
    "\n",
    "def extract_text_between_headings(text):\n",
    "    \"\"\"\n",
    "    Extracts the text between the first and second headings in the given text.\n",
    "\n",
    "    Parameters:\n",
    "        text (str): The input document containing headings and content.\n",
    "\n",
    "    Returns:\n",
    "        str: Extracted text between the first and second headings, or None if not found.\n",
    "    \"\"\"\n",
    "    # Regex to match the first heading and the second heading\n",
    "    pattern = r\"#\\s*[^\\n]+\\s*(.*?)\\s*##\\s*[^\\n]+\"\n",
    "    match = re.search(pattern, text, re.DOTALL)\n",
    "\n",
    "    # Extract the text between the headings\n",
    "    return match.group(1).strip() if match else None\n",
    "dict_tool_link = {}\n",
    "dict_tool_doc = {}\n",
    "    # dict_tool_doc[name_tools[i]] = extract_text_between_headings(fetch_documents(links_tools[i]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "b200eaaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Serialize data into file:\n",
    "# json.dump( dict_tool_doc, open( \"tools_doc_json.json\", 'w' ) )\n",
    "# json.dump( dict_tool_link, open( \"tools_link_json.json\", 'w' ) )\n",
    "# Read data from file:\n",
    "# data = json.load( open( \"tools_json.json\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "a16c35c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_tool_link = json.load( open( \"tools_link_json.json\") )\n",
    "dict_tool_doc = json.load( open( \"tools_doc_json.json\") )\n",
    "def lowercase_keys(input_dict):\n",
    "    \"\"\"\n",
    "    Returns a new dictionary with all keys converted to lowercase.\n",
    "    \"\"\"\n",
    "    return {k.lower(): v for k, v in input_dict.items()}\n",
    "\n",
    "dict_tool_link = lowercase_keys(dict_tool_link)\n",
    "dict_tool_doc = lowercase_keys(dict_tool_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "15bfaf08",
   "metadata": {},
   "outputs": [],
   "source": [
    "Initial_prompt = \"\"\"You are an expert python developer. You will be given a description of a python function. \n",
    "\n",
    "You job is to estimate and extract the following information:\n",
    "\n",
    "- What exactly does this python do. What is the detailed objective of the function. Please write 1-5 lines\n",
    "- Suggest or extract the name of the the function\n",
    "- What would be the inputs/arguements required into this function to make it work. Please all mentioned the type of each input\n",
    "- WHat would be output produced by this input. Please mention the output type \n",
    "\n",
    "Here is the description of the function you need to create:\n",
    "<description>\n",
    "{desc}\n",
    "</description>\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "5b542135",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=0, model=\"gpt-4o-mini\", streaming=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "e7cfc715",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class FunctionInstructions(BaseModel):\n",
    "    \"\"\"Instructions for defining a python function\"\"\"\n",
    "    objective: str = Field(description= \"what does this pythion function do\")\n",
    "    name: str = Field(description=\"name of the python function\")\n",
    "    input : List[str] = Field(description= \"what would be the input arguements to this function along with the types\")\n",
    "    output: List[str] = Field(description=\"what would be the output/return attributes for the function along with the types\")\n",
    "    name_toolkit: str = Field(description=\"what would be the toolkit/ code SDK that will be used\")\n",
    "    code: str = Field(description=\"the final python code\")\n",
    "# Annotated[str, operator.add]\n",
    "\n",
    "class CodebuilderState(BaseModel):\n",
    "    \"\"\"Instructions for defining a python function\"\"\"\n",
    "    code: str = Field(description= \"tailored code for the python function\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "0ae12f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def functional_analysis_node(state: FunctionInstructions):\n",
    "  print(\"functional_analysis_node\")\n",
    "  llm_with_structured_output = llm.with_structured_output(FunctionInstructions)\n",
    "  functionalReport: FunctionInstructions = llm_with_structured_output.invoke(\n",
    "      [SystemMessage(content=Initial_prompt.format(desc = state.objective))])\n",
    "  return {  \"messages\": [AIMessage(content=\"Generated JSON code!\")],\n",
    "           \"objective\": functionalReport.objective,\n",
    "           \"name\": functionalReport.name,\n",
    "           \"input\": functionalReport.input,\n",
    "           \"output\": functionalReport.output}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "4fe0565c",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_code_prompt = \"\"\"You are an expert Python developer tasked with creating Python functions (tools) based on user requests.\n",
    "\n",
    "            Your process is as follows:\n",
    "            1. Understand the user's request for a tool (e.g., \"tool to send a discord message\").\n",
    "            2. Find relevant Python SDKs for the core task.\n",
    "            3. See if Composio offers an integration for the relevant service (e.g., 'discord').\n",
    "            4. Analyze the results:\n",
    "                - If Composio has an integration, prioritize generating code that utilizes Composio (assume this involves calling a hypothetical 'composio.run_action()' function). Include a comment explaining this choice.\n",
    "                - If Composio does not have a clear integration, choose the most promising Python SDK found\n",
    "                - If no suitable SDK is found, state that you cannot create the function.\n",
    "            5. Generate *only* the complete, runnable Python function code based on your decision.\n",
    "                - The function should have clear arguments based on the user's likely intent (e.g., for discord, `channel_id` and `message_text`).\n",
    "                - Include a comprehensive docstring explaining the function, its arguments, and what it returns.\n",
    "                - Use type hints for all arguments and the return type.\n",
    "                - If using a standard SDK, add a comment indicating which SDK is intended (e.g., `# Uses discord.py`).\n",
    "                - If using Composio, structure the function to call `composio.run_action('service_name', 'action_name', params={{...}})` (you'll need to infer 'service_name' and 'action_name' and necessary params). Add comments explaining this structure.\n",
    "            6. Do not include any explanatory text before or after the code block. Output only the Python code for the function.\n",
    "\n",
    "    Here are some details about the python function you will be creating:\n",
    "    <objective>\n",
    "    {objective}\n",
    "    </objective>\n",
    "\n",
    "    <input schema>\n",
    "    {inputs}\n",
    "    </input schema>\n",
    "\n",
    "    <output schema>\n",
    "    {output}\n",
    "    </output schema>\n",
    "\n",
    "    <name of function>\n",
    "    {name}\n",
    "    </name of function>\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "77bee104",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_code_prompt = \"\"\"\n",
    "You are a skilled code generation assistant. Your task is to create executable code using the following information:\n",
    "- SDK Documentation: The provided documentation outlines the functionalities and usage details of the SDK. Use this as the reference for constructing your code.\n",
    "- Objective: A clear description of what the code is intended to achieve.\n",
    "- Input: The expected input for the code (e.g., variables, parameters, data types).\n",
    "- Output: The desired result or outcome of the code (e.g., format, type, or structure).\n",
    "- SDK Name: The name of the SDK that must be used in the code.\n",
    "\n",
    "Your goal is to generate executable code that:\n",
    "- Adheres to the requirements outlined above.\n",
    "- Follows standard coding practices and is optimized for readability and efficiency.\n",
    "- Utilizes the specified SDK appropriately based on the documentation provided.\n",
    "- Only return a self contained function\n",
    "- Your output should only contain a code block containing the required function and nothing else. Please do no include any explainantions\n",
    "- Write your code in python\n",
    "- Please also provide which API keys will be required and define the API keys as part of the function\n",
    "- Please also write the doc string for the python function\n",
    "\n",
    "Here are some details about the python function you will be creating:\n",
    "<objective>\n",
    "{objective}\n",
    "</objective>\n",
    "\n",
    "<input schema>\n",
    "{inputs}\n",
    "</input schema>\n",
    "\n",
    "<output schema>\n",
    "{output}\n",
    "</output schema>\n",
    "\n",
    "<name of function>\n",
    "{name}\n",
    "</name of function>\n",
    "\n",
    "Documentation for SDK that might be helpful:\n",
    "<documentation>\n",
    "{docs}\n",
    "</documentation>\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "5a2a51bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "Best_sdk_prompt = \"\"\"\n",
    "You are a highly specialized language model designed to assist in selecting the most suitable SDK for a given use case. You are provided with the following:\n",
    "- A dictionary containing pairs of SDK names and their respective descriptions.\n",
    "- Requirements for a piece of code, including the objective, input, and output.\n",
    "\n",
    "Your task is to:\n",
    "- Identify the SDK from the provided dictionary whose description best matches the given use case described in the code requirements.\n",
    "- Also give preferences to SDKs that are generally more well known or are used more frequently in the industry (Use google tools for anything search related)\n",
    "- Return only the name of the matching SDK without any additional text or formatting.\n",
    "\n",
    "Input Example:\n",
    "Dictionary:\n",
    "{{\n",
    "\"SDK_A\": \"Provides tools for web scraping and data extraction.\",\n",
    "\"SDK_B\": \"Enables natural language processing for unstructured text.\",\n",
    "\"SDK_C\": \"Facilitates the integration of payment gateways in applications.\"\n",
    "}}\n",
    "Code Requirements:\n",
    "Objective: Extract data from multiple web pages.\n",
    "Input: URLs of the web pages.\n",
    "Output: Structured data in JSON format.\n",
    "\n",
    "Expected Output:\n",
    "SDK_A\n",
    "\n",
    "\n",
    "Input :\n",
    "<dictionary>\n",
    "{dictionary}\n",
    "</dictionary>\n",
    "\n",
    "<objective>\n",
    "{objective}\n",
    "</objective>\n",
    "\n",
    "<input schema>\n",
    "{inputs}\n",
    "</input schema>\n",
    "\n",
    "<output schema>\n",
    "{output}\n",
    "</output schema>\n",
    "\n",
    "<name of function>\n",
    "{name}\n",
    "</name of function>\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "caa0cd7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def sdk_production_node(state: FunctionInstructions):\n",
    "    objective_agent: str = state.objective\n",
    "    name: str = state.name\n",
    "    input_args : List[str] = state.input\n",
    "    output_args: List[str] = state.output\n",
    "    response = llm.invoke([SystemMessage(content=Best_sdk_prompt.format(\n",
    "          objective=objective_agent,\n",
    "          inputs=input_args,\n",
    "          output=output_args,\n",
    "          name=name,\n",
    "          dictionary = dict_tool_doc\n",
    "    ))])\n",
    "    code_snips = response\n",
    "    return {\n",
    "            \"name_toolkit\": response.content.lower()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "deb96806",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def code_production_node(state: FunctionInstructions):\n",
    "    objective_agent: str = state.objective\n",
    "    name: str = state.name\n",
    "    input_args : List[str] = state.input\n",
    "    output_args: List[str] = state.output\n",
    "    toolkit: str = state.name_toolkit\n",
    "    docs = fetch_documents(dict_tool_link[toolkit])\n",
    "    response = llm.invoke([SystemMessage(content=write_code_prompt.format(\n",
    "          objective=objective_agent,\n",
    "          inputs=input_args,\n",
    "          output=output_args,\n",
    "          name=name,\n",
    "          docs = docs,\n",
    "    ))])\n",
    "    print(response.content)\n",
    "    code_snips = response\n",
    "    return {\n",
    "            \"code\": response.content}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "a698194b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "workflow = StateGraph(FunctionInstructions)\n",
    "workflow.add_node(\"func_analysis\", functional_analysis_node)\n",
    "workflow.add_node(\"sdk_write\", sdk_production_node)\n",
    "workflow.add_node(\"code_write\", code_production_node)\n",
    "checkpointer = InMemorySaver()\n",
    "workflow.add_edge(\"code_write\", END)\n",
    "workflow.add_edge(\"sdk_write\",\"code_write\")\n",
    "workflow.add_edge(\"func_analysis\",\"sdk_write\")\n",
    "workflow.add_edge(START, \"func_analysis\")\n",
    "tool_infograph = workflow.compile(checkpointer=checkpointer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f3217ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class toolcollector(MessagesState):\n",
    "    tool : List[FunctionInstructions] = Field(description= \"what tools are there\")\n",
    "    total_code: List[str]\n",
    "\n",
    "tool_desc_prompt = \"\"\"\n",
    "You are an AI assistant designed to analyze Python code. Your task is to identify all function definitions in the provided Python snippet that are decorated with @tool. You must return a dictionary where:\n",
    "- The keys are the names of the identified functions.\n",
    "- The values are descriptions of what each function is supposed to do. If a function contains a docstring, extract it as the description. If a docstring is missing, infer the function's purpose from its structure and comments.\n",
    "Example Input:\n",
    "@tool\n",
    "def calculate_area(length, width):\n",
    "    \"Calculates the area of a rectangle.\"\n",
    "    return length * width\n",
    "\n",
    "@tool\n",
    "def greet(name):\n",
    "    return f\"Hello, {{name}}!\"\n",
    "\n",
    "\n",
    "Expected Output:\n",
    "{\n",
    "    \"calculate_area\": \"Calculates the area of a rectangle.\",\n",
    "    \"greet\": \"Greets a user by name.\"\n",
    "}\n",
    "\n",
    "\n",
    "Instructions:\n",
    "- Identify functions that have the @tool decorator.\n",
    "- Extract function names and descriptions (either from docstrings or inferred).\n",
    "- Return the output as a structured JSON.\n",
    "- Please only return a json object that can be converted into a json directly. DO NOT RETURN ANYTHING OTHER THAN A JSON\n",
    "\n",
    "Python code:\n",
    "<code>\n",
    "from typing import Literal\n",
    "from langgraph.graph import StateGraph, MessagesState, START, END\n",
    "from langchain_core.messages import AIMessage\n",
    "from langchain_core.tools import tool\n",
    "from langgraph.prebuilt import ToolNode\n",
    "\n",
    "# Define the tools needed by the LLM\n",
    "\n",
    "@tool\n",
    "def get_weather(location: str):\n",
    "    \\\"\\\"\\\"Call to get the current weather.\\\"\\\"\\\"\n",
    "    if location.lower() in [\"sf\", \"san francisco\"]:\n",
    "        return \"It's 60 degrees and foggy.\"\n",
    "    else:\n",
    "        return \"It's 90 degrees and sunny.\"\n",
    "\n",
    "\n",
    "@tool\n",
    "def get_coolest_cities():\n",
    "    \\\"\\\"\\\"Get a list of coolest cities\\\"\\\"\\\"\n",
    "    return \"nyc, sf\"\n",
    "\n",
    "tools = [get_weather, get_coolest_cities]\n",
    "\n",
    "# Bind the model(llm) with tools\n",
    "model_with_tools = ChatAnthropic(\n",
    "    model=\"claude-3-haiku-20240307\", temperature=0\n",
    ").bind_tools(tools)\n",
    "\n",
    "# Generate a tool node.\n",
    "tool_node = ToolNode(tools)\n",
    "\n",
    "# conditional edge\n",
    "def should_continue(state: MessagesState):\n",
    "    messages = state[\"messages\"]\n",
    "    last_message = messages[-1]\n",
    "    if last_message.tool_calls:\n",
    "        return \"tools\"\n",
    "    return END\n",
    "</code>\n",
    "\n",
    "\"\"\"\n",
    "import uuid\n",
    "\n",
    "def graph_map_step(state: toolcollector):\n",
    "    # Extract nodes and edges from json_objects\n",
    "    \n",
    "    response_1 = llm.invoke([SystemMessage(content=tool_desc_prompt)])\n",
    "    json_objects = json.loads(response_1.content)\n",
    "    print(json_objects)\n",
    "    uuid_str = uuid.uuid4()\n",
    "    config = {\"configurable\": {\"thread_id\": str(uuid_str)}}\n",
    "    send = []\n",
    "    for key in json_objects:\n",
    "        print(key + \" \" + json_objects[key])\n",
    "        for output in tool_infograph.stream({\"objective\":key + \" \" + json_objects[key], \"name\": key, \"input\":[], \"output\": [], \"name_toolkit\": \"\", \"code\":\"\"}, config, stream_mode=\"updates\"):\n",
    "            print(output)\n",
    "        send.append(tool_infograph.get_state(config).values[\"code\"])\n",
    "    return {\n",
    "        \"total_code\": send\n",
    "    }\n",
    "\n",
    "# \"objective\":json_objects[key], \"name\": key, \"input\":[], \"output\": [], \"name_toolkit\": \"\", \"code\":\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "0ae3ec1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow1 = StateGraph(toolcollector)\n",
    "# workflow1.add_node(\"tool_infograph\", tool_infograph)\n",
    "workflow1.add_node(\"graph_map_step\", graph_map_step)\n",
    "\n",
    "workflow1.add_edge(START, \"graph_map_step\")\n",
    "workflow1.add_edge(\"graph_map_step\", END)\n",
    "infograph = workflow1.compile()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "450809e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'get_weather': 'Call to get the current weather.', 'get_coolest_cities': 'Get a list of coolest cities'}\n",
      "get_weather Call to get the current weather.\n",
      "functional_analysis_node\n",
      "{'func_analysis': {'objective': 'The function retrieves the current weather information for a specified location, typically using an external weather API to fetch real-time data.', 'name': 'get_weather', 'input': ['location: str', 'api_key: str'], 'output': ['weather_data: dict']}}\n",
      "{'sdk_write': {'name_toolkit': 'openweathermap'}}\n",
      "```python\n",
      "import os\n",
      "from langchain_community.utilities import OpenWeatherMapAPIWrapper\n",
      "\n",
      "def get_weather(location: str, api_key: str) -> dict:\n",
      "    \"\"\"\n",
      "    Retrieves the current weather information for a specified location using the OpenWeatherMap API.\n",
      "\n",
      "    Parameters:\n",
      "    location (str): The location for which to retrieve the weather information.\n",
      "    api_key (str): The API key for authenticating with the OpenWeatherMap API.\n",
      "\n",
      "    Returns:\n",
      "    dict: A dictionary containing the current weather data for the specified location.\n",
      "    \"\"\"\n",
      "    os.environ[\"OPENWEATHERMAP_API_KEY\"] = api_key\n",
      "    weather = OpenWeatherMapAPIWrapper()\n",
      "    weather_data = weather.run(location)\n",
      "    return weather_data\n",
      "```\n",
      "{'code_write': {'code': '```python\\nimport os\\nfrom langchain_community.utilities import OpenWeatherMapAPIWrapper\\n\\ndef get_weather(location: str, api_key: str) -> dict:\\n    \"\"\"\\n    Retrieves the current weather information for a specified location using the OpenWeatherMap API.\\n\\n    Parameters:\\n    location (str): The location for which to retrieve the weather information.\\n    api_key (str): The API key for authenticating with the OpenWeatherMap API.\\n\\n    Returns:\\n    dict: A dictionary containing the current weather data for the specified location.\\n    \"\"\"\\n    os.environ[\"OPENWEATHERMAP_API_KEY\"] = api_key\\n    weather = OpenWeatherMapAPIWrapper()\\n    weather_data = weather.run(location)\\n    return weather_data\\n```'}}\n",
      "get_coolest_cities Get a list of coolest cities\n",
      "functional_analysis_node\n",
      "{'func_analysis': {'objective': 'The function retrieves a list of cities that are considered the coolest based on certain criteria, such as climate, culture, or popularity.', 'name': 'get_coolest_cities', 'input': ['criteria: str', 'limit: int'], 'output': ['coolest_cities: list of str']}}\n",
      "{'sdk_write': {'name_toolkit': 'hyperbrowser_browser_agent_tools'}}\n",
      "```python\n",
      "from langchain_hyperbrowser import HyperbrowserBrowserUseTool\n",
      "\n",
      "def get_coolest_cities(criteria: str, limit: int) -> list:\n",
      "    \"\"\"\n",
      "    Retrieves a list of cities that are considered the coolest based on certain criteria,\n",
      "    such as climate, culture, or popularity.\n",
      "\n",
      "    Parameters:\n",
      "    criteria (str): The criteria to filter the coolest cities.\n",
      "    limit (int): The maximum number of cities to retrieve.\n",
      "\n",
      "    Returns:\n",
      "    list: A list of the coolest cities based on the specified criteria.\n",
      "    \"\"\"\n",
      "    # Define API key\n",
      "    HYPERBROWSER_API_KEY = \"<your-api-key>\"\n",
      "    \n",
      "    # Initialize the browser tool\n",
      "    tool = HyperbrowserBrowserUseTool()\n",
      "    \n",
      "    # Create the task to retrieve coolest cities\n",
      "    task = f\"Find the top {limit} coolest cities based on {criteria}.\"\n",
      "    \n",
      "    # Run the tool with the task\n",
      "    result = tool.run({\"task\": task})\n",
      "    \n",
      "    # Extract and return the list of coolest cities\n",
      "    return result.get('data', [])\n",
      "```\n",
      "{'code_write': {'code': '```python\\nfrom langchain_hyperbrowser import HyperbrowserBrowserUseTool\\n\\ndef get_coolest_cities(criteria: str, limit: int) -> list:\\n    \"\"\"\\n    Retrieves a list of cities that are considered the coolest based on certain criteria,\\n    such as climate, culture, or popularity.\\n\\n    Parameters:\\n    criteria (str): The criteria to filter the coolest cities.\\n    limit (int): The maximum number of cities to retrieve.\\n\\n    Returns:\\n    list: A list of the coolest cities based on the specified criteria.\\n    \"\"\"\\n    # Define API key\\n    HYPERBROWSER_API_KEY = \"<your-api-key>\"\\n    \\n    # Initialize the browser tool\\n    tool = HyperbrowserBrowserUseTool()\\n    \\n    # Create the task to retrieve coolest cities\\n    task = f\"Find the top {limit} coolest cities based on {criteria}.\"\\n    \\n    # Run the tool with the task\\n    result = tool.run({\"task\": task})\\n    \\n    # Extract and return the list of coolest cities\\n    return result.get(\\'data\\', [])\\n```'}}\n",
      "\"Output from node 'graph_map_step':\"\n",
      "'---'\n",
      "{ 'total_code': [ '```python\\n'\n",
      "                  'import os\\n'\n",
      "                  'from langchain_community.utilities import '\n",
      "                  'OpenWeatherMapAPIWrapper\\n'\n",
      "                  '\\n'\n",
      "                  'def get_weather(location: str, api_key: str) -> dict:\\n'\n",
      "                  '    \"\"\"\\n'\n",
      "                  '    Retrieves the current weather information for a '\n",
      "                  'specified location using the OpenWeatherMap API.\\n'\n",
      "                  '\\n'\n",
      "                  '    Parameters:\\n'\n",
      "                  '    location (str): The location for which to retrieve the '\n",
      "                  'weather information.\\n'\n",
      "                  '    api_key (str): The API key for authenticating with the '\n",
      "                  'OpenWeatherMap API.\\n'\n",
      "                  '\\n'\n",
      "                  '    Returns:\\n'\n",
      "                  '    dict: A dictionary containing the current weather data '\n",
      "                  'for the specified location.\\n'\n",
      "                  '    \"\"\"\\n'\n",
      "                  '    os.environ[\"OPENWEATHERMAP_API_KEY\"] = api_key\\n'\n",
      "                  '    weather = OpenWeatherMapAPIWrapper()\\n'\n",
      "                  '    weather_data = weather.run(location)\\n'\n",
      "                  '    return weather_data\\n'\n",
      "                  '```',\n",
      "                  '```python\\n'\n",
      "                  'from langchain_hyperbrowser import '\n",
      "                  'HyperbrowserBrowserUseTool\\n'\n",
      "                  '\\n'\n",
      "                  'def get_coolest_cities(criteria: str, limit: int) -> list:\\n'\n",
      "                  '    \"\"\"\\n'\n",
      "                  '    Retrieves a list of cities that are considered the '\n",
      "                  'coolest based on certain criteria,\\n'\n",
      "                  '    such as climate, culture, or popularity.\\n'\n",
      "                  '\\n'\n",
      "                  '    Parameters:\\n'\n",
      "                  '    criteria (str): The criteria to filter the coolest '\n",
      "                  'cities.\\n'\n",
      "                  '    limit (int): The maximum number of cities to retrieve.\\n'\n",
      "                  '\\n'\n",
      "                  '    Returns:\\n'\n",
      "                  '    list: A list of the coolest cities based on the '\n",
      "                  'specified criteria.\\n'\n",
      "                  '    \"\"\"\\n'\n",
      "                  '    # Define API key\\n'\n",
      "                  '    HYPERBROWSER_API_KEY = \"<your-api-key>\"\\n'\n",
      "                  '    \\n'\n",
      "                  '    # Initialize the browser tool\\n'\n",
      "                  '    tool = HyperbrowserBrowserUseTool()\\n'\n",
      "                  '    \\n'\n",
      "                  '    # Create the task to retrieve coolest cities\\n'\n",
      "                  '    task = f\"Find the top {limit} coolest cities based on '\n",
      "                  '{criteria}.\"\\n'\n",
      "                  '    \\n'\n",
      "                  '    # Run the tool with the task\\n'\n",
      "                  '    result = tool.run({\"task\": task})\\n'\n",
      "                  '    \\n'\n",
      "                  '    # Extract and return the list of coolest cities\\n'\n",
      "                  \"    return result.get('data', [])\\n\"\n",
      "                  '```']}\n",
      "'\\n---\\n'\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "from langgraph.graph import END, StateGraph, START, MessagesState\n",
    "from langgraph.prebuilt import ToolNode\n",
    "\n",
    "inputs = {\n",
    "    \"messages\": [\n",
    "        (\"user\", \"Create a python function that gets all my emails from gmail and filter them based on senders\"),\n",
    "    ]\n",
    "}\n",
    "\n",
    "code_snip = dict()\n",
    "for output in infograph.stream(inputs):\n",
    "    for key, value in output.items():\n",
    "        pprint.pprint(f\"Output from node '{key}':\")\n",
    "        pprint.pprint(\"---\")\n",
    "        pprint.pprint(value, indent=2, width=80, depth=None)\n",
    "        code_snip = value\n",
    "    pprint.pprint(\"\\n---\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "b8b1a563",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```python\n",
      "from langchain_hyperbrowser import HyperbrowserBrowserUseTool\n",
      "\n",
      "def get_coolest_cities(criteria: str, limit: int) -> list:\n",
      "    \"\"\"\n",
      "    Retrieves a list of cities that are considered the coolest based on certain criteria,\n",
      "    such as climate, culture, or popularity.\n",
      "\n",
      "    Parameters:\n",
      "    criteria (str): The criteria to filter the coolest cities.\n",
      "    limit (int): The maximum number of cities to retrieve.\n",
      "\n",
      "    Returns:\n",
      "    list: A list of the coolest cities based on the specified criteria.\n",
      "    \"\"\"\n",
      "    # Define API key\n",
      "    HYPERBROWSER_API_KEY = \"<your-api-key>\"\n",
      "    \n",
      "    # Initialize the browser tool\n",
      "    tool = HyperbrowserBrowserUseTool()\n",
      "    \n",
      "    # Create the task to retrieve coolest cities\n",
      "    task = f\"Find the top {limit} coolest cities based on {criteria}.\"\n",
      "    \n",
      "    # Run the tool with the task\n",
      "    result = tool.run({\"task\": task})\n",
      "    \n",
      "    # Extract and return the list of coolest cities\n",
      "    return result.get('data', [])\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "print(code_snip['total_code'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a886a64c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_map_step2(state: toolcollector):\n",
    "    # Extract nodes and edges from json_objects\n",
    "    \n",
    "    response_1 = llm.invoke([SystemMessage(content=tool_desc_prompt)])\n",
    "    json_objects = json.loads(response_1.content)\n",
    "    print(json_objects)\n",
    "    sends = []\n",
    "    for key in json_objects:\n",
    "        sends.append(Send(\"tool_infograph\", {\"objective\":key + \" \" + json_objects[key], \"name\": key, \"input\":[], \"output\": [], \"name_toolkit\": \"\", \"code\":\"\"}))    \n",
    "    return sends\n",
    "# \"objective\":json_objects[key], \"name\": key, \"input\":[], \"output\": [], \"name_toolkit\": \"\", \"code\":\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "a11ba401",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow2 = StateGraph(toolcollector)\n",
    "workflow2.add_node(\"tool_infograph\", tool_infograph)\n",
    "workflow2.add_node(\"graph_map_step2\", graph_map_step2)\n",
    "\n",
    "workflow2.add_edge(START, \"graph_map_step2\")\n",
    "workflow2.add_conditional_edges(START,graph_map_step2, [\"tool_infograph\"])\n",
    "workflow2.add_edge(\"tool_infograph\", END)\n",
    "infograph2 = workflow2.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "4be54077",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'get_weather': 'Call to get the current weather.', 'get_coolest_cities': 'Get a list of coolest cities'}\n"
     ]
    },
    {
     "ename": "InvalidUpdateError",
     "evalue": "Expected dict, got [Send(node='tool_infograph', arg={'objective': 'get_weather Call to get the current weather.', 'name': 'get_weather', 'input': [], 'output': [], 'name_toolkit': '', 'code': ''}), Send(node='tool_infograph', arg={'objective': 'get_coolest_cities Get a list of coolest cities', 'name': 'get_coolest_cities', 'input': [], 'output': [], 'name_toolkit': '', 'code': ''})]\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_GRAPH_NODE_RETURN_VALUE",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mInvalidUpdateError\u001b[39m                        Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[84]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m      5\u001b[39m inputs = {\n\u001b[32m      6\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m: [\n\u001b[32m      7\u001b[39m         (\u001b[33m\"\u001b[39m\u001b[33muser\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mCreate a python function that gets all my emails from gmail and filter them based on senders\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m      8\u001b[39m     ]\n\u001b[32m      9\u001b[39m }\n\u001b[32m     11\u001b[39m code_snip = \u001b[38;5;28mdict\u001b[39m()\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m \u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43minfograph\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpprint\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpprint\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mOutput from node \u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mkey\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m'\u001b[39;49m\u001b[33;43m:\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\langgraph\\pregel\\__init__.py:2377\u001b[39m, in \u001b[36mPregel.stream\u001b[39m\u001b[34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, checkpoint_during, debug, subgraphs)\u001b[39m\n\u001b[32m   2371\u001b[39m     \u001b[38;5;66;03m# Similarly to Bulk Synchronous Parallel / Pregel model\u001b[39;00m\n\u001b[32m   2372\u001b[39m     \u001b[38;5;66;03m# computation proceeds in steps, while there are channel updates.\u001b[39;00m\n\u001b[32m   2373\u001b[39m     \u001b[38;5;66;03m# Channel updates from step N are only visible in step N+1\u001b[39;00m\n\u001b[32m   2374\u001b[39m     \u001b[38;5;66;03m# channels are guaranteed to be immutable for the duration of the step,\u001b[39;00m\n\u001b[32m   2375\u001b[39m     \u001b[38;5;66;03m# with channel updates applied only at the transition between steps.\u001b[39;00m\n\u001b[32m   2376\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m loop.tick(input_keys=\u001b[38;5;28mself\u001b[39m.input_channels):\n\u001b[32m-> \u001b[39m\u001b[32m2377\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrunner\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtick\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2378\u001b[39m \u001b[43m            \u001b[49m\u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtasks\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2379\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstep_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2380\u001b[39m \u001b[43m            \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2381\u001b[39m \u001b[43m            \u001b[49m\u001b[43mget_waiter\u001b[49m\u001b[43m=\u001b[49m\u001b[43mget_waiter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2382\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   2383\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# emit output\u001b[39;49;00m\n\u001b[32m   2384\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01myield from\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2385\u001b[39m \u001b[38;5;66;03m# emit output\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\langgraph\\pregel\\runner.py:158\u001b[39m, in \u001b[36mPregelRunner.tick\u001b[39m\u001b[34m(self, tasks, reraise, timeout, retry_policy, get_waiter)\u001b[39m\n\u001b[32m    156\u001b[39m t = tasks[\u001b[32m0\u001b[39m]\n\u001b[32m    157\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m158\u001b[39m     \u001b[43mrun_with_retry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    159\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    160\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    161\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfigurable\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    162\u001b[39m \u001b[43m            \u001b[49m\u001b[43mCONFIG_KEY_CALL\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    163\u001b[39m \u001b[43m                \u001b[49m\u001b[43m_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    164\u001b[39m \u001b[43m                \u001b[49m\u001b[43mweakref\u001b[49m\u001b[43m.\u001b[49m\u001b[43mref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    165\u001b[39m \u001b[43m                \u001b[49m\u001b[43mretry\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    166\u001b[39m \u001b[43m                \u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweakref\u001b[49m\u001b[43m.\u001b[49m\u001b[43mref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    167\u001b[39m \u001b[43m                \u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    168\u001b[39m \u001b[43m                \u001b[49m\u001b[43msubmit\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msubmit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    169\u001b[39m \u001b[43m                \u001b[49m\u001b[43mreraise\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreraise\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    170\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    171\u001b[39m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    172\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    173\u001b[39m     \u001b[38;5;28mself\u001b[39m.commit(t, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    174\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\langgraph\\pregel\\retry.py:39\u001b[39m, in \u001b[36mrun_with_retry\u001b[39m\u001b[34m(task, retry_policy, configurable)\u001b[39m\n\u001b[32m     37\u001b[39m     task.writes.clear()\n\u001b[32m     38\u001b[39m     \u001b[38;5;66;03m# run the task\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m39\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtask\u001b[49m\u001b[43m.\u001b[49m\u001b[43mproc\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m.\u001b[49m\u001b[43minput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     40\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ParentCommand \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m     41\u001b[39m     ns: \u001b[38;5;28mstr\u001b[39m = config[CONF][CONFIG_KEY_CHECKPOINT_NS]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\langgraph\\utils\\runnable.py:624\u001b[39m, in \u001b[36mRunnableSeq.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    622\u001b[39m                 \u001b[38;5;28minput\u001b[39m = context.run(step.invoke, \u001b[38;5;28minput\u001b[39m, config, **kwargs)\n\u001b[32m    623\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m624\u001b[39m             \u001b[38;5;28minput\u001b[39m = \u001b[43mstep\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    625\u001b[39m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[32m    626\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\langgraph\\utils\\runnable.py:376\u001b[39m, in \u001b[36mRunnableCallable.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    374\u001b[39m         run_manager.on_chain_end(ret)\n\u001b[32m    375\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m376\u001b[39m     ret = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    377\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.recurse \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, Runnable):\n\u001b[32m    378\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ret.invoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\langgraph\\pregel\\write.py:99\u001b[39m, in \u001b[36mChannelWrite._write\u001b[39m\u001b[34m(self, input, config)\u001b[39m\n\u001b[32m     90\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_write\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Any, config: RunnableConfig) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     91\u001b[39m     writes = [\n\u001b[32m     92\u001b[39m         ChannelWriteEntry(write.channel, \u001b[38;5;28minput\u001b[39m, write.skip_none, write.mapper)\n\u001b[32m     93\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(write, ChannelWriteEntry) \u001b[38;5;129;01mand\u001b[39;00m write.value \u001b[38;5;129;01mis\u001b[39;00m PASSTHROUGH\n\u001b[32m   (...)\u001b[39m\u001b[32m     97\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m write \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.writes\n\u001b[32m     98\u001b[39m     ]\n\u001b[32m---> \u001b[39m\u001b[32m99\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdo_write\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    100\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    101\u001b[39m \u001b[43m        \u001b[49m\u001b[43mwrites\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    102\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    103\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\langgraph\\pregel\\write.py:144\u001b[39m, in \u001b[36mChannelWrite.do_write\u001b[39m\u001b[34m(config, writes, require_at_least_one_of)\u001b[39m\n\u001b[32m    142\u001b[39m     tuples.append((TASKS, w))\n\u001b[32m    143\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(w, ChannelWriteTupleEntry):\n\u001b[32m--> \u001b[39m\u001b[32m144\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ww := \u001b[43mw\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m(\u001b[49m\u001b[43mw\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m    145\u001b[39m         tuples.extend(ww)\n\u001b[32m    146\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(w, ChannelWriteEntry):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\langgraph\\graph\\state.py:770\u001b[39m, in \u001b[36mCompiledStateGraph.attach_node.<locals>._get_updates\u001b[39m\u001b[34m(input)\u001b[39m\n\u001b[32m    765\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    766\u001b[39m     msg = create_error_message(\n\u001b[32m    767\u001b[39m         message=\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mExpected dict, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28minput\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    768\u001b[39m         error_code=ErrorCode.INVALID_GRAPH_NODE_RETURN_VALUE,\n\u001b[32m    769\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m770\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m InvalidUpdateError(msg)\n",
      "\u001b[31mInvalidUpdateError\u001b[39m: Expected dict, got [Send(node='tool_infograph', arg={'objective': 'get_weather Call to get the current weather.', 'name': 'get_weather', 'input': [], 'output': [], 'name_toolkit': '', 'code': ''}), Send(node='tool_infograph', arg={'objective': 'get_coolest_cities Get a list of coolest cities', 'name': 'get_coolest_cities', 'input': [], 'output': [], 'name_toolkit': '', 'code': ''})]\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_GRAPH_NODE_RETURN_VALUE",
      "During task with name 'graph_map_step' and id '69c448bf-8c43-55bd-4fdd-5a9c3b97d10b'"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "from langgraph.graph import END, StateGraph, START, MessagesState\n",
    "from langgraph.prebuilt import ToolNode\n",
    "\n",
    "inputs = {\n",
    "    \"messages\": [\n",
    "        (\"user\", \"Create a python function that gets all my emails from gmail and filter them based on senders\"),\n",
    "    ]\n",
    "}\n",
    "\n",
    "code_snip = dict()\n",
    "for output in infograph.stream(inputs):\n",
    "    for key, value in output.items():\n",
    "        pprint.pprint(f\"Output from node '{key}':\")\n",
    "        pprint.pprint(\"---\")\n",
    "        pprint.pprint(value, indent=2, width=80, depth=None)\n",
    "        code_snip = value\n",
    "    pprint.pprint(\"\\n---\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33bbe063",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9003dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd06b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "tool_desc_prompt = \"\"\"\n",
    "You are an AI assistant designed to analyze Python code. Your task is to identify all function definitions in the provided Python snippet that are decorated with @tool. You must return a dictionary where:\n",
    "- The keys are the names of the identified functions.\n",
    "- The values are descriptions of what each function is supposed to do. If a function contains a docstring, extract it as the description. If a docstring is missing, infer the function's purpose from its structure and comments.\n",
    "Example Input:\n",
    "@tool\n",
    "def calculate_area(length, width):\n",
    "    \"Calculates the area of a rectangle.\"\n",
    "    return length * width\n",
    "\n",
    "@tool\n",
    "def greet(name):\n",
    "    return f\"Hello, {{name}}!\"\n",
    "\n",
    "\n",
    "Expected Output:\n",
    "{\n",
    "    \"calculate_area\": \"Calculates the area of a rectangle.\",\n",
    "    \"greet\": \"Greets a user by name.\"\n",
    "}\n",
    "\n",
    "\n",
    "Instructions:\n",
    "- Identify functions that have the @tool decorator.\n",
    "- Extract function names and descriptions (either from docstrings or inferred).\n",
    "- Return the output as a structured Python dictionary.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def graph_map_step(state: toolcollector):\n",
    "    # Extract nodes and edges from json_objects\n",
    "    response_1 = llm.invoke([SystemMessage(content=tool_desc_prompt)])\n",
    "    json_objects = json.loads(response_1.content)\n",
    "    sends = []\n",
    "    for key in json_objects:\n",
    "        sends.append(Send(\"tool_create\", {\"objective\": key + \" \" + json_objects[key]}))    \n",
    "    return sends\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e4c477",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
