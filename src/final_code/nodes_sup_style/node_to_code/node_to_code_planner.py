from typing import List
from pydantic import BaseModel, Field
from langchain_core.prompts import ChatPromptTemplate

from src.final_code.nodes_sup_style.node_to_code.node_to_code_base import NodeBuilderState
from src.final_code.llms.model_factory import get_model

llm = get_model()

node_building_strategies="""
<NodeBuildingStrategies>
<prompting+tool_binding>
When to use:
a. if the node's function is to answer a user_query which needs factual information to answer
b. personal assistant which may need to respond with which tool should be used if any to meet the requirements 
c. Whenever the node involves a LLM(model) needing sensors or actuators, for information retrieval, and allowing llm(model) to 'DO' stuff
d. One of the best use cases of this technique is when you know that there would be 'n' different tools which could be used in different scenarios to handle a query in a particular domain.

Algorithm:
1. Write a prompt to handle the user query
2. Use the toolset_generation capbility to bind the prompts obtained in step 1 with appropriate tools. 

IMPORTANT: this node is not responsible for execution, that will be handled separately, PLAN SHOULD NOT INCLUDE TOOL CALLING/EXECUTION
</prompting+tool_binding>

<prompting+structured_output>
When not to use: 
DONOT USE if the user query needs to be answered based on factual data, refer to 'prompting + tool_binding' for the same.
do not rely on LLM's ability to answer factual data, always refer to tool_binding strategy

When to use:
a. Reduced Hallucinations: By enforcing adherence to a JSON Schema, structured outputs minimize the chance of models generating incorrect or irrelevant data. 
b. Simplified Prompting: You don't need overly complex or specific prompts to get consistently formatted output, as the schema provides the structure. 
c. Reliable Type-Safety: Structured outputs ensure that the model always generates data that fits the defined schema, eliminating the need for validation or retries due to format errors. 
d. Building Complex Workflows: Structured outputs are particularly useful for building multi-step workflows where the output of one step serves as input for the next. 
e. Integration with Databases and APIs: When integrating with systems that require structured data formats (like databases or APIs), structured outputs ensure consistency and avoid integration problems. 
f. Function Calling and Data Extraction: Structured outputs are recommended for function calling and extracting structured data from various sources. 
g. Consistent and Verifiable Output: The predictable format of structured outputs makes it easier to test, debug, and evaluate applications that rely on them. 
h. Explicit Refusals: You can now detect model refusals programmatically when using structured outputs, as the model will explicitly return a structured error message instead of a text-based one. 


Algorithm::
1. Generate a prompt for the given ask
2. Use structured output functionality along with the prompt received in step 1

</prompting+structured_output>


<Interrupt>
Use case: 
a. Reviewing tool calls: Humans can review, edit, or approve tool calls requested by the LLM before tool execution.
b. Validating LLM outputs: Humans can review, edit, or approve content generated by the LLM.
c. Providing context: Enable the LLM to explicitly request human input for clarification or additional details or to support multi-turn conversations.

Algorithm:
1. follow the interrupt pattern
</Interrupt>
</NodeBuildingStrategies>
"""


class Plan(BaseModel):
    """Plan to follow in future"""
    justification: str = Field(description = "justification for the proposed plan, which of the nodebuildingstrategies you have chosen for building this plan")
    steps: List[str] = Field(
        description="different steps to follow, should be in sorted order"
    )

planner_prompt = ChatPromptTemplate.from_messages(
    [
        (
            "system",
            """For the given node information by user, come up with a simple step by step plan to implement the node.
This plan should involve individual tasks, that if executed correctly will yield the correct answer. Do not add any superfluous steps.

You need to closely adhere to one of the following node building strategies:
{node_building_strategies}

Read the complete 'NodeBuildingStrategies' section to select the best strategy and create a tailored plan accordingly, make sure to follow the algorithm in the selected section.

Filters on plan: 
1. The result of the final step should be the final answer
2. Do not add steps like extract _user_query , tool_execution, or update state
""",


        ),
        ("placeholder", "{messages}"),
    ]
)
planner = planner_prompt | llm.with_structured_output(Plan) 

plan_filter_prompt = ChatPromptTemplate.from_template("""
You have been provided with a plan
<Plan>
{original_plan}
</Plan>

You are tasked with filtering this prompt
1. Do not focus on input manipulation - any plan steps like extracting input/decomposing etc.
2. Do not focus on updating output to state - store ouput to final state

Check each step of the plan, and filter them to give final plan
""")

def plan_step(state: NodeBuilderState):
    plan: Plan = planner.invoke({"messages": state["messages"], "node_building_strategies": node_building_strategies})
    return {"plan": plan.steps}