from langchain_core.prompts import PromptTemplate
from langchain_core.messages import HumanMessage, AIMessage
from final_code.states.AgentBuilderState import AgentBuilderState, AgentInstructions
from final_code.llms.model_factory import get_model
from langgraph.checkpoint.memory import InMemorySaver

llm = get_model()

CODE_GEN_PROMPT = PromptTemplate.from_template("""
You are an expert Python programmer specializing in AI agent development via the Langgraph and Langchain SDK. Your primary task is to generate compilable, logical, and complete Python code for a LangGraph state graph based on user 'INPUT' section below. You must prioritize LLM-based implementations for relevant tasks.

<JSON>                                              
{json_schema}
</JSON>

<IMPLEMENTATION_PATTERNS_2025>
## Pattern 1: LLM with Tool Calling
**When to use:** Node needs to interact with external systems, APIs, databases, or perform specific actions.

**Example Implementation:**
IF the tools corresponding to a node are composio tools use the below format: 
                                               
```python
                                               
from langchain_core.tools import tool
from langchain_openai import ChatOpenAI
        
from composio_langgraph import Action, ComposioToolSet, App
composio_toolset = ComposioToolSet()
tools = composio_toolset.get_tools(actions=[composio_action_name])  # Replace with actual action names
```                                               
ELSE IF the tools corresponding to a node are not composio tools and instead provide pycode:
``` python
def search_customer_database(customer_id: str) -> str:
    '''Search for customer information by ID.'''
    # Direct implementation - no nested LLM calls
    return f"Customer {{customer_id}} data retrieved"
tools = [search_customer_database]
```

Finally use the following format:
```python
def node_name(state: GraphState) -> GraphState:
    '''Node purpose: [Clear description]'''
    agent = create_react_agent(
        model=ChatOpenAI(model="gpt-4o", temperature=0.7),
        tools=tools,
        prompt="The prompt for the agent to follow, also mention which tools to use, if any.")
    response = agent.invoke([HumanMessage(content="Perform action based on state")])
    return {{
        "messages": [response["messages"]]
    }}
```
## Pattern 2: LLM with Structured Output
**When to use:** Node needs to make decisions, classify inputs, or extract structured data.

**Example Implementation:**
<Example1>
```python
from pydantic import BaseModel, Field
from typing import Literal

class IntentClassification(BaseModel):
    '''Structured output for intent classification.'''
    intent: Literal["support", "sales", "billing"] = Field(description="Classified intent")
    confidence: float = Field(description="Confidence score 0-1")
    reasoning: str = Field(description="Brief explanation of classification")

def intent_classifier_node(state: GraphState) -> GraphState:
    # Reasoning: This node needs structured decision making for routing
    llm = ChatOpenAI(model="gpt-4o", temperature=0)
    structured_llm = llm.with_structured_output(IntentClassification)
    
    user_message = state["messages"][-1].content
    prompt = f"Classify this user message: {{user_message}}"
    
    result = structured_llm.invoke(prompt)
    return {{
        "messages": [("system", f"Intent classified as: {{result.intent}}")],
        "intent": result.intent,
        "confidence": result.confidence
    }}
```

</Example1>
<Example2>
Here is an example of how to use structured output. In this example, we want the LLM to generate and fill up the pydantic class Joke, based on user query. 
``` python
from typing import Optional
from pydantic import BaseModel, Field

# Pydantic class for structured output
class Joke(BaseModel):
    setup: str = Field(description="The setup of the joke")
    punchline: str = Field(description="The punchline to the joke")
    rating: Optional[int] = Field(
        default=None, description="How funny the joke is, from 1 to 10"
    )

class JokeBuilderState(MessagesState):
    joke: Joke = Field(description= "joke generated by the GenerateJoke node.")

def GenerateJoke(state: JokeBuilderState):
    structured_llm = llm.with_structured_output(Joke)
    joke: Joke = structured_llm.invoke("Tell me a joke about cats")
    return {{ "joke": joke }}
```
</Example2>

## Pattern 3: Human-in-the-Loop with Interrupt
**When to use:** Node requires human approval, creative input, or oversight.

*** Example Implementation:***
```python
from langgraph.types import interrupt

def human_node(state: State):
    value = interrupt(
        # Any JSON serializable value to surface to the human.
        # For example, a question or a piece of text or a set of keys in the state
       {{
          "text_to_revise": state["some_text"]
       }}
    )
    # Update the state with the human's input or route the graph based on the input.
    return {{
        "some_text": value
    }}
```

## Pattern 4: Multi-Step LLM Processing
**When to use:** Complex tasks requiring multiple LLM operations.

**Example Implementation:**
```python
def content_enhancement_node(state: GraphState) -> dict:
    # Reasoning: Multi-step enhancement requires sequential LLM processing
    llm = ChatOpenAI(model="gpt-4o", temperature=0.7)
    
    raw_content = state.get("raw_content", "")
    
    # Step 1: Structure the content
    structured_prompt = f"Structure this content logically: {{raw_content}}"
    structured = llm.invoke(structured_prompt).content
    
    # Step 2: Enhance with examples
    enhanced_prompt = f"Add relevant examples to: {{structured}}"
    enhanced = llm.invoke(enhanced_prompt).content
    
    return {{
        "messages": [("system", "Content enhanced with structure and examples")],
        "enhanced_content": enhanced,
        "processing_steps": ["structured", "enhanced"]
    }}
```
</IMPLEMENTATION_PATTERNS_2025>

<CODE_GENERATION_INSTRUCTIONS>

Generate a single, self-contained, and compilable Python script following this structure:

### 1. Imports and Setup
```python
from typing import Dict, Any, List, Optional, Literal
from langgraph.graph import StateGraph, START, END, MessagesState
from langgraph.checkpoint.memory import MemorySaver, InMemorySaver
from langchain_openai import ChatOpenAI
from langchain_google_genai import ChatGoogleGenerativeAI
from pydantic import BaseModel, Field
from langchain_core.tools import tool
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage
import re
import json
```

### 2. State Definition
Always use MessagesState as base and extend as needed:
```python
class GraphState(MessagesState):
    \"\"\" The GraphState represents the state of the LangGraph workflow.
    Below is the definition of MessagesState, the AnyMessage refers to AIMessage, HumanMessage, or SystemMessage etc.
    the add_messages is a reducer, which means that when doing return {{\"messages\": [AIMessage(content=\"...\")]}}, it will append the new message to the messages variable and not override it..
    class MessagesState(TypedDict):
        messages: Annotated[list[AnyMessage], add_messages]
    \"\"\" 
    # Add domain-specific fields based on your analysis
    intent: Optional[str] = None
    confidence: Optional[float] = None
    processing_complete: bool = False
    # Add other fields as required by your architecture
```

### 3. Tool Definitions (if needed)
Define tools before node functions that use them:
```python
@tool
def example_tool(param: str) -> str:
    '''Tool description for LLM understanding.'''
    # Direct implementation - avoid nested LLM calls
    return f"Tool result for {{param}}"
```

### 4. Node Implementation
For each node, include reasoning comments:
```python
def node_name(state: GraphState) -> GraphState:
    \"\"\"
    Node purpose: [Clear description]
    Implementation reasoning: [Why this pattern was chosen]
    \"\"\"
    # Implementation here
    return {{"field": "value"}}
```

### 5. Routing Functions
<Information>
<Edges>
Edges define how the logic is routed and how the graph decides to stop. This is a big part of how your agents work and how different nodes communicate with each other. There are a few key types of edges:

Normal Edges: Go directly from one node to the next.
Conditional Edges: Call a function to determine which node(s) to go to next.
A node can have MULTIPLE outgoing edges. If a node has multiple out-going edges, all of those destination nodes will be executed in parallel as a part of the next superstep.

<NonConditionalEdges>
If you always want to go from node A to node B, you can use the add_edge method directly.

``` python
graph.add_edge("node_a", "node_b")
```
</NonConditionalEdges>

<ConditionalEdges>
If you want to optionally route to 1 or more edges (or optionally terminate), you can use the add_conditional_edges method. This method accepts the name of a node and a "routing function" to call after that node is executed:

``` python
graph.add_conditional_edges("node_a", routing_function)
```

Similar to nodes, the routing_function accepts the current state of the graph and returns a value.

By default, the return value routing_function is used as the name of the node (or list of nodes) to send the state to next. All those nodes will be run in parallel as a part of the next superstep.

You can optionally provide a dictionary that maps the routing_function's output to the name of the next node.

``` python 
graph.add_conditional_edges("node_a", routing_function, {{True: "node_b", False: "node_c"}})
```
</ConditionalEdges>
</Edges>

<Command>
It can be useful to combine control flow (edges) and state updates (nodes). 
For example, you might want to BOTH perform state updates AND decide which node to go to next in the SAME node. 
LangGraph provides a way to do so by returning a Command object from node functions:

``` python
def my_node(state: State) -> Command[Literal["my_other_node"]]:
    return Command(
        # state update
        update={{"foo": "bar"}},
        # control flow
        goto="my_other_node"
    )
``` 

With Command you can also achieve dynamic control flow behavior (identical to conditional edges):

``` python
def my_node(state: State) -> Command[Literal["my_other_node"]]:
    if state["foo"] == "bar":
        return Command(update={{"foo": "baz"}}, goto="my_other_node")
```
                                               
Important:
When returning Command in your node functions, you must add return type annotations with the list of node names the node is routing to, e.g. Command[Literal["my_other_node"]]. This is necessary for the graph rendering and tells LangGraph that my_node can navigate to my_other_node.

Navigating to a node in a parent graph:
If you are using subgraphs, you might want to navigate from a node within a subgraph to a different subgraph (i.e. a different node in the parent graph). To do so, you can specify graph=Command.PARENT in Command:

``` python
def my_node(state: State) -> Command[Literal["other_subgraph"]]:
    return Command(
        update={{"foo": "bar"}},
        goto="other_subgraph",  # where `other_subgraph` is a node in the parent graph
        graph=Command.PARENT
    )
```
                                          
Note:
Setting graph to Command.PARENT will navigate to the closest parent graph.
State updates with Command.PARENT
When you send updates from a subgraph node to a parent graph node for a key that's shared by both parent and subgraph state schemas, you must define a reducer for the key you're updating in the parent graph state. See this example.

This is particularly useful when implementing multi-agent handoffs.
</Command>

<CommandOrConditionalEdge>
Use Command 
1. when you need to both update the graph state and route to a different node. 
2. For example, when implementing multi-agent handoffs where it's important to route to a different agent and pass some information to that agent.
Use conditional edges to route between nodes conditionally without updating the state.
</CommandOrConditionalEdge>

Output: python code with appropriate inline comments
Follow the below algorithm to generate output: 
if: non-conditional edge, then: refer to implementation in 'NonConditionalEdge' for implementation
else if: either the return type of the function is Command or according to 'CommandOrConditionalEdge' we should use Command functionality, then: refer to 'Command' section for implementation
else if : according to 'CommandOrConditionalEdge' conditional_edge should be used, then: refer to 'ConditionalEdges' section for implementation.

###6: Final Graph Compilation
```python
If there are 'interrupt' nodes in the graph, you must use a checkpointer to save the state of the graph.
checkpointer = InMemorySaver()
app = workflow.compile(
    checkpointer=checkpointer # Required for `interrupt` to work
)
# Run the graph until the interrupt
thread_config = {{"configurable": {{"thread_id": "some_id"}}}}
app.invoke(some_input, config=thread_config)

Else if there are no 'interrupt' nodes in the graph, you can compile the graph without a checkpointer.
app = workflow.compile()
# Run the graph
app.invoke(some_input)
                                               

DONOT ADD '__main__' block or any other boilerplate code, the code should be self-contained and compilable.
```
</CODE_GENERATION_INSTRUCTIONS>

<QUALITY_CHECKLIST>
Before finalizing your code, verify:
- [ ] All imports are included and correct
- [ ] GraphState properly extends MessagesState  
- [ ] LLM calls include proper error handling
- [ ] Tools are self-contained (no nested LLM calls)
- [ ] Structured output uses proper Pydantic models
- [ ] Conditional edges handle all possible routing outcomes
- [ ] Code is compilable and logically consistent
</QUALITY_CHECKLIST>

<KEY_EXTRACTION_INSTRUCTIONS>
After generating the complete Python script, add a section titled:

## Required Keys and Credentials

List all environment variables, API keys, and external dependencies needed as comment :
- Environment variables (e.g., OPENAI_API_KEY, GOOGLE_API_KEY)
- Tool-specific credentials 
- External service configurations
- Database connection strings (if applicable)

If no external keys are needed, state: "No external API keys required for this implementation."
</KEY_EXTRACTION_INSTRUCTIONS>

Please return only complete and compilable langgraph python code
""")

def code_node(state: AgentBuilderState):
    """
    LangGraph node to generate the final Python code for the agent.
    It uses the gathered agent_instructions and the CODE_GEN_PROMPT.
    """
    instructions: AgentInstructions = state["agent_instructions"]
 
    # Invoke LLM to generate code based on the detailed prompt and instructions
    code_output = llm.invoke([HumanMessage(content=CODE_GEN_PROMPT.format(
        json_schema=state["json_schema"].model_dump_json(indent=2),
        justification = state["justification"],
        objective=instructions.objective,
        usecases=instructions.usecases,
        examples=instructions.examples
    ))])
    # Return the generated Python code and an AI message
    return {
        "messages": [AIMessage(content="Generated final python code!")],
        "python_code": code_output.content,
    }
