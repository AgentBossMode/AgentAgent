{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "! pip install langchain-mcp-adapters langgraph \"langchain[anthropic]\" \"langchain[openai]\" langgraph-swarm httpx markdownify"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Swarm Researcher Example "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prompts import planner_prompt, researcher_prompt\n",
    "from utils import fetch_doc, print_stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "from langchain_openai import AzureChatOpenAI, ChatOpenAI\n",
    "\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langgraph_swarm import create_handoff_tool, create_swarm\n",
    "from langchain_deepseek import ChatDeepSeek\n",
    "\n",
    "model = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    ")\n",
    "\n",
    "# Handoff tools\n",
    "transfer_to_planner_agent = create_handoff_tool(\n",
    "    agent_name=\"planner_agent\",\n",
    "    description=\"Transfer the user to the planner_agent for clarifying questions related to the user's request.\",\n",
    ")\n",
    "transfer_to_researcher_agent = create_handoff_tool(\n",
    "    agent_name=\"researcher_agent\",\n",
    "    description=\"Transfer the user to the researcher_agent to perform research and implement the solution to the user's request.\",\n",
    ")\n",
    "\n",
    "# LLMS.txt\n",
    "llms_txt = \"LangGraph:https://langchain-ai.github.io/langgraph/llms.txt\"\n",
    "num_urls = 3\n",
    "planner_prompt_formatted = planner_prompt.format(llms_txt=llms_txt, num_urls=num_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"I am based on OpenAI's GPT-3 model, which is designed for natural language understanding and generation. If you have any questions or need assistance, feel free to ask!\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 37, 'prompt_tokens': 11, 'total_tokens': 48, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_b376dfbbd5', 'id': 'chatcmpl-BHFupPZcQAOOT1OaxvuOMCHUfaWiS', 'finish_reason': 'stop', 'logprobs': None}, id='run-cdd59881-9fd1-4cfe-a295-aaab2973dfc6-0', usage_metadata={'input_tokens': 11, 'output_tokens': 37, 'total_tokens': 48, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.invoke(\"what model are you\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='' additional_kwargs={'tool_calls': [{'id': 'call_0_3fe25f6e-3cdc-4001-ab3c-ca02f49e94f1', 'function': {'arguments': '{\"a\":2,\"b\":4}', 'name': 'multiply_numbers'}, 'type': 'function', 'index': 0}], 'refusal': None} response_metadata={'token_usage': {'completion_tokens': 23, 'prompt_tokens': 146, 'total_tokens': 169, 'completion_tokens_details': None, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}, 'prompt_cache_hit_tokens': 0, 'prompt_cache_miss_tokens': 146}, 'model_name': 'deepseek-chat', 'system_fingerprint': 'fp_3d5141a69a_prod0225', 'id': 'e389b2a9-ca11-4bac-b9d0-495e2f0272d1', 'finish_reason': 'tool_calls', 'logprobs': None} id='run-e19cf3ff-bb97-43d0-8183-91a19ce2e469-0' tool_calls=[{'name': 'multiply_numbers', 'args': {'a': 2, 'b': 4}, 'id': 'call_0_3fe25f6e-3cdc-4001-ab3c-ca02f49e94f1', 'type': 'tool_call'}] usage_metadata={'input_tokens': 146, 'output_tokens': 23, 'total_tokens': 169, 'input_token_details': {'cache_read': 0}, 'output_token_details': {}}\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.tools import tool\n",
    "\n",
    "@tool\n",
    "def multiply_numbers(a: int, b: int) -> int:\n",
    "    \"\"\"Multiply two numbers.\"\"\"\n",
    "    return a * b\n",
    "    \n",
    "model_with_tool = model.bind_tools([multiply_numbers])\n",
    "\n",
    "result = model_with_tool.invoke(\"multiply 2 and 4. Do not try to answer on your own via reasoning, try using tools at your disposal such as multiply_numbers\")\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.tool_calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace '('planner_agent:5d3edfdb-c24d-2f28-0b5c-a1c230a12ee1',)'\n",
      "Update from node 'agent'\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Name: planner_agent\n",
      "Tool Calls:\n",
      "  fetch_doc (call_Em7AQfFWb06IyLcrHm53LJMD)\n",
      " Call ID: call_Em7AQfFWb06IyLcrHm53LJMD\n",
      "  Args:\n",
      "    url: https://langchain-ai.github.io/langgraph/llms.txt\n",
      "\n",
      "\n",
      "\n",
      "Namespace '('planner_agent:5d3edfdb-c24d-2f28-0b5c-a1c230a12ee1',)'\n",
      "Update from node 'tools'\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: fetch_doc\n",
      "\n",
      "# LangGraph\n",
      "## Tutorials\n",
      "[Learn the basics](https://langchain-ai.github.io/langgraph/tutorials/introduction/): LLM should read this page when needing to build a LangGraph chatbot or when learning about chat agents with memory, human-in-the-loop functionality, and state management. This page provides a comprehensive LangGraph quickstart tutorial covering building a support chatbot with web search capability, conversation memory, human review routing, custom state management, and time travel functionality to explore alternative conversation paths.\n",
      "[Local Deploy](https://langchain-ai.github.io/langgraph/tutorials/langgraph-platform/local-server/): LLM should read this page when setting up a LangGraph app locally using `langgraph dev` and troubleshooting LangGraph server deployment. This page contains a quickstart guide for launching a LangGraph server locally, including installation steps, app creation from templates, environment setup, API testing with Python/JS SDKs, and links to deployment options and further documentation.\n",
      "[Workflows and Agents](https://langchain-ai.github.io/langgraph/tutorials/workflows/): LLM should read this page when implementing agent systems, designing workflow architectures, or troubleshooting LLM orchestration strategies. The page covers patterns for LLM system design, comparing workflows (predefined paths) vs agents (dynamic control), with implementations of prompt chaining, parallelization, routing, orchestrator-worker, evaluator-optimizer, and agent patterns using both graph and functional APIs in LangGraph.\n",
      "## Concepts\n",
      "[Concepts](https://langchain-ai.github.io/langgraph/concepts/): LLM should read this page when needing to understand LangGraph's key concepts or when planning to deploy LangGraph applications. Comprehensive guide covering LangGraph fundamentals (graph primitives, agents, multi-agent systems, breakpoints, persistence), features (time travel, memory, streaming), and LangGraph Platform deployment options (self-hosted, cloud, enterprise).\n",
      "[Agent architectures](https://langchain-ai.github.io/langgraph/concepts/agentic\\_concepts/): LLM should read this page when designing agent architectures, implementing control flows for LLM applications, or customizing agent behavior patterns. This page covers different LLM agent architectures including routers, tool calling agents (ReAct), structured outputs, memory systems, planning capabilities, and advanced customization options like human-in-the-loop, parallelization, subgraphs, and reflection mechanisms.\n",
      "[Application Structure](https://langchain-ai.github.io/langgraph/concepts/application\\_structure/): LLM should read this page when needing to understand LangGraph application structure, preparing to deploy a LangGraph application, or troubleshooting configuration issues. This page details the structure of LangGraph applications, including required components (graphs, langgraph.json config file, dependency files, optional .env), file organization patterns for Python/JavaScript projects, configuration file format with all supported fields, and how to specify dependencies, graphs, and environment variables.\n",
      "[Assistants](https://langchain-ai.github.io/langgraph/concepts/assistants/): LLM should read this page when looking for information about LangGraph assistants, understanding assistant configuration in LangGraph Platform, or learning about versioning agent configurations. This page explains LangGraph assistants, which allow developers to modify agent configurations (prompts, models, etc.) without changing graph logic, supports versioning for tracking changes, and is available only in LangGraph Platform (not open source).\n",
      "[Authentication & Access Control](https://langchain-ai.github.io/langgraph/concepts/auth/): LLM should read this page when implementing authentication in LangGraph Platform, designing access control for LangGraph applications, or troubleshooting security issues in LangGraph deployments. This page explains LangGraph's authentication and authorization system, covering the difference between authentication and authorization, system architecture, implementing custom auth handlers, common access patterns, and supported resources/actions for access control.\n",
      "[Bring Your Own Cloud (BYOC)](https://langchain-ai.github.io/langgraph/concepts/bring\\_your\\_own\\_cloud/): LLM should read this page when learning about LangGraph Platform deployment options, understanding Bring Your Own Cloud architecture, or managing deployments in AWS. This page explains LangGraph's BYOC deployment model, detailing how it separates control plane (managed by LangChain) from data plane (in customer's AWS account), outlines AWS requirements, infrastructure setup via Terraform, required permissions, and explains the deployment workflow.\n",
      "[Deployment Options](https://langchain-ai.github.io/langgraph/concepts/deployment\\_options/): LLM should read this page when needing information about LangGraph deployment options, comparing different deployment methods, or understanding LangGraph Platform plans. This page outlines four deployment options for LangGraph Platform: Self-Hosted Lite (available for all plans), Self-Hosted Enterprise (Enterprise plan only), Cloud SaaS (Plus and Enterprise plans), and Bring Your Own Cloud (Enterprise plan only, AWS-only).\n",
      "[Double Texting](https://langchain-ai.github.io/langgraph/concepts/double\\_texting/): LLM should read this page when handling concurrent user interactions in LangGraph Platform, implementing double-texting safeguards, or designing stateful conversation systems. This page explains four approaches to handling \"double texting\" in LangGraph (when users send a second message before the first completes): Reject, Enqueue, Interrupt, and Rollback, noting these features are currently only available in LangGraph Platform.\n",
      "[Durable Execution](https://langchain-ai.github.io/langgraph/concepts/durable\\_execution/): LLM should read this page when needing to understand durable execution in LangGraph, implementing workflow persistence, or troubleshooting workflow resumption. This page explains durable execution in LangGraph: how workflows save progress to resume later, requirements (checkpointers and thread IDs), determinism guidelines for consistent replay, using tasks to encapsulate non-deterministic operations, and approaches for pausing/resuming workflows.\n",
      "[FAQ](https://langchain-ai.github.io/langgraph/concepts/faq/): LLM should read this page when needing to understand differences between LangGraph and LangChain, exploring deployment options for LangGraph Platform, or determining compatibility with various LLMs. FAQ covering LangGraph basics, comparisons with other frameworks, deployment options (free self-hosted, Cloud SaaS, BYOC, Enterprise), compatibility with different LLMs including OSS models, and feature differences between open-source LangGraph and proprietary LangGraph Platform.\n",
      "[Functional API](https://langchain-ai.github.io/langgraph/concepts/functional\\_api/): LLM should read this page when implementing workflows with persistent state, adding human-in-the-loop features, or converting existing code to use LangGraph. The page documents LangGraph's Functional API, which allows adding persistence, memory, and human-in-the-loop capabilities with minimal code changes using @entrypoint and @task decorators, handling serialization requirements, state management, and common patterns for parallel execution and error handling.\n",
      "[Why LangGraph?](https://langchain-ai.github.io/langgraph/concepts/high\\_level/): LLM should read this page when understanding LangGraph's core capabilities, exploring LLM application infrastructure, or evaluating agent/workflow persistence options. LangGraph provides infrastructure for LLM applications with three key benefits: persistence for memory and human-in-the-loop capabilities, streaming of workflow events and LLM outputs, and tools for debugging and deployment via LangGraph Platform.\n",
      "[Human-in-the-loop](https://langchain-ai.github.io/langgraph/concepts/human\\_in\\_the\\_loop/): LLM should read this page when implementing human-in-the-loop workflows in LangGraph, designing approval systems with LLMs, or creating interactive multi-turn conversation agents. This page explains human-in-the-loop patterns in LangGraph using the interrupt function, showing how to pause graph execution for human review/input and resume with Command. Includes design patterns for approval workflows, state editing, tool call reviews, and multi-turn conversations, with code examples and warnings about execution flow and common pitfalls.\n",
      "[LangGraph CLI](https://langchain-ai.github.io/langgraph/concepts/langgraph\\_cli/): LLM should read this page when looking for information about LangGraph CLI installation or when needing to deploy a LangGraph API server locally. The page covers LangGraph CLI installation methods (Homebrew, pip), key commands (build, dev, up, dockerfile), and features like hot reloading, debugger support, and database management for running LangGraph servers.\n",
      "[Cloud SaaS](https://langchain-ai.github.io/langgraph/concepts/langgraph\\_cloud/): LLM should read this page when learning about LangGraph's Cloud SaaS offering, understanding deployment options for LangGraph Servers, or planning autoscaling infrastructure for LangGraph applications. This page describes LangGraph Cloud SaaS, a managed deployment service for LangGraph Servers with details on deployment types (Development/Production), revisions, persistence, autoscaling capabilities (up to 10 containers), LangSmith integration, IP whitelisting, and automatic deletion policies after 28 days of non-use.\n",
      "[LangGraph Platform](https://langchain-ai.github.io/langgraph/concepts/langgraph\\_platform/): LLM should read this page when seeking information about LangGraph Platform's components or evaluating production deployment options for agentic applications. The page details the LangGraph Platform, a commercial solution for deploying agentic applications, including its components (Server, Studio, CLI, SDK, Remote Graph) and key benefits like streaming support, background runs, long run handling, burstiness management, and human-in-the-loop capabilities.\n",
      "[LangGraph Server](https://langchain-ai.github.io/langgraph/concepts/langgraph\\_server/): LLM should read this page when developing applications with LangGraph Server, deploying agent-based applications, or integrating persistent state management in agent workflows. LangGraph Server provides an API for creating and managing agent applications with key features like streaming endpoints, background runs, task queues, persistence, webhooks, cron jobs, and monitoring capabilities through a structured system of assistants, threads, runs, and stores.\n",
      "[LangGraph Studio](https://langchain-ai.github.io/langgraph/concepts/langgraph\\_studio/): LLM should read this page when looking for information about LangGraph Studio features, needing to troubleshoot LangGraph Studio issues, or learning how to connect a LangGraph application to the Studio. LangGraph Studio is a specialized agent IDE for visualizing, interacting with, and debugging LLM applications, offering features such as graph visualization, state editing, assistant management, and integration with LangSmith, with instructions for connecting via deployed applications or local development servers, plus troubleshooting FAQs.\n",
      "[LangGraph Glossary](https://langchain-ai.github.io/langgraph/concepts/low\\_level/): LLM should read this page when needing to understand LangGraph terminology, implementing agent workflows as graphs, or developing modular multi-step AI systems. The page covers core LangGraph concepts including StateGraph, nodes, edges, state management, messaging, persistence, configuration, human-in-the-loop features, subgraphs, and visualization capabilities.\n",
      "[Memory](https://langchain-ai.github.io/langgraph/concepts/memory/): LLM should read this page when implementing memory systems for AI agents, managing conversation context across sessions, or designing systems that require both short-term and long-term information retention. This page explains memory systems in LangGraph, covering short-term (thread-scoped) memory for managing conversation history and long-term memory across threads, with techniques for handling long conversations, summarizing past interactions, and organizing persistent memories in namespaces.\n",
      "[Multi-agent Systems](https://langchain-ai.github.io/langgraph/concepts/multi\\_agent/): LLM should read this page when implementing multi-agent systems, troubleshooting complex agent architectures, or designing agent communication patterns. Multi-agent systems organize LLMs into modular architectures (network, supervisor, hierarchical, custom) with different communication patterns, using Command objects for handoffs between agents, and supporting various state management approaches.\n",
      "[Persistence](https://langchain-ai.github.io/langgraph/concepts/persistence/): LLM should read this page when needing to understand LangGraph persistence mechanisms, implementing stateful workflows, or managing conversation history across interactions. This page covers LangGraph's persistence features including checkpointers, threads, state snapshots, replay functionality, forking state, cross-thread memory via InMemoryStore, and semantic search capabilities for stored memories.\n",
      "[LangGraph Platform Plans](https://langchain-ai.github.io/langgraph/concepts/plans/): LLM should read this page when determining LangGraph Platform pricing tiers, comparing deployment options, or researching features available across different plans. This page outlines LangGraph Platform plans (Developer, Plus, Enterprise), detailing deployment options, usage limitations, feature availability, and pricing structure for agentic application deployment.\n",
      "[LangGraph Platform Architecture](https://langchain-ai.github.io/langgraph/concepts/platform\\_architecture/): LLM should read this page when needing to understand LangGraph Platform's technical architecture or troubleshooting deployment issues. The page details how LangGraph Platform uses Postgres for persistent storage of user/run data and Redis for worker communication (run cancellation, output streaming) and ephemeral metadata storage (retry attempts).\n",
      "[LangGraph's Runtime (Pregel)](https://langchain-ai.github.io/langgraph/concepts/pregel/): LLM should read this page when learning about LangGraph's runtime, implementing applications with Pregel directly, or understanding how LangGraph executes graph applications. Explains LangGraph's Pregel runtime which manages graph application execution through a three-phase process (Plan, Execution, Update), describes different channel types (LastValue, Topic, Context, BinaryOperatorAggregate), provides direct implementation examples, and contrasts the StateGraph API with the Functional API.\n",
      "[LangGraph Platform: Scalability & Resilience](https://langchain-ai.github.io/langgraph/concepts/scalability\\_and\\_resilience/): LLM should read this page when needing to understand LangGraph Platform's scaling capabilities, designing high-availability LangGraph deployments, or troubleshooting resilience issues. This page details LangGraph Platform's horizontal scaling features including stateless server instances, queue worker scaling, resilience mechanisms for handling crashes, and database failover strategies in Postgres and Redis.\n",
      "[LangGraph SDK](https://langchain-ai.github.io/langgraph/concepts/sdk/): LLM should read this page when looking for installation instructions for LangGraph SDK, needing to choose between sync and async Python clients, or requiring SDK API references. The page covers LangGraph SDK installation for Python and JS, provides API reference links, explains the difference between synchronous and asynchronous Python clients, and includes code examples for both client types.\n",
      "[Self-Hosted](https://langchain-ai.github.io/langgraph/concepts/self\\_hosted/): LLM should read this page when looking for LangGraph deployment options, understanding self-hosted versions, or seeking requirements for self-hosting LangGraph. This page details two self-hosted deployment options for LangGraph Platform: Self-Hosted Lite (limited to 1M nodes/year) and Self-Hosted Enterprise (full version requiring license). Includes requirements, deployment process using Redis/Postgres, Docker, and optional Kubernetes deployment via Helm chart.\n",
      "[Streaming](https://langchain-ai.github.io/langgraph/concepts/streaming/): LLM should read this page when implementing streaming features in LangGraph applications, understanding different streaming modes, or building responsive LLM applications. This page explains streaming in LangGraph, covering the main types (workflow progress, LLM tokens, custom updates) and streaming modes (values, updates, custom, messages, debug, events), with details on how to use multiple modes simultaneously and differences between LangGraph library and Platform implementations.\n",
      "[Template Applications](https://langchain-ai.github.io/langgraph/concepts/template\\_applications/): LLM should read this page when looking for LangGraph template applications, setting up a new LangGraph project, or finding reference implementations for agentic workflows. This page presents LangGraph template applications with installation requirements, available templates (including ReAct Agent, Memory Agent, Retrieval Agent, etc.), instructions for creating new apps using the CLI, deployment options, and links to further learning resources.\n",
      "[Time Travel ⏱️](https://langchain-ai.github.io/langgraph/concepts/time-travel/): LLM should read this page when debugging LLM-based agent behavior, analyzing decision-making paths, or exploring alternative execution branches in LangGraph. This page explains LangGraph's Time Travel debugging features: Replaying (reproducing past actions up to specific checkpoints) and Forking (creating alternative execution paths from specific points), with code examples for retrieving checkpoints, configuring replay, and creating forked states.\n",
      "## How Tos\n",
      "[How-to Guides](https://langchain-ai.github.io/langgraph/how-tos/): LLM should read this page when looking for specific implementation techniques in LangGraph or when trying to deploy LangGraph applications to production environments. This page contains an extensive collection of how-to guides for LangGraph, covering graph fundamentals, persistence, memory management, human-in-the-loop features, tool calling, multi-agent systems, streaming, and deployment options through LangGraph Platform.\n",
      "[How to implement handoffs between agents](https://langchain-ai.github.io/langgraph/how-tos/agent-handoffs/): LLM should read this page when implementing multi-agent systems that require agent coordination, when building systems with specialized agents that need to work together, or when needing to implement handoffs between agents. This page explains how to implement handoffs between agents in LangGraph using Command objects, both directly from agent nodes and through specialized handoff tools, with code examples for creating multi-agent systems.\n",
      "[How to run a graph asynchronously](https://langchain-ai.github.io/langgraph/how-tos/async/): LLM should read this page when needing to implement asynchronous graph execution in LangGraph or when optimizing IO-bound LLM applications. This page explains how to convert synchronous graphs to asynchronous in LangGraph, including updating node definitions with async/await, using StateGraph with TypedDict, implementing conditional edges, and streaming results.\n",
      "[How to integrate LangGraph with AutoGen, CrewAI, and other frameworks](https://langchain-ai.github.io/langgraph/how-tos/autogen-integration/): LLM should read this page when integrating LangGraph with other agent frameworks, building multi-agent systems, or adding persistence features to agents. The page demonstrates how to combine LangGraph with AutoGen by calling AutoGen agents inside LangGraph nodes, showing code examples for setting up the integration with memory and conversation persistence.\n",
      "[How to integrate LangGraph (functional API) with AutoGen, CrewAI, and other frameworks](https://langchain-ai.github.io/langgraph/how-tos/autogen-integration-functional/): LLM should read this page when integrating LangGraph with other agent frameworks, building multi-agent systems with different frameworks, or adding LangGraph features to existing agent systems. This page demonstrates how to integrate LangGraph's functional API with AutoGen, including code examples for creating a workflow that calls AutoGen agents, leveraging LangGraph's memory and persistence features.\n",
      "[How to create branches for parallel node execution](https://langchain-ai.github.io/langgraph/how-tos/branching/): LLM should read this page when needing to implement parallel node execution in LangGraph, optimizing graph performance, or handling conditional branching in workflows. This page explains how to create branches for parallel execution in LangGraph using fan-out/fan-in mechanisms, reducer functions for state accumulation, handling exceptions during parallel execution, and implementing conditional branching logic between nodes.\n",
      "[How to combine control flow and state updates with Command](https://langchain-ai.github.io/langgraph/how-tos/command): LLM should read this page when learning how to combine control flow with state updates in LangGraph, understanding Command objects, or navigating between parent graphs and subgraphs. This page explains how to use Command objects to simultaneously update state and control flow between nodes, demonstrates using Command.PARENT to navigate from subgraphs to parent graphs, and includes examples of implementing reducers for state updates across graph hierarchies.\n",
      "[How to add runtime configuration to your graph](https://langchain-ai.github.io/langgraph/how-tos/configuration/): LLM should read this page when implementing runtime configuration for LangGraph, adding model selection options to agents, or enabling dynamic system messages. This page demonstrates how to configure LangGraph at runtime, including selecting different LLMs dynamically and adding custom configuration options like system messages through the configurable dictionary.\n",
      "[How to use the pre-built ReAct agent](https://langchain-ai.github.io/langgraph/how-tos/create-react-agent/): LLM should read this page when implementing a ReAct agent, needing pre-built agent solutions, or learning how to integrate tools with LLM agents. This page covers how to use the pre-built ReAct agent in LangGraph, including setup instructions, creating a weather checking tool, implementing the agent architecture, and examples of running the agent with and without tool calls.\n",
      "[How to add human-in-the-loop processes to the prebuilt ReAct agent](https://langchain-ai.github.io/langgraph/how-tos/create-react-agent-hitl/): LLM should read this page when implementing human-in-the-loop processes for ReAct agents, debugging tool calls, or learning about interrupts in LangGraph. This guide demonstrates how to add human-in-the-loop functionality to prebuilt ReAct agents using interrupt\\_before=[\"tools\"], working with MemorySaver checkpoints, and showing how to approve or edit tool calls before they execute.\n",
      "[How to add thread-level memory to a ReAct Agent](https://langchain-ai.github.io/langgraph/how-tos/create-react-agent-memory/): LLM should read this page when adding memory to ReAct agents, implementing thread-level persistence in LangGraph, or building stateful conversational agents. This guide demonstrates how to add memory to a ReAct agent using LangGraph's checkpointer interface, with code examples showing MemorySaver implementation, thread\\_id configuration, and persistent chat context across multiple interactions.\n",
      "[How to return structured output from the prebuilt ReAct agent](https://langchain-ai.github.io/langgraph/how-tos/create-react-agent-structured-output/): LLM should read this page when implementing structured output with ReAct agents, customizing agent response formats, or working with LangGraph agents. This page explains how to return structured output from prebuilt ReAct agents by providing a response\\_format parameter with a Pydantic schema, including examples with weather data and options for customizing the prompt.\n",
      "[How to add a custom system prompt to the prebuilt ReAct agent](https://langchain-ai.github.io/langgraph/how-tos/create-react-agent-system-prompt/): LLM should read this page when learning to customize ReAct agents, needing to add system prompts to agents, or working with LangGraph's prebuilt agents. This tutorial demonstrates how to add a custom system prompt to a prebuilt ReAct agent, with code examples showing model setup, tool creation, and using the prompt parameter in the create\\_react\\_agent function.\n",
      "[How to add cross-thread persistence to your graph](https://langchain-ai.github.io/langgraph/how-tos/cross-thread-persistence): LLM should read this page when needing to implement persistence across multiple threads in LangGraph, when storing user data between conversations, or when implementing shared memory in graph-based LLM applications. This page demonstrates how to use LangGraph's Store API to persist data across threads, including creating an InMemoryStore with embedding search capabilities, passing stores to graph nodes, and accessing user-specific memories in different conversation threads.\n",
      "[How to add cross-thread persistence (functional API)](https://langchain-ai.github.io/langgraph/how-tos/cross-thread-persistence-functional): LLM should read this page when needing to implement cross-thread persistence in LangGraph functional API, storing user data across different conversation threads, or creating shared memory between workflows. This page explains how to add cross-thread persistence to LangGraph using the Store interface, including defining a store, configuring the entrypoint decorator, and implementing a workflow that can store and retrieve user information across different conversation threads.\n",
      "[How to do a Self-hosted deployment of LangGraph](https://langchain-ai.github.io/langgraph/how-tos/deploy-self-hosted/): LLM should read this page when implementing a self-hosted deployment of LangGraph, configuring required environment variables, or building Docker images for LangGraph applications. This page explains how to deploy LangGraph applications using Docker, covering environment requirements (Redis, Postgres), how to build Docker images with the LangGraph CLI, configuration using environment variables, and deployment options using Docker or Docker Compose.\n",
      "[How to disable streaming for models that don't support it](https://langchain-ai.github.io/langgraph/how-tos/disable-streaming/): LLM should read this page when handling models that don't support streaming, implementing LangGraph with non-streaming models, or troubleshooting streaming errors with OpenAI's O1 models. This page explains how to use the disable\\_streaming=True parameter with ChatOpenAI to make non-streaming models work with LangGraph's astream\\_events API, with code examples showing the error case and proper implementation.\n",
      "[How to edit graph state](https://langchain-ai.github.io/langgraph/how-tos/human\\_in\\_the\\_loop/edit-graph-state/): LLM should read this page when needing to implement human intervention in LangGraph workflows, wanting to edit graph state during execution, or implementing breakpoints in agent systems. This page explains how to edit graph state in LangGraph using breakpoints, including implementing human-in-the-loop interactions, setting up interruptions before specific nodes, and updating state during agent execution.\n",
      "[How to Review Tool Calls](https://langchain-ai.github.io/langgraph/how-tos/human\\_in\\_the\\_loop/review-tool-calls/): LLM should read this page when implementing human review of tool calls, creating interactive agent workflows, or building approval systems for AI actions. This page explains how to implement human-in-the-loop review for tool calls in LangGraph, including approving tool calls, modifying tool calls manually, and providing natural language feedback to agents with complete code examples and explanations.\n",
      "[How to view and update past graph state](https://langchain-ai.github.io/langgraph/how-tos/human\\_in\\_the\\_loop/time-travel/): LLM should read this page when needing to access or modify past states in LangGraph, when debugging agent execution, or when implementing user interventions in agent workflows. This page demonstrates how to view and update past graph states in LangGraph using get\\_state and update\\_state methods, with examples of replaying execution from checkpoints and branching workflows.\n",
      "[How to wait for user input using interrupt](https://langchain-ai.github.io/langgraph/how-tos/human\\_in\\_the\\_loop/wait-user-input/): LLM should read this page when implementing wait-for-user functions in LangGraph, implementing human-in-the-loop interactions, or learning how to use the interrupt() function. This page explains how to pause graph execution to collect user input using LangGraph's interrupt() function, with examples of simple feedback collection and more complex agent interactions that ask clarifying questions.\n",
      "[How to define input/output schema for your graph](https://langchain-ai.github.io/langgraph/how-tos/input\\_output\\_schema/): LLM should read this page when needing to define separate input/output schemas for LangGraph, implementing schema-based data filtering, or understanding schema definitions in StateGraph. This page explains how to define distinct input and output schemas for a StateGraph, showing how input schema validates the provided data structure while output schema filters internal data to return only relevant information, with code examples demonstrating implementation.\n",
      "[How to handle large numbers of tools](https://langchain-ai.github.io/langgraph/how-tos/many-tools/): LLM should read this page when handling large tool collections, implementing dynamic tool selection, or creating retrieval-based tool management in LangGraph. This page demonstrates how to manage large numbers of tools by using vector search to dynamically select relevant tools based on user queries, implementing tool selection nodes in LangGraph, and handling tool selection errors with retry mechanisms.\n",
      "[How to create map-reduce branches for parallel execution](https://langchain-ai.github.io/langgraph/how-tos/map-reduce/): LLM should read this page when learning to implement parallel execution in LangGraph, creating map-reduce operations, or handling dynamic task decomposition. This guide explains how to use LangGraph's Send API to create map-reduce workflows, breaking tasks into parallel sub-tasks and recombining results, with examples showing joke generation across multiple subjects.\n",
      "[How to add summary of the conversation history](https://langchain-ai.github.io/langgraph/how-tos/memory/add-summary-conversation-history/): LLM should read this page when implementing conversation summarization, managing context windows, or building chatbots with memory management. This page demonstrates how to add summary functionality to conversation history using LangGraph, including checking conversation length, creating summaries, and removing old messages while maintaining context.\n",
      "[How to delete messages](https://langchain-ai.github.io/langgraph/how-tos/memory/delete-messages): LLM should read this page when attempting to manage message history in LangGraph, needing to delete specific messages from conversational state, or implementing memory management in LLM applications. This page explains how to delete messages from a LangGraph application using RemoveMessage modifiers, covering both manual deletion with message IDs and programmatic deletion within graph logic to maintain conversation history limits.\n",
      "[How to manage conversation history](https://langchain-ai.github.io/langgraph/how-tos/memory/manage-conversation-history/): LLM should read this page when managing conversation history in LangGraph, preventing context window issues, or implementing custom message filtering. This page explains how to manage conversation history in LangGraph to prevent context window overflow by implementing message filtering functions that control which messages are sent to the LLM.\n",
      "[How to add semantic search to your agent's memory](https://langchain-ai.github.io/langgraph/how-tos/memory/semantic-search/): LLM should read this page when implementing semantic search in agent memory, enabling memory-aware AI assistants, or configuring advanced memory retrieval systems. This page demonstrates how to add semantic search to LangGraph agent memory stores, covering basic setup with embeddings, storing memories, searching by semantic similarity, integrating memory in agents and ReAct agents, and advanced usage like multi-vector indexing and selective memory indexing.\n",
      "[How to add multi-turn conversation in a multi-agent application](https://langchain-ai.github.io/langgraph/how-tos/multi-agent-multi-turn-convo/): LLM should read this page when implementing multi-turn conversations between agents, creating interactive agent systems with human input, or learning about langgraph interrupts and agent handoffs. This page demonstrates how to build a multi-agent system with multi-turn conversations, including human-in-the-loop interactions, agent handoffs, and state management using LangGraph, Command objects, and interrupts.\n",
      "[How to add multi-turn conversation in a multi-agent application (functional API)](https://langchain-ai.github.io/langgraph/how-tos/multi-agent-multi-turn-convo-functional/): LLM should read this page when building multi-turn conversational agents, implementing agent-to-agent handoffs, or using interrupts to collect user input in LangGraph. This guide demonstrates how to create a multi-agent system with multi-turn conversations using LangGraph's functional API, featuring agent handoffs, interrupt mechanics for user input, and a complete example of travel and hotel advisor agents that can transfer control between each other.\n",
      "[How to build a multi-agent network](https://langchain-ai.github.io/langgraph/how-tos/multi-agent-network/): LLM should read this page when implementing multi-agent networks, setting up agent communication via handoffs, or building travel assistance agents. This page explains how to create a fully-connected multi-agent network with LangGraph where agents can communicate with each other via handoffs, including custom agent implementation and using prebuilt ReAct agents with tools.\n",
      "[How to build a multi-agent network (functional API)](https://langchain-ai.github.io/langgraph/how-tos/multi-agent-network-functional/): LLM should read this page when building multi-agent systems, implementing agent handoffs between specialists, or creating fully-connected agent networks. This guide demonstrates how to create a multi-agent network using LangGraph's functional API, with tasks for individual agents and entrypoint functions to manage agent handoffs based on tool calls.\n",
      "[How to add node retry policies](https://langchain-ai.github.io/langgraph/how-tos/node-retries/): LLM should read this page when implementing error handling in LangGraph nodes, configuring API retry mechanisms, or troubleshooting node failures in graph workflows. Shows how to add custom retry policies to LangGraph nodes, including specifying which exceptions to retry on, setting max attempts, intervals, backoff factors, and implementing different retry behaviors for different node types.\n",
      "[How to pass config to tools](https://langchain-ai.github.io/langgraph/how-tos/pass-config-to-tools/): LLM should read this page when implementing secure tool configuration in LangChain, passing user-specific parameters to tools, or configuring tools with runtime values. This page explains how to pass configuration to LangChain tools using RunnableConfig, allowing application-controlled values (like user IDs) to be securely passed to tools without LLM control, with examples of implementing tools that access user-specific data.\n",
      "[How to pass private state between nodes](https://langchain-ai.github.io/langgraph/how-tos/pass\\_private\\_state/): LLM should read this page when implementing data sharing between specific nodes in LangGraph, handling private state in graph workflows, or designing multi-node sequential processes with selective data visibility. This page demonstrates how to pass private data between specific nodes in a LangGraph without making it part of the main schema, using typed dictionaries to define both public and private states, and showing a three-node example where private data flows only between the first two nodes.\n",
      "[How to add thread-level persistence to your graph](https://langchain-ai.github.io/langgraph/how-tos/persistence/): LLM should read this page when implementing persistence in LangGraph, needing to preserve context across user interactions, or learning about thread-level state management. This page explains how to add thread-level persistence to LangGraph applications using MemorySaver, including code examples for creating stateful conversations where context is maintained across multiple interactions.\n",
      "[How to add thread-level persistence (functional API)](https://langchain-ai.github.io/langgraph/how-tos/persistence-functional/): LLM should read this page when implementing thread-level persistence in LangGraph, creating conversational agents with memory, or using functional API with state management. This page explains how to add thread-level persistence to LangGraph functional API workflows using checkpointers, including code examples for creating a simple chatbot with memory across conversation turns.\n",
      "[How to use MongoDB checkpointer for persistence](https://langchain-ai.github.io/langgraph/how-tos/persistence\\_mongodb/): LLM should read this page when implementing persistence in LangGraph agents, setting up MongoDB for state checkpointing, or working with MongoDB connections in LangGraph applications. This page explains how to use the MongoDB checkpointer for LangGraph persistence, covering connection methods (direct, client-based, async), basic setup requirements, and practical examples of saving and retrieving agent state between interactions.\n",
      "[How to use Postgres checkpointer for persistence](https://langchain-ai.github.io/langgraph/how-tos/persistence\\_postgres/): LLM should read this page when setting up persistence for LangGraph agents, implementing PostgreSQL as a checkpoint storage backend, or working with either synchronous or asynchronous database connections. This page details how to use PostgreSQL for persisting LangGraph agent state, covering setup and configuration of PostgresSaver and AsyncPostgresSaver with different connection methods (pool, direct connection, connection string).\n",
      "[How to create a custom checkpointer using Redis](https://langchain-ai.github.io/langgraph/how-tos/persistence\\_redis/): LLM should read this page when implementing persistence in LangGraph applications, creating custom checkpoint mechanisms for agents, or working with Redis as a storage backend. This page demonstrates how to create custom checkpointers for LangGraph agents using Redis, including implementations for both synchronous and asynchronous interfaces that save and retrieve agent state.\n",
      "[How to create a ReAct agent from scratch](https://langchain-ai.github.io/langgraph/how-tos/react-agent-from-scratch/): LLM should read this page when needing to create a custom ReAct agent, wanting more control than prebuilt agents, or implementing ReAct from scratch with LangGraph. This guide shows how to build a custom ReAct agent using LangGraph, covering state definition, model/tool setup, node/edge configuration, graph creation, and testing the implementation with a weather query example.\n",
      "[How to create a ReAct agent from scratch (Functional API)](https://langchain-ai.github.io/langgraph/how-tos/react-agent-from-scratch-functional): LLM should read this page when creating a ReAct agent using LangGraph's Functional API, implementing tool-calling workflows, or building conversational agents with thread persistence. This page explains how to build a ReAct agent from scratch using LangGraph's Functional API, including model and tool setup, defining tasks for model/tool calling, creating an entrypoint for orchestration, and adding thread-level persistence for conversational experiences.\n",
      "[How to force tool-calling agent to structure output](https://langchain-ai.github.io/langgraph/how-tos/react-agent-structured-output): LLM should read this page when needing to force tool-calling agents to produce structured output, implementing consistent output formats for downstream software, or choosing between single-LLM vs two-LLM structured output approaches. The page explains two methods for implementing structured output with tool-calling agents: binding output as a tool (single LLM approach) and using two LLMs with structured output conversion, with code examples for both approaches using LangGraph.\n",
      "[How to create and control loops](https://langchain-ai.github.io/langgraph/how-tos/recursion-limit/): LLM should read this page when building loops in computational graphs, needing to implement termination conditions, or handling recursion limits in LangGraph. The page explains how to create graphs with loops using conditional edges for termination, set recursion limits, handle GraphRecursionError, and implement complex loops with branches.\n",
      "[How to review tool calls (Functional API)](https://langchain-ai.github.io/langgraph/how-tos/review-tool-calls-functional/): LLM should read this page when implementing human review of tool calls, creating ReAct agents with Functional API, or adding human-in-the-loop workflows. This page demonstrates how to review tool calls before execution in a ReAct agent using LangGraph's Functional API, including accepting, revising, or generating custom tool messages with the interrupt function.\n",
      "[How to pass custom run ID or set tags and metadata for graph runs in LangSmith](https://langchain-ai.github.io/langgraph/how-tos/run-id-langsmith/): LLM should read this page when needing to customize trace information in LangSmith for LangGraph runs or when debugging graph runs with custom identifiers. The page explains how to pass custom run\\_id, set tags, add metadata, and customize run names for LangGraph traces in LangSmith using RunnableConfig, with examples showing implementation with a ReAct agent.\n",
      "[How to create a sequence of steps](https://langchain-ai.github.io/langgraph/how-tos/sequence/): LLM should read this page when implementing sequential workflows in LangGraph, creating multi-step processes in applications, or learning about state management in graph-based systems. This page explains how to create sequences in LangGraph, covering methods for building sequential graphs using .add\\_node/.add\\_edge or the shorthand .add\\_sequence, defining state with TypedDict, creating nodes as functions that update state, and compiling/invoking graphs with examples.\n",
      "[How to use Pydantic model as graph state](https://langchain-ai.github.io/langgraph/how-tos/state-model): LLM should read this page when implementing Pydantic models for state validation in LangGraph, handling complex state schema definitions, or troubleshooting validation errors in graph nodes. This guide explains how to use Pydantic BaseModel as a state schema in LangGraph for runtime validation, covering basic implementation, limitations, validation behavior across multiple nodes, serialization patterns, type coercion, and working with message models.\n",
      "[How to update graph state from nodes](https://langchain-ai.github.io/langgraph/how-tos/state-reducers/): LLM should read this page when needing to update state in LangGraph, designing graphs with nodes that modify state, or implementing reducers for state management. This page explains how to define state schemas in LangGraph using TypedDict, how nodes can update state, and how to use reducers to control state updates, with specific examples using message handling.\n",
      "[How to stream](https://langchain-ai.github.io/langgraph/how-tos/streaming/): LLM should read this page when needing to implement streaming in LangGraph applications, understanding different streaming modes, or troubleshooting LLM response delivery. This page explains how to stream LLM outputs using LangGraph, covering different streaming modes (values, updates, custom, messages, debug), with code examples for each mode and how to combine multiple streaming modes.\n",
      "[How to stream data from within a tool](https://langchain-ai.github.io/langgraph/how-tos/streaming-events-from-within-tools/): LLM should read this page when implementing streaming functionality in tools, integrating LLM outputs with custom data streams, or developing LangGraph applications with real-time feedback. This page explains how to stream data from within tools using LangGraph, covering custom data streaming with stream\\_mode=\"custom\", LLM token streaming with stream\\_mode=\"messages\", and implementation approaches both with and without LangChain.\n",
      "[How to stream LLM tokens from specific nodes](https://langchain-ai.github.io/langgraph/how-tos/streaming-specific-nodes/): LLM should read this page when needing to filter token streaming from specific nodes in LangGraph, implementing selective streaming in multi-node workflows, or controlling which node outputs are displayed. Guide explains how to stream LLM tokens from specific nodes using stream\\_mode=\"messages\" and filtering by the langgraph\\_node metadata field, with complete code examples for implementing this in StateGraph applications.\n",
      "[How to stream from subgraphs](https://langchain-ai.github.io/langgraph/how-tos/streaming-subgraphs/): LLM should read this page when needing to stream outputs from subgraphs in LangGraph, implementing nested graph streaming, or debugging hierarchical graph execution. This page explains how to stream outputs from subgraphs in LangGraph by using the subgraphs=True parameter in the parent graph's stream() method, with a complete code example showing the difference between regular streaming and subgraph streaming.\n",
      "[How to stream LLM tokens from your graph](https://langchain-ai.github.io/langgraph/how-tos/streaming-tokens): LLM should read this page when needing to stream LLM tokens from a LangGraph application, implementing custom token streaming, or filtering streamed outputs. This page explains how to stream individual LLM tokens from LangGraph nodes using graph.stream() with different stream\\_mode options, including examples with and without LangChain, async implementations, and how to filter streamed tokens using metadata.\n",
      "[How to use subgraphs](https://langchain-ai.github.io/langgraph/how-tos/subgraph/): LLM should read this page when building complex systems with subgraphs, implementing multi-agent systems, or needing to share state between parent graphs and subgraphs. The page explains two methods for using subgraphs: adding compiled subgraphs when schemas share keys, and invoking subgraphs via node functions when schemas differ, with code examples for both approaches.\n",
      "[How to add thread-level persistence to a subgraph](https://langchain-ai.github.io/langgraph/how-tos/subgraph-persistence/): LLM should read this page when implementing persistence in nested LangGraph architectures, adding thread-level storage to subgraphs, or debugging state propagation in LangGraph applications. This guide demonstrates how to add thread-level persistence to subgraphs by passing a checkpointer only to the parent graph during compilation, accessing persisted states from both parent and child graphs, and retrieving subgraph state using the proper configuration parameters.\n",
      "[How to transform inputs and outputs of a subgraph](https://langchain-ai.github.io/langgraph/how-tos/subgraph-transform-state/): LLM should read this page when needing to work with nested subgraphs, transforming state between parent and child graphs, or integrating independent state components in LangGraph. This page demonstrates how to transform inputs and outputs between parent graphs and subgraphs with different state structures, showing implementation of three nested graphs (parent, child, grandchild) with separate state dictionaries and transformation functions.\n",
      "[How to view and update state in subgraphs](https://langchain-ai.github.io/langgraph/how-tos/subgraphs-manage-state/): LLM should read this page when working with state management in nested subgraphs, implementing human-in-the-loop patterns, or debugging complex graph flows. This guide covers viewing and updating state in LangGraph subgraphs, including how to resume execution from breakpoints, modify subgraph state, act as specific nodes, and work with multi-level nested subgraphs.\n",
      "[How to call tools using ToolNode](https://langchain-ai.github.io/langgraph/how-tos/tool-calling/): LLM should read this page when learning how to implement tool calling with LangGraph, when working with the ToolNode component, or when building ReAct agents. This page covers using LangGraph's ToolNode for tool calling, including setup, manual invocation, working with chat models, building a ReAct agent, handling single and parallel tool calls, and error handling.\n",
      "[How to handle tool calling errors](https://langchain-ai.github.io/langgraph/how-tos/tool-calling-errors/): LLM should read this page when handling tool call errors, implementing error handling for LLM-tool interactions, or creating fallback strategies for failed tool calls. This page covers strategies for handling tool calling errors in LangGraph, including using the prebuilt ToolNode with built-in error handling, implementing custom error handling patterns, and fallback mechanisms with model upgrades when tools fail.\n",
      "[How to update graph state from tools](https://langchain-ai.github.io/langgraph/how-tos/update-state-from-tools/): LLM should read this page when needing to update graph state from tools in LangGraph, implementing personalized responses based on tool updates, or using Command objects to modify state. This page details how to update graph state from tools using Command objects, creating personalized agents with state tracking, and implementing dynamic prompt construction based on updated state values.\n",
      "[How to interact with the deployment using RemoteGraph](https://langchain-ai.github.io/langgraph/how-tos/use-remote-graph/): LLM should read this page when needing to interact with LangGraph Platform deployments remotely, when implementing RemoteGraph interfaces, or when using deployed graphs as subgraphs. This page explains how to use RemoteGraph to interact with LangGraph Platform deployments, covering initialization methods (URL-based or client-based), synchronous/asynchronous invocation, thread-level persistence, and using RemoteGraph as a subgraph in larger applications.\n",
      "[How to visualize your graph](https://langchain-ai.github.io/langgraph/how-tos/visualization): LLM should read this page when needing to visualize LangGraph graphs, looking for graph visualization methods, or working with graph visualization in Python. Comprehensive guide for visualizing graphs in LangGraph with multiple methods: Mermaid syntax, Mermaid.ink API for PNG rendering, Pyppeteer-based visualization, and Graphviz, with customization options for colors, styles, and layout.\n",
      "[How to wait for user input (Functional API)](https://langchain-ai.github.io/langgraph/how-tos/wait-user-input-functional/): LLM should read this page when implementing human-in-the-loop workflows, integrating user input into agent systems, or adding interruption capabilities to LangGraph applications. The page explains how to use the `interrupt()` function in LangGraph's Functional API to pause execution for human input, with examples for both simple workflows and ReAct agents, including code implementations with checkpointing.\n",
      "\n",
      "\n",
      "\n",
      "Namespace '('planner_agent:5d3edfdb-c24d-2f28-0b5c-a1c230a12ee1',)'\n",
      "Update from node 'agent'\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Name: planner_agent\n",
      "\n",
      "To create a LangGraph application that generates a joke based on a user-provided topic and checks if the joke has a punchline, I need to clarify a few aspects of your request:\n",
      "\n",
      "1. **Joke Generation**: Do you have a specific method or model in mind for generating jokes, or should I use a general joke generation approach?\n",
      "2. **Punchline Check**: How do you want to define or check if a joke has a punchline? Should it be a simple keyword check, or do you want a more complex analysis?\n",
      "3. **User Interaction**: How do you envision the user providing the topic? Should it be through a chat interface, a web form, or another method?\n",
      "4. **Output Format**: What format do you want the output to be in? Should it be plain text, structured data, or something else?\n",
      "5. **Additional Features**: Are there any other features or functionalities you would like to include in this application?\n",
      "\n",
      "Once I have this information, I can refine the project scope and provide relevant documentation links for implementation.\n",
      "\n",
      "\n",
      "\n",
      "Namespace '()'\n",
      "Update from node 'planner_agent'\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Name: planner_agent\n",
      "\n",
      "To create a LangGraph application that generates a joke based on a user-provided topic and checks if the joke has a punchline, I need to clarify a few aspects of your request:\n",
      "\n",
      "1. **Joke Generation**: Do you have a specific method or model in mind for generating jokes, or should I use a general joke generation approach?\n",
      "2. **Punchline Check**: How do you want to define or check if a joke has a punchline? Should it be a simple keyword check, or do you want a more complex analysis?\n",
      "3. **User Interaction**: How do you envision the user providing the topic? Should it be through a chat interface, a web form, or another method?\n",
      "4. **Output Format**: What format do you want the output to be in? Should it be plain text, structured data, or something else?\n",
      "5. **Additional Features**: Are there any other features or functionalities you would like to include in this application?\n",
      "\n",
      "Once I have this information, I can refine the project scope and provide relevant documentation links for implementation.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "===\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "\n",
    "# Planner agent\n",
    "planner_agent = create_react_agent(\n",
    "    model,\n",
    "    prompt=planner_prompt_formatted,\n",
    "    tools=[fetch_doc, transfer_to_researcher_agent],\n",
    "    name=\"planner_agent\",\n",
    ")\n",
    "\n",
    "# Researcher agent\n",
    "researcher_agent = create_react_agent(\n",
    "    model,\n",
    "    prompt=researcher_prompt,\n",
    "    tools=[fetch_doc, transfer_to_planner_agent],\n",
    "    name=\"researcher_agent\",\n",
    ")\n",
    "\n",
    "# Swarm\n",
    "checkpointer = InMemorySaver()\n",
    "agent_swarm = create_swarm(\n",
    "    [planner_agent, researcher_agent], default_active_agent=\"planner_agent\"\n",
    ")\n",
    "app = agent_swarm.compile(checkpointer=checkpointer)\n",
    "\n",
    "# Input\n",
    "request = \"Create a LangGraph application that is a prompt chain: it takes a topic from a user, generates a joke, and checks if the joke has a punchline.\"\n",
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "print_stream(\n",
    "    app.stream(\n",
    "        {\"messages\": [{\"role\": \"user\", \"content\": request}]}, config, subgraphs=True\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace '('planner_agent:a999c959-286c-f687-d2e3-9f77d9dddea9',)'\n",
      "Update from node 'agent'\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Name: planner_agent\n",
      "Tool Calls:\n",
      "  transfer_to_researcher_agent (call_KU6H9mDNgg2X7xIOqzvcEFb9)\n",
      " Call ID: call_KU6H9mDNgg2X7xIOqzvcEFb9\n",
      "  Args:\n",
      "\n",
      "\n",
      "\n",
      "Namespace '()'\n",
      "Update from node 'planner_agent'\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: transfer_to_researcher_agent\n",
      "\n",
      "Successfully transferred to researcher_agent\n",
      "\n",
      "\n",
      "\n",
      "Namespace '('researcher_agent:877fc2f9-f988-2334-c7c2-1ea726a657e0',)'\n",
      "Update from node 'agent'\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Name: researcher_agent\n",
      "Tool Calls:\n",
      "  fetch_doc (call_ZEsDN3P3BdrtD3hpHhvvklrG)\n",
      " Call ID: call_ZEsDN3P3BdrtD3hpHhvvklrG\n",
      "  Args:\n",
      "    url: https://langchain-ai.github.io/langgraph/tutorials/introduction/\n",
      "  fetch_doc (call_l5I5GeOl7jwtUTPaNVbqDE5Q)\n",
      " Call ID: call_l5I5GeOl7jwtUTPaNVbqDE5Q\n",
      "  Args:\n",
      "    url: https://langchain-ai.github.io/langgraph/tutorials/workflows/\n",
      "  fetch_doc (call_mttpDaH3SXIC2cchYUHv9mYF)\n",
      " Call ID: call_mttpDaH3SXIC2cchYUHv9mYF\n",
      "  Args:\n",
      "    url: https://langchain-ai.github.io/langgraph/how-tos/create-react-agent/\n",
      "  fetch_doc (call_eTobd3kxEqtX8Eg8cuSzer8C)\n",
      " Call ID: call_eTobd3kxEqtX8Eg8cuSzer8C\n",
      "  Args:\n",
      "    url: https://langchain-ai.github.io/langgraph/how-tos/tool-calling/\n",
      "  fetch_doc (call_Gw1hlJ7g215gzL6VPu5xSUBa)\n",
      " Call ID: call_Gw1hlJ7g215gzL6VPu5xSUBa\n",
      "  Args:\n",
      "    url: https://langchain-ai.github.io/langgraph/how-tos/update-state-from-tools/\n",
      "\n",
      "\n",
      "\n",
      "Namespace '('researcher_agent:877fc2f9-f988-2334-c7c2-1ea726a657e0',)'\n",
      "Update from node 'tools'\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: fetch_doc\n",
      "\n",
      "How to update graph state from tools\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "[Skip to content](#how-to-update-graph-state-from-tools)\n",
      "\n",
      "**Join us at  [Interrupt: The Agent AI Conference by LangChain](https://interrupt.langchain.com/) on May 13 & 14 in San Francisco!**\n",
      "\n",
      "[![logo](../../static/wordmark_dark.svg)\n",
      "![logo](../../static/wordmark_light.svg)](../..)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "How to update graph state from tools\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Initializing search\n",
      "\n",
      "[GitHub](https://github.com/langchain-ai/langgraph \"Go to repository\")\n",
      "\n",
      "* [Home](../..)\n",
      "* [API reference](../../reference/graphs/)\n",
      "\n",
      "\n",
      "\n",
      "[![logo](../../static/wordmark_dark.svg)\n",
      "![logo](../../static/wordmark_light.svg)](../..)\n",
      "\n",
      "[GitHub](https://github.com/langchain-ai/langgraph \"Go to repository\")\n",
      "\n",
      "* [Home](../..)\n",
      "\n",
      "  Home\n",
      "  + Get started\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "    Get started\n",
      "    - [Learn the basics](../../tutorials/introduction/)\n",
      "    - [Deployment](../../tutorials/deployment/)\n",
      "  + Guides\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "    Guides\n",
      "    - [How-to Guides](../)\n",
      "\n",
      "      How-to Guides\n",
      "      * LangGraph\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "        LangGraph\n",
      "        + [LangGraph](../../how-tos#langgraph)\n",
      "        + [Graph API Basics](../../how-tos#graph-api-basics)\n",
      "        + [Controllability](../../how-tos#controllability)\n",
      "        + [Persistence](../../how-tos#persistence)\n",
      "        + [Memory](../../how-tos#memory)\n",
      "        + [Human-in-the-loop](../../how-tos#human-in-the-loop)\n",
      "        + [Streaming](../../how-tos#streaming)\n",
      "        + Tool calling\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "          Tool calling\n",
      "          - [Tool calling](../../how-tos#tool-calling)\n",
      "          - [How to call tools using ToolNode](../tool-calling/)\n",
      "          - [How to handle tool calling errors](../tool-calling-errors/)\n",
      "          - [How to pass runtime values to tools](../pass-run-time-values-to-tools/)\n",
      "          - How to update graph state from tools\n",
      "\n",
      "            [How to update graph state from tools](./)\n",
      "\n",
      "\n",
      "\n",
      "            Table of contents\n",
      "            * [Setup](#setup)\n",
      "            * [Define tool](#define-tool)\n",
      "            * [Define prompt](#define-prompt)\n",
      "            * [Define graph](#define-graph)\n",
      "            * [Use it!](#use-it)\n",
      "          - [How to pass config to tools](../pass-config-to-tools/)\n",
      "          - [How to handle large numbers of tools](../many-tools/)\n",
      "        + [Subgraphs](../../how-tos#subgraphs)\n",
      "        + [Multi-agent](../../how-tos#multi-agent)\n",
      "        + [State Management](../../how-tos#state-management)\n",
      "        + [Other](../../how-tos#other)\n",
      "        + [Prebuilt ReAct Agent](../../how-tos#prebuilt-react-agent)\n",
      "      * [LangGraph Platform](../../how-tos#langgraph-platform)\n",
      "    - [Concepts](../../concepts/)\n",
      "    - [Tutorials](../../tutorials/)\n",
      "  + Resources\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "    Resources\n",
      "    - [Prebuilt Agents](../../prebuilt/)\n",
      "    - [Companies using LangGraph](../../adopters/)\n",
      "    - [LLMS-txt](../../llms-txt-overview/)\n",
      "    - [FAQ](../../concepts/faq/)\n",
      "    - [Troubleshooting](../../troubleshooting/errors/)\n",
      "    - [LangGraph Academy Course](https://academy.langchain.com/courses/intro-to-langgraph)\n",
      "* [API reference](../../reference/graphs/)\n",
      "\n",
      "Table of contents\n",
      "\n",
      "* [Setup](#setup)\n",
      "* [Define tool](#define-tool)\n",
      "* [Define prompt](#define-prompt)\n",
      "* [Define graph](#define-graph)\n",
      "* [Use it!](#use-it)\n",
      "\n",
      "1. [Home](../..)\n",
      "2. [Guides](../)\n",
      "3. [How-to Guides](../)\n",
      "4. [LangGraph](../../how-tos#langgraph)\n",
      "5. [Tool calling](../../how-tos#tool-calling)\n",
      "\n",
      "How to update graph state from tools[¶](#how-to-update-graph-state-from-tools \"Permanent link\")\n",
      "===============================================================================================\n",
      "\n",
      "Prerequisites\n",
      "\n",
      "This guide assumes familiarity with the following:\n",
      "\n",
      "* [Command](../../concepts/low_level/#command)\n",
      "\n",
      "A common use case is updating graph state from inside a tool. For example, in a customer support application you might want to look up customer account number or ID in the beginning of the conversation. To update the graph state from the tool, you can return `Command(update={\"my_custom_key\": \"foo\", \"messages\": [...]})` from the tool:\n",
      "\n",
      "```\n",
      "@tool\n",
      "def lookup_user_info(tool_call_id: Annotated[str, InjectedToolCallId], config: RunnableConfig):\n",
      "    \"\"\"Use this to look up user information to better assist them with their questions.\"\"\"\n",
      "    user_info = get_user_info(config)\n",
      "    return Command(\n",
      "        update={\n",
      "            # update the state keys\n",
      "            \"user_info\": user_info,\n",
      "            # update the message history\n",
      "            \"messages\": [ToolMessage(\"Successfully looked up user information\", tool_call_id=tool_call_id)]\n",
      "        }\n",
      "    )\n",
      "\n",
      "```\n",
      "\n",
      "Important\n",
      "\n",
      "If you want to use tools that return `Command` and update graph state, you can either use prebuilt [`create_react_agent`](../../reference/prebuilt/#langgraph.prebuilt.chat_agent_executor.create_react_agent) / [`ToolNode`](../../reference/prebuilt/#langgraph.prebuilt.tool_node.ToolNode) components, or implement your own tool-executing node that collects `Command` objects returned by the tools and returns a list of them, e.g.:\n",
      "\n",
      "```\n",
      "def call_tools(state):\n",
      "    ...\n",
      "    commands = [tools_by_name[tool_call[\"name\"]].invoke(tool_call) for tool_call in tool_calls]\n",
      "    return commands\n",
      "\n",
      "```\n",
      "\n",
      "This guide shows how you can do this using LangGraph's prebuilt components ([`create_react_agent`](../../reference/prebuilt/#langgraph.prebuilt.chat_agent_executor.create_react_agent) / [`ToolNode`](../../reference/prebuilt/#langgraph.prebuilt.tool_node.ToolNode)).\n",
      "\n",
      "Note\n",
      "\n",
      "Support for tools that return [`Command`](../../reference/types/#langgraph.types.Command) was added in LangGraph `v0.2.59`.\n",
      "\n",
      "Setup[¶](#setup \"Permanent link\")\n",
      "---------------------------------\n",
      "\n",
      "First, let's install the required packages and set our API keys:\n",
      "\n",
      "```\n",
      "%%capture --no-stderr\n",
      "%pip install -U langgraph langchain-openai\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "import os\n",
      "import getpass\n",
      "\n",
      "\n",
      "def _set_if_undefined(var: str):\n",
      "    if not os.environ.get(var):\n",
      "        os.environ[var] = getpass.getpass(f\"Please provide your {var}\")\n",
      "\n",
      "\n",
      "_set_if_undefined(\"OPENAI_API_KEY\")\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "Please provide your OPENAI_API_KEY ········\n",
      "\n",
      "```\n",
      "\n",
      "Set up [LangSmith](https://smith.langchain.com) for LangGraph development\n",
      "\n",
      "Sign up for LangSmith to quickly spot issues and improve the performance of your LangGraph projects. LangSmith lets you use trace data to debug, test, and monitor your LLM apps built with LangGraph — read more about how to get started [here](https://docs.smith.langchain.com).\n",
      "\n",
      "Let's create a simple ReAct style agent that can look up user information and personalize the response based on the user info.\n",
      "\n",
      "Define tool[¶](#define-tool \"Permanent link\")\n",
      "---------------------------------------------\n",
      "\n",
      "First, let's define the tool that we'll be using to look up user information. We'll use a naive implementation that simply looks user information up using a dictionary:\n",
      "\n",
      "```\n",
      "USER_INFO = [\n",
      "    {\"user_id\": \"1\", \"name\": \"Bob Dylan\", \"location\": \"New York, NY\"},\n",
      "    {\"user_id\": \"2\", \"name\": \"Taylor Swift\", \"location\": \"Beverly Hills, CA\"},\n",
      "]\n",
      "\n",
      "USER_ID_TO_USER_INFO = {info[\"user_id\"]: info for info in USER_INFO}\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "from langgraph.prebuilt.chat_agent_executor import AgentState\n",
      "from langgraph.types import Command\n",
      "from langchain_core.tools import tool\n",
      "from langchain_core.tools.base import InjectedToolCallId\n",
      "from langchain_core.messages import ToolMessage\n",
      "from langchain_core.runnables import RunnableConfig\n",
      "\n",
      "from typing_extensions import Any, Annotated\n",
      "\n",
      "\n",
      "class State(AgentState):\n",
      "    # updated by the tool\n",
      "    user_info: dict[str, Any]\n",
      "\n",
      "\n",
      "@tool\n",
      "def lookup_user_info(\n",
      "    tool_call_id: Annotated[str, InjectedToolCallId], config: RunnableConfig\n",
      "):\n",
      "    \"\"\"Use this to look up user information to better assist them with their questions.\"\"\"\n",
      "    user_id = config.get(\"configurable\", {}).get(\"user_id\")\n",
      "    if user_id is None:\n",
      "        raise ValueError(\"Please provide user ID\")\n",
      "\n",
      "    if user_id not in USER_ID_TO_USER_INFO:\n",
      "        raise ValueError(f\"User '{user_id}' not found\")\n",
      "\n",
      "    user_info = USER_ID_TO_USER_INFO[user_id]\n",
      "    return Command(\n",
      "        update={\n",
      "            # update the state keys\n",
      "            \"user_info\": user_info,\n",
      "            # update the message history\n",
      "            \"messages\": [\n",
      "                ToolMessage(\n",
      "                    \"Successfully looked up user information\", tool_call_id=tool_call_id\n",
      "                )\n",
      "            ],\n",
      "        }\n",
      "    )\n",
      "\n",
      "```\n",
      "\n",
      "API Reference: [Command](https://langchain-ai.github.io/langgraph/reference/types/#langgraph.types.Command) | [tool](https://python.langchain.com/api_reference/core/tools/langchain_core.tools.convert.tool.html) | [InjectedToolCallId](https://python.langchain.com/api_reference/core/tools/langchain_core.tools.base.InjectedToolCallId.html) | [ToolMessage](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.tool.ToolMessage.html) | [RunnableConfig](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.config.RunnableConfig.html)\n",
      "\n",
      "Define prompt[¶](#define-prompt \"Permanent link\")\n",
      "-------------------------------------------------\n",
      "\n",
      "Let's now add personalization: we'll respond differently to the user based on the state values AFTER the state has been updated from the tool. To achieve this, let's define a function that will dynamically construct the system prompt based on the graph state. It will be called every time the LLM is called and the function output will be passed to the LLM:\n",
      "\n",
      "```\n",
      "def prompt(state: State):\n",
      "    user_info = state.get(\"user_info\")\n",
      "    if user_info is None:\n",
      "        return state[\"messages\"]\n",
      "\n",
      "    system_msg = (\n",
      "        f\"User name is {user_info['name']}. User lives in {user_info['location']}\"\n",
      "    )\n",
      "    return [{\"role\": \"system\", \"content\": system_msg}] + state[\"messages\"]\n",
      "\n",
      "```\n",
      "\n",
      "Define graph[¶](#define-graph \"Permanent link\")\n",
      "-----------------------------------------------\n",
      "\n",
      "Finally, let's combine this into a single graph using the prebuilt `create_react_agent`:\n",
      "\n",
      "```\n",
      "from langgraph.prebuilt import create_react_agent\n",
      "from langchain_openai import ChatOpenAI\n",
      "\n",
      "model = ChatOpenAI(model=\"gpt-4o\")\n",
      "\n",
      "agent = create_react_agent(\n",
      "    model,\n",
      "    # pass the tool that can update state\n",
      "    [lookup_user_info],\n",
      "    state_schema=State,\n",
      "    # pass dynamic prompt function\n",
      "    prompt=prompt,\n",
      ")\n",
      "\n",
      "```\n",
      "\n",
      "API Reference: [create\\_react\\_agent](https://langchain-ai.github.io/langgraph/reference/prebuilt/#langgraph.prebuilt.chat_agent_executor.create_react_agent) | [ChatOpenAI](https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html)\n",
      "\n",
      "Use it![¶](#use-it \"Permanent link\")\n",
      "------------------------------------\n",
      "\n",
      "Let's now try running our agent. We'll need to provide user ID in the config so that our tool knows what information to look up:\n",
      "\n",
      "```\n",
      "for chunk in agent.stream(\n",
      "    {\"messages\": [(\"user\", \"hi, what should i do this weekend?\")]},\n",
      "    # provide user ID in the config\n",
      "    {\"configurable\": {\"user_id\": \"1\"}},\n",
      "):\n",
      "    print(chunk)\n",
      "    print(\"\\n\")\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "{'agent': {'messages': [AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_7LSUh6ZDvGJAUvlWvXiCK4Gf', 'function': {'arguments': '{}', 'name': 'lookup_user_info'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 11, 'prompt_tokens': 56, 'total_tokens': 67, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_9d50cd990b', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-57eeb216-e35d-4501-aaac-b5c6b26fb17c-0', tool_calls=[{'name': 'lookup_user_info', 'args': {}, 'id': 'call_7LSUh6ZDvGJAUvlWvXiCK4Gf', 'type': 'tool_call'}], usage_metadata={'input_tokens': 56, 'output_tokens': 11, 'total_tokens': 67, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}}\n",
      "\n",
      "\n",
      "{'tools': {'user_info': {'user_id': '1', 'name': 'Bob Dylan', 'location': 'New York, NY'}, 'messages': [ToolMessage(content='Successfully looked up user information', name='lookup_user_info', id='168d8ff8-b021-4c8b-a11a-3b50c30a072c', tool_call_id='call_7LSUh6ZDvGJAUvlWvXiCK4Gf')]}}\n",
      "\n",
      "\n",
      "{'agent': {'messages': [AIMessage(content=\"Hi Bob! Since you're in New York, NY, there are plenty of exciting things to do over the weekend. Here are some suggestions:\\n\\n1. **Explore Central Park**: Take a leisurely walk, rent a bike, or have a picnic in this iconic park.\\n\\n2. **Visit a Museum**: Check out The Metropolitan Museum of Art or the Museum of Modern Art (MoMA) for an enriching cultural experience.\\n\\n3. **Broadway Show**: Catch a Broadway show or an off-Broadway performance for some world-class entertainment.\\n\\n4. **Food Tour**: Explore different neighborhoods like Greenwich Village or Williamsburg for diverse culinary experiences.\\n\\n5. **Brooklyn Bridge Walk**: Take a walk across the Brooklyn Bridge for stunning views of the city skyline.\\n\\n6. **Visit a Rooftop Bar**: Enjoy a drink with a view at one of New York’s many rooftop bars.\\n\\n7. **Explore a New Neighborhood**: Discover the unique charm of areas like SoHo, Chelsea, or Astoria.\\n\\n8. **Live Music**: Check out live music venues for a night of great performances.\\n\\n9. **Art Galleries**: Visit some of the smaller art galleries around Chelsea or the Lower East Side.\\n\\n10. **Attend a Local Event**: Look up any local events or festivals happening this weekend.\\n\\nFeel free to let me know if you want more details on any of these activities!\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 285, 'prompt_tokens': 95, 'total_tokens': 380, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_9d50cd990b', 'finish_reason': 'stop', 'logprobs': None}, id='run-f13ce15b-02b6-40e6-8264-c4d9edd0d03a-0', usage_metadata={'input_tokens': 95, 'output_tokens': 285, 'total_tokens': 380, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}}\n",
      "\n",
      "```\n",
      "\n",
      "We can see that the model correctly recommended some New York activities for Bob Dylan! Let's try getting recommendations for Taylor Swift:\n",
      "\n",
      "```\n",
      "for chunk in agent.stream(\n",
      "    {\"messages\": [(\"user\", \"hi, what should i do this weekend?\")]},\n",
      "    {\"configurable\": {\"user_id\": \"2\"}},\n",
      "):\n",
      "    print(chunk)\n",
      "    print(\"\\n\")\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "{'agent': {'messages': [AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_5HLtJtzcgmKbtmK6By21wW5Y', 'function': {'arguments': '{}', 'name': 'lookup_user_info'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 11, 'prompt_tokens': 56, 'total_tokens': 67, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_c7ca0ebaca', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-bacacd7d-76cc-4f6b-9e9b-d9e6f00b9391-0', tool_calls=[{'name': 'lookup_user_info', 'args': {}, 'id': 'call_5HLtJtzcgmKbtmK6By21wW5Y', 'type': 'tool_call'}], usage_metadata={'input_tokens': 56, 'output_tokens': 11, 'total_tokens': 67, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}}\n",
      "\n",
      "\n",
      "{'tools': {'user_info': {'user_id': '2', 'name': 'Taylor Swift', 'location': 'Beverly Hills, CA'}, 'messages': [ToolMessage(content='Successfully looked up user information', name='lookup_user_info', id='d81ef31e-6d77-4f13-ae86-e2e6ba567e3d', tool_call_id='call_5HLtJtzcgmKbtmK6By21wW5Y')]}}\n",
      "\n",
      "\n",
      "{'agent': {'messages': [AIMessage(content=\"Hi Taylor! Since you're in Beverly Hills, here are a few suggestions for a fun weekend:\\n\\n1. **Hiking at Runyon Canyon**: Enjoy a scenic hike with beautiful views of Los Angeles. It's a great way to get some exercise and enjoy the outdoors.\\n\\n2. **Visit Rodeo Drive**: Spend some time shopping or window shopping at the famous Rodeo Drive. You might even spot some celebrities!\\n\\n3. **Explore the Getty Center**: Check out the art collections and beautiful gardens at the Getty Center. The architecture and views are stunning.\\n\\n4. **Relax at a Spa**: Treat yourself to a relaxing day at one of Beverly Hills' luxurious spas.\\n\\n5. **Dining Out**: Try a new restaurant or visit your favorite spot for a delicious meal. Beverly Hills has a fantastic dining scene.\\n\\n6. **Attend a Local Event**: Check out any local events or concerts happening this weekend. Beverly Hills often hosts exciting events.\\n\\nEnjoy your weekend!\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 198, 'prompt_tokens': 95, 'total_tokens': 293, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_c7ca0ebaca', 'finish_reason': 'stop', 'logprobs': None}, id='run-2057df76-f192-4c69-a66a-1f0a86bf5d66-0', usage_metadata={'input_tokens': 95, 'output_tokens': 198, 'total_tokens': 293, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}}\n",
      "\n",
      "```\n",
      "\n",
      "Was this page helpful?\n",
      "\n",
      "Thanks for your feedback!\n",
      "\n",
      "Thanks for your feedback! Please help us improve this page by adding to the discussion below.\n",
      "\n",
      "Comments\n",
      "--------\n",
      "\n",
      "Back to top\n",
      "\n",
      "\n",
      "[Previous\n",
      "\n",
      "How to pass runtime values to tools](../pass-run-time-values-to-tools/)\n",
      "[Next\n",
      "\n",
      "How to pass config to tools](../pass-config-to-tools/)\n",
      "\n",
      "Copyright © 2025 LangChain, Inc | [Consent Preferences](#__consent)\n",
      "\n",
      "Made with\n",
      "[Material for MkDocs Insiders](https://squidfunk.github.io/mkdocs-material/)\n",
      "\n",
      "#### Cookie consent\n",
      "\n",
      "We use cookies to recognize your repeated visits and preferences, as well as to measure the effectiveness of our documentation and whether users find what they're searching for. **Clicking \"Accept\" makes our documentation better. Thank you!** ❤️\n",
      "\n",
      "* Google Analytics\n",
      "* GitHub\n",
      "\n",
      "Accept\n",
      "Reject\n",
      "\n",
      "\n",
      "\n",
      "Namespace '('researcher_agent:877fc2f9-f988-2334-c7c2-1ea726a657e0',)'\n",
      "Update from node 'agent'\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Name: researcher_agent\n",
      "\n",
      "Based on the information gathered from the LangGraph documentation, here is a structured plan to create a LangGraph application that generates jokes based on user input, checks for punchlines, and saves the jokes.\n",
      "\n",
      "### Application Overview\n",
      "The application will:\n",
      "1. Accept a topic from the user via a command-line interface.\n",
      "2. Generate a joke using OpenAI's GPT model.\n",
      "3. Check if the joke has a punchline.\n",
      "4. If the punchline is missing, generate a new joke.\n",
      "5. Save the jokes for future reference.\n",
      "\n",
      "### Implementation Steps\n",
      "\n",
      "#### 1. Setup Environment\n",
      "Make sure to install the necessary packages:\n",
      "```bash\n",
      "pip install langgraph langchain-openai\n",
      "```\n",
      "\n",
      "#### 2. Define the State\n",
      "Define the state schema that will hold the necessary information:\n",
      "```python\n",
      "from typing import TypedDict, Annotated\n",
      "from langgraph.graph import StateGraph, MessagesState\n",
      "\n",
      "class State(TypedDict):\n",
      "    topic: str\n",
      "    joke: str\n",
      "    punchline_check: bool\n",
      "```\n",
      "\n",
      "#### 3. Define the Joke Generation Function\n",
      "Create a function that generates a joke based on the provided topic:\n",
      "```python\n",
      "from langchain_openai import ChatOpenAI\n",
      "\n",
      "llm = ChatOpenAI(model=\"gpt-4\", temperature=0)\n",
      "\n",
      "def generate_joke(state: State) -> str:\n",
      "    prompt = f\"Tell me a joke about {state['topic']}.\"\n",
      "    response = llm.invoke(prompt)\n",
      "    return response.content\n",
      "```\n",
      "\n",
      "#### 4. Define the Punchline Check Function\n",
      "Create a function to check if the joke has a punchline:\n",
      "```python\n",
      "def check_punchline(joke: str) -> bool:\n",
      "    return \"?\" in joke or \"!\" in joke  # Simple check for punchline\n",
      "```\n",
      "\n",
      "#### 5. Define the Workflow\n",
      "Set up the workflow using LangGraph:\n",
      "```python\n",
      "graph_builder = StateGraph(State)\n",
      "\n",
      "# Add nodes for generating jokes and checking punchlines\n",
      "graph_builder.add_node(\"generate_joke\", generate_joke)\n",
      "graph_builder.add_node(\"check_punchline\", check_punchline)\n",
      "\n",
      "# Define edges\n",
      "graph_builder.add_edge(START, \"generate_joke\")\n",
      "graph_builder.add_conditional_edges(\"generate_joke\", check_punchline, {\n",
      "    \"True\": END,  # If punchline exists, end the workflow\n",
      "    \"False\": \"generate_joke\"  # If no punchline, generate a new joke\n",
      "})\n",
      "\n",
      "# Compile the graph\n",
      "joke_graph = graph_builder.compile()\n",
      "```\n",
      "\n",
      "#### 6. Save Jokes\n",
      "Implement a function to save jokes to a file or database:\n",
      "```python\n",
      "def save_joke(joke: str):\n",
      "    with open(\"jokes.txt\", \"a\") as f:\n",
      "        f.write(joke + \"\\n\")\n",
      "```\n",
      "\n",
      "#### 7. Main Execution Loop\n",
      "Create a command-line interface to interact with the user:\n",
      "```python\n",
      "if __name__ == \"__main__\":\n",
      "    topic = input(\"Enter a topic for a joke: \")\n",
      "    state = {\"topic\": topic, \"joke\": \"\", \"punchline_check\": False}\n",
      "    \n",
      "    # Run the graph\n",
      "    result = joke_graph.invoke(state)\n",
      "    \n",
      "    # Save the joke\n",
      "    save_joke(result[\"joke\"])\n",
      "    \n",
      "    print(\"Joke saved:\", result[\"joke\"])\n",
      "```\n",
      "\n",
      "### Conclusion\n",
      "This application will allow users to input a topic, generate a joke, check for a punchline, and save the joke. The use of LangGraph's state management and workflow capabilities will ensure that the application is robust and easy to extend in the future. \n",
      "\n",
      "### Next Steps\n",
      "- Test the application to ensure it works as expected.\n",
      "- Consider adding more features, such as retrieving jokes from a database or providing a user interface for joke browsing.\n",
      "\n",
      "\n",
      "\n",
      "Namespace '()'\n",
      "Update from node 'researcher_agent'\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Name: researcher_agent\n",
      "\n",
      "Based on the information gathered from the LangGraph documentation, here is a structured plan to create a LangGraph application that generates jokes based on user input, checks for punchlines, and saves the jokes.\n",
      "\n",
      "### Application Overview\n",
      "The application will:\n",
      "1. Accept a topic from the user via a command-line interface.\n",
      "2. Generate a joke using OpenAI's GPT model.\n",
      "3. Check if the joke has a punchline.\n",
      "4. If the punchline is missing, generate a new joke.\n",
      "5. Save the jokes for future reference.\n",
      "\n",
      "### Implementation Steps\n",
      "\n",
      "#### 1. Setup Environment\n",
      "Make sure to install the necessary packages:\n",
      "```bash\n",
      "pip install langgraph langchain-openai\n",
      "```\n",
      "\n",
      "#### 2. Define the State\n",
      "Define the state schema that will hold the necessary information:\n",
      "```python\n",
      "from typing import TypedDict, Annotated\n",
      "from langgraph.graph import StateGraph, MessagesState\n",
      "\n",
      "class State(TypedDict):\n",
      "    topic: str\n",
      "    joke: str\n",
      "    punchline_check: bool\n",
      "```\n",
      "\n",
      "#### 3. Define the Joke Generation Function\n",
      "Create a function that generates a joke based on the provided topic:\n",
      "```python\n",
      "from langchain_openai import ChatOpenAI\n",
      "\n",
      "llm = ChatOpenAI(model=\"gpt-4\", temperature=0)\n",
      "\n",
      "def generate_joke(state: State) -> str:\n",
      "    prompt = f\"Tell me a joke about {state['topic']}.\"\n",
      "    response = llm.invoke(prompt)\n",
      "    return response.content\n",
      "```\n",
      "\n",
      "#### 4. Define the Punchline Check Function\n",
      "Create a function to check if the joke has a punchline:\n",
      "```python\n",
      "def check_punchline(joke: str) -> bool:\n",
      "    return \"?\" in joke or \"!\" in joke  # Simple check for punchline\n",
      "```\n",
      "\n",
      "#### 5. Define the Workflow\n",
      "Set up the workflow using LangGraph:\n",
      "```python\n",
      "graph_builder = StateGraph(State)\n",
      "\n",
      "# Add nodes for generating jokes and checking punchlines\n",
      "graph_builder.add_node(\"generate_joke\", generate_joke)\n",
      "graph_builder.add_node(\"check_punchline\", check_punchline)\n",
      "\n",
      "# Define edges\n",
      "graph_builder.add_edge(START, \"generate_joke\")\n",
      "graph_builder.add_conditional_edges(\"generate_joke\", check_punchline, {\n",
      "    \"True\": END,  # If punchline exists, end the workflow\n",
      "    \"False\": \"generate_joke\"  # If no punchline, generate a new joke\n",
      "})\n",
      "\n",
      "# Compile the graph\n",
      "joke_graph = graph_builder.compile()\n",
      "```\n",
      "\n",
      "#### 6. Save Jokes\n",
      "Implement a function to save jokes to a file or database:\n",
      "```python\n",
      "def save_joke(joke: str):\n",
      "    with open(\"jokes.txt\", \"a\") as f:\n",
      "        f.write(joke + \"\\n\")\n",
      "```\n",
      "\n",
      "#### 7. Main Execution Loop\n",
      "Create a command-line interface to interact with the user:\n",
      "```python\n",
      "if __name__ == \"__main__\":\n",
      "    topic = input(\"Enter a topic for a joke: \")\n",
      "    state = {\"topic\": topic, \"joke\": \"\", \"punchline_check\": False}\n",
      "    \n",
      "    # Run the graph\n",
      "    result = joke_graph.invoke(state)\n",
      "    \n",
      "    # Save the joke\n",
      "    save_joke(result[\"joke\"])\n",
      "    \n",
      "    print(\"Joke saved:\", result[\"joke\"])\n",
      "```\n",
      "\n",
      "### Conclusion\n",
      "This application will allow users to input a topic, generate a joke, check for a punchline, and save the joke. The use of LangGraph's state management and workflow capabilities will ensure that the application is robust and easy to extend in the future. \n",
      "\n",
      "### Next Steps\n",
      "- Test the application to ensure it works as expected.\n",
      "- Consider adding more features, such as retrieving jokes from a database or providing a user interface for joke browsing.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "===\n",
      "\n"
     ]
    }
   ],
   "source": [
    "request = \"1. Topic will be provided via command-line interface. 2. Use OpenAI's GPT. 3. Evaluate joke quality. 4. Generate new joke if punchline is missing. 5. Save the jokes\"\n",
    "print_stream(\n",
    "    app.stream(\n",
    "        {\"messages\": [{\"role\": \"user\", \"content\": request}]}, config, subgraphs=True\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Traces \n",
    "\n",
    "Example planning trace - \n",
    "\n",
    "https://smith.langchain.com/public/7a428dfc-b8b8-4e28-bc05-9508a848082c/r\n",
    "\n",
    "Example researcher trace - \n",
    "\n",
    "https://smith.langchain.com/public/1c9d0e28-56c4-4241-922b-afd614464edd/r\n",
    "\n",
    "### Test Code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.graph import END, START, StateGraph\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "\n",
    "# Define state schema for our joke generator\n",
    "class State(TypedDict):\n",
    "    topic: str  # The topic provided by the user\n",
    "    joke: str  # The generated joke\n",
    "    has_punchline: bool  # Whether the joke has a punchline\n",
    "\n",
    "\n",
    "# Create the nodes for our graph\n",
    "def generate_joke(state: State):\n",
    "    \"\"\"Generate a joke with a punchline based on the provided topic.\"\"\"\n",
    "    # Initialize the LLM\n",
    "    llm = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "\n",
    "    # Create a prompt that explicitly asks for a setup followed by a punchline\n",
    "    prompt = f\"Create a joke about {state['topic']}. Format it with a setup followed by 'punchline:' and then the actual punchline.\"\n",
    "\n",
    "    # Generate the joke\n",
    "    response = llm.invoke([HumanMessage(content=prompt)])\n",
    "\n",
    "    # Return the joke\n",
    "    return {\"joke\": response.content}\n",
    "\n",
    "\n",
    "def check_punchline(state: State):\n",
    "    \"\"\"Check if the joke contains the word 'punchline'.\"\"\"\n",
    "    # Check if the joke contains the word 'punchline'\n",
    "    if \"punchline\" in state[\"joke\"].lower():\n",
    "        return {\"has_punchline\": True}\n",
    "    else:\n",
    "        return {\"has_punchline\": False}\n",
    "\n",
    "\n",
    "# Define conditional edge decision function\n",
    "def decide_next_step(state: State):\n",
    "    \"\"\"Decide whether to end the process.\"\"\"\n",
    "    if state[\"has_punchline\"]:\n",
    "        return \"end\"\n",
    "    else:\n",
    "        # If we were to handle cases without punchlines, we'd route to a fix node\n",
    "        # But per requirements, we'll just end without feedback\n",
    "        return \"end\"\n",
    "\n",
    "\n",
    "# Build the graph\n",
    "def build_graph():\n",
    "    # Create a graph with the defined state schema\n",
    "    graph = StateGraph(State)\n",
    "\n",
    "    # Add nodes\n",
    "    graph.add_node(\"generate_joke\", generate_joke)\n",
    "    graph.add_node(\"check_punchline\", check_punchline)\n",
    "\n",
    "    # Add edges\n",
    "    graph.add_edge(START, \"generate_joke\")\n",
    "    graph.add_edge(\"generate_joke\", \"check_punchline\")\n",
    "    graph.add_conditional_edges(\"check_punchline\", decide_next_step, {\"end\": END})\n",
    "\n",
    "    # Compile the graph\n",
    "    return graph.compile()\n",
    "\n",
    "\n",
    "joke_graph = build_graph()\n",
    "\n",
    "# Invoke the graph with the provided topic\n",
    "result = joke_graph.invoke({\"topic\": \"cats\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'topic': 'cats',\n",
       " 'joke': 'Why are cats bad storytellers?\\n\\nPunchline: Because they always paws in the middle of the sentence.',\n",
       " 'has_punchline': True}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
