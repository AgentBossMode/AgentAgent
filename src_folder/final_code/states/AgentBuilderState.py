from typing import List, Literal
from pydantic import Field
from final_code.states.ReqAnalysis import ReqAnalysis, DryRuns
from final_code.states.DryRunState import UseCaseAnalysis
from final_code.states.BaseCopilotRenderingState import BaseCopilotRenderingState
from final_code.states.ToolOptions import ToolOptions
from final_code.pydantic_models.UtGen import UtGeneration
from final_code.pydantic_models.PytestReport import TestResult
from final_code.pydantic_models.Questions import Questions


class AgentBuilderState(BaseCopilotRenderingState):
    """
    Represents the state of the main agent building graph.
    Inherits from MessagesState to include a list of messages.
    """
    req_analysis: ReqAnalysis= Field(description="The requirement analysis generated by the model.")
    dry_runs: DryRuns = Field(default=None, description="The dry run information for the agent")
    react_flow_created: bool = Field(default=False, description="Flag to indicate if the ReactFlow graph has been created")
    justification: str = Field(description="Justification for the agent_architecture")
    use_cases: List[UseCaseAnalysis] = Field(default_factory=list,description="List of use cases with their names, descriptions, and dry runs.")
    env_variables: List[str] = Field(description="List if all environment variables required to run the python code")
    tool_set: set = Field(description="The composio tool slugs")
    tool_options: ToolOptions = Field(description="The list of tools provided by the user")
    questions: Questions = Field(description="List of questions to ask the user, explain why you are asking this question")
    answers: dict = Field(description="answers to the questions")
    # eval pipeline
    packages: list[str]
    pytest_results: str = Field(description="The pytest results")
    issue_type: Literal["syntax_error", "runtime_error", "assertion_fail"] = Field(description="identify the type of issue")
    file_that_needs_fixes: Literal["python_code", "mock_tools_code", "pytest_code"] = Field(description="identify the file to fix")
    fix_needed: str = Field(description="detailed explanation of fixes needed, in a diff format")
    attempts: int = Field(default=5, description="Number of attempts made to fix the issue")
    attempt_num: int = Field(default=1, description="Current attempt number")
    utGeneration: UtGeneration = Field(description="The unit test generation object containing final_response_uts and trajectory_uts")
    pytest_report: dict = Field(description="The pytest json report")
    syntax_issues: List[TestResult] = Field(default_factory=list, description="List of syntax issues found in the code")
    assertion_failures: List[TestResult] = Field(default_factory=list, description="List of assertion failures found in the code")
    exception_caught: str = Field(default="", description="Log of any exceptions or failures that occurred during node execution")