{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "! pip install langchain-mcp-adapters langgraph \"langchain[anthropic]\" \"langchain[openai]\" langgraph-swarm httpx markdownify"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Swarm Researcher Example "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prompts import planner_prompt, researcher_prompt\n",
    "from utils import fetch_doc, print_stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "from langchain_openai import AzureChatOpenAI, ChatOpenAI\n",
    "\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langgraph_swarm import create_handoff_tool, create_swarm\n",
    "from langchain_deepseek import ChatDeepSeek\n",
    "\n",
    "model = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    ")\n",
    "\n",
    "# Handoff tools\n",
    "transfer_to_planner_agent = create_handoff_tool(\n",
    "    agent_name=\"planner_agent\",\n",
    "    description=\"Transfer the user to the planner_agent for clarifying questions related to the user's request.\",\n",
    ")\n",
    "transfer_to_researcher_agent = create_handoff_tool(\n",
    "    agent_name=\"researcher_agent\",\n",
    "    description=\"Transfer the user to the researcher_agent to perform research and implement the solution to the user's request.\",\n",
    ")\n",
    "\n",
    "# LLMS.txt\n",
    "llms_txt = \"LangGraph:https://langchain-ai.github.io/langgraph/llms.txt\"\n",
    "num_urls = 3\n",
    "planner_prompt_formatted = planner_prompt.format(llms_txt=llms_txt, num_urls=num_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"I am based on OpenAI's GPT-3 model, which is designed for natural language understanding and generation. If you have any questions or need assistance, feel free to ask!\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 37, 'prompt_tokens': 11, 'total_tokens': 48, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_b376dfbbd5', 'id': 'chatcmpl-BHFupPZcQAOOT1OaxvuOMCHUfaWiS', 'finish_reason': 'stop', 'logprobs': None}, id='run-cdd59881-9fd1-4cfe-a295-aaab2973dfc6-0', usage_metadata={'input_tokens': 11, 'output_tokens': 37, 'total_tokens': 48, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.invoke(\"what model are you\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='' additional_kwargs={'tool_calls': [{'id': 'call_0_3fe25f6e-3cdc-4001-ab3c-ca02f49e94f1', 'function': {'arguments': '{\"a\":2,\"b\":4}', 'name': 'multiply_numbers'}, 'type': 'function', 'index': 0}], 'refusal': None} response_metadata={'token_usage': {'completion_tokens': 23, 'prompt_tokens': 146, 'total_tokens': 169, 'completion_tokens_details': None, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}, 'prompt_cache_hit_tokens': 0, 'prompt_cache_miss_tokens': 146}, 'model_name': 'deepseek-chat', 'system_fingerprint': 'fp_3d5141a69a_prod0225', 'id': 'e389b2a9-ca11-4bac-b9d0-495e2f0272d1', 'finish_reason': 'tool_calls', 'logprobs': None} id='run-e19cf3ff-bb97-43d0-8183-91a19ce2e469-0' tool_calls=[{'name': 'multiply_numbers', 'args': {'a': 2, 'b': 4}, 'id': 'call_0_3fe25f6e-3cdc-4001-ab3c-ca02f49e94f1', 'type': 'tool_call'}] usage_metadata={'input_tokens': 146, 'output_tokens': 23, 'total_tokens': 169, 'input_token_details': {'cache_read': 0}, 'output_token_details': {}}\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.tools import tool\n",
    "\n",
    "@tool\n",
    "def multiply_numbers(a: int, b: int) -> int:\n",
    "    \"\"\"Multiply two numbers.\"\"\"\n",
    "    return a * b\n",
    "    \n",
    "model_with_tool = model.bind_tools([multiply_numbers])\n",
    "\n",
    "result = model_with_tool.invoke(\"multiply 2 and 4. Do not try to answer on your own via reasoning, try using tools at your disposal such as multiply_numbers\")\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.tool_calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace '('planner_agent:71fa9f4e-4688-35be-22b0-11f34ea5e5d6',)'\n",
      "Update from node 'agent'\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Name: planner_agent\n",
      "Tool Calls:\n",
      "  fetch_doc (call_GI0SAM3hZivywouyKEJPfmOp)\n",
      " Call ID: call_GI0SAM3hZivywouyKEJPfmOp\n",
      "  Args:\n",
      "    url: https://langchain-ai.github.io/langgraph/llms.txt\n",
      "\n",
      "\n",
      "\n",
      "Namespace '('planner_agent:71fa9f4e-4688-35be-22b0-11f34ea5e5d6',)'\n",
      "Update from node 'tools'\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: fetch_doc\n",
      "\n",
      "# LangGraph\n",
      "## Tutorials\n",
      "[Learn the basics](https://langchain-ai.github.io/langgraph/tutorials/introduction/): LLM should read this page when needing to build a LangGraph chatbot or when learning about chat agents with memory, human-in-the-loop functionality, and state management. This page provides a comprehensive LangGraph quickstart tutorial covering building a support chatbot with web search capability, conversation memory, human review routing, custom state management, and time travel functionality to explore alternative conversation paths.\n",
      "[Local Deploy](https://langchain-ai.github.io/langgraph/tutorials/langgraph-platform/local-server/): LLM should read this page when setting up a LangGraph app locally using `langgraph dev` and troubleshooting LangGraph server deployment. This page contains a quickstart guide for launching a LangGraph server locally, including installation steps, app creation from templates, environment setup, API testing with Python/JS SDKs, and links to deployment options and further documentation.\n",
      "[Workflows and Agents](https://langchain-ai.github.io/langgraph/tutorials/workflows/): LLM should read this page when implementing agent systems, designing workflow architectures, or troubleshooting LLM orchestration strategies. The page covers patterns for LLM system design, comparing workflows (predefined paths) vs agents (dynamic control), with implementations of prompt chaining, parallelization, routing, orchestrator-worker, evaluator-optimizer, and agent patterns using both graph and functional APIs in LangGraph.\n",
      "## Concepts\n",
      "[Concepts](https://langchain-ai.github.io/langgraph/concepts/): LLM should read this page when needing to understand LangGraph's key concepts or when planning to deploy LangGraph applications. Comprehensive guide covering LangGraph fundamentals (graph primitives, agents, multi-agent systems, breakpoints, persistence), features (time travel, memory, streaming), and LangGraph Platform deployment options (self-hosted, cloud, enterprise).\n",
      "[Agent architectures](https://langchain-ai.github.io/langgraph/concepts/agentic\\_concepts/): LLM should read this page when designing agent architectures, implementing control flows for LLM applications, or customizing agent behavior patterns. This page covers different LLM agent architectures including routers, tool calling agents (ReAct), structured outputs, memory systems, planning capabilities, and advanced customization options like human-in-the-loop, parallelization, subgraphs, and reflection mechanisms.\n",
      "[Application Structure](https://langchain-ai.github.io/langgraph/concepts/application\\_structure/): LLM should read this page when needing to understand LangGraph application structure, preparing to deploy a LangGraph application, or troubleshooting configuration issues. This page details the structure of LangGraph applications, including required components (graphs, langgraph.json config file, dependency files, optional .env), file organization patterns for Python/JavaScript projects, configuration file format with all supported fields, and how to specify dependencies, graphs, and environment variables.\n",
      "[Assistants](https://langchain-ai.github.io/langgraph/concepts/assistants/): LLM should read this page when looking for information about LangGraph assistants, understanding assistant configuration in LangGraph Platform, or learning about versioning agent configurations. This page explains LangGraph assistants, which allow developers to modify agent configurations (prompts, models, etc.) without changing graph logic, supports versioning for tracking changes, and is available only in LangGraph Platform (not open source).\n",
      "[Authentication & Access Control](https://langchain-ai.github.io/langgraph/concepts/auth/): LLM should read this page when implementing authentication in LangGraph Platform, designing access control for LangGraph applications, or troubleshooting security issues in LangGraph deployments. This page explains LangGraph's authentication and authorization system, covering the difference between authentication and authorization, system architecture, implementing custom auth handlers, common access patterns, and supported resources/actions for access control.\n",
      "[Bring Your Own Cloud (BYOC)](https://langchain-ai.github.io/langgraph/concepts/bring\\_your\\_own\\_cloud/): LLM should read this page when learning about LangGraph Platform deployment options, understanding Bring Your Own Cloud architecture, or managing deployments in AWS. This page explains LangGraph's BYOC deployment model, detailing how it separates control plane (managed by LangChain) from data plane (in customer's AWS account), outlines AWS requirements, infrastructure setup via Terraform, required permissions, and explains the deployment workflow.\n",
      "[Deployment Options](https://langchain-ai.github.io/langgraph/concepts/deployment\\_options/): LLM should read this page when needing information about LangGraph deployment options, comparing different deployment methods, or understanding LangGraph Platform plans. This page outlines four deployment options for LangGraph Platform: Self-Hosted Lite (available for all plans), Self-Hosted Enterprise (Enterprise plan only), Cloud SaaS (Plus and Enterprise plans), and Bring Your Own Cloud (Enterprise plan only, AWS-only).\n",
      "[Double Texting](https://langchain-ai.github.io/langgraph/concepts/double\\_texting/): LLM should read this page when handling concurrent user interactions in LangGraph Platform, implementing double-texting safeguards, or designing stateful conversation systems. This page explains four approaches to handling \"double texting\" in LangGraph (when users send a second message before the first completes): Reject, Enqueue, Interrupt, and Rollback, noting these features are currently only available in LangGraph Platform.\n",
      "[Durable Execution](https://langchain-ai.github.io/langgraph/concepts/durable\\_execution/): LLM should read this page when needing to understand durable execution in LangGraph, implementing workflow persistence, or troubleshooting workflow resumption. This page explains durable execution in LangGraph: how workflows save progress to resume later, requirements (checkpointers and thread IDs), determinism guidelines for consistent replay, using tasks to encapsulate non-deterministic operations, and approaches for pausing/resuming workflows.\n",
      "[FAQ](https://langchain-ai.github.io/langgraph/concepts/faq/): LLM should read this page when needing to understand differences between LangGraph and LangChain, exploring deployment options for LangGraph Platform, or determining compatibility with various LLMs. FAQ covering LangGraph basics, comparisons with other frameworks, deployment options (free self-hosted, Cloud SaaS, BYOC, Enterprise), compatibility with different LLMs including OSS models, and feature differences between open-source LangGraph and proprietary LangGraph Platform.\n",
      "[Functional API](https://langchain-ai.github.io/langgraph/concepts/functional\\_api/): LLM should read this page when implementing workflows with persistent state, adding human-in-the-loop features, or converting existing code to use LangGraph. The page documents LangGraph's Functional API, which allows adding persistence, memory, and human-in-the-loop capabilities with minimal code changes using @entrypoint and @task decorators, handling serialization requirements, state management, and common patterns for parallel execution and error handling.\n",
      "[Why LangGraph?](https://langchain-ai.github.io/langgraph/concepts/high\\_level/): LLM should read this page when understanding LangGraph's core capabilities, exploring LLM application infrastructure, or evaluating agent/workflow persistence options. LangGraph provides infrastructure for LLM applications with three key benefits: persistence for memory and human-in-the-loop capabilities, streaming of workflow events and LLM outputs, and tools for debugging and deployment via LangGraph Platform.\n",
      "[Human-in-the-loop](https://langchain-ai.github.io/langgraph/concepts/human\\_in\\_the\\_loop/): LLM should read this page when implementing human-in-the-loop workflows in LangGraph, designing approval systems with LLMs, or creating interactive multi-turn conversation agents. This page explains human-in-the-loop patterns in LangGraph using the interrupt function, showing how to pause graph execution for human review/input and resume with Command. Includes design patterns for approval workflows, state editing, tool call reviews, and multi-turn conversations, with code examples and warnings about execution flow and common pitfalls.\n",
      "[LangGraph CLI](https://langchain-ai.github.io/langgraph/concepts/langgraph\\_cli/): LLM should read this page when looking for information about LangGraph CLI installation or when needing to deploy a LangGraph API server locally. The page covers LangGraph CLI installation methods (Homebrew, pip), key commands (build, dev, up, dockerfile), and features like hot reloading, debugger support, and database management for running LangGraph servers.\n",
      "[Cloud SaaS](https://langchain-ai.github.io/langgraph/concepts/langgraph\\_cloud/): LLM should read this page when learning about LangGraph's Cloud SaaS offering, understanding deployment options for LangGraph Servers, or planning autoscaling infrastructure for LangGraph applications. This page describes LangGraph Cloud SaaS, a managed deployment service for LangGraph Servers with details on deployment types (Development/Production), revisions, persistence, autoscaling capabilities (up to 10 containers), LangSmith integration, IP whitelisting, and automatic deletion policies after 28 days of non-use.\n",
      "[LangGraph Platform](https://langchain-ai.github.io/langgraph/concepts/langgraph\\_platform/): LLM should read this page when seeking information about LangGraph Platform's components or evaluating production deployment options for agentic applications. The page details the LangGraph Platform, a commercial solution for deploying agentic applications, including its components (Server, Studio, CLI, SDK, Remote Graph) and key benefits like streaming support, background runs, long run handling, burstiness management, and human-in-the-loop capabilities.\n",
      "[LangGraph Server](https://langchain-ai.github.io/langgraph/concepts/langgraph\\_server/): LLM should read this page when developing applications with LangGraph Server, deploying agent-based applications, or integrating persistent state management in agent workflows. LangGraph Server provides an API for creating and managing agent applications with key features like streaming endpoints, background runs, task queues, persistence, webhooks, cron jobs, and monitoring capabilities through a structured system of assistants, threads, runs, and stores.\n",
      "[LangGraph Studio](https://langchain-ai.github.io/langgraph/concepts/langgraph\\_studio/): LLM should read this page when looking for information about LangGraph Studio features, needing to troubleshoot LangGraph Studio issues, or learning how to connect a LangGraph application to the Studio. LangGraph Studio is a specialized agent IDE for visualizing, interacting with, and debugging LLM applications, offering features such as graph visualization, state editing, assistant management, and integration with LangSmith, with instructions for connecting via deployed applications or local development servers, plus troubleshooting FAQs.\n",
      "[LangGraph Glossary](https://langchain-ai.github.io/langgraph/concepts/low\\_level/): LLM should read this page when needing to understand LangGraph terminology, implementing agent workflows as graphs, or developing modular multi-step AI systems. The page covers core LangGraph concepts including StateGraph, nodes, edges, state management, messaging, persistence, configuration, human-in-the-loop features, subgraphs, and visualization capabilities.\n",
      "[Memory](https://langchain-ai.github.io/langgraph/concepts/memory/): LLM should read this page when implementing memory systems for AI agents, managing conversation context across sessions, or designing systems that require both short-term and long-term information retention. This page explains memory systems in LangGraph, covering short-term (thread-scoped) memory for managing conversation history and long-term memory across threads, with techniques for handling long conversations, summarizing past interactions, and organizing persistent memories in namespaces.\n",
      "[Multi-agent Systems](https://langchain-ai.github.io/langgraph/concepts/multi\\_agent/): LLM should read this page when implementing multi-agent systems, troubleshooting complex agent architectures, or designing agent communication patterns. Multi-agent systems organize LLMs into modular architectures (network, supervisor, hierarchical, custom) with different communication patterns, using Command objects for handoffs between agents, and supporting various state management approaches.\n",
      "[Persistence](https://langchain-ai.github.io/langgraph/concepts/persistence/): LLM should read this page when needing to understand LangGraph persistence mechanisms, implementing stateful workflows, or managing conversation history across interactions. This page covers LangGraph's persistence features including checkpointers, threads, state snapshots, replay functionality, forking state, cross-thread memory via InMemoryStore, and semantic search capabilities for stored memories.\n",
      "[LangGraph Platform Plans](https://langchain-ai.github.io/langgraph/concepts/plans/): LLM should read this page when determining LangGraph Platform pricing tiers, comparing deployment options, or researching features available across different plans. This page outlines LangGraph Platform plans (Developer, Plus, Enterprise), detailing deployment options, usage limitations, feature availability, and pricing structure for agentic application deployment.\n",
      "[LangGraph Platform Architecture](https://langchain-ai.github.io/langgraph/concepts/platform\\_architecture/): LLM should read this page when needing to understand LangGraph Platform's technical architecture or troubleshooting deployment issues. The page details how LangGraph Platform uses Postgres for persistent storage of user/run data and Redis for worker communication (run cancellation, output streaming) and ephemeral metadata storage (retry attempts).\n",
      "[LangGraph's Runtime (Pregel)](https://langchain-ai.github.io/langgraph/concepts/pregel/): LLM should read this page when learning about LangGraph's runtime, implementing applications with Pregel directly, or understanding how LangGraph executes graph applications. Explains LangGraph's Pregel runtime which manages graph application execution through a three-phase process (Plan, Execution, Update), describes different channel types (LastValue, Topic, Context, BinaryOperatorAggregate), provides direct implementation examples, and contrasts the StateGraph API with the Functional API.\n",
      "[LangGraph Platform: Scalability & Resilience](https://langchain-ai.github.io/langgraph/concepts/scalability\\_and\\_resilience/): LLM should read this page when needing to understand LangGraph Platform's scaling capabilities, designing high-availability LangGraph deployments, or troubleshooting resilience issues. This page details LangGraph Platform's horizontal scaling features including stateless server instances, queue worker scaling, resilience mechanisms for handling crashes, and database failover strategies in Postgres and Redis.\n",
      "[LangGraph SDK](https://langchain-ai.github.io/langgraph/concepts/sdk/): LLM should read this page when looking for installation instructions for LangGraph SDK, needing to choose between sync and async Python clients, or requiring SDK API references. The page covers LangGraph SDK installation for Python and JS, provides API reference links, explains the difference between synchronous and asynchronous Python clients, and includes code examples for both client types.\n",
      "[Self-Hosted](https://langchain-ai.github.io/langgraph/concepts/self\\_hosted/): LLM should read this page when looking for LangGraph deployment options, understanding self-hosted versions, or seeking requirements for self-hosting LangGraph. This page details two self-hosted deployment options for LangGraph Platform: Self-Hosted Lite (limited to 1M nodes/year) and Self-Hosted Enterprise (full version requiring license). Includes requirements, deployment process using Redis/Postgres, Docker, and optional Kubernetes deployment via Helm chart.\n",
      "[Streaming](https://langchain-ai.github.io/langgraph/concepts/streaming/): LLM should read this page when implementing streaming features in LangGraph applications, understanding different streaming modes, or building responsive LLM applications. This page explains streaming in LangGraph, covering the main types (workflow progress, LLM tokens, custom updates) and streaming modes (values, updates, custom, messages, debug, events), with details on how to use multiple modes simultaneously and differences between LangGraph library and Platform implementations.\n",
      "[Template Applications](https://langchain-ai.github.io/langgraph/concepts/template\\_applications/): LLM should read this page when looking for LangGraph template applications, setting up a new LangGraph project, or finding reference implementations for agentic workflows. This page presents LangGraph template applications with installation requirements, available templates (including ReAct Agent, Memory Agent, Retrieval Agent, etc.), instructions for creating new apps using the CLI, deployment options, and links to further learning resources.\n",
      "[Time Travel ⏱️](https://langchain-ai.github.io/langgraph/concepts/time-travel/): LLM should read this page when debugging LLM-based agent behavior, analyzing decision-making paths, or exploring alternative execution branches in LangGraph. This page explains LangGraph's Time Travel debugging features: Replaying (reproducing past actions up to specific checkpoints) and Forking (creating alternative execution paths from specific points), with code examples for retrieving checkpoints, configuring replay, and creating forked states.\n",
      "## How Tos\n",
      "[How-to Guides](https://langchain-ai.github.io/langgraph/how-tos/): LLM should read this page when looking for specific implementation techniques in LangGraph or when trying to deploy LangGraph applications to production environments. This page contains an extensive collection of how-to guides for LangGraph, covering graph fundamentals, persistence, memory management, human-in-the-loop features, tool calling, multi-agent systems, streaming, and deployment options through LangGraph Platform.\n",
      "[How to implement handoffs between agents](https://langchain-ai.github.io/langgraph/how-tos/agent-handoffs/): LLM should read this page when implementing multi-agent systems that require agent coordination, when building systems with specialized agents that need to work together, or when needing to implement handoffs between agents. This page explains how to implement handoffs between agents in LangGraph using Command objects, both directly from agent nodes and through specialized handoff tools, with code examples for creating multi-agent systems.\n",
      "[How to run a graph asynchronously](https://langchain-ai.github.io/langgraph/how-tos/async/): LLM should read this page when needing to implement asynchronous graph execution in LangGraph or when optimizing IO-bound LLM applications. This page explains how to convert synchronous graphs to asynchronous in LangGraph, including updating node definitions with async/await, using StateGraph with TypedDict, implementing conditional edges, and streaming results.\n",
      "[How to integrate LangGraph with AutoGen, CrewAI, and other frameworks](https://langchain-ai.github.io/langgraph/how-tos/autogen-integration/): LLM should read this page when integrating LangGraph with other agent frameworks, building multi-agent systems, or adding persistence features to agents. The page demonstrates how to combine LangGraph with AutoGen by calling AutoGen agents inside LangGraph nodes, showing code examples for setting up the integration with memory and conversation persistence.\n",
      "[How to integrate LangGraph (functional API) with AutoGen, CrewAI, and other frameworks](https://langchain-ai.github.io/langgraph/how-tos/autogen-integration-functional/): LLM should read this page when integrating LangGraph with other agent frameworks, building multi-agent systems with different frameworks, or adding LangGraph features to existing agent systems. This page demonstrates how to integrate LangGraph's functional API with AutoGen, including code examples for creating a workflow that calls AutoGen agents, leveraging LangGraph's memory and persistence features.\n",
      "[How to create branches for parallel node execution](https://langchain-ai.github.io/langgraph/how-tos/branching/): LLM should read this page when needing to implement parallel node execution in LangGraph, optimizing graph performance, or handling conditional branching in workflows. This page explains how to create branches for parallel execution in LangGraph using fan-out/fan-in mechanisms, reducer functions for state accumulation, handling exceptions during parallel execution, and implementing conditional branching logic between nodes.\n",
      "[How to combine control flow and state updates with Command](https://langchain-ai.github.io/langgraph/how-tos/command): LLM should read this page when learning how to combine control flow with state updates in LangGraph, understanding Command objects, or navigating between parent graphs and subgraphs. This page explains how to use Command objects to simultaneously update state and control flow between nodes, demonstrates using Command.PARENT to navigate from subgraphs to parent graphs, and includes examples of implementing reducers for state updates across graph hierarchies.\n",
      "[How to add runtime configuration to your graph](https://langchain-ai.github.io/langgraph/how-tos/configuration/): LLM should read this page when implementing runtime configuration for LangGraph, adding model selection options to agents, or enabling dynamic system messages. This page demonstrates how to configure LangGraph at runtime, including selecting different LLMs dynamically and adding custom configuration options like system messages through the configurable dictionary.\n",
      "[How to use the pre-built ReAct agent](https://langchain-ai.github.io/langgraph/how-tos/create-react-agent/): LLM should read this page when implementing a ReAct agent, needing pre-built agent solutions, or learning how to integrate tools with LLM agents. This page covers how to use the pre-built ReAct agent in LangGraph, including setup instructions, creating a weather checking tool, implementing the agent architecture, and examples of running the agent with and without tool calls.\n",
      "[How to add human-in-the-loop processes to the prebuilt ReAct agent](https://langchain-ai.github.io/langgraph/how-tos/create-react-agent-hitl/): LLM should read this page when implementing human-in-the-loop processes for ReAct agents, debugging tool calls, or learning about interrupts in LangGraph. This guide demonstrates how to add human-in-the-loop functionality to prebuilt ReAct agents using interrupt\\_before=[\"tools\"], working with MemorySaver checkpoints, and showing how to approve or edit tool calls before they execute.\n",
      "[How to add thread-level memory to a ReAct Agent](https://langchain-ai.github.io/langgraph/how-tos/create-react-agent-memory/): LLM should read this page when adding memory to ReAct agents, implementing thread-level persistence in LangGraph, or building stateful conversational agents. This guide demonstrates how to add memory to a ReAct agent using LangGraph's checkpointer interface, with code examples showing MemorySaver implementation, thread\\_id configuration, and persistent chat context across multiple interactions.\n",
      "[How to return structured output from the prebuilt ReAct agent](https://langchain-ai.github.io/langgraph/how-tos/create-react-agent-structured-output/): LLM should read this page when implementing structured output with ReAct agents, customizing agent response formats, or working with LangGraph agents. This page explains how to return structured output from prebuilt ReAct agents by providing a response\\_format parameter with a Pydantic schema, including examples with weather data and options for customizing the prompt.\n",
      "[How to add a custom system prompt to the prebuilt ReAct agent](https://langchain-ai.github.io/langgraph/how-tos/create-react-agent-system-prompt/): LLM should read this page when learning to customize ReAct agents, needing to add system prompts to agents, or working with LangGraph's prebuilt agents. This tutorial demonstrates how to add a custom system prompt to a prebuilt ReAct agent, with code examples showing model setup, tool creation, and using the prompt parameter in the create\\_react\\_agent function.\n",
      "[How to add cross-thread persistence to your graph](https://langchain-ai.github.io/langgraph/how-tos/cross-thread-persistence): LLM should read this page when needing to implement persistence across multiple threads in LangGraph, when storing user data between conversations, or when implementing shared memory in graph-based LLM applications. This page demonstrates how to use LangGraph's Store API to persist data across threads, including creating an InMemoryStore with embedding search capabilities, passing stores to graph nodes, and accessing user-specific memories in different conversation threads.\n",
      "[How to add cross-thread persistence (functional API)](https://langchain-ai.github.io/langgraph/how-tos/cross-thread-persistence-functional): LLM should read this page when needing to implement cross-thread persistence in LangGraph functional API, storing user data across different conversation threads, or creating shared memory between workflows. This page explains how to add cross-thread persistence to LangGraph using the Store interface, including defining a store, configuring the entrypoint decorator, and implementing a workflow that can store and retrieve user information across different conversation threads.\n",
      "[How to do a Self-hosted deployment of LangGraph](https://langchain-ai.github.io/langgraph/how-tos/deploy-self-hosted/): LLM should read this page when implementing a self-hosted deployment of LangGraph, configuring required environment variables, or building Docker images for LangGraph applications. This page explains how to deploy LangGraph applications using Docker, covering environment requirements (Redis, Postgres), how to build Docker images with the LangGraph CLI, configuration using environment variables, and deployment options using Docker or Docker Compose.\n",
      "[How to disable streaming for models that don't support it](https://langchain-ai.github.io/langgraph/how-tos/disable-streaming/): LLM should read this page when handling models that don't support streaming, implementing LangGraph with non-streaming models, or troubleshooting streaming errors with OpenAI's O1 models. This page explains how to use the disable\\_streaming=True parameter with ChatOpenAI to make non-streaming models work with LangGraph's astream\\_events API, with code examples showing the error case and proper implementation.\n",
      "[How to edit graph state](https://langchain-ai.github.io/langgraph/how-tos/human\\_in\\_the\\_loop/edit-graph-state/): LLM should read this page when needing to implement human intervention in LangGraph workflows, wanting to edit graph state during execution, or implementing breakpoints in agent systems. This page explains how to edit graph state in LangGraph using breakpoints, including implementing human-in-the-loop interactions, setting up interruptions before specific nodes, and updating state during agent execution.\n",
      "[How to Review Tool Calls](https://langchain-ai.github.io/langgraph/how-tos/human\\_in\\_the\\_loop/review-tool-calls/): LLM should read this page when implementing human review of tool calls, creating interactive agent workflows, or building approval systems for AI actions. This page explains how to implement human-in-the-loop review for tool calls in LangGraph, including approving tool calls, modifying tool calls manually, and providing natural language feedback to agents with complete code examples and explanations.\n",
      "[How to view and update past graph state](https://langchain-ai.github.io/langgraph/how-tos/human\\_in\\_the\\_loop/time-travel/): LLM should read this page when needing to access or modify past states in LangGraph, when debugging agent execution, or when implementing user interventions in agent workflows. This page demonstrates how to view and update past graph states in LangGraph using get\\_state and update\\_state methods, with examples of replaying execution from checkpoints and branching workflows.\n",
      "[How to wait for user input using interrupt](https://langchain-ai.github.io/langgraph/how-tos/human\\_in\\_the\\_loop/wait-user-input/): LLM should read this page when implementing wait-for-user functions in LangGraph, implementing human-in-the-loop interactions, or learning how to use the interrupt() function. This page explains how to pause graph execution to collect user input using LangGraph's interrupt() function, with examples of simple feedback collection and more complex agent interactions that ask clarifying questions.\n",
      "[How to define input/output schema for your graph](https://langchain-ai.github.io/langgraph/how-tos/input\\_output\\_schema/): LLM should read this page when needing to define separate input/output schemas for LangGraph, implementing schema-based data filtering, or understanding schema definitions in StateGraph. This page explains how to define distinct input and output schemas for a StateGraph, showing how input schema validates the provided data structure while output schema filters internal data to return only relevant information, with code examples demonstrating implementation.\n",
      "[How to handle large numbers of tools](https://langchain-ai.github.io/langgraph/how-tos/many-tools/): LLM should read this page when handling large tool collections, implementing dynamic tool selection, or creating retrieval-based tool management in LangGraph. This page demonstrates how to manage large numbers of tools by using vector search to dynamically select relevant tools based on user queries, implementing tool selection nodes in LangGraph, and handling tool selection errors with retry mechanisms.\n",
      "[How to create map-reduce branches for parallel execution](https://langchain-ai.github.io/langgraph/how-tos/map-reduce/): LLM should read this page when learning to implement parallel execution in LangGraph, creating map-reduce operations, or handling dynamic task decomposition. This guide explains how to use LangGraph's Send API to create map-reduce workflows, breaking tasks into parallel sub-tasks and recombining results, with examples showing joke generation across multiple subjects.\n",
      "[How to add summary of the conversation history](https://langchain-ai.github.io/langgraph/how-tos/memory/add-summary-conversation-history/): LLM should read this page when implementing conversation summarization, managing context windows, or building chatbots with memory management. This page demonstrates how to add summary functionality to conversation history using LangGraph, including checking conversation length, creating summaries, and removing old messages while maintaining context.\n",
      "[How to delete messages](https://langchain-ai.github.io/langgraph/how-tos/memory/delete-messages): LLM should read this page when attempting to manage message history in LangGraph, needing to delete specific messages from conversational state, or implementing memory management in LLM applications. This page explains how to delete messages from a LangGraph application using RemoveMessage modifiers, covering both manual deletion with message IDs and programmatic deletion within graph logic to maintain conversation history limits.\n",
      "[How to manage conversation history](https://langchain-ai.github.io/langgraph/how-tos/memory/manage-conversation-history/): LLM should read this page when managing conversation history in LangGraph, preventing context window issues, or implementing custom message filtering. This page explains how to manage conversation history in LangGraph to prevent context window overflow by implementing message filtering functions that control which messages are sent to the LLM.\n",
      "[How to add semantic search to your agent's memory](https://langchain-ai.github.io/langgraph/how-tos/memory/semantic-search/): LLM should read this page when implementing semantic search in agent memory, enabling memory-aware AI assistants, or configuring advanced memory retrieval systems. This page demonstrates how to add semantic search to LangGraph agent memory stores, covering basic setup with embeddings, storing memories, searching by semantic similarity, integrating memory in agents and ReAct agents, and advanced usage like multi-vector indexing and selective memory indexing.\n",
      "[How to add multi-turn conversation in a multi-agent application](https://langchain-ai.github.io/langgraph/how-tos/multi-agent-multi-turn-convo/): LLM should read this page when implementing multi-turn conversations between agents, creating interactive agent systems with human input, or learning about langgraph interrupts and agent handoffs. This page demonstrates how to build a multi-agent system with multi-turn conversations, including human-in-the-loop interactions, agent handoffs, and state management using LangGraph, Command objects, and interrupts.\n",
      "[How to add multi-turn conversation in a multi-agent application (functional API)](https://langchain-ai.github.io/langgraph/how-tos/multi-agent-multi-turn-convo-functional/): LLM should read this page when building multi-turn conversational agents, implementing agent-to-agent handoffs, or using interrupts to collect user input in LangGraph. This guide demonstrates how to create a multi-agent system with multi-turn conversations using LangGraph's functional API, featuring agent handoffs, interrupt mechanics for user input, and a complete example of travel and hotel advisor agents that can transfer control between each other.\n",
      "[How to build a multi-agent network](https://langchain-ai.github.io/langgraph/how-tos/multi-agent-network/): LLM should read this page when implementing multi-agent networks, setting up agent communication via handoffs, or building travel assistance agents. This page explains how to create a fully-connected multi-agent network with LangGraph where agents can communicate with each other via handoffs, including custom agent implementation and using prebuilt ReAct agents with tools.\n",
      "[How to build a multi-agent network (functional API)](https://langchain-ai.github.io/langgraph/how-tos/multi-agent-network-functional/): LLM should read this page when building multi-agent systems, implementing agent handoffs between specialists, or creating fully-connected agent networks. This guide demonstrates how to create a multi-agent network using LangGraph's functional API, with tasks for individual agents and entrypoint functions to manage agent handoffs based on tool calls.\n",
      "[How to add node retry policies](https://langchain-ai.github.io/langgraph/how-tos/node-retries/): LLM should read this page when implementing error handling in LangGraph nodes, configuring API retry mechanisms, or troubleshooting node failures in graph workflows. Shows how to add custom retry policies to LangGraph nodes, including specifying which exceptions to retry on, setting max attempts, intervals, backoff factors, and implementing different retry behaviors for different node types.\n",
      "[How to pass config to tools](https://langchain-ai.github.io/langgraph/how-tos/pass-config-to-tools/): LLM should read this page when implementing secure tool configuration in LangChain, passing user-specific parameters to tools, or configuring tools with runtime values. This page explains how to pass configuration to LangChain tools using RunnableConfig, allowing application-controlled values (like user IDs) to be securely passed to tools without LLM control, with examples of implementing tools that access user-specific data.\n",
      "[How to pass private state between nodes](https://langchain-ai.github.io/langgraph/how-tos/pass\\_private\\_state/): LLM should read this page when implementing data sharing between specific nodes in LangGraph, handling private state in graph workflows, or designing multi-node sequential processes with selective data visibility. This page demonstrates how to pass private data between specific nodes in a LangGraph without making it part of the main schema, using typed dictionaries to define both public and private states, and showing a three-node example where private data flows only between the first two nodes.\n",
      "[How to add thread-level persistence to your graph](https://langchain-ai.github.io/langgraph/how-tos/persistence/): LLM should read this page when implementing persistence in LangGraph, needing to preserve context across user interactions, or learning about thread-level state management. This page explains how to add thread-level persistence to LangGraph applications using MemorySaver, including code examples for creating stateful conversations where context is maintained across multiple interactions.\n",
      "[How to add thread-level persistence (functional API)](https://langchain-ai.github.io/langgraph/how-tos/persistence-functional/): LLM should read this page when implementing thread-level persistence in LangGraph, creating conversational agents with memory, or using functional API with state management. This page explains how to add thread-level persistence to LangGraph functional API workflows using checkpointers, including code examples for creating a simple chatbot with memory across conversation turns.\n",
      "[How to use MongoDB checkpointer for persistence](https://langchain-ai.github.io/langgraph/how-tos/persistence\\_mongodb/): LLM should read this page when implementing persistence in LangGraph agents, setting up MongoDB for state checkpointing, or working with MongoDB connections in LangGraph applications. This page explains how to use the MongoDB checkpointer for LangGraph persistence, covering connection methods (direct, client-based, async), basic setup requirements, and practical examples of saving and retrieving agent state between interactions.\n",
      "[How to use Postgres checkpointer for persistence](https://langchain-ai.github.io/langgraph/how-tos/persistence\\_postgres/): LLM should read this page when setting up persistence for LangGraph agents, implementing PostgreSQL as a checkpoint storage backend, or working with either synchronous or asynchronous database connections. This page details how to use PostgreSQL for persisting LangGraph agent state, covering setup and configuration of PostgresSaver and AsyncPostgresSaver with different connection methods (pool, direct connection, connection string).\n",
      "[How to create a custom checkpointer using Redis](https://langchain-ai.github.io/langgraph/how-tos/persistence\\_redis/): LLM should read this page when implementing persistence in LangGraph applications, creating custom checkpoint mechanisms for agents, or working with Redis as a storage backend. This page demonstrates how to create custom checkpointers for LangGraph agents using Redis, including implementations for both synchronous and asynchronous interfaces that save and retrieve agent state.\n",
      "[How to create a ReAct agent from scratch](https://langchain-ai.github.io/langgraph/how-tos/react-agent-from-scratch/): LLM should read this page when needing to create a custom ReAct agent, wanting more control than prebuilt agents, or implementing ReAct from scratch with LangGraph. This guide shows how to build a custom ReAct agent using LangGraph, covering state definition, model/tool setup, node/edge configuration, graph creation, and testing the implementation with a weather query example.\n",
      "[How to create a ReAct agent from scratch (Functional API)](https://langchain-ai.github.io/langgraph/how-tos/react-agent-from-scratch-functional): LLM should read this page when creating a ReAct agent using LangGraph's Functional API, implementing tool-calling workflows, or building conversational agents with thread persistence. This page explains how to build a ReAct agent from scratch using LangGraph's Functional API, including model and tool setup, defining tasks for model/tool calling, creating an entrypoint for orchestration, and adding thread-level persistence for conversational experiences.\n",
      "[How to force tool-calling agent to structure output](https://langchain-ai.github.io/langgraph/how-tos/react-agent-structured-output): LLM should read this page when needing to force tool-calling agents to produce structured output, implementing consistent output formats for downstream software, or choosing between single-LLM vs two-LLM structured output approaches. The page explains two methods for implementing structured output with tool-calling agents: binding output as a tool (single LLM approach) and using two LLMs with structured output conversion, with code examples for both approaches using LangGraph.\n",
      "[How to create and control loops](https://langchain-ai.github.io/langgraph/how-tos/recursion-limit/): LLM should read this page when building loops in computational graphs, needing to implement termination conditions, or handling recursion limits in LangGraph. The page explains how to create graphs with loops using conditional edges for termination, set recursion limits, handle GraphRecursionError, and implement complex loops with branches.\n",
      "[How to review tool calls (Functional API)](https://langchain-ai.github.io/langgraph/how-tos/review-tool-calls-functional/): LLM should read this page when implementing human review of tool calls, creating ReAct agents with Functional API, or adding human-in-the-loop workflows. This page demonstrates how to review tool calls before execution in a ReAct agent using LangGraph's Functional API, including accepting, revising, or generating custom tool messages with the interrupt function.\n",
      "[How to pass custom run ID or set tags and metadata for graph runs in LangSmith](https://langchain-ai.github.io/langgraph/how-tos/run-id-langsmith/): LLM should read this page when needing to customize trace information in LangSmith for LangGraph runs or when debugging graph runs with custom identifiers. The page explains how to pass custom run\\_id, set tags, add metadata, and customize run names for LangGraph traces in LangSmith using RunnableConfig, with examples showing implementation with a ReAct agent.\n",
      "[How to create a sequence of steps](https://langchain-ai.github.io/langgraph/how-tos/sequence/): LLM should read this page when implementing sequential workflows in LangGraph, creating multi-step processes in applications, or learning about state management in graph-based systems. This page explains how to create sequences in LangGraph, covering methods for building sequential graphs using .add\\_node/.add\\_edge or the shorthand .add\\_sequence, defining state with TypedDict, creating nodes as functions that update state, and compiling/invoking graphs with examples.\n",
      "[How to use Pydantic model as graph state](https://langchain-ai.github.io/langgraph/how-tos/state-model): LLM should read this page when implementing Pydantic models for state validation in LangGraph, handling complex state schema definitions, or troubleshooting validation errors in graph nodes. This guide explains how to use Pydantic BaseModel as a state schema in LangGraph for runtime validation, covering basic implementation, limitations, validation behavior across multiple nodes, serialization patterns, type coercion, and working with message models.\n",
      "[How to update graph state from nodes](https://langchain-ai.github.io/langgraph/how-tos/state-reducers/): LLM should read this page when needing to update state in LangGraph, designing graphs with nodes that modify state, or implementing reducers for state management. This page explains how to define state schemas in LangGraph using TypedDict, how nodes can update state, and how to use reducers to control state updates, with specific examples using message handling.\n",
      "[How to stream](https://langchain-ai.github.io/langgraph/how-tos/streaming/): LLM should read this page when needing to implement streaming in LangGraph applications, understanding different streaming modes, or troubleshooting LLM response delivery. This page explains how to stream LLM outputs using LangGraph, covering different streaming modes (values, updates, custom, messages, debug), with code examples for each mode and how to combine multiple streaming modes.\n",
      "[How to stream data from within a tool](https://langchain-ai.github.io/langgraph/how-tos/streaming-events-from-within-tools/): LLM should read this page when implementing streaming functionality in tools, integrating LLM outputs with custom data streams, or developing LangGraph applications with real-time feedback. This page explains how to stream data from within tools using LangGraph, covering custom data streaming with stream\\_mode=\"custom\", LLM token streaming with stream\\_mode=\"messages\", and implementation approaches both with and without LangChain.\n",
      "[How to stream LLM tokens from specific nodes](https://langchain-ai.github.io/langgraph/how-tos/streaming-specific-nodes/): LLM should read this page when needing to filter token streaming from specific nodes in LangGraph, implementing selective streaming in multi-node workflows, or controlling which node outputs are displayed. Guide explains how to stream LLM tokens from specific nodes using stream\\_mode=\"messages\" and filtering by the langgraph\\_node metadata field, with complete code examples for implementing this in StateGraph applications.\n",
      "[How to stream from subgraphs](https://langchain-ai.github.io/langgraph/how-tos/streaming-subgraphs/): LLM should read this page when needing to stream outputs from subgraphs in LangGraph, implementing nested graph streaming, or debugging hierarchical graph execution. This page explains how to stream outputs from subgraphs in LangGraph by using the subgraphs=True parameter in the parent graph's stream() method, with a complete code example showing the difference between regular streaming and subgraph streaming.\n",
      "[How to stream LLM tokens from your graph](https://langchain-ai.github.io/langgraph/how-tos/streaming-tokens): LLM should read this page when needing to stream LLM tokens from a LangGraph application, implementing custom token streaming, or filtering streamed outputs. This page explains how to stream individual LLM tokens from LangGraph nodes using graph.stream() with different stream\\_mode options, including examples with and without LangChain, async implementations, and how to filter streamed tokens using metadata.\n",
      "[How to use subgraphs](https://langchain-ai.github.io/langgraph/how-tos/subgraph/): LLM should read this page when building complex systems with subgraphs, implementing multi-agent systems, or needing to share state between parent graphs and subgraphs. The page explains two methods for using subgraphs: adding compiled subgraphs when schemas share keys, and invoking subgraphs via node functions when schemas differ, with code examples for both approaches.\n",
      "[How to add thread-level persistence to a subgraph](https://langchain-ai.github.io/langgraph/how-tos/subgraph-persistence/): LLM should read this page when implementing persistence in nested LangGraph architectures, adding thread-level storage to subgraphs, or debugging state propagation in LangGraph applications. This guide demonstrates how to add thread-level persistence to subgraphs by passing a checkpointer only to the parent graph during compilation, accessing persisted states from both parent and child graphs, and retrieving subgraph state using the proper configuration parameters.\n",
      "[How to transform inputs and outputs of a subgraph](https://langchain-ai.github.io/langgraph/how-tos/subgraph-transform-state/): LLM should read this page when needing to work with nested subgraphs, transforming state between parent and child graphs, or integrating independent state components in LangGraph. This page demonstrates how to transform inputs and outputs between parent graphs and subgraphs with different state structures, showing implementation of three nested graphs (parent, child, grandchild) with separate state dictionaries and transformation functions.\n",
      "[How to view and update state in subgraphs](https://langchain-ai.github.io/langgraph/how-tos/subgraphs-manage-state/): LLM should read this page when working with state management in nested subgraphs, implementing human-in-the-loop patterns, or debugging complex graph flows. This guide covers viewing and updating state in LangGraph subgraphs, including how to resume execution from breakpoints, modify subgraph state, act as specific nodes, and work with multi-level nested subgraphs.\n",
      "[How to call tools using ToolNode](https://langchain-ai.github.io/langgraph/how-tos/tool-calling/): LLM should read this page when learning how to implement tool calling with LangGraph, when working with the ToolNode component, or when building ReAct agents. This page covers using LangGraph's ToolNode for tool calling, including setup, manual invocation, working with chat models, building a ReAct agent, handling single and parallel tool calls, and error handling.\n",
      "[How to handle tool calling errors](https://langchain-ai.github.io/langgraph/how-tos/tool-calling-errors/): LLM should read this page when handling tool call errors, implementing error handling for LLM-tool interactions, or creating fallback strategies for failed tool calls. This page covers strategies for handling tool calling errors in LangGraph, including using the prebuilt ToolNode with built-in error handling, implementing custom error handling patterns, and fallback mechanisms with model upgrades when tools fail.\n",
      "[How to update graph state from tools](https://langchain-ai.github.io/langgraph/how-tos/update-state-from-tools/): LLM should read this page when needing to update graph state from tools in LangGraph, implementing personalized responses based on tool updates, or using Command objects to modify state. This page details how to update graph state from tools using Command objects, creating personalized agents with state tracking, and implementing dynamic prompt construction based on updated state values.\n",
      "[How to interact with the deployment using RemoteGraph](https://langchain-ai.github.io/langgraph/how-tos/use-remote-graph/): LLM should read this page when needing to interact with LangGraph Platform deployments remotely, when implementing RemoteGraph interfaces, or when using deployed graphs as subgraphs. This page explains how to use RemoteGraph to interact with LangGraph Platform deployments, covering initialization methods (URL-based or client-based), synchronous/asynchronous invocation, thread-level persistence, and using RemoteGraph as a subgraph in larger applications.\n",
      "[How to visualize your graph](https://langchain-ai.github.io/langgraph/how-tos/visualization): LLM should read this page when needing to visualize LangGraph graphs, looking for graph visualization methods, or working with graph visualization in Python. Comprehensive guide for visualizing graphs in LangGraph with multiple methods: Mermaid syntax, Mermaid.ink API for PNG rendering, Pyppeteer-based visualization, and Graphviz, with customization options for colors, styles, and layout.\n",
      "[How to wait for user input (Functional API)](https://langchain-ai.github.io/langgraph/how-tos/wait-user-input-functional/): LLM should read this page when implementing human-in-the-loop workflows, integrating user input into agent systems, or adding interruption capabilities to LangGraph applications. The page explains how to use the `interrupt()` function in LangGraph's Functional API to pause execution for human input, with examples for both simple workflows and ReAct agents, including code implementations with checkpointing.\n",
      "\n",
      "\n",
      "\n",
      "Namespace '('planner_agent:71fa9f4e-4688-35be-22b0-11f34ea5e5d6',)'\n",
      "Update from node 'agent'\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Name: planner_agent\n",
      "\n",
      "To build a nutritional LLM agent capable of calorie logging and providing nutritional advice based on user goals and coach guidance, I need to clarify a few aspects of your project:\n",
      "\n",
      "1. **User Goals**: What specific user goals should the agent support? For example, weight loss, muscle gain, or general health improvement?\n",
      "2. **Calorie Logging**: How do you envision the calorie logging feature? Should it allow users to input meals manually, or should it integrate with existing food databases or APIs?\n",
      "3. **Nutritional Advice**: What kind of nutritional advice do you want the agent to provide? Should it be based on general guidelines, or should it consider individual dietary restrictions and preferences?\n",
      "4. **Coach Guidance**: How do you want the coach guidance to be implemented? Should it be a predefined set of responses, or should it adapt based on user interactions?\n",
      "5. **Platform and Deployment**: Do you have a preferred platform for deploying this agent (e.g., web, mobile, etc.)? \n",
      "\n",
      "Once I have this information, I can refine the project scope and provide relevant resources for implementation.\n",
      "\n",
      "\n",
      "\n",
      "Namespace '()'\n",
      "Update from node 'planner_agent'\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Name: planner_agent\n",
      "\n",
      "To build a nutritional LLM agent capable of calorie logging and providing nutritional advice based on user goals and coach guidance, I need to clarify a few aspects of your project:\n",
      "\n",
      "1. **User Goals**: What specific user goals should the agent support? For example, weight loss, muscle gain, or general health improvement?\n",
      "2. **Calorie Logging**: How do you envision the calorie logging feature? Should it allow users to input meals manually, or should it integrate with existing food databases or APIs?\n",
      "3. **Nutritional Advice**: What kind of nutritional advice do you want the agent to provide? Should it be based on general guidelines, or should it consider individual dietary restrictions and preferences?\n",
      "4. **Coach Guidance**: How do you want the coach guidance to be implemented? Should it be a predefined set of responses, or should it adapt based on user interactions?\n",
      "5. **Platform and Deployment**: Do you have a preferred platform for deploying this agent (e.g., web, mobile, etc.)? \n",
      "\n",
      "Once I have this information, I can refine the project scope and provide relevant resources for implementation.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "===\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "\n",
    "# Planner agent\n",
    "planner_agent = create_react_agent(\n",
    "    model,\n",
    "    prompt=planner_prompt_formatted,\n",
    "    tools=[fetch_doc, transfer_to_researcher_agent],\n",
    "    name=\"planner_agent\",\n",
    ")\n",
    "\n",
    "# Researcher agent\n",
    "researcher_agent = create_react_agent(\n",
    "    model,\n",
    "    prompt=researcher_prompt,\n",
    "    tools=[fetch_doc, transfer_to_planner_agent],\n",
    "    name=\"researcher_agent\",\n",
    ")\n",
    "\n",
    "# Swarm\n",
    "checkpointer = InMemorySaver()\n",
    "agent_swarm = create_swarm(\n",
    "    [planner_agent, researcher_agent], default_active_agent=\"planner_agent\"\n",
    ")\n",
    "app = agent_swarm.compile(checkpointer=checkpointer)\n",
    "\n",
    "# Input\n",
    "request = \"Build a nutritional LLM agent for me, which is capable of calorie logging and providing nutritional advice based on users goals and coach guidance\"\n",
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "print_stream(\n",
    "    app.stream(\n",
    "        {\"messages\": [{\"role\": \"user\", \"content\": request}]}, config, subgraphs=True\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace '('planner_agent:2c1178dd-4786-6db9-557f-4086cb1a0169',)'\n",
      "Update from node 'agent'\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Name: planner_agent\n",
      "\n",
      "Thank you for the clarifications! Based on your input, here’s the refined project scope for the nutritional LLM agent:\n",
      "\n",
      "## Scope:\n",
      "- **User Goals**: Support for weight loss, muscle gain, and health improvement.\n",
      "- **Calorie Logging**: Integration with the NutritionX API for meal input and calorie tracking.\n",
      "- **Nutritional Advice**: Provide fact-based guidelines from accredited sources, considering individual dietary restrictions and preferences.\n",
      "- **Coach Guidance**: Allow coaches to share documents that answer a set of questions, which will be stored in a vector store and retrieved using Retrieval-Augmented Generation (RAG).\n",
      "- **Platform**: Deployment as a web application.\n",
      "\n",
      "## URLs:\n",
      "1. [Workflows and Agents](https://langchain-ai.github.io/langgraph/tutorials/workflows/) - For implementing agent systems and designing workflows.\n",
      "2. [Memory](https://langchain-ai.github.io/langgraph/concepts/memory/) - To manage conversation context and dietary preferences.\n",
      "3. [How to integrate LangGraph with AutoGen, CrewAI, and other frameworks](https://langchain-ai.github.io/langgraph/how-tos/autogen-integration/) - For integrating with external APIs like NutritionX and implementing RAG.\n",
      "\n",
      "I will now transfer this information to the research agent for further implementation.\n",
      "Tool Calls:\n",
      "  transfer_to_researcher_agent (call_wKLaKJXg880vqPOQ815L3meV)\n",
      " Call ID: call_wKLaKJXg880vqPOQ815L3meV\n",
      "  Args:\n",
      "\n",
      "\n",
      "\n",
      "Namespace '()'\n",
      "Update from node 'planner_agent'\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: transfer_to_researcher_agent\n",
      "\n",
      "Successfully transferred to researcher_agent\n",
      "\n",
      "\n",
      "\n",
      "Namespace '('researcher_agent:b384fb56-366b-f510-dc81-ab34c0a884a8',)'\n",
      "Update from node 'agent'\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Name: researcher_agent\n",
      "Tool Calls:\n",
      "  fetch_doc (call_LoqSG8N0nRAnKICkpz9LCgd0)\n",
      " Call ID: call_LoqSG8N0nRAnKICkpz9LCgd0\n",
      "  Args:\n",
      "    url: https://langchain-ai.github.io/langgraph/tutorials/workflows/\n",
      "  fetch_doc (call_3n6R3bnGB2u3B1pMRE8u0MvC)\n",
      " Call ID: call_3n6R3bnGB2u3B1pMRE8u0MvC\n",
      "  Args:\n",
      "    url: https://langchain-ai.github.io/langgraph/concepts/memory/\n",
      "  fetch_doc (call_ACn2CCMJyXLJbJ5f4JflXqbK)\n",
      " Call ID: call_ACn2CCMJyXLJbJ5f4JflXqbK\n",
      "  Args:\n",
      "    url: https://langchain-ai.github.io/langgraph/how-tos/autogen-integration/\n",
      "\n",
      "\n",
      "\n",
      "Namespace '('researcher_agent:b384fb56-366b-f510-dc81-ab34c0a884a8',)'\n",
      "Update from node 'tools'\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: fetch_doc\n",
      "\n",
      "How to integrate LangGraph with AutoGen, CrewAI, and other frameworks\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "[Skip to content](#how-to-integrate-langgraph-with-autogen-crewai-and-other-frameworks)\n",
      "\n",
      "**Join us at  [Interrupt: The Agent AI Conference by LangChain](https://interrupt.langchain.com/) on May 13 & 14 in San Francisco!**\n",
      "\n",
      "[![logo](../../static/wordmark_dark.svg)\n",
      "![logo](../../static/wordmark_light.svg)](../..)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "How to integrate LangGraph with AutoGen, CrewAI, and other frameworks\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Initializing search\n",
      "\n",
      "[GitHub](https://github.com/langchain-ai/langgraph \"Go to repository\")\n",
      "\n",
      "* [Home](../..)\n",
      "* [API reference](../../reference/graphs/)\n",
      "\n",
      "\n",
      "\n",
      "[![logo](../../static/wordmark_dark.svg)\n",
      "![logo](../../static/wordmark_light.svg)](../..)\n",
      "\n",
      "[GitHub](https://github.com/langchain-ai/langgraph \"Go to repository\")\n",
      "\n",
      "* [Home](../..)\n",
      "\n",
      "  Home\n",
      "  + Get started\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "    Get started\n",
      "    - [Learn the basics](../../tutorials/introduction/)\n",
      "    - [Deployment](../../tutorials/deployment/)\n",
      "  + Guides\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "    Guides\n",
      "    - [How-to Guides](../)\n",
      "\n",
      "      How-to Guides\n",
      "      * LangGraph\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "        LangGraph\n",
      "        + [LangGraph](../../how-tos#langgraph)\n",
      "        + [Graph API Basics](../../how-tos#graph-api-basics)\n",
      "        + [Controllability](../../how-tos#controllability)\n",
      "        + [Persistence](../../how-tos#persistence)\n",
      "        + [Memory](../../how-tos#memory)\n",
      "        + [Human-in-the-loop](../../how-tos#human-in-the-loop)\n",
      "        + [Streaming](../../how-tos#streaming)\n",
      "        + [Tool calling](../../how-tos#tool-calling)\n",
      "        + [Subgraphs](../../how-tos#subgraphs)\n",
      "        + [Multi-agent](../../how-tos#multi-agent)\n",
      "        + [State Management](../../how-tos#state-management)\n",
      "        + Other\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "          Other\n",
      "          - [Other](../../how-tos#other)\n",
      "          - [How to run a graph asynchronously](../async/)\n",
      "          - [How to force tool-calling agent to structure output](../react-agent-structured-output/)\n",
      "          - [How to pass custom run ID or set tags and metadata for graph runs in LangSmith](../run-id-langsmith/)\n",
      "          - How to integrate LangGraph with AutoGen, CrewAI, and other frameworks\n",
      "\n",
      "            [How to integrate LangGraph with AutoGen, CrewAI, and other frameworks](./)\n",
      "\n",
      "\n",
      "\n",
      "            Table of contents\n",
      "            * [Setup](#setup)\n",
      "            * [Define AutoGen agent](#define-autogen-agent)\n",
      "            * [Create the graph](#create-the-graph)\n",
      "            * [Run the graph](#run-the-graph)\n",
      "          - [How to integrate LangGraph (functional API) with AutoGen, CrewAI, and other frameworks](../autogen-integration-functional/)\n",
      "        + [Prebuilt ReAct Agent](../../how-tos#prebuilt-react-agent)\n",
      "      * [LangGraph Platform](../../how-tos#langgraph-platform)\n",
      "    - [Concepts](../../concepts/)\n",
      "    - [Tutorials](../../tutorials/)\n",
      "  + Resources\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "    Resources\n",
      "    - [Prebuilt Agents](../../prebuilt/)\n",
      "    - [Companies using LangGraph](../../adopters/)\n",
      "    - [LLMS-txt](../../llms-txt-overview/)\n",
      "    - [FAQ](../../concepts/faq/)\n",
      "    - [Troubleshooting](../../troubleshooting/errors/)\n",
      "    - [LangGraph Academy Course](https://academy.langchain.com/courses/intro-to-langgraph)\n",
      "* [API reference](../../reference/graphs/)\n",
      "\n",
      "Table of contents\n",
      "\n",
      "* [Setup](#setup)\n",
      "* [Define AutoGen agent](#define-autogen-agent)\n",
      "* [Create the graph](#create-the-graph)\n",
      "* [Run the graph](#run-the-graph)\n",
      "\n",
      "1. [Home](../..)\n",
      "2. [Guides](../)\n",
      "3. [How-to Guides](../)\n",
      "4. [LangGraph](../../how-tos#langgraph)\n",
      "5. [Other](../../how-tos#other)\n",
      "\n",
      "How to integrate LangGraph with AutoGen, CrewAI, and other frameworks[¶](#how-to-integrate-langgraph-with-autogen-crewai-and-other-frameworks \"Permanent link\")\n",
      "===============================================================================================================================================================\n",
      "\n",
      "LangGraph is a framework for building agentic and multi-agent applications. LangGraph can be easily integrated with other agent frameworks.\n",
      "\n",
      "The primary reasons you might want to integrate LangGraph with other agent frameworks:\n",
      "\n",
      "* create [multi-agent systems](../../concepts/multi_agent) where individual agents are built with different frameworks\n",
      "* leverage LangGraph to add features like [persistence](../../concepts/persistence), [streaming](../../concepts/streaming), [short and long-term memory](../../concepts/memory) and more\n",
      "\n",
      "The simplest way to integrate agents from other frameworks is by calling those agents inside a LangGraph [node](../../concepts/low_level/#nodes):\n",
      "\n",
      "```\n",
      "from langgraph.graph import StateGraph, MessagesState, START\n",
      "\n",
      "autogen_agent = autogen.AssistantAgent(name=\"assistant\", ...)\n",
      "user_proxy = autogen.UserProxyAgent(name=\"user_proxy\", ...)\n",
      "\n",
      "def call_autogen_agent(state: MessagesState):\n",
      "    response = user_proxy.initiate_chat(\n",
      "        autogen_agent,\n",
      "        message=state[\"messages\"][-1],\n",
      "        ...\n",
      "    )\n",
      "    ...\n",
      "\n",
      "graph = (\n",
      "    StateGraph(MessagesState)\n",
      "    .add_node(call_autogen_agent)\n",
      "    .add_edge(START, \"call_autogen_agent\")\n",
      "    .compile()\n",
      ")\n",
      "\n",
      "graph.invoke({\n",
      "    \"messages\": [\n",
      "        {\n",
      "            \"role\": \"user\",\n",
      "            \"content\": \"Find numbers between 10 and 30 in fibonacci sequence\",\n",
      "        }\n",
      "    ]\n",
      "})\n",
      "\n",
      "```\n",
      "\n",
      "In this guide we show how to build a LangGraph chatbot that integrates with AutoGen, but you can follow the same approach with other frameworks.\n",
      "\n",
      "Setup[¶](#setup \"Permanent link\")\n",
      "---------------------------------\n",
      "\n",
      "```\n",
      "%pip install autogen langgraph\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "import getpass\n",
      "import os\n",
      "\n",
      "\n",
      "def _set_env(var: str):\n",
      "    if not os.environ.get(var):\n",
      "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
      "\n",
      "\n",
      "_set_env(\"OPENAI_API_KEY\")\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "OPENAI_API_KEY:  ········\n",
      "\n",
      "```\n",
      "\n",
      "Define AutoGen agent[¶](#define-autogen-agent \"Permanent link\")\n",
      "---------------------------------------------------------------\n",
      "\n",
      "Here we define our AutoGen agent. Adapted from official tutorial [here](https://github.com/microsoft/autogen/blob/0.2/notebook/agentchat_web_info.ipynb).\n",
      "\n",
      "```\n",
      "import autogen\n",
      "import os\n",
      "\n",
      "config_list = [{\"model\": \"gpt-4o\", \"api_key\": os.environ[\"OPENAI_API_KEY\"]}]\n",
      "\n",
      "llm_config = {\n",
      "    \"timeout\": 600,\n",
      "    \"cache_seed\": 42,\n",
      "    \"config_list\": config_list,\n",
      "    \"temperature\": 0,\n",
      "}\n",
      "\n",
      "autogen_agent = autogen.AssistantAgent(\n",
      "    name=\"assistant\",\n",
      "    llm_config=llm_config,\n",
      ")\n",
      "\n",
      "user_proxy = autogen.UserProxyAgent(\n",
      "    name=\"user_proxy\",\n",
      "    human_input_mode=\"NEVER\",\n",
      "    max_consecutive_auto_reply=10,\n",
      "    is_termination_msg=lambda x: x.get(\"content\", \"\").rstrip().endswith(\"TERMINATE\"),\n",
      "    code_execution_config={\n",
      "        \"work_dir\": \"web\",\n",
      "        \"use_docker\": False,\n",
      "    },  # Please set use_docker=True if docker is available to run the generated code. Using docker is safer than running the generated code directly.\n",
      "    llm_config=llm_config,\n",
      "    system_message=\"Reply TERMINATE if the task has been solved at full satisfaction. Otherwise, reply CONTINUE, or the reason why the task is not solved yet.\",\n",
      ")\n",
      "\n",
      "```\n",
      "\n",
      "---\n",
      "\n",
      "Create the graph[¶](#create-the-graph \"Permanent link\")\n",
      "-------------------------------------------------------\n",
      "\n",
      "We will now create a LangGraph chatbot graph that calls AutoGen agent.\n",
      "\n",
      "```\n",
      "from langchain_core.messages import convert_to_openai_messages\n",
      "from langgraph.graph import StateGraph, MessagesState, START\n",
      "from langgraph.checkpoint.memory import MemorySaver\n",
      "\n",
      "\n",
      "def call_autogen_agent(state: MessagesState):\n",
      "    # convert to openai-style messages\n",
      "    messages = convert_to_openai_messages(state[\"messages\"])\n",
      "    response = user_proxy.initiate_chat(\n",
      "        autogen_agent,\n",
      "        message=messages[-1],\n",
      "        # pass previous message history as context\n",
      "        carryover=messages[:-1],\n",
      "    )\n",
      "    # get the final response from the agent\n",
      "    content = response.chat_history[-1][\"content\"]\n",
      "    return {\"messages\": {\"role\": \"assistant\", \"content\": content}}\n",
      "\n",
      "\n",
      "# add short-term memory for storing conversation history\n",
      "checkpointer = MemorySaver()\n",
      "\n",
      "builder = StateGraph(MessagesState)\n",
      "builder.add_node(call_autogen_agent)\n",
      "builder.add_edge(START, \"call_autogen_agent\")\n",
      "graph = builder.compile(checkpointer=checkpointer)\n",
      "\n",
      "```\n",
      "\n",
      "API Reference: [convert\\_to\\_openai\\_messages](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.utils.convert_to_openai_messages.html) | [StateGraph](https://langchain-ai.github.io/langgraph/reference/graphs/#langgraph.graph.state.StateGraph) | [START](https://langchain-ai.github.io/langgraph/reference/constants/#langgraph.constants.START) | [MemorySaver](https://langchain-ai.github.io/langgraph/reference/checkpoints/#langgraph.checkpoint.memory.MemorySaver)\n",
      "\n",
      "```\n",
      "from IPython.display import display, Image\n",
      "\n",
      "display(Image(graph.get_graph().draw_mermaid_png()))\n",
      "\n",
      "```\n",
      "\n",
      "![](data:image/png;base64,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)\n",
      "\n",
      "Run the graph[¶](#run-the-graph \"Permanent link\")\n",
      "-------------------------------------------------\n",
      "\n",
      "We can now run the graph.\n",
      "\n",
      "```\n",
      "# pass the thread ID to persist agent outputs for future interactions\n",
      "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
      "\n",
      "for chunk in graph.stream(\n",
      "    {\n",
      "        \"messages\": [\n",
      "            {\n",
      "                \"role\": \"user\",\n",
      "                \"content\": \"Find numbers between 10 and 30 in fibonacci sequence\",\n",
      "            }\n",
      "        ]\n",
      "    },\n",
      "    config,\n",
      "):\n",
      "    print(chunk)\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\u001b[33muser_proxy\u001b[0m (to assistant):\n",
      "\n",
      "Find numbers between 10 and 30 in fibonacci sequence\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33massistant\u001b[0m (to user_proxy):\n",
      "\n",
      "To find numbers between 10 and 30 in the Fibonacci sequence, we can generate the Fibonacci sequence and check which numbers fall within this range. Here's a plan:\n",
      "\n",
      "1. Generate Fibonacci numbers starting from 0.\n",
      "2. Continue generating until the numbers exceed 30.\n",
      "3. Collect and print the numbers that are between 10 and 30.\n",
      "\n",
      "Let's implement this in Python:\n",
      "\n",
      "\\`\\`\\`python\n",
      "# filename: fibonacci_range.py\n",
      "\n",
      "def fibonacci_sequence():\n",
      "    a, b = 0, 1\n",
      "    while a <= 30:\n",
      "        if 10 <= a <= 30:\n",
      "            print(a)\n",
      "        a, b = b, a + b\n",
      "\n",
      "fibonacci_sequence()\n",
      "\\`\\`\\`\n",
      "\n",
      "This script will print the Fibonacci numbers between 10 and 30. Please execute the code to see the result.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> EXECUTING CODE BLOCK 0 (inferred language is python)...\u001b[0m\n",
      "\u001b[33muser_proxy\u001b[0m (to assistant):\n",
      "\n",
      "exitcode: 0 (execution succeeded)\n",
      "Code output: \n",
      "13\n",
      "21\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33massistant\u001b[0m (to user_proxy):\n",
      "\n",
      "The Fibonacci numbers between 10 and 30 are 13 and 21. \n",
      "\n",
      "These numbers are part of the Fibonacci sequence, which is generated by adding the two preceding numbers to get the next number, starting from 0 and 1. \n",
      "\n",
      "The sequence goes: 0, 1, 1, 2, 3, 5, 8, 13, 21, 34, ...\n",
      "\n",
      "As you can see, 13 and 21 are the only numbers in this sequence that fall between 10 and 30.\n",
      "\n",
      "TERMINATE\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "{'call_autogen_agent': {'messages': {'role': 'assistant', 'content': 'The Fibonacci numbers between 10 and 30 are 13 and 21. \\n\\nThese numbers are part of the Fibonacci sequence, which is generated by adding the two preceding numbers to get the next number, starting from 0 and 1. \\n\\nThe sequence goes: 0, 1, 1, 2, 3, 5, 8, 13, 21, 34, ...\\n\\nAs you can see, 13 and 21 are the only numbers in this sequence that fall between 10 and 30.\\n\\nTERMINATE'}}}\n",
      "\n",
      "```\n",
      "\n",
      "Since we're leveraging LangGraph's [persistence](https://langchain-ai.github.io/langgraph/concepts/persistence/) features we can now continue the conversation using the same thread ID -- LangGraph will automatically pass previous history to the AutoGen agent:\n",
      "\n",
      "```\n",
      "for chunk in graph.stream(\n",
      "    {\n",
      "        \"messages\": [\n",
      "            {\n",
      "                \"role\": \"user\",\n",
      "                \"content\": \"Multiply the last number by 3\",\n",
      "            }\n",
      "        ]\n",
      "    },\n",
      "    config,\n",
      "):\n",
      "    print(chunk)\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\u001b[33muser_proxy\u001b[0m (to assistant):\n",
      "\n",
      "Multiply the last number by 3\n",
      "Context: \n",
      "Find numbers between 10 and 30 in fibonacci sequence\n",
      "The Fibonacci numbers between 10 and 30 are 13 and 21. \n",
      "\n",
      "These numbers are part of the Fibonacci sequence, which is generated by adding the two preceding numbers to get the next number, starting from 0 and 1. \n",
      "\n",
      "The sequence goes: 0, 1, 1, 2, 3, 5, 8, 13, 21, 34, ...\n",
      "\n",
      "As you can see, 13 and 21 are the only numbers in this sequence that fall between 10 and 30.\n",
      "\n",
      "TERMINATE\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33massistant\u001b[0m (to user_proxy):\n",
      "\n",
      "The last number in the Fibonacci sequence between 10 and 30 is 21. Multiplying 21 by 3 gives:\n",
      "\n",
      "21 * 3 = 63\n",
      "\n",
      "TERMINATE\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "{'call_autogen_agent': {'messages': {'role': 'assistant', 'content': 'The last number in the Fibonacci sequence between 10 and 30 is 21. Multiplying 21 by 3 gives:\\n\\n21 * 3 = 63\\n\\nTERMINATE'}}}\n",
      "\n",
      "```\n",
      "\n",
      "Was this page helpful?\n",
      "\n",
      "Thanks for your feedback!\n",
      "\n",
      "Thanks for your feedback! Please help us improve this page by adding to the discussion below.\n",
      "\n",
      "Comments\n",
      "--------\n",
      "\n",
      "Back to top\n",
      "\n",
      "\n",
      "[Previous\n",
      "\n",
      "How to pass custom run ID or set tags and metadata for graph runs in LangSmith](../run-id-langsmith/)\n",
      "[Next\n",
      "\n",
      "How to integrate LangGraph (functional API) with AutoGen, CrewAI, and other frameworks](../autogen-integration-functional/)\n",
      "\n",
      "Copyright © 2025 LangChain, Inc | [Consent Preferences](#__consent)\n",
      "\n",
      "Made with\n",
      "[Material for MkDocs Insiders](https://squidfunk.github.io/mkdocs-material/)\n",
      "\n",
      "#### Cookie consent\n",
      "\n",
      "We use cookies to recognize your repeated visits and preferences, as well as to measure the effectiveness of our documentation and whether users find what they're searching for. **Clicking \"Accept\" makes our documentation better. Thank you!** ❤️\n",
      "\n",
      "* Google Analytics\n",
      "* GitHub\n",
      "\n",
      "Accept\n",
      "Reject\n",
      "\n",
      "\n",
      "\n",
      "Namespace '('researcher_agent:b384fb56-366b-f510-dc81-ab34c0a884a8',)'\n",
      "Update from node 'agent'\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Name: researcher_agent\n",
      "\n",
      "Based on the information gathered from the URLs, here’s a comprehensive plan for building your nutritional LLM agent:\n",
      "\n",
      "### Implementation Plan for Nutritional LLM Agent\n",
      "\n",
      "1. **User Goals Support**:\n",
      "   - Implement features to support weight loss, muscle gain, and health improvement. This can be achieved by defining specific prompts and workflows that guide users based on their goals.\n",
      "\n",
      "2. **Calorie Logging**:\n",
      "   - Integrate with the **NutritionX API** to allow users to log their meals and track calorie intake. This will involve setting up API calls to fetch nutritional information and logging user inputs.\n",
      "\n",
      "3. **Nutritional Advice**:\n",
      "   - Provide fact-based nutritional advice using guidelines from accredited sources. This can be implemented by creating a knowledge base that pulls information from trusted sources and allows the agent to respond to user queries with accurate data.\n",
      "   - Consider individual dietary restrictions and preferences by implementing a user profile system that stores this information and adjusts advice accordingly.\n",
      "\n",
      "4. **Coach Guidance**:\n",
      "   - Allow coaches to upload documents that answer common questions. These documents can be indexed and stored in a vector store for retrieval using Retrieval-Augmented Generation (RAG).\n",
      "   - Implement a mechanism for the agent to query this vector store and provide relevant documents based on user inquiries.\n",
      "\n",
      "5. **Memory Management**:\n",
      "   - Utilize **short-term memory** to manage ongoing conversations and user interactions. This will help maintain context during sessions.\n",
      "   - Implement **long-term memory** to store user preferences, dietary restrictions, and past interactions, allowing for personalized advice over time.\n",
      "\n",
      "6. **Agent Architecture**:\n",
      "   - Use the **LangGraph** framework to build the agent. This will involve creating workflows that define how the agent interacts with users, processes inputs, and retrieves information.\n",
      "   - Implement **prompt chaining** to break down complex tasks into manageable steps, ensuring that the agent can handle user requests effectively.\n",
      "\n",
      "7. **Deployment**:\n",
      "   - Deploy the agent as a web application, ensuring that it is accessible to users. This will involve setting up a web server and creating a user-friendly interface for interaction.\n",
      "\n",
      "### Key Features to Implement:\n",
      "- **User Profile Management**: Store user goals, dietary restrictions, and preferences.\n",
      "- **Meal Logging Interface**: Allow users to input meals and track calories via the NutritionX API.\n",
      "- **Nutritional Advice Engine**: Provide tailored advice based on user profiles and accredited guidelines.\n",
      "- **Document Upload and Retrieval**: Enable coaches to upload documents and retrieve them based on user queries.\n",
      "- **Memory Management**: Implement both short-term and long-term memory for personalized interactions.\n",
      "\n",
      "### Next Steps:\n",
      "- Begin by setting up the development environment and integrating the NutritionX API.\n",
      "- Create user profiles and implement memory management features.\n",
      "- Develop the core functionalities for calorie logging and nutritional advice.\n",
      "- Test the agent with various user scenarios to ensure it meets the defined goals.\n",
      "\n",
      "This plan outlines a structured approach to building your nutritional LLM agent, leveraging the capabilities of LangGraph and integrating with external APIs for a comprehensive user experience. If you need further assistance or specific code examples, feel free to ask!\n",
      "\n",
      "\n",
      "\n",
      "Namespace '()'\n",
      "Update from node 'researcher_agent'\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Name: researcher_agent\n",
      "\n",
      "Based on the information gathered from the URLs, here’s a comprehensive plan for building your nutritional LLM agent:\n",
      "\n",
      "### Implementation Plan for Nutritional LLM Agent\n",
      "\n",
      "1. **User Goals Support**:\n",
      "   - Implement features to support weight loss, muscle gain, and health improvement. This can be achieved by defining specific prompts and workflows that guide users based on their goals.\n",
      "\n",
      "2. **Calorie Logging**:\n",
      "   - Integrate with the **NutritionX API** to allow users to log their meals and track calorie intake. This will involve setting up API calls to fetch nutritional information and logging user inputs.\n",
      "\n",
      "3. **Nutritional Advice**:\n",
      "   - Provide fact-based nutritional advice using guidelines from accredited sources. This can be implemented by creating a knowledge base that pulls information from trusted sources and allows the agent to respond to user queries with accurate data.\n",
      "   - Consider individual dietary restrictions and preferences by implementing a user profile system that stores this information and adjusts advice accordingly.\n",
      "\n",
      "4. **Coach Guidance**:\n",
      "   - Allow coaches to upload documents that answer common questions. These documents can be indexed and stored in a vector store for retrieval using Retrieval-Augmented Generation (RAG).\n",
      "   - Implement a mechanism for the agent to query this vector store and provide relevant documents based on user inquiries.\n",
      "\n",
      "5. **Memory Management**:\n",
      "   - Utilize **short-term memory** to manage ongoing conversations and user interactions. This will help maintain context during sessions.\n",
      "   - Implement **long-term memory** to store user preferences, dietary restrictions, and past interactions, allowing for personalized advice over time.\n",
      "\n",
      "6. **Agent Architecture**:\n",
      "   - Use the **LangGraph** framework to build the agent. This will involve creating workflows that define how the agent interacts with users, processes inputs, and retrieves information.\n",
      "   - Implement **prompt chaining** to break down complex tasks into manageable steps, ensuring that the agent can handle user requests effectively.\n",
      "\n",
      "7. **Deployment**:\n",
      "   - Deploy the agent as a web application, ensuring that it is accessible to users. This will involve setting up a web server and creating a user-friendly interface for interaction.\n",
      "\n",
      "### Key Features to Implement:\n",
      "- **User Profile Management**: Store user goals, dietary restrictions, and preferences.\n",
      "- **Meal Logging Interface**: Allow users to input meals and track calories via the NutritionX API.\n",
      "- **Nutritional Advice Engine**: Provide tailored advice based on user profiles and accredited guidelines.\n",
      "- **Document Upload and Retrieval**: Enable coaches to upload documents and retrieve them based on user queries.\n",
      "- **Memory Management**: Implement both short-term and long-term memory for personalized interactions.\n",
      "\n",
      "### Next Steps:\n",
      "- Begin by setting up the development environment and integrating the NutritionX API.\n",
      "- Create user profiles and implement memory management features.\n",
      "- Develop the core functionalities for calorie logging and nutritional advice.\n",
      "- Test the agent with various user scenarios to ensure it meets the defined goals.\n",
      "\n",
      "This plan outlines a structured approach to building your nutritional LLM agent, leveraging the capabilities of LangGraph and integrating with external APIs for a comprehensive user experience. If you need further assistance or specific code examples, feel free to ask!\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "===\n",
      "\n"
     ]
    }
   ],
   "source": [
    "request = \"1. It should support weight loss, muscle gain, health improvement. 2. Integrate with nutritionx api. 3. Should be fact based guidelines from accredited sources, also consider individual dietary restrictions and preferences. 4. Coach should be allowed to share documents answering a set of questions, which can be fed to a vector store and fetched via RAG. 5. Web\"\n",
    "print_stream(\n",
    "    app.stream(\n",
    "        {\"messages\": [{\"role\": \"user\", \"content\": request}]}, config, subgraphs=True\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace '('researcher_agent:0aff52d1-71f4-fd57-05ce-67b32e793037',)'\n",
      "Update from node 'agent'\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Name: researcher_agent\n",
      "\n",
      "To implement the nutritional LLM agent as per the outlined plan, I will provide a structured code example that covers the key components. This will include setting up the environment, integrating with the NutritionX API, managing user profiles, and implementing memory management. \n",
      "\n",
      "### Step 1: Set Up the Environment\n",
      "\n",
      "First, ensure you have the necessary packages installed. You can do this using pip:\n",
      "\n",
      "```bash\n",
      "pip install langgraph langchain requests\n",
      "```\n",
      "\n",
      "### Step 2: Integrate with NutritionX API\n",
      "\n",
      "Create a function to interact with the NutritionX API for calorie logging.\n",
      "\n",
      "```python\n",
      "import requests\n",
      "\n",
      "NUTRITIONX_API_URL = \"https://api.nutritionx.com/v1/food\"  # Example URL, replace with actual endpoint\n",
      "API_KEY = \"your_nutritionx_api_key\"  # Replace with your actual API key\n",
      "\n",
      "def log_meal(meal_name):\n",
      "    headers = {\n",
      "        \"Authorization\": f\"Bearer {API_KEY}\",\n",
      "        \"Content-Type\": \"application/json\"\n",
      "    }\n",
      "    response = requests.get(f\"{NUTRITIONX_API_URL}?query={meal_name}\", headers=headers)\n",
      "    if response.status_code == 200:\n",
      "        return response.json()  # Return the nutritional information\n",
      "    else:\n",
      "        return None  # Handle errors appropriately\n",
      "```\n",
      "\n",
      "### Step 3: User Profile Management\n",
      "\n",
      "Create a simple user profile management system to store user goals and dietary restrictions.\n",
      "\n",
      "```python\n",
      "class UserProfile:\n",
      "    def __init__(self, user_id):\n",
      "        self.user_id = user_id\n",
      "        self.goals = []\n",
      "        self.dietary_restrictions = []\n",
      "\n",
      "    def set_goals(self, goals):\n",
      "        self.goals = goals\n",
      "\n",
      "    def set_dietary_restrictions(self, restrictions):\n",
      "        self.dietary_restrictions = restrictions\n",
      "\n",
      "    def get_profile(self):\n",
      "        return {\n",
      "            \"goals\": self.goals,\n",
      "            \"dietary_restrictions\": self.dietary_restrictions\n",
      "        }\n",
      "```\n",
      "\n",
      "### Step 4: Memory Management\n",
      "\n",
      "Implement short-term and long-term memory management using LangGraph.\n",
      "\n",
      "```python\n",
      "from langgraph.graph import StateGraph, MessagesState, START\n",
      "from langgraph.checkpoint.memory import MemorySaver\n",
      "\n",
      "class MemoryManager:\n",
      "    def __init__(self):\n",
      "        self.checkpointer = MemorySaver()\n",
      "\n",
      "    def save_memory(self, user_id, memory_data):\n",
      "        # Save memory data to a persistent store\n",
      "        pass  # Implement storage logic here\n",
      "\n",
      "    def retrieve_memory(self, user_id):\n",
      "        # Retrieve memory data from a persistent store\n",
      "        pass  # Implement retrieval logic here\n",
      "```\n",
      "\n",
      "### Step 5: Build the Agent Workflow\n",
      "\n",
      "Create the main workflow for the agent using LangGraph.\n",
      "\n",
      "```python\n",
      "from langgraph.graph import StateGraph, MessagesState\n",
      "\n",
      "def nutritional_advice(state: MessagesState):\n",
      "    user_profile = state.get(\"user_profile\")\n",
      "    meal_name = state[\"messages\"][-1][\"content\"]\n",
      "    \n",
      "    # Log the meal and get nutritional info\n",
      "    nutritional_info = log_meal(meal_name)\n",
      "    \n",
      "    # Generate advice based on user profile and nutritional info\n",
      "    advice = f\"Based on your goals of {user_profile['goals']}, here is the nutritional info for {meal_name}: {nutritional_info}\"\n",
      "    \n",
      "    return {\"messages\": {\"role\": \"assistant\", \"content\": advice}}\n",
      "\n",
      "# Build the workflow\n",
      "graph = StateGraph(MessagesState)\n",
      "graph.add_node(nutritional_advice)\n",
      "graph.add_edge(START, \"nutritional_advice\")\n",
      "compiled_graph = graph.compile(checkpointer=MemorySaver())\n",
      "```\n",
      "\n",
      "### Step 6: Run the Agent\n",
      "\n",
      "Finally, create a function to run the agent and interact with users.\n",
      "\n",
      "```python\n",
      "def run_agent(user_id, user_input):\n",
      "    user_profile = UserProfile(user_id)\n",
      "    # Set user goals and dietary restrictions as needed\n",
      "    user_profile.set_goals([\"weight loss\"])\n",
      "    user_profile.set_dietary_restrictions([\"gluten-free\"])\n",
      "\n",
      "    state = {\n",
      "        \"messages\": [{\"role\": \"user\", \"content\": user_input}],\n",
      "        \"user_profile\": user_profile.get_profile()\n",
      "    }\n",
      "\n",
      "    response = compiled_graph.invoke(state)\n",
      "    print(response[\"messages\"][\"content\"])\n",
      "\n",
      "# Example usage\n",
      "run_agent(\"user123\", \"What can you tell me about a salad?\")\n",
      "```\n",
      "\n",
      "### Summary\n",
      "\n",
      "This code provides a basic structure for your nutritional LLM agent, including:\n",
      "- Integration with the NutritionX API for calorie logging.\n",
      "- User profile management to store goals and dietary restrictions.\n",
      "- Memory management for storing and retrieving user data.\n",
      "- A workflow to generate nutritional advice based on user input.\n",
      "\n",
      "You can expand upon this foundation by adding more features, improving error handling, and refining the user interface for a web application. If you need further assistance or specific features, feel free to ask!\n",
      "\n",
      "\n",
      "\n",
      "Namespace '()'\n",
      "Update from node 'researcher_agent'\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Name: researcher_agent\n",
      "\n",
      "To implement the nutritional LLM agent as per the outlined plan, I will provide a structured code example that covers the key components. This will include setting up the environment, integrating with the NutritionX API, managing user profiles, and implementing memory management. \n",
      "\n",
      "### Step 1: Set Up the Environment\n",
      "\n",
      "First, ensure you have the necessary packages installed. You can do this using pip:\n",
      "\n",
      "```bash\n",
      "pip install langgraph langchain requests\n",
      "```\n",
      "\n",
      "### Step 2: Integrate with NutritionX API\n",
      "\n",
      "Create a function to interact with the NutritionX API for calorie logging.\n",
      "\n",
      "```python\n",
      "import requests\n",
      "\n",
      "NUTRITIONX_API_URL = \"https://api.nutritionx.com/v1/food\"  # Example URL, replace with actual endpoint\n",
      "API_KEY = \"your_nutritionx_api_key\"  # Replace with your actual API key\n",
      "\n",
      "def log_meal(meal_name):\n",
      "    headers = {\n",
      "        \"Authorization\": f\"Bearer {API_KEY}\",\n",
      "        \"Content-Type\": \"application/json\"\n",
      "    }\n",
      "    response = requests.get(f\"{NUTRITIONX_API_URL}?query={meal_name}\", headers=headers)\n",
      "    if response.status_code == 200:\n",
      "        return response.json()  # Return the nutritional information\n",
      "    else:\n",
      "        return None  # Handle errors appropriately\n",
      "```\n",
      "\n",
      "### Step 3: User Profile Management\n",
      "\n",
      "Create a simple user profile management system to store user goals and dietary restrictions.\n",
      "\n",
      "```python\n",
      "class UserProfile:\n",
      "    def __init__(self, user_id):\n",
      "        self.user_id = user_id\n",
      "        self.goals = []\n",
      "        self.dietary_restrictions = []\n",
      "\n",
      "    def set_goals(self, goals):\n",
      "        self.goals = goals\n",
      "\n",
      "    def set_dietary_restrictions(self, restrictions):\n",
      "        self.dietary_restrictions = restrictions\n",
      "\n",
      "    def get_profile(self):\n",
      "        return {\n",
      "            \"goals\": self.goals,\n",
      "            \"dietary_restrictions\": self.dietary_restrictions\n",
      "        }\n",
      "```\n",
      "\n",
      "### Step 4: Memory Management\n",
      "\n",
      "Implement short-term and long-term memory management using LangGraph.\n",
      "\n",
      "```python\n",
      "from langgraph.graph import StateGraph, MessagesState, START\n",
      "from langgraph.checkpoint.memory import MemorySaver\n",
      "\n",
      "class MemoryManager:\n",
      "    def __init__(self):\n",
      "        self.checkpointer = MemorySaver()\n",
      "\n",
      "    def save_memory(self, user_id, memory_data):\n",
      "        # Save memory data to a persistent store\n",
      "        pass  # Implement storage logic here\n",
      "\n",
      "    def retrieve_memory(self, user_id):\n",
      "        # Retrieve memory data from a persistent store\n",
      "        pass  # Implement retrieval logic here\n",
      "```\n",
      "\n",
      "### Step 5: Build the Agent Workflow\n",
      "\n",
      "Create the main workflow for the agent using LangGraph.\n",
      "\n",
      "```python\n",
      "from langgraph.graph import StateGraph, MessagesState\n",
      "\n",
      "def nutritional_advice(state: MessagesState):\n",
      "    user_profile = state.get(\"user_profile\")\n",
      "    meal_name = state[\"messages\"][-1][\"content\"]\n",
      "    \n",
      "    # Log the meal and get nutritional info\n",
      "    nutritional_info = log_meal(meal_name)\n",
      "    \n",
      "    # Generate advice based on user profile and nutritional info\n",
      "    advice = f\"Based on your goals of {user_profile['goals']}, here is the nutritional info for {meal_name}: {nutritional_info}\"\n",
      "    \n",
      "    return {\"messages\": {\"role\": \"assistant\", \"content\": advice}}\n",
      "\n",
      "# Build the workflow\n",
      "graph = StateGraph(MessagesState)\n",
      "graph.add_node(nutritional_advice)\n",
      "graph.add_edge(START, \"nutritional_advice\")\n",
      "compiled_graph = graph.compile(checkpointer=MemorySaver())\n",
      "```\n",
      "\n",
      "### Step 6: Run the Agent\n",
      "\n",
      "Finally, create a function to run the agent and interact with users.\n",
      "\n",
      "```python\n",
      "def run_agent(user_id, user_input):\n",
      "    user_profile = UserProfile(user_id)\n",
      "    # Set user goals and dietary restrictions as needed\n",
      "    user_profile.set_goals([\"weight loss\"])\n",
      "    user_profile.set_dietary_restrictions([\"gluten-free\"])\n",
      "\n",
      "    state = {\n",
      "        \"messages\": [{\"role\": \"user\", \"content\": user_input}],\n",
      "        \"user_profile\": user_profile.get_profile()\n",
      "    }\n",
      "\n",
      "    response = compiled_graph.invoke(state)\n",
      "    print(response[\"messages\"][\"content\"])\n",
      "\n",
      "# Example usage\n",
      "run_agent(\"user123\", \"What can you tell me about a salad?\")\n",
      "```\n",
      "\n",
      "### Summary\n",
      "\n",
      "This code provides a basic structure for your nutritional LLM agent, including:\n",
      "- Integration with the NutritionX API for calorie logging.\n",
      "- User profile management to store goals and dietary restrictions.\n",
      "- Memory management for storing and retrieving user data.\n",
      "- A workflow to generate nutritional advice based on user input.\n",
      "\n",
      "You can expand upon this foundation by adding more features, improving error handling, and refining the user interface for a web application. If you need further assistance or specific features, feel free to ask!\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "===\n",
      "\n"
     ]
    }
   ],
   "source": [
    "request = \"Plan looks good, please execute and provide code for this\"\n",
    "print_stream(\n",
    "    app.stream(\n",
    "        {\"messages\": [{\"role\": \"user\", \"content\": request}]}, config, subgraphs=True\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Traces \n",
    "\n",
    "Example planning trace - \n",
    "\n",
    "https://smith.langchain.com/public/7a428dfc-b8b8-4e28-bc05-9508a848082c/r\n",
    "\n",
    "Example researcher trace - \n",
    "\n",
    "https://smith.langchain.com/public/1c9d0e28-56c4-4241-922b-afd614464edd/r\n",
    "\n",
    "### Test Code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.graph import END, START, StateGraph\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "\n",
    "# Define state schema for our joke generator\n",
    "class State(TypedDict):\n",
    "    topic: str  # The topic provided by the user\n",
    "    joke: str  # The generated joke\n",
    "    has_punchline: bool  # Whether the joke has a punchline\n",
    "\n",
    "\n",
    "# Create the nodes for our graph\n",
    "def generate_joke(state: State):\n",
    "    \"\"\"Generate a joke with a punchline based on the provided topic.\"\"\"\n",
    "    # Initialize the LLM\n",
    "    llm = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "\n",
    "    # Create a prompt that explicitly asks for a setup followed by a punchline\n",
    "    prompt = f\"Create a joke about {state['topic']}. Format it with a setup followed by 'punchline:' and then the actual punchline.\"\n",
    "\n",
    "    # Generate the joke\n",
    "    response = llm.invoke([HumanMessage(content=prompt)])\n",
    "\n",
    "    # Return the joke\n",
    "    return {\"joke\": response.content}\n",
    "\n",
    "\n",
    "def check_punchline(state: State):\n",
    "    \"\"\"Check if the joke contains the word 'punchline'.\"\"\"\n",
    "    # Check if the joke contains the word 'punchline'\n",
    "    if \"punchline\" in state[\"joke\"].lower():\n",
    "        return {\"has_punchline\": True}\n",
    "    else:\n",
    "        return {\"has_punchline\": False}\n",
    "\n",
    "\n",
    "# Define conditional edge decision function\n",
    "def decide_next_step(state: State):\n",
    "    \"\"\"Decide whether to end the process.\"\"\"\n",
    "    if state[\"has_punchline\"]:\n",
    "        return \"end\"\n",
    "    else:\n",
    "        # If we were to handle cases without punchlines, we'd route to a fix node\n",
    "        # But per requirements, we'll just end without feedback\n",
    "        return \"end\"\n",
    "\n",
    "\n",
    "# Build the graph\n",
    "def build_graph():\n",
    "    # Create a graph with the defined state schema\n",
    "    graph = StateGraph(State)\n",
    "\n",
    "    # Add nodes\n",
    "    graph.add_node(\"generate_joke\", generate_joke)\n",
    "    graph.add_node(\"check_punchline\", check_punchline)\n",
    "\n",
    "    # Add edges\n",
    "    graph.add_edge(START, \"generate_joke\")\n",
    "    graph.add_edge(\"generate_joke\", \"check_punchline\")\n",
    "    graph.add_conditional_edges(\"check_punchline\", decide_next_step, {\"end\": END})\n",
    "\n",
    "    # Compile the graph\n",
    "    return graph.compile()\n",
    "\n",
    "\n",
    "joke_graph = build_graph()\n",
    "\n",
    "# Invoke the graph with the provided topic\n",
    "result = joke_graph.invoke({\"topic\": \"cats\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'topic': 'cats',\n",
       " 'joke': 'Why was the cat sitting on the computer?\\n\\nPunchline: It wanted to keep an eye on the mouse!',\n",
       " 'has_punchline': True}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
