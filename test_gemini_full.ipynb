{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "34b2dfad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imports complete\n"
     ]
    }
   ],
   "source": [
    "# --- Imports and Environment Setup ---\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv() # Load environment variables from .env file\n",
    "\n",
    "import logging\n",
    "\n",
    "from pydantic import Field, BaseModel # For data validation and settings management\n",
    "from typing import List, Literal # For type hinting\n",
    "\n",
    "from langgraph.graph import MessagesState, StateGraph, START, END # Core LangGraph components for building stateful graphs\n",
    "from langgraph.types import Command, interrupt # For controlling graph flow, e.g., human-in-the-loop\n",
    "\n",
    "from langchain_core.prompts import PromptTemplate # For creating flexible prompts for LLMs\n",
    "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage # For structuring messages in conversations\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI # Google Generative AI model wrapper\n",
    "\n",
    "from experiments.gemini_phase1_toolgen import tool_compile_graph\n",
    "\n",
    "print(\"imports complete\")\n",
    "\n",
    "# --- Logging Configuration ---\n",
    "# Set up the logger for application-wide logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,  # Set to DEBUG for detailed logs, INFO for general information\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\", # Log message format\n",
    "    handlers=[\n",
    "        # logging.FileHandler(\"scraper.log\"),  # Option to log to a file (currently commented out)\n",
    "        logging.StreamHandler()  # Log to console\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__) # Get a logger instance for the current module\n",
    "\n",
    "# --- LLM Initialization ---\n",
    "# Initialize the Language Model (LLM) to be used throughout the application\n",
    "# Using Google's Gemini Flash model with a temperature of 0 for deterministic outputs\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash-preview-05-20\", temperature=0)\n",
    "\n",
    "\n",
    "# --- Agent Requirement Analysis ---\n",
    "# This section defines the process for understanding user requirements for building an AI agent.\n",
    "\n",
    "# Prompt template for the Requirement Analysis LLM\n",
    "REQ_ANALYSIS_PROMPT = \"\"\"Your job is to get information from a user about what kind of agent they wish to build.\n",
    "\n",
    "You should get the following information from them:\n",
    "\n",
    "- What the objective of the agent is\n",
    "- Various usecases of the agent\n",
    "- Some examples of what the agent will be doing (Input and expected output pairs)\n",
    "\n",
    "If you are not able to discern this info, ask them to clarify, you can suggest and see if user confirms the suggestions.\n",
    "\n",
    "After you are able to discern all the information, call the tool AgentInstruction\"\"\"\n",
    "\n",
    "class AgentInstructions(BaseModel):\n",
    "    \"\"\"\n",
    "    Pydantic model to structure the instructions for building the Agent.\n",
    "    This model is used as a tool for the LLM to output structured information.\n",
    "    \"\"\"\n",
    "    objective: str = Field(description=\"What is the primary objective of the agent\")\n",
    "    usecases: List[str] = Field(description=\"What are the various responsibilities of the agent which it needs to fulfill\")\n",
    "    examples: str = Field(description=\"What are some examples of the usage of the agent (input query and expected output from the agent) ?\")\n",
    "\n",
    "class AgentBuilderState(MessagesState):\n",
    "    \"\"\"\n",
    "    Represents the state of the main agent building graph.\n",
    "    Inherits from MessagesState to include a list of messages.\n",
    "    \"\"\"\n",
    "    agent_instructions: AgentInstructions = Field(description=\"The requirement analysis generated by the model.\")\n",
    "    python_code: str = Field(description=\"The Python code generated for the agent\")\n",
    "\n",
    "def requirement_analysis_node(state: AgentBuilderState) -> Command[Literal[\"requirement_analysis_node\", \"code_node\"]]:\n",
    "    \"\"\"\n",
    "    LangGraph node for performing requirement analysis.\n",
    "    It interacts with the LLM to gather agent specifications from the user.\n",
    "    If information is insufficient, it interrupts the graph for user input.\n",
    "    Otherwise, it proceeds to the code generation node.\n",
    "    \"\"\"\n",
    "    logger.info(\"Executing requirement_analysis_node\")\n",
    "    llm_with_tool = llm.bind_tools([AgentInstructions]) # Bind the AgentInstructions Pydantic model as a tool\n",
    "    \n",
    "    # Invoke the LLM with the system prompt and current message history\n",
    "    response = llm_with_tool.invoke([SystemMessage(content=REQ_ANALYSIS_PROMPT)] + state[\"messages\"])\n",
    "    \n",
    "    if not response.tool_calls:\n",
    "        logger.info(\"LLM requires more information or is in conversation.\")\n",
    "        value = interrupt(response.content)\n",
    "        return Command(goto=\"requirement_analysis_node\", update={\"messages\": [response, HumanMessage(content=value)]})\n",
    "        \n",
    "    logger.info(\"LLM successfully called AgentInstructions tool.\")\n",
    "    agent_instructions_args = response.tool_calls[0][\"args\"]\n",
    "    agent_instructions = AgentInstructions(**agent_instructions_args)\n",
    "    \n",
    "    return Command(\n",
    "        goto=\"code_node\",\n",
    "        update={\"messages\": [response], \"agent_instructions\": agent_instructions}\n",
    "    )\n",
    "\n",
    "# --- Agent Code Generation ---\n",
    "# This section focuses on generating the main Python code for the agent based on the gathered requirements.\n",
    "\n",
    "CODE_GEN_PROMPT = PromptTemplate.from_template(\"\"\"\n",
    "You are an expert Python programmer specializing in AI agent development via the Langgraph and Langchain SDK . Your primary task is to generate compilable, logical, and complete Python code for a LangGraph state graph based on user input below. You must prioritize LLM-based implementations for relevant tasks and consider advanced graph architectures.\n",
    "\n",
    "**Input:**\n",
    "<INPUT>\n",
    "<OBJECTIVE>\n",
    "{objective}\n",
    "</OBJECTIVE>\n",
    "<USECASES>\n",
    "{usecases}\n",
    "</USECASES>\n",
    "<EXAMPLES>\n",
    "{examples}\n",
    "</EXAMPLES>\n",
    "</INPUT>\n",
    "---\n",
    "**Phase 1: Evaluating best architecture for the given INPUT**\n",
    "\n",
    "1.  **Identify Potential Architectures:** Consider if the described INPUT aligns with or would benefit from known advanced LangGraph architectures such as:\n",
    "    * **Plan and Execute**: Does the INPUT imply an agent which might need a planning step (e.g., breaking down a complex task) followed by the execution of those plans by one or more action nodes?\n",
    "    * **Agent Supervisor / Hierarchical Agent Teams**: Is the INPUT best served by a supervisor agent dispatching tasks to specialized worker agents, or a hierarchy of agents making decisions and delegating?\n",
    "    * **Multi-Agent Collaboration (e.g., Swarm Architecture)**: Does the problem benefit from multiple agents working in parallel or collaboratively, perhaps sharing insights or contributing to a common goal?\n",
    "    * **Reflection / Self-Correction (e.g., Self-Discover frameworks)**: Are there indications of iterative refinement, where results are evaluated and the process is adjusted?\n",
    "    * **Human in the Loop (HITL)**: Does the `description` of any node, or the overall process, imply a need for human review, approval, correction, or explicit input at specific stages (e.g., before executing a critical action, when confidence is low, or for subjective assessments)?\n",
    "\n",
    "2.  **Architectural Decision:**\n",
    "    * If you determine that one or more of these architectures are strongly applicable to the INPUT, choose to implement it.\n",
    "    * If no specific advanced architecture seems directly applicable for the given INPUT, proceed with a standard stateful graph construction based on the explicit langgraph nodes and edges.\n",
    "\n",
    "3.  **Initial Comment:** At the very beginning of your generated Python script, include a comment block stating:\n",
    "    * Which LangGraph architecture(s) (if any) you've identified and chosen to implement, with a brief justification based on your interpretation of the INPUT, provide dry runs of the usecases/examples.\n",
    "    * If you are proceeding with a standard graph, mention that.\n",
    "\n",
    "4. Generate a JSON representation of the architecture you have chosen, including:\n",
    "a.  `nodes`: A dictionary where each key is a unique node ID. The value for each node ID is an object containing:\n",
    "    * `id`: The node's identifier.\n",
    "    * `schema_info`: A string describing the structure of the `GraphState` (e.g., \"GraphState:\\\\n type: TypedDict\\\\n fields:\\\\n - name: input\\\\n type: str...\"). You will need to parse this to define the `GraphState` TypedDict.\n",
    "    * `input_schema`: The expected input schema for the node (typically \"GraphState\").\n",
    "    * `output_schema`: The schema of the output produced by the node (typically \"GraphState\", indicating a partial update).\n",
    "    * `description`: A natural language description of what the node does. This is crucial for determining implementation strategy and overall architecture.\n",
    "    * `function_name`: The suggested Python function name for this node.\n",
    "    * `code` (optional): A string containing Python code for the node's function. **Treat this `code` primarily as an illustration or a very basic version. Prioritize LLM-based solutions if the `description` suggests a more robust approach is needed.**\n",
    "\n",
    "b.  `edges`: A list of objects, each describing a directed edge in the graph. Each edge object contains:\n",
    "    * `source`: The ID of the source node (or \"__START__\" for the graph's entry point).\n",
    "    * `target`: The ID of the target node (or \"__END__\" for a graph termination point).\n",
    "    * `routing_conditions`: A natural language description of the condition under which this edge is taken, especially for conditional edges.\n",
    "    * `conditional`: A boolean flag, `true` if the edge is part of a conditional branch, `false` otherwise.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "** Phase 2: Graph Creation\n",
    "**Phase 2: Python Code Generation**\n",
    "\n",
    "Generate a single, self-contained, and compilable Python script that implements your chosen strategy.\n",
    "\n",
    "1.  **Imports:** Include all necessary Python libraries (e.g., `typing`, `langgraph.graph`, `langgraph.checkpoint.memory`, LLM client libraries like `langchain_openai`, `langchain_google_genai`, `langchain_core.pydantic_v1`, `langchain_core.tools`, `re`).\n",
    "\n",
    "2.  **State Definition (`GraphState`):**\n",
    "    * Define a `GraphState` class using `MessagesState` (langgraph prebuilt class).\n",
    "\n",
    "3.  **Node Implementation (Python Functions):**\n",
    "    For each conceptual node in your chosen architecture (these may map directly to JSON you define):\n",
    "    * Create a Python function. This function must accept the `GraphState` and return a dictionary representing the partial update to the state.\n",
    "    * **Decision Logic for Implementation (Prioritize LLM, No Mock Data):**\n",
    "        * **Default to LLM-Based Solutions:** Your default stance should be to implement an **LLM-based solution** if the node's `description` (from JSON or your architectural design) suggests tasks like:\n",
    "            * Natural Language Understanding (NLU)\n",
    "            * Complex classification or routing\n",
    "            * Content generation or summarization\n",
    "            * Tool selection and usage\n",
    "            * Planning or complex decision-making.\n",
    "            * Any task where an LLM would provide more robust, flexible, or intelligent behavior than simple hardcoded logic.\n",
    "        * **Handling Provided `code`:** If `code` is present in the JSON for a node, treat it as a **low-priority hint or a simplistic example**. Do **not** simply copy it if an LLM approach is more appropriate for the described task.\n",
    "        * **Algorithmic Logic (Use Sparingly):** Only use purely algorithmic Python code (like from the `code` attribute or written new) if the node's task is genuinely simple, deterministic (e.g., basic data formatting, fixed calculation), *and* an LLM would offer no significant benefit for that specific, narrow function.\n",
    "        * **Functional LLM Calls:** When an LLM is used, instantiate a generic model (e.g., `llm = ChatOpenAI(model=\"gpt-3.5-turbo\")` or `llm = ChatGoogleGenerativeAI(model=\"gemini-pro\")`) and include a **functional, descriptive prompt** relevant to the node's task. Ensure the code for the LLM call is complete and not just a comment. Add a `TODO` comment for the user to specify API keys and potentially refine the model/prompt.\n",
    "        * **No Mock Data:** Generated functions must be logical and aim for completeness. **Avoid using mock data or overly simplistic placeholder logic** where an LLM or a proper algorithmic implementation is expected.\n",
    "        * **Structured Output & Tools:** If the task implies structured output from an LLM or the use of tools, define necessary Pydantic models and/or LangChain tools, and integrate them with the LLM call.\n",
    "            * Define a Pydantic model (e.g., `from langchain_core.pydantic_v1 import BaseModel, Field`) representing the desired structured output.\n",
    "            * If implementing an LLM call, configure it to use the Pydantic model for its output (e.g., with OpenAI's function calling/tool usage features, or by instructing the LLM to generate JSON conforming to the model).\n",
    "        * **Tool Definition and Usage:** If a node's `description` (or your architectural design) implies the LLM within that node needs to interact with external systems, perform specific actions, or fetch data (e.g., \"search customer database,\" \"get weather update\"):\n",
    "                * Define these capabilities as discrete LangChain tools using the `@tool` decorator (e.g., `from langchain_core.tools import tool`).\n",
    "                * **Crucially, each tool's internal Python function should be self-contained and directly perform its advertised action** (e.g., make a specific API call to an external service, run a local script, perform a calculation, retrieve data algorithmically). **Avoid embedding a *new, separate general-purpose LLM call within the tool's own implementation logic* unless the tool's explicit and documented purpose is to be a specialized, self-contained sub-agent (which is an advanced case).** The primary LLM within the graph node is responsible for *deciding to call* the tool and for interpreting its output.\n",
    "                * Bind these well-defined tools to the LLM instance operating within that graph node. The node's LLM will then intelligently decide when to call a tool and with what inputs.\n",
    "        * **Human in the Loop Nodes:** If you've designed a HITL step as a dedicated node, its function might primarily format data for human review and then process the subsequent human input (which would be added to the state, potentially by an external mechanism or a subsequent node). The graph might pause using an interruption mechanism tied to this node.\n",
    "        * **State Coherence:** Ensure variable assignments and updates within node functions are coherent with the `GraphState` definition and how state is managed in LangGraph.\n",
    "\n",
    "4.  **Graph Construction (`StatefulGraph`):**\n",
    "    * Instantiate `StatefulGraph(GraphState)`.\n",
    "    * Add each implemented node function to the graph using `graph.add_node(\"node_id\", node_function)`.\n",
    "    * Set the graph's entry point using `graph.add_edge(START, \"entry_node_id\")` where `\"entry_node_id\"` is the target of the edge originating from `\"__START__\"`.\n",
    "\n",
    "5.  **Edge Implementation:**\n",
    "    * Iterate through the `edges` list in the JSON.\n",
    "    * **Regular Edges:** If `conditional` is `false`:\n",
    "        * If `target` is `__END__`, use `graph.add_edge(source_node_id, END)`.\n",
    "        * Otherwise, use `graph.add_edge(source_node_id, target_node_id)`.\n",
    "    * **Conditional Edges:** If `conditional` is `true`:\n",
    "        * The `source` node of these conditional edges is expected to produce some output in the `GraphState` (e.g., an `intent` field) that determines the next path.\n",
    "        * Create a separate routing function (e.g., `def route_after_source_node(state: GraphState) -> str:`).\n",
    "        * This routing function must inspect the relevant fields in the `state` and return the string ID of the next node to execute, based on the logic described in the `routing_conditions` for each conditional edge originating from that source.\n",
    "        * Use `graph.add_conditional_edges(source_node_id, routing_function, {{ \"target_id_1\": \"target_id_1\", \"target_id_2\": \"target_id_2\", ... \"__END__\": END }})`. The keys in the dictionary are the possible return values from your routing function, and the values are the actual node IDs or `END`.\n",
    "\n",
    "6.  **Compilation:**\n",
    "    * Instantiate an `InMemoryCheckpointer`: `checkpointer = InMemoryCheckpointer()`.\n",
    "    * Compile the graph: `final_app = graph.compile(checkpointer=checkpointer)`. The compiled graph must be assigned to a variable named `final_app`.\n",
    "\n",
    "---\n",
    "**Phase 3: Required Keys/Credentials Identification**\n",
    "\n",
    "After generating the complete Python script, add a separate section at the end of your response, clearly titled:\n",
    "`## Required Keys and Credentials`\n",
    "\n",
    "In this section, list all environment variables or API keys a user would need to set for the generated code to execute successfully (e.g., `OPENAI_API_KEY`, `GOOGLE_API_KEY`, tool-specific keys). If no external keys are needed, state that.\n",
    "\n",
    "---\n",
    "**Important Considerations (General):**\n",
    "* The primary goal is **compilable, logical, and functionally plausible Python code** that intelligently interprets the JSON input.\n",
    "* Focus on creating a system that leverages LLMs effectively for tasks suited to them.\n",
    "* Ensure node functions correctly update and return relevant parts of the `GraphState`.\n",
    "* If the provided `code` in the JSON uses specific libraries (e.g., `re`), make sure the corresponding import is included at the top of the script.\n",
    "* Handle `__START__` and `__END__` correctly in edge definitions. `langgraph.graph.START` and `langgraph.graph.END` should be used.\n",
    "\n",
    "Please generate the Python code and the list of required keys now:\n",
    "\"\"\")\n",
    "\n",
    "def code_node(state: AgentBuilderState):\n",
    "    \"\"\"\n",
    "    LangGraph node to generate the final Python code for the agent.\n",
    "    It uses the gathered agent_instructions and the CODE_GEN_PROMPT.\n",
    "    \"\"\"\n",
    "    logger.info(\"Executing code_node\")\n",
    "    instructions: AgentInstructions = state[\"agent_instructions\"]\n",
    "    \n",
    "    # Invoke LLM to generate code based on the detailed prompt and instructions\n",
    "    code_output = llm.invoke([HumanMessage(content=CODE_GEN_PROMPT.format(\n",
    "        objective=instructions.objective,\n",
    "        usecases=instructions.usecases,\n",
    "        examples=instructions.examples\n",
    "    ))])\n",
    "    \n",
    "    logger.info(\"Python code generated by LLM.\")\n",
    "    # Return the generated Python code and an AI message\n",
    "    return {\n",
    "        \"messages\": [AIMessage(content=\"Generated final python code!\")],\n",
    "        \"python_code\": code_output.content,\n",
    "    }\n",
    "\n",
    "\n",
    "# --- Main Agent Generation Graph ---\n",
    "# This is the primary graph that orchestrates the entire agent building process.\n",
    "\n",
    "\n",
    "# --- Example Invocation (Optional) ---\n",
    "# This section can be used to demonstrate how to run the main graph.\n",
    "# For example:\n",
    "# if __name__ == \"__main__\":\n",
    "#     logger.info(\"Starting agent generation process...\")\n",
    "#     initial_input = {\"messages\": [HumanMessage(content=\"I want to build an agent that can tell me the weather.\")]}\n",
    "#     config = {\"configurable\": {\"thread_id\": \"user-thread-1\"}}\n",
    "    \n",
    "#     for event in agent_generator_graph.stream(initial_input, config=config):\n",
    "#         for key, value in event.items():\n",
    "#             logger.info(f\"Event from graph: {key} - {value}\")\n",
    "#             if key == \"requirement_analysis\" and isinstance(value, dict) and 'messages' in value:\n",
    "#                 last_message = value['messages'][-1]\n",
    "#                 if isinstance(last_message, HumanMessage) and last_message.content.startswith(\"__interrupt__\"):\n",
    "#                     # This is a simplified way to handle interruption for demo.\n",
    "#                     # In a real app, you'd present this to the user and get their input.\n",
    "#                     user_response = input(f\"Agent asks: {last_message.content[len('__interrupt__'):].strip()} Your response: \")\n",
    "#                     # How to reinvoke or continue with new input needs careful handling with stream/invoke\n",
    "#                     # For simplicity, this example doesn't fully implement the interactive loop here.\n",
    "#                     logger.info(f\"User input received: {user_response} (manual continuation needed for stream)\")\n",
    "\n",
    "\n",
    "#     final_state = agent_generator_graph.get_state(config)\n",
    "#     logger.info(\"\\n--- Final Generated Python Code ---\")\n",
    "#     print(final_state.values.get(\"python_code\"))\n",
    "#     logger.info(\"Agent generation process complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6dbeb933",
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_compile_prompt = \"\"\"\n",
    "You are an expert LangGraph code refactoring AI. Your task is to analyze the provided Python code for a LangGraph implementation and automatically correct it to ensure adherence to best practices, specifically concerning tool definition within nodes and the consistency of state objects.\n",
    "\n",
    "Please perform the following analysis and apply corrections directly to the code:\n",
    "\n",
    "Tool Definition and Usage in Nodes:\n",
    "\n",
    "Correct Invocation: Identify and fix any instances where tools called within graph nodes are not defined or invoked correctly according to LangGraph's protocols.\n",
    "Schema Adherence: Ensure that the inputs provided to tools and the outputs received from them strictly adhere to their defined schemas. Modify the code to align with these schemas if discrepancies are found.\n",
    "Tool Registration (if applicable): If tools are not correctly registered or made available to the nodes that intend to use them, update the code to ensure proper registration.\n",
    "Error Handling: Implement or improve error handling for tool execution failures within the nodes, making the graph more robust.\n",
    "State Object Management and Consistency:\n",
    "\n",
    "State Definition Review (and potential refinement): While the primary goal is to ensure consistency with the existing definition, if the state definition itself is unclear or problematic for achieving consistency, you may suggest minor refactorings to the state definition (clearly noting these changes).\n",
    "Node-State Interaction: For each node:\n",
    "Input State: Correct any instances where the node incorrectly accesses or misinterprets information from the input state.\n",
    "Output State: Rectify how the node updates the state object to ensure it's consistent with its defined purpose, the overall graph flow, and the state definition. Ensure all modifications are explicit and correct.\n",
    "Type Consistency: Enforce that the data types of values being read from and written to the state are consistent across different nodes and with the state definition. Apply necessary type conversions or corrections.\n",
    "Immutability (where applicable): If parts of the state are intended to be immutable but are modified, adjust the node logic to respect this or ensure modifications are handled through proper state update mechanisms.\n",
    "State Transitions: Refine the logic of state changes between nodes if it leads to inconsistencies or deviates from the intended graph objective.\n",
    "Overall Code Health (related to tools and state):\n",
    "\n",
    "Clarity and Readability: Refactor code related to tool usage and state manipulation to improve its clarity and readability, potentially by adding comments or restructuring logic.\n",
    "Modularity: If tool definitions or state interactions can be better encapsulated within their respective nodes for improved modularity, make these changes.\n",
    "Input:\n",
    "You will be provided with the Python code for the LangGraph implementation.\n",
    "\n",
    "Output:\n",
    "Provide the updated and corrected LangGraph Python code.\n",
    "input code:\n",
    "<input_code>\n",
    "{compiled_code}\n",
    "</input_code>\n",
    "\"\"\"\n",
    "def dfs_analysis_node(state: AgentBuilderState): # Renamed for clarity\n",
    "    \"\"\"\n",
    "    LangGraph node to analyse the code\n",
    "    \"\"\"\n",
    "    main_agent_code = state['python_code']\n",
    "    \n",
    "    # Use LLM to merge the main agent code with the generated tool definitions\n",
    "    response = llm.invoke([HumanMessage(content=analysis_compile_prompt.format(\n",
    "        compiled_code=main_agent_code,\n",
    "    ))])\n",
    "    \n",
    "    logger.info(\"Main agent code updated with fixes.\")\n",
    "    # The response from this LLM call is expected to be the final, complete Python code\n",
    "    return {\n",
    "        \"messages\": [AIMessage(content=response.content)], # Storing the LLM's final code as a message for now\n",
    "        \"python_code\": response.content # Update compiled_code with the final merged code\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "042547f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from langchain_core.language_models.chat_models import BaseChatModel\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "33cf274d",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = \"\"\"\n",
    "You are an expert software engineer.\n",
    "\n",
    "You will be given a langgraph code. You need to fix it and make it runnable.\n",
    "If you are not aware about some piece of code or a prebuilt function,use the get_langgraph_docs_index tool to get an index of the LangGraph docs first,\n",
    "then follow up with the get_request tool. Be persistent - if your first page does\n",
    "not result in confident information, keep digging!\n",
    "\n",
    "Make sure it is correct, complete, and executable without modification.\n",
    "Make sure that any generated code is contained in a properly formatted markdown code block.\n",
    "\n",
    "You can use the following URLs with your \"get_langgraph_docs_content\" tool to help answer questions:\n",
    "\n",
    "{langgraph_llms_txt}\n",
    "\"\"\"\n",
    "\n",
    "@tool\n",
    "def get_langgraph_docs_content(url: str) -> str:\n",
    "    \"\"\"Sends a get request to a webpage and returns plain text\n",
    "    extracted via BeautifulSoup.\"\"\"\n",
    "    res = requests.get(url).text\n",
    "    soup = BeautifulSoup(res, features=\"html.parser\")\n",
    "    return soup.get_text()\n",
    "\n",
    "def create_base_agent(model):\n",
    "    langgraph_llms_txt = requests.get(\n",
    "        \"https://langchain-ai.github.io/langgraph/llms.txt\"\n",
    "    ).text\n",
    "    return create_react_agent(\n",
    "        model=model,\n",
    "        tools=[get_langgraph_docs_content],\n",
    "        prompt=SYSTEM_PROMPT.format(langgraph_llms_txt=langgraph_llms_txt),\n",
    "    ).with_config(run_name=\"Base Agent\")\n",
    "\n",
    "def create_judge_graph(sandbox: Sandbox):\n",
    "    def run_reflection(state: dict) -> dict | None:\n",
    "        evaluator = create_e2b_pyright_evaluator(\n",
    "            sandbox=sandbox,\n",
    "            code_extraction_strategy=\"markdown_code_blocks\",\n",
    "        )\n",
    "\n",
    "        result = evaluator(outputs=state)\n",
    "\n",
    "        code_extraction_failed = result[\"metadata\"] and result[\"metadata\"].get(\n",
    "            \"code_extraction_failed\"\n",
    "        )\n",
    "\n",
    "        if not result[\"score\"] and not code_extraction_failed:\n",
    "            return {\n",
    "                \"messages\": [\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": f\"I ran pyright and found some problems with the code you generated: {result['comment']}\\n\\n\"\n",
    "                        \"Try to fix it. Make sure to regenerate the entire code snippet. \"\n",
    "                        \"If you are not sure what is wrong, search for more information by pulling more information \"\n",
    "                        \"from the LangGraph docs.\",\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "\n",
    "    return (\n",
    "        StateGraph(MessagesState)\n",
    "        .add_node(\"run_reflection\", run_reflection)\n",
    "        .add_edge(\"__start__\", \"run_reflection\")\n",
    "        .compile()\n",
    "    ).with_config(run_name=\"Judge Agent\")\n",
    "\n",
    "_GLOBAL_SANDBOX = None\n",
    "\n",
    "def get_or_create_sandbox():\n",
    "    global _GLOBAL_SANDBOX\n",
    "    if _GLOBAL_SANDBOX is None:\n",
    "        _GLOBAL_SANDBOX = Sandbox(\"OpenEvalsPython\")\n",
    "    return _GLOBAL_SANDBOX\n",
    "\n",
    "def create_reflection_agent(config: RunnableConfig = None):\n",
    "    if config is None:\n",
    "        config = {}\n",
    "    configurable = config.get(\"configurable\", {})\n",
    "    sandbox = configurable.get(\"sandbox\", None)\n",
    "    model = configurable.get(\"model\", None)\n",
    "    if sandbox is None:\n",
    "        sandbox = get_or_create_sandbox()\n",
    "    if model is None:\n",
    "        model = init_chat_model(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            max_tokens=4096,\n",
    "        )\n",
    "    judge = create_judge_graph(sandbox)\n",
    "    return (\n",
    "        create_reflection_graph(create_base_agent(model), judge, MessagesState)\n",
    "        .compile()\n",
    "        .with_config(run_name=\"Mini Chat LangChain\")\n",
    "    )\n",
    "\n",
    "def code_reflection_node_updated(state: AgentBuilderState):\n",
    "    \"\"\"\n",
    "    LangGraph node to run code reflection and fixing using E2B sandbox - updated for AgentBuilderState\n",
    "    \"\"\"\n",
    "    logger.info(\"Executing code_reflection_node to test and fix code in E2B sandbox.\")\n",
    "    python_code = state['python_code']\n",
    "    \n",
    "    # Create reflection agent\n",
    "    reflection_agent = create_reflection_agent({})\n",
    "    \n",
    "    # Run the reflection agent with the generated code\n",
    "    result = reflection_agent.invoke({\n",
    "        \"messages\": [HumanMessage(content=f\"Please review and fix this LangGraph code to make it compilable and runnable:\\n\\n```python\\n{python_code}\\n```\")]\n",
    "    })\n",
    "    \n",
    "    # Extract the final message content as the fixed code\n",
    "    final_message = result[\"messages\"][-1]\n",
    "    fixed_code = final_message.content if hasattr(final_message, 'content') else str(final_message)\n",
    "    \n",
    "    logger.info(\"Code reflection and fixing completed.\")\n",
    "    return {\n",
    "        \"messages\": [AIMessage(content=\"Code has been tested and fixed using E2B sandbox reflection.\")],\n",
    "        \"python_code\": fixed_code\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f2b87204",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-05 22:08:41,435 - INFO - Main agent generator graph compiled.\n"
     ]
    }
   ],
   "source": [
    "main_workflow = StateGraph(AgentBuilderState)\n",
    "\n",
    "# Add all nodes to the main workflow\n",
    "main_workflow.add_node(\"requirement_analysis_node\", requirement_analysis_node)\n",
    "main_workflow.add_node(\"code_node\", code_node)\n",
    "main_workflow.add_node(\"tool_subgraph_processing\", tool_compile_graph)\n",
    "main_workflow.add_node(\"dfs_analysis_node\", dfs_analysis_node)\n",
    "main_workflow.add_node(\"code_reflection_node\", code_reflection_node_updated)\n",
    "\n",
    "# Define the updated workflow edges\n",
    "main_workflow.add_edge(START, \"requirement_analysis_node\")\n",
    "main_workflow.add_edge(\"requirement_analysis_node\", \"code_node\")\n",
    "main_workflow.add_edge(\"code_node\", \"tool_subgraph_processing\")\n",
    "main_workflow.add_edge(\"tool_subgraph_processing\", \"dfs_analysis_node\")\n",
    "main_workflow.add_edge(\"dfs_analysis_node\", \"code_reflection_node\")\n",
    "main_workflow.add_edge(\"code_reflection_node\", END)\n",
    "agent_generator_graph = main_workflow.compile()\n",
    "logger.info(\"Main agent generator graph compiled.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c9f76f3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-05 22:08:51,151 - INFO - Executing requirement_analysis_node\n",
      "2025-06-05 22:08:53,210 - INFO - LLM successfully called AgentInstructions tool.\n",
      "2025-06-05 22:08:53,218 - INFO - Executing code_node\n",
      "2025-06-05 22:10:27,781 - INFO - Python code generated by LLM.\n",
      "2025-06-05 22:10:27,798 - INFO - Executing graph_map_step to identify and generate tool implementations.\n",
      "2025-06-05 22:10:33,081 - INFO - Generating implementation for tool: search_social_media - Description: Searches social media platforms for trending topic...\n",
      "2025-06-05 22:10:33,084 - INFO - Generating implementation for tool: search_influencers - Description: Searches for influencers or relevant individuals i...\n",
      "2025-06-05 22:10:33,095 - INFO - Executing functional_analysis_node for: Searches social media platforms for trending topic...\n",
      "2025-06-05 22:10:33,119 - INFO - Executing functional_analysis_node for: Searches for influencers or relevant individuals i...\n",
      "2025-06-05 22:10:36,802 - INFO - Functional analysis complete for search_influencers_by_topic.\n",
      "2025-06-05 22:10:36,810 - INFO - Executing sdk_production_node for function: search_influencers_by_topic\n",
      "2025-06-05 22:10:39,352 - INFO - Functional analysis complete for search_social_media.\n",
      "2025-06-05 22:10:39,359 - INFO - Executing sdk_production_node for function: search_social_media\n",
      "2025-06-05 22:11:06,519 - INFO - SDK identified: google_search for function search_influencers_by_topic\n",
      "2025-06-05 22:11:06,526 - INFO - Executing code_production_node for function: search_influencers_by_topic using SDK: google_search\n",
      "2025-06-05 22:11:07,982 - INFO - HTTP Request: GET https://python.langchain.com/docs/integrations/tools/google_search \"HTTP/1.1 308 Permanent Redirect\"\n",
      "2025-06-05 22:11:08,037 - INFO - HTTP Request: GET https://python.langchain.com/docs/integrations/tools/google_search/ \"HTTP/1.1 200 OK\"\n",
      "2025-06-05 22:11:08,447 - INFO - Fetched documentation for SDK: google_search\n",
      "2025-06-05 22:11:23,611 - INFO - SDK identified: tavily_search for function search_social_media\n",
      "2025-06-05 22:11:23,618 - INFO - Executing code_production_node for function: search_social_media using SDK: tavily_search\n",
      "2025-06-05 22:11:25,059 - INFO - HTTP Request: GET https://python.langchain.com/docs/integrations/tools/tavily_search \"HTTP/1.1 308 Permanent Redirect\"\n",
      "2025-06-05 22:11:25,420 - INFO - HTTP Request: GET https://python.langchain.com/docs/integrations/tools/tavily_search/ \"HTTP/1.1 200 OK\"\n",
      "2025-06-05 22:11:25,864 - INFO - Fetched documentation for SDK: tavily_search\n",
      "2025-06-05 22:11:45,124 - INFO - Code generated for function: search_social_media\n",
      "2025-06-05 22:15:14,605 - INFO - Code generated for function: search_influencers_by_topic\n",
      "2025-06-05 22:15:14,624 - INFO - Executing compile_tool_code_node to merge tool codes with main agent code.\n",
      "2025-06-05 22:15:52,509 - INFO - Main agent code compiled with tool function definitions.\n",
      "2025-06-05 22:16:58,251 - INFO - Main agent code updated with fixes.\n",
      "2025-06-05 22:16:58,260 - INFO - Executing code_reflection_node to test and fix code in E2B sandbox.\n",
      "2025-06-05 22:16:59,573 - INFO - Request POST https://api.e2b.app/sandboxes\n",
      "2025-06-05 22:17:00,419 - INFO - HTTP Request: POST https://api.e2b.app/sandboxes \"HTTP/1.1 201 Created\"\n",
      "2025-06-05 22:17:00,422 - INFO - Response 201\n",
      "2025-06-05 22:18:12,775 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-06-05 22:18:13,723 - INFO - Request: POST https://49983-ieqy2iax4laybt337gjtz-fa354746.e2b.app/files\n",
      "2025-06-05 22:18:14,208 - INFO - Response: 200 https://49983-ieqy2iax4laybt337gjtz-fa354746.e2b.app/files\n",
      "2025-06-05 22:18:14,211 - INFO - HTTP Request: POST https://49983-ieqy2iax4laybt337gjtz-fa354746.e2b.app/files?username=user&path=openevals%2Foutputs.py \"HTTP/1.1 200 OK\"\n",
      "2025-06-05 22:18:26,802 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-06-05 22:19:16,774 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-06-05 22:19:16,837 - INFO - Request: POST https://49983-ieqy2iax4laybt337gjtz-fa354746.e2b.app/files\n",
      "2025-06-05 22:19:17,080 - INFO - Response: 200 https://49983-ieqy2iax4laybt337gjtz-fa354746.e2b.app/files\n",
      "2025-06-05 22:19:17,083 - INFO - HTTP Request: POST https://49983-ieqy2iax4laybt337gjtz-fa354746.e2b.app/files?username=user&path=openevals%2Foutputs.py \"HTTP/1.1 200 OK\"\n",
      "2025-06-05 22:19:23,020 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-06-05 22:20:08,691 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-06-05 22:20:08,747 - INFO - Request: POST https://49983-ieqy2iax4laybt337gjtz-fa354746.e2b.app/files\n",
      "2025-06-05 22:20:09,099 - INFO - Response: 200 https://49983-ieqy2iax4laybt337gjtz-fa354746.e2b.app/files\n",
      "2025-06-05 22:20:09,103 - INFO - HTTP Request: POST https://49983-ieqy2iax4laybt337gjtz-fa354746.e2b.app/files?username=user&path=openevals%2Foutputs.py \"HTTP/1.1 200 OK\"\n",
      "2025-06-05 22:20:13,605 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-06-05 22:21:07,265 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-06-05 22:21:07,416 - INFO - Request: POST https://49983-ieqy2iax4laybt337gjtz-fa354746.e2b.app/files\n",
      "2025-06-05 22:21:07,782 - INFO - Response: 200 https://49983-ieqy2iax4laybt337gjtz-fa354746.e2b.app/files\n",
      "2025-06-05 22:21:07,784 - INFO - HTTP Request: POST https://49983-ieqy2iax4laybt337gjtz-fa354746.e2b.app/files?username=user&path=openevals%2Foutputs.py \"HTTP/1.1 200 OK\"\n",
      "2025-06-05 22:21:12,400 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-06-05 22:22:02,264 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-06-05 22:22:02,634 - INFO - Request: POST https://49983-ieqy2iax4laybt337gjtz-fa354746.e2b.app/files\n",
      "2025-06-05 22:22:02,975 - INFO - Response: 502 https://49983-ieqy2iax4laybt337gjtz-fa354746.e2b.app/files\n",
      "2025-06-05 22:22:02,978 - INFO - HTTP Request: POST https://49983-ieqy2iax4laybt337gjtz-fa354746.e2b.app/files?username=user&path=openevals%2Foutputs.py \"HTTP/1.1 502 Bad Gateway\"\n"
     ]
    },
    {
     "ename": "TimeoutException",
     "evalue": "The sandbox was not found: This error is likely due to sandbox timeout. You can modify the sandbox timeout by passing 'timeoutMs' when starting the sandbox or calling '.setTimeout' on the sandbox with the desired timeout.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTimeoutException\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m query= \u001b[33m\"\u001b[39m\u001b[33m need to create a worklow with the objective of managing my social media.  It should be able to tell me trends from social media for sports, get me all the relevant people I should contant for a spsonsoring a specific post,  suggest me content I should be posting, make content for me if I give it a description * Identifying social media trends in sports. *   Finding relevant people for sponsoring specific posts. *   Suggesting content to post. *   Generating content based on a description.  Examples: Given I ask it about trends in football, it goes through recent viral reels in football and tell me Q&A reels are trending If I tell it I want people who would be interested in a post about UCL football, it gives me current players like Dembele etc If I tell it I want to make a post about UCL football, it goes through what is trending and an agent that thinks about social media posts and tells me we should post a highlight reel\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      3\u001b[39m config = {\u001b[33m\"\u001b[39m\u001b[33mconfigurable\u001b[39m\u001b[33m\"\u001b[39m: {\u001b[33m\"\u001b[39m\u001b[33mthread_id\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33msome_id\u001b[39m\u001b[33m\"\u001b[39m}}\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m result = \u001b[43magent_generator_graph\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mHumanMessage\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m \n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\langgraph\\pregel\\__init__.py:2739\u001b[39m, in \u001b[36mPregel.invoke\u001b[39m\u001b[34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, checkpoint_during, debug, **kwargs)\u001b[39m\n\u001b[32m   2737\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2738\u001b[39m     chunks = []\n\u001b[32m-> \u001b[39m\u001b[32m2739\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2740\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   2741\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2742\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2743\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_keys\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2744\u001b[39m \u001b[43m    \u001b[49m\u001b[43minterrupt_before\u001b[49m\u001b[43m=\u001b[49m\u001b[43minterrupt_before\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2745\u001b[39m \u001b[43m    \u001b[49m\u001b[43minterrupt_after\u001b[49m\u001b[43m=\u001b[49m\u001b[43minterrupt_after\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2746\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcheckpoint_during\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcheckpoint_during\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2747\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdebug\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdebug\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2748\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2749\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   2750\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvalues\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\n\u001b[32m   2751\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlatest\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\langgraph\\pregel\\__init__.py:2377\u001b[39m, in \u001b[36mPregel.stream\u001b[39m\u001b[34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, checkpoint_during, debug, subgraphs)\u001b[39m\n\u001b[32m   2371\u001b[39m     \u001b[38;5;66;03m# Similarly to Bulk Synchronous Parallel / Pregel model\u001b[39;00m\n\u001b[32m   2372\u001b[39m     \u001b[38;5;66;03m# computation proceeds in steps, while there are channel updates.\u001b[39;00m\n\u001b[32m   2373\u001b[39m     \u001b[38;5;66;03m# Channel updates from step N are only visible in step N+1\u001b[39;00m\n\u001b[32m   2374\u001b[39m     \u001b[38;5;66;03m# channels are guaranteed to be immutable for the duration of the step,\u001b[39;00m\n\u001b[32m   2375\u001b[39m     \u001b[38;5;66;03m# with channel updates applied only at the transition between steps.\u001b[39;00m\n\u001b[32m   2376\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m loop.tick(input_keys=\u001b[38;5;28mself\u001b[39m.input_channels):\n\u001b[32m-> \u001b[39m\u001b[32m2377\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrunner\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtick\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2378\u001b[39m \u001b[43m            \u001b[49m\u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtasks\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2379\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstep_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2380\u001b[39m \u001b[43m            \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2381\u001b[39m \u001b[43m            \u001b[49m\u001b[43mget_waiter\u001b[49m\u001b[43m=\u001b[49m\u001b[43mget_waiter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2382\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   2383\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# emit output\u001b[39;49;00m\n\u001b[32m   2384\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01myield from\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2385\u001b[39m \u001b[38;5;66;03m# emit output\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\langgraph\\pregel\\runner.py:158\u001b[39m, in \u001b[36mPregelRunner.tick\u001b[39m\u001b[34m(self, tasks, reraise, timeout, retry_policy, get_waiter)\u001b[39m\n\u001b[32m    156\u001b[39m t = tasks[\u001b[32m0\u001b[39m]\n\u001b[32m    157\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m158\u001b[39m     \u001b[43mrun_with_retry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    159\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    160\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    161\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfigurable\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    162\u001b[39m \u001b[43m            \u001b[49m\u001b[43mCONFIG_KEY_CALL\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    163\u001b[39m \u001b[43m                \u001b[49m\u001b[43m_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    164\u001b[39m \u001b[43m                \u001b[49m\u001b[43mweakref\u001b[49m\u001b[43m.\u001b[49m\u001b[43mref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    165\u001b[39m \u001b[43m                \u001b[49m\u001b[43mretry\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    166\u001b[39m \u001b[43m                \u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweakref\u001b[49m\u001b[43m.\u001b[49m\u001b[43mref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    167\u001b[39m \u001b[43m                \u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    168\u001b[39m \u001b[43m                \u001b[49m\u001b[43msubmit\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msubmit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    169\u001b[39m \u001b[43m                \u001b[49m\u001b[43mreraise\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreraise\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    170\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    171\u001b[39m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    172\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    173\u001b[39m     \u001b[38;5;28mself\u001b[39m.commit(t, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    174\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\langgraph\\pregel\\retry.py:39\u001b[39m, in \u001b[36mrun_with_retry\u001b[39m\u001b[34m(task, retry_policy, configurable)\u001b[39m\n\u001b[32m     37\u001b[39m     task.writes.clear()\n\u001b[32m     38\u001b[39m     \u001b[38;5;66;03m# run the task\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m39\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtask\u001b[49m\u001b[43m.\u001b[49m\u001b[43mproc\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m.\u001b[49m\u001b[43minput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     40\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ParentCommand \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m     41\u001b[39m     ns: \u001b[38;5;28mstr\u001b[39m = config[CONF][CONFIG_KEY_CHECKPOINT_NS]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\langgraph\\utils\\runnable.py:622\u001b[39m, in \u001b[36mRunnableSeq.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    620\u001b[39m     \u001b[38;5;66;03m# run in context\u001b[39;00m\n\u001b[32m    621\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(config, run) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[32m--> \u001b[39m\u001b[32m622\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    623\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    624\u001b[39m     \u001b[38;5;28minput\u001b[39m = step.invoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\langgraph\\utils\\runnable.py:376\u001b[39m, in \u001b[36mRunnableCallable.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    374\u001b[39m         run_manager.on_chain_end(ret)\n\u001b[32m    375\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m376\u001b[39m     ret = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    377\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.recurse \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, Runnable):\n\u001b[32m    378\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ret.invoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 107\u001b[39m, in \u001b[36mcode_reflection_node_updated\u001b[39m\u001b[34m(state)\u001b[39m\n\u001b[32m    104\u001b[39m reflection_agent = create_reflection_agent({})\n\u001b[32m    106\u001b[39m \u001b[38;5;66;03m# Run the reflection agent with the generated code\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m107\u001b[39m result = \u001b[43mreflection_agent\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    108\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mHumanMessage\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mPlease review and fix this LangGraph code to make it compilable and runnable:\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m```python\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mpython_code\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m```\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m    109\u001b[39m \u001b[43m\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    111\u001b[39m \u001b[38;5;66;03m# Extract the final message content as the fixed code\u001b[39;00m\n\u001b[32m    112\u001b[39m final_message = result[\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m][-\u001b[32m1\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\langgraph\\pregel\\__init__.py:2739\u001b[39m, in \u001b[36mPregel.invoke\u001b[39m\u001b[34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, checkpoint_during, debug, **kwargs)\u001b[39m\n\u001b[32m   2737\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2738\u001b[39m     chunks = []\n\u001b[32m-> \u001b[39m\u001b[32m2739\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2740\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   2741\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2742\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2743\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_keys\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2744\u001b[39m \u001b[43m    \u001b[49m\u001b[43minterrupt_before\u001b[49m\u001b[43m=\u001b[49m\u001b[43minterrupt_before\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2745\u001b[39m \u001b[43m    \u001b[49m\u001b[43minterrupt_after\u001b[49m\u001b[43m=\u001b[49m\u001b[43minterrupt_after\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2746\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcheckpoint_during\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcheckpoint_during\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2747\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdebug\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdebug\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2748\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2749\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   2750\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvalues\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\n\u001b[32m   2751\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlatest\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\langgraph\\pregel\\__init__.py:2377\u001b[39m, in \u001b[36mPregel.stream\u001b[39m\u001b[34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, checkpoint_during, debug, subgraphs)\u001b[39m\n\u001b[32m   2371\u001b[39m     \u001b[38;5;66;03m# Similarly to Bulk Synchronous Parallel / Pregel model\u001b[39;00m\n\u001b[32m   2372\u001b[39m     \u001b[38;5;66;03m# computation proceeds in steps, while there are channel updates.\u001b[39;00m\n\u001b[32m   2373\u001b[39m     \u001b[38;5;66;03m# Channel updates from step N are only visible in step N+1\u001b[39;00m\n\u001b[32m   2374\u001b[39m     \u001b[38;5;66;03m# channels are guaranteed to be immutable for the duration of the step,\u001b[39;00m\n\u001b[32m   2375\u001b[39m     \u001b[38;5;66;03m# with channel updates applied only at the transition between steps.\u001b[39;00m\n\u001b[32m   2376\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m loop.tick(input_keys=\u001b[38;5;28mself\u001b[39m.input_channels):\n\u001b[32m-> \u001b[39m\u001b[32m2377\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrunner\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtick\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2378\u001b[39m \u001b[43m            \u001b[49m\u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtasks\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2379\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstep_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2380\u001b[39m \u001b[43m            \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2381\u001b[39m \u001b[43m            \u001b[49m\u001b[43mget_waiter\u001b[49m\u001b[43m=\u001b[49m\u001b[43mget_waiter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2382\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   2383\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# emit output\u001b[39;49;00m\n\u001b[32m   2384\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01myield from\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2385\u001b[39m \u001b[38;5;66;03m# emit output\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\langgraph\\pregel\\runner.py:158\u001b[39m, in \u001b[36mPregelRunner.tick\u001b[39m\u001b[34m(self, tasks, reraise, timeout, retry_policy, get_waiter)\u001b[39m\n\u001b[32m    156\u001b[39m t = tasks[\u001b[32m0\u001b[39m]\n\u001b[32m    157\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m158\u001b[39m     \u001b[43mrun_with_retry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    159\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    160\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    161\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfigurable\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    162\u001b[39m \u001b[43m            \u001b[49m\u001b[43mCONFIG_KEY_CALL\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    163\u001b[39m \u001b[43m                \u001b[49m\u001b[43m_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    164\u001b[39m \u001b[43m                \u001b[49m\u001b[43mweakref\u001b[49m\u001b[43m.\u001b[49m\u001b[43mref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    165\u001b[39m \u001b[43m                \u001b[49m\u001b[43mretry\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    166\u001b[39m \u001b[43m                \u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweakref\u001b[49m\u001b[43m.\u001b[49m\u001b[43mref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    167\u001b[39m \u001b[43m                \u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    168\u001b[39m \u001b[43m                \u001b[49m\u001b[43msubmit\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msubmit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    169\u001b[39m \u001b[43m                \u001b[49m\u001b[43mreraise\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreraise\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    170\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    171\u001b[39m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    172\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    173\u001b[39m     \u001b[38;5;28mself\u001b[39m.commit(t, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    174\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\langgraph\\pregel\\retry.py:39\u001b[39m, in \u001b[36mrun_with_retry\u001b[39m\u001b[34m(task, retry_policy, configurable)\u001b[39m\n\u001b[32m     37\u001b[39m     task.writes.clear()\n\u001b[32m     38\u001b[39m     \u001b[38;5;66;03m# run the task\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m39\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtask\u001b[49m\u001b[43m.\u001b[49m\u001b[43mproc\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m.\u001b[49m\u001b[43minput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     40\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ParentCommand \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m     41\u001b[39m     ns: \u001b[38;5;28mstr\u001b[39m = config[CONF][CONFIG_KEY_CHECKPOINT_NS]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\langgraph\\utils\\runnable.py:622\u001b[39m, in \u001b[36mRunnableSeq.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    620\u001b[39m     \u001b[38;5;66;03m# run in context\u001b[39;00m\n\u001b[32m    621\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(config, run) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[32m--> \u001b[39m\u001b[32m622\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    623\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    624\u001b[39m     \u001b[38;5;28minput\u001b[39m = step.invoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\langgraph\\pregel\\__init__.py:2739\u001b[39m, in \u001b[36mPregel.invoke\u001b[39m\u001b[34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, checkpoint_during, debug, **kwargs)\u001b[39m\n\u001b[32m   2737\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2738\u001b[39m     chunks = []\n\u001b[32m-> \u001b[39m\u001b[32m2739\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2740\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   2741\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2742\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2743\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_keys\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2744\u001b[39m \u001b[43m    \u001b[49m\u001b[43minterrupt_before\u001b[49m\u001b[43m=\u001b[49m\u001b[43minterrupt_before\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2745\u001b[39m \u001b[43m    \u001b[49m\u001b[43minterrupt_after\u001b[49m\u001b[43m=\u001b[49m\u001b[43minterrupt_after\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2746\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcheckpoint_during\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcheckpoint_during\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2747\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdebug\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdebug\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2748\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2749\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   2750\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvalues\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\n\u001b[32m   2751\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlatest\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\langgraph\\pregel\\__init__.py:2377\u001b[39m, in \u001b[36mPregel.stream\u001b[39m\u001b[34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, checkpoint_during, debug, subgraphs)\u001b[39m\n\u001b[32m   2371\u001b[39m     \u001b[38;5;66;03m# Similarly to Bulk Synchronous Parallel / Pregel model\u001b[39;00m\n\u001b[32m   2372\u001b[39m     \u001b[38;5;66;03m# computation proceeds in steps, while there are channel updates.\u001b[39;00m\n\u001b[32m   2373\u001b[39m     \u001b[38;5;66;03m# Channel updates from step N are only visible in step N+1\u001b[39;00m\n\u001b[32m   2374\u001b[39m     \u001b[38;5;66;03m# channels are guaranteed to be immutable for the duration of the step,\u001b[39;00m\n\u001b[32m   2375\u001b[39m     \u001b[38;5;66;03m# with channel updates applied only at the transition between steps.\u001b[39;00m\n\u001b[32m   2376\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m loop.tick(input_keys=\u001b[38;5;28mself\u001b[39m.input_channels):\n\u001b[32m-> \u001b[39m\u001b[32m2377\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrunner\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtick\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2378\u001b[39m \u001b[43m            \u001b[49m\u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtasks\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2379\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstep_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2380\u001b[39m \u001b[43m            \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2381\u001b[39m \u001b[43m            \u001b[49m\u001b[43mget_waiter\u001b[49m\u001b[43m=\u001b[49m\u001b[43mget_waiter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2382\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   2383\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# emit output\u001b[39;49;00m\n\u001b[32m   2384\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01myield from\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2385\u001b[39m \u001b[38;5;66;03m# emit output\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\langgraph\\pregel\\runner.py:158\u001b[39m, in \u001b[36mPregelRunner.tick\u001b[39m\u001b[34m(self, tasks, reraise, timeout, retry_policy, get_waiter)\u001b[39m\n\u001b[32m    156\u001b[39m t = tasks[\u001b[32m0\u001b[39m]\n\u001b[32m    157\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m158\u001b[39m     \u001b[43mrun_with_retry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    159\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    160\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    161\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfigurable\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    162\u001b[39m \u001b[43m            \u001b[49m\u001b[43mCONFIG_KEY_CALL\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    163\u001b[39m \u001b[43m                \u001b[49m\u001b[43m_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    164\u001b[39m \u001b[43m                \u001b[49m\u001b[43mweakref\u001b[49m\u001b[43m.\u001b[49m\u001b[43mref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    165\u001b[39m \u001b[43m                \u001b[49m\u001b[43mretry\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    166\u001b[39m \u001b[43m                \u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweakref\u001b[49m\u001b[43m.\u001b[49m\u001b[43mref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    167\u001b[39m \u001b[43m                \u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    168\u001b[39m \u001b[43m                \u001b[49m\u001b[43msubmit\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msubmit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    169\u001b[39m \u001b[43m                \u001b[49m\u001b[43mreraise\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreraise\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    170\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    171\u001b[39m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    172\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    173\u001b[39m     \u001b[38;5;28mself\u001b[39m.commit(t, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    174\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\langgraph\\pregel\\retry.py:39\u001b[39m, in \u001b[36mrun_with_retry\u001b[39m\u001b[34m(task, retry_policy, configurable)\u001b[39m\n\u001b[32m     37\u001b[39m     task.writes.clear()\n\u001b[32m     38\u001b[39m     \u001b[38;5;66;03m# run the task\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m39\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtask\u001b[49m\u001b[43m.\u001b[49m\u001b[43mproc\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m.\u001b[49m\u001b[43minput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     40\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ParentCommand \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m     41\u001b[39m     ns: \u001b[38;5;28mstr\u001b[39m = config[CONF][CONFIG_KEY_CHECKPOINT_NS]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\langgraph\\utils\\runnable.py:622\u001b[39m, in \u001b[36mRunnableSeq.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    620\u001b[39m     \u001b[38;5;66;03m# run in context\u001b[39;00m\n\u001b[32m    621\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(config, run) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[32m--> \u001b[39m\u001b[32m622\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    623\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    624\u001b[39m     \u001b[38;5;28minput\u001b[39m = step.invoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\langgraph\\utils\\runnable.py:376\u001b[39m, in \u001b[36mRunnableCallable.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    374\u001b[39m         run_manager.on_chain_end(ret)\n\u001b[32m    375\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m376\u001b[39m     ret = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    377\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.recurse \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, Runnable):\n\u001b[32m    378\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ret.invoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 42\u001b[39m, in \u001b[36mcreate_judge_graph.<locals>.run_reflection\u001b[39m\u001b[34m(state)\u001b[39m\n\u001b[32m     36\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrun_reflection\u001b[39m(state: \u001b[38;5;28mdict\u001b[39m) -> \u001b[38;5;28mdict\u001b[39m | \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     37\u001b[39m     evaluator = create_e2b_pyright_evaluator(\n\u001b[32m     38\u001b[39m         sandbox=sandbox,\n\u001b[32m     39\u001b[39m         code_extraction_strategy=\u001b[33m\"\u001b[39m\u001b[33mmarkdown_code_blocks\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     40\u001b[39m     )\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m     result = \u001b[43mevaluator\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     44\u001b[39m     code_extraction_failed = result[\u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m] \u001b[38;5;129;01mand\u001b[39;00m result[\u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m].get(\n\u001b[32m     45\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mcode_extraction_failed\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     46\u001b[39m     )\n\u001b[32m     48\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m result[\u001b[33m\"\u001b[39m\u001b[33mscore\u001b[39m\u001b[33m\"\u001b[39m] \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m code_extraction_failed:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\openevals\\code\\base.py:173\u001b[39m, in \u001b[36m_create_base_code_evaluator.<locals>._wrapped_evaluator\u001b[39m\u001b[34m(inputs, outputs, reference_outputs, **kwargs)\u001b[39m\n\u001b[32m    167\u001b[39m         normalized_outputs = code_extractor(outputs)\n\u001b[32m    168\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m scorer(\n\u001b[32m    169\u001b[39m         outputs=normalized_outputs,\n\u001b[32m    170\u001b[39m         **kwargs,\n\u001b[32m    171\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m173\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_run_evaluator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    174\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    175\u001b[39m \u001b[43m    \u001b[49m\u001b[43mscorer\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_score_wrapper\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    176\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfeedback_key\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfeedback_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    177\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    178\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    179\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreference_outputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreference_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    180\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    181\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\openevals\\utils.py:102\u001b[39m, in \u001b[36m_run_evaluator\u001b[39m\u001b[34m(run_name, scorer, feedback_key, ls_framework, **kwargs)\u001b[39m\n\u001b[32m     94\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_run_evaluator\u001b[39m(\n\u001b[32m     95\u001b[39m     *,\n\u001b[32m     96\u001b[39m     run_name: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    100\u001b[39m     **kwargs: Any,\n\u001b[32m    101\u001b[39m ) -> Union[EvaluatorResult, \u001b[38;5;28mlist\u001b[39m[EvaluatorResult]]:\n\u001b[32m--> \u001b[39m\u001b[32m102\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_run_evaluator_untyped\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore\u001b[39;49;00m\n\u001b[32m    103\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    104\u001b[39m \u001b[43m        \u001b[49m\u001b[43mscorer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mscorer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    105\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfeedback_key\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfeedback_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    106\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_raw_outputs\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    107\u001b[39m \u001b[43m        \u001b[49m\u001b[43mls_framework\u001b[49m\u001b[43m=\u001b[49m\u001b[43mls_framework\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    108\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    109\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\openevals\\utils.py:193\u001b[39m, in \u001b[36m_run_evaluator_untyped\u001b[39m\u001b[34m(run_name, scorer, feedback_key, return_raw_outputs, ls_framework, **kwargs)\u001b[39m\n\u001b[32m    187\u001b[39m                 t.log_feedback(\n\u001b[32m    188\u001b[39m                     key=results[\u001b[33m\"\u001b[39m\u001b[33mkey\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m    189\u001b[39m                     score=results[\u001b[33m\"\u001b[39m\u001b[33mscore\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m    190\u001b[39m                     comment=results[\u001b[33m\"\u001b[39m\u001b[33mcomment\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m    191\u001b[39m                 )\n\u001b[32m    192\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m193\u001b[39m     results = \u001b[43m_run_scorer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    194\u001b[39m     _add_metadata_to_run_tree(run_name, ls_framework, results)\n\u001b[32m    196\u001b[39m \u001b[38;5;66;03m# Return single result or list of results\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\langsmith\\run_helpers.py:635\u001b[39m, in \u001b[36mtraceable.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(langsmith_extra, *args, **kwargs)\u001b[39m\n\u001b[32m    633\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m func_accepts_parent_run:\n\u001b[32m    634\u001b[39m         kwargs[\u001b[33m\"\u001b[39m\u001b[33mrun_tree\u001b[39m\u001b[33m\"\u001b[39m] = run_container[\u001b[33m\"\u001b[39m\u001b[33mnew_run\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m--> \u001b[39m\u001b[32m635\u001b[39m     function_result = \u001b[43mrun_container\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcontext\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    636\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    637\u001b[39m     _cleanup_traceback(e)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\openevals\\utils.py:124\u001b[39m, in \u001b[36m_run_evaluator_untyped.<locals>._run_scorer\u001b[39m\u001b[34m(**kwargs)\u001b[39m\n\u001b[32m    121\u001b[39m \u001b[38;5;129m@traceable\u001b[39m(name=run_name)\n\u001b[32m    122\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_run_scorer\u001b[39m(**kwargs: Any):\n\u001b[32m    123\u001b[39m     \u001b[38;5;66;03m# Get the initial score\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m124\u001b[39m     score = \u001b[43mscorer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    126\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m return_raw_outputs:\n\u001b[32m    127\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m score\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\openevals\\code\\base.py:168\u001b[39m, in \u001b[36m_create_base_code_evaluator.<locals>._wrapped_evaluator.<locals>._score_wrapper\u001b[39m\u001b[34m(outputs, **kwargs)\u001b[39m\n\u001b[32m    166\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    167\u001b[39m     normalized_outputs = code_extractor(outputs)\n\u001b[32m--> \u001b[39m\u001b[32m168\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mscorer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    169\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnormalized_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    170\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    171\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\openevals\\code\\e2b\\pyright.py:74\u001b[39m, in \u001b[36mcreate_e2b_pyright_evaluator.<locals>._scorer\u001b[39m\u001b[34m(outputs, **kwargs)\u001b[39m\n\u001b[32m     72\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_scorer\u001b[39m(outputs: \u001b[38;5;28mstr\u001b[39m, **kwargs: Any):\n\u001b[32m     73\u001b[39m     cwd = sandbox_project_directory \u001b[38;5;129;01mor\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mopenevals\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m74\u001b[39m     \u001b[43msandbox\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfiles\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mcwd\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m/outputs.py\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     75\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     76\u001b[39m         cmd = sandbox.commands.run(cmd=E2B_COMMAND, cwd=cwd)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\e2b\\sandbox_sync\\filesystem\\filesystem.py:238\u001b[39m, in \u001b[36mFilesystem.write\u001b[39m\u001b[34m(self, path_or_files, data_or_user, user_or_request_timeout, request_timeout_or_none)\u001b[39m\n\u001b[32m    236\u001b[39m err = handle_envd_api_exception(r)\n\u001b[32m    237\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m err:\n\u001b[32m--> \u001b[39m\u001b[32m238\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m err\n\u001b[32m    240\u001b[39m write_files = r.json()\n\u001b[32m    242\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(write_files, \u001b[38;5;28mlist\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(write_files) == \u001b[32m0\u001b[39m:\n",
      "\u001b[31mTimeoutException\u001b[39m: The sandbox was not found: This error is likely due to sandbox timeout. You can modify the sandbox timeout by passing 'timeoutMs' when starting the sandbox or calling '.setTimeout' on the sandbox with the desired timeout.",
      "During task with name 'run_reflection' and id '91d53146-1dd8-4bbf-9a44-9c058676ec06'",
      "During task with name 'reflection' and id 'b65a865f-05f2-8758-d927-de235e0e826f'",
      "During task with name 'code_reflection_node' and id 'c91ef159-292a-2a7e-d319-d1d78c22d488'"
     ]
    }
   ],
   "source": [
    "# Run the graph until the interrupt is hit.\n",
    "query= \" need to create a worklow with the objective of managing my social media.  It should be able to tell me trends from social media for sports, get me all the relevant people I should contant for a spsonsoring a specific post,  suggest me content I should be posting, make content for me if I give it a description * Identifying social media trends in sports. *   Finding relevant people for sponsoring specific posts. *   Suggesting content to post. *   Generating content based on a description.  Examples: Given I ask it about trends in football, it goes through recent viral reels in football and tell me Q&A reels are trending If I tell it I want people who would be interested in a post about UCL football, it gives me current players like Dembele etc If I tell it I want to make a post about UCL football, it goes through what is trending and an agent that thinks about social media posts and tells me we should post a highlight reel\"\n",
    "config = {\"configurable\": {\"thread_id\": \"some_id\"}}\n",
    "result = agent_generator_graph.invoke({\"messages\": HumanMessage(content=query)}, config=config) \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38beb199",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checkpointer for the main agent generation graph\n",
    "main_workflow = StateGraph(AgentBuilderState) # Define state type\n",
    "\n",
    "# Add nodes to the main workflow\n",
    "main_workflow.add_node(\"requirement_analysis_node\", requirement_analysis_node)\n",
    "main_workflow.add_node(\"code_node\", code_node)\n",
    "main_workflow.add_node(\"tool_subgraph_processing\", tool_compile_graph) # Renamed node\n",
    "\n",
    "# Define edges for the main workflow\n",
    "main_workflow.add_edge(START, \"requirement_analysis_node\")             # Start with requirement analysis\n",
    "main_workflow.add_edge(\"requirement_analysis_node\", \"code_node\")        # Then generate initial code\n",
    "main_workflow.add_edge(\"code_node\", \"tool_subgraph_processing\")    # Then process/implement tools via sub-graph\n",
    "main_workflow.add_edge(\"tool_subgraph_processing\", END)            # End after tool processing\n",
    "\n",
    "# Compile the main agent generation graph\n",
    "agent_generator_graph = main_workflow.compile()\n",
    "logger.info(\"Main agent generator graph compiled.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb7f79e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
