{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b0cdf334",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_factory import get_model, ModelName\n",
    "llm = get_model(ModelName.GPT41MINI)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7667b15",
   "metadata": {},
   "source": [
    "# ORIGINAL CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd9082cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph_supervisor import create_supervisor\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "model = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "# Create specialized agents\n",
    "\n",
    "def add(a: float, b: float) -> float:\n",
    "    \"\"\"Add two numbers.\"\"\"\n",
    "    return a + b\n",
    "\n",
    "def multiply(a: float, b: float) -> float:\n",
    "    \"\"\"Multiply two numbers.\"\"\"\n",
    "    return a * b\n",
    "\n",
    "def web_search(query: str) -> str:\n",
    "    \"\"\"Search the web for information.\"\"\"\n",
    "    return (\n",
    "        \"Here are the headcounts for each of the FAANG companies in 2024:\\n\"\n",
    "        \"1. **Facebook (Meta)**: 67,317 employees.\\n\"\n",
    "        \"2. **Apple**: 164,000 employees.\\n\"\n",
    "        \"3. **Amazon**: 1,551,000 employees.\\n\"\n",
    "        \"4. **Netflix**: 14,000 employees.\\n\"\n",
    "        \"5. **Google (Alphabet)**: 181,269 employees.\"\n",
    "    )\n",
    "\n",
    "math_agent = create_react_agent(\n",
    "    model=model,\n",
    "    tools=[add, multiply],\n",
    "    name=\"math_expert\",\n",
    "    prompt=\"You are a math expert. Always use one tool at a time.\"\n",
    ")\n",
    "\n",
    "research_agent = create_react_agent(\n",
    "    model=model,\n",
    "    tools=[web_search],\n",
    "    name=\"research_expert\",\n",
    "    prompt=\"You are a world class researcher with access to web search. Do not do any math.\"\n",
    ")\n",
    "\n",
    "# Create supervisor workflow\n",
    "workflow = create_supervisor(\n",
    "    [research_agent, math_agent],\n",
    "    model=model,\n",
    "    prompt=(\n",
    "        \"You are a team supervisor managing a research expert and a math expert. \"\n",
    "        \"For current events, use research_agent. \"\n",
    "        \"For math problems, use math_agent.\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# Compile and run\n",
    "app = workflow.compile()\n",
    "result = app.invoke({\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"what's the combined headcount of the FAANG companies in 2024?\"\n",
    "        }\n",
    "    ]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4fd1fc9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "code = \"\"\" \n",
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph_supervisor import create_supervisor\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "model = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "# Create specialized agents\n",
    "\n",
    "def add(a: float, b: float) -> float:\n",
    "    \\\"\\\"\\\"Add two numbers.\\\"\\\"\\\"\n",
    "    return a + b\n",
    "\n",
    "def multiply(a: float, b: float) -> float:\n",
    "    \\\"\\\"\\\"Multiply two numbers.\\\"\\\"\\\"\n",
    "    return a * b\n",
    "\n",
    "def web_search(query: str) -> str:\n",
    "    \\\"\\\"\\\"Search the web for information.\\\"\\\"\\\"\n",
    "    return (\n",
    "        \"Here are the headcounts for each of the FAANG companies in 2024:\\n\"\n",
    "        \"1. **Facebook (Meta)**: 67,317 employees.\\n\"\n",
    "        \"2. **Apple**: 164,000 employees.\\n\"\n",
    "        \"3. **Amazon**: 1,551,000 employees.\\n\"\n",
    "        \"4. **Netflix**: 14,000 employees.\\n\"\n",
    "        \"5. **Google (Alphabet)**: 181,269 employees.\"\n",
    "    )\n",
    "\n",
    "math_agent = create_react_agent(\n",
    "    model=model,\n",
    "    tools=[add, multiply],\n",
    "    name=\"math_expert\",\n",
    "    prompt=\"You are a math expert. Always use one tool at a time.\"\n",
    ")\n",
    "\n",
    "research_agent = create_react_agent(\n",
    "    model=model,\n",
    "    tools=[web_search],\n",
    "    name=\"research_expert\",\n",
    "    prompt=\"You are a world class researcher with access to web search. Do not do any math.\"\n",
    ")\n",
    "\n",
    "# Create supervisor workflow\n",
    "workflow = create_supervisor(\n",
    "    [research_agent, math_agent],\n",
    "    model=model,\n",
    "    prompt=(\n",
    "        \"You are a team supervisor managing a research expert and a math expert. \"\n",
    "        \"For current events, use research_agent. \"\n",
    "        \"For math problems, use math_agent.\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# Compile and run\n",
    "app = workflow.compile()\n",
    "result = app.invoke({\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"what's the combined headcount of the FAANG companies in 2024?\"\n",
    "        }\n",
    "    ]\n",
    "})\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eae1637",
   "metadata": {},
   "source": [
    "# JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8ae69658",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'nodes': [{'id': '__start__',\n",
       "   'type': 'runnable',\n",
       "   'data': {'id': ['langchain', 'schema', 'runnable', 'RunnablePassthrough'],\n",
       "    'name': '__start__'}},\n",
       "  {'id': 'supervisor',\n",
       "   'type': 'runnable',\n",
       "   'data': {'id': ['langgraph', 'graph', 'state', 'CompiledStateGraph'],\n",
       "    'name': 'supervisor'}},\n",
       "  {'id': 'research_expert',\n",
       "   'type': 'runnable',\n",
       "   'data': {'id': ['langgraph', 'utils', 'runnable', 'RunnableCallable'],\n",
       "    'name': 'research_expert'}},\n",
       "  {'id': 'math_expert',\n",
       "   'type': 'runnable',\n",
       "   'data': {'id': ['langgraph', 'utils', 'runnable', 'RunnableCallable'],\n",
       "    'name': 'math_expert'}},\n",
       "  {'id': '__end__'}],\n",
       " 'edges': [{'source': '__start__', 'target': 'supervisor'},\n",
       "  {'source': 'math_expert', 'target': 'supervisor'},\n",
       "  {'source': 'research_expert', 'target': 'supervisor'},\n",
       "  {'source': 'supervisor', 'target': 'math_expert', 'conditional': True},\n",
       "  {'source': 'supervisor', 'target': 'research_expert', 'conditional': True},\n",
       "  {'source': 'supervisor', 'target': '__end__', 'conditional': True}]}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "app.get_graph().to_json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4967f79c",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_str = \"\"\" \n",
    "{{'nodes': [{{'id': '__start__',\n",
    "   'type': 'runnable',\n",
    "   'data': {{'id': ['langchain', 'schema', 'runnable', 'RunnablePassthrough'],\n",
    "    'name': '__start__'}},\n",
    "  {{'id': 'supervisor',\n",
    "   'type': 'runnable',\n",
    "   'data': {{'id': ['langgraph', 'graph', 'state', 'CompiledStateGraph'],\n",
    "    'name': 'supervisor'}},\n",
    "  {{'id': 'research_expert',\n",
    "   'type': 'runnable',\n",
    "   'data': {{'id': ['langgraph', 'utils', 'runnable', 'RunnableCallable'],\n",
    "    'name': 'research_expert'}},\n",
    "  {{'id': 'math_expert',\n",
    "   'type': 'runnable',\n",
    "   'data': {{'id': ['langgraph', 'utils', 'runnable', 'RunnableCallable'],\n",
    "    'name': 'math_expert'}},\n",
    "  {{'id': '__end__'}],\n",
    " 'edges': [{{'source': '__start__', 'target': 'supervisor'}},\n",
    "  {{'source': 'math_expert', 'target': 'supervisor'},\n",
    "  {{'source': 'research_expert', 'target': 'supervisor'}},\n",
    "  {{'source': 'supervisor', 'target': 'math_expert', 'conditional': True},\n",
    "  {{'source': 'supervisor', 'target': 'research_expert', 'conditional': True},\n",
    "  {{'source': 'supervisor', 'target': '__end__', 'conditional': True}]}}\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf2c194",
   "metadata": {},
   "source": [
    "# USE CASE DRY RUNS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6aadee4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYS_PROMPT= ChatPromptTemplate.from_template(\"\"\"\n",
    "You are given the json of a workflow graph below.\n",
    "{json_str}\n",
    "You are supposed to write use cases for the graph.\n",
    "You will also do dry run of the graph with the use cases.\n",
    "The use cases should be in the format of a list of dictionaries.\n",
    "Each dictionary should have the following\n",
    "keys:\n",
    "- name: The name of the use case\n",
    "- description: The description of the use case\n",
    "- dry_run: The dry run of the use case\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "77c20848",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cases  = llm.invoke(SYS_PROMPT.format(json_str=json_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2cd9a900",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Here are some use cases for the given workflow graph along with their dry runs.\n",
      "\n",
      "```json\n",
      "[\n",
      "  {\n",
      "    \"name\": \"Simple Math Query\",\n",
      "    \"description\": \"A user asks a math-related question. The supervisor routes the query to the math_expert for processing, then returns to the supervisor and ends the workflow.\",\n",
      "    \"dry_run\": [\n",
      "      \"__start__ receives input\",\n",
      "      \"Input passed to supervisor\",\n",
      "      \"Supervisor evaluates input and routes to math_expert\",\n",
      "      \"math_expert processes the math query\",\n",
      "      \"Result returned to supervisor\",\n",
      "      \"Supervisor decides to end workflow\",\n",
      "      \"Workflow ends at __end__\"\n",
      "    ]\n",
      "  },\n",
      "  {\n",
      "    \"name\": \"Research Question\",\n",
      "    \"description\": \"A user asks a research-related question. The supervisor routes the query to the research_expert for processing, then returns to the supervisor and ends the workflow.\",\n",
      "    \"dry_run\": [\n",
      "      \"__start__ receives input\",\n",
      "      \"Input passed to supervisor\",\n",
      "      \"Supervisor evaluates input and routes to research_expert\",\n",
      "      \"research_expert processes the research query\",\n",
      "      \"Result returned to supervisor\",\n",
      "      \"Supervisor decides to end workflow\",\n",
      "      \"Workflow ends at __end__\"\n",
      "    ]\n",
      "  },\n",
      "  {\n",
      "    \"name\": \"No Further Processing Needed\",\n",
      "    \"description\": \"The supervisor determines that no expert processing is needed and ends the workflow immediately after receiving input.\",\n",
      "    \"dry_run\": [\n",
      "      \"__start__ receives input\",\n",
      "      \"Input passed to supervisor\",\n",
      "      \"Supervisor decides no expert processing is needed\",\n",
      "      \"Workflow ends at __end__\"\n",
      "    ]\n",
      "  }\n",
      "]\n",
      "```\n",
      "\n",
      "### Explanation\n",
      "\n",
      "- The graph starts at `__start__` which passes control to the `supervisor`.\n",
      "- The `supervisor` can route the input conditionally to either `math_expert` or `research_expert`.\n",
      "- After the expert processes the input, control returns to the `supervisor`.\n",
      "- The `supervisor` can also decide to end the workflow directly.\n",
      "- The workflow ends at `__end__`.\n",
      "\n",
      "These use cases cover the main paths through the graph: routing to math expert, routing to research expert, and ending without expert involvement.\n"
     ]
    }
   ],
   "source": [
    "use_cases.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bdb1a2f",
   "metadata": {},
   "source": [
    "# GENERATING TEST CASES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b248ac9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import html2text\n",
    "import httpx\n",
    "\n",
    "def fetch_documents(url: str) -> str:\n",
    "    \"\"\"Fetch a document from a URL and return the markdownified text.\n",
    "\n",
    "    Args:\n",
    "        url (str): The URL of the document to fetch.\n",
    "\n",
    "    Returns:\n",
    "        str: The markdownified text of the document.\n",
    "    \"\"\"\n",
    "    httpx_client = httpx.Client(follow_redirects=True, timeout=10)\n",
    "\n",
    "    try:\n",
    "        response = httpx_client.get(url, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        html_content = response\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    \n",
    "        img_tags = soup.find_all('img')\n",
    "        for img_tag in img_tags:\n",
    "            img_tag.decompose()\n",
    "\n",
    "        target_div = soup.find('div', class_= \"theme-doc-markdown markdown\") #langchain\n",
    "        \n",
    "        if not target_div:\n",
    "            target_div = soup.find('article') #langraph\n",
    "\n",
    "        if not target_div:\n",
    "            target_div = soup.find('html') #langraph\n",
    "\n",
    "        if not target_div:\n",
    "            return html2text.html2text(str(soup))\n",
    "        \n",
    "        return html2text.html2text(str(target_div))\n",
    "    except (httpx.HTTPStatusError, httpx.RequestError) as e:\n",
    "        return f\"Encountered an HTTP error: {str(e)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "3470b926",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_GEN_PROMPT = ChatPromptTemplate.from_template(\"\"\"\n",
    "You are given the json of a workflow graph below.\n",
    "<JSON>\n",
    "{json_str}\n",
    "</JSON>\n",
    "You are also given the code of the graph below.\n",
    "<CODE>\n",
    "{code}\n",
    "</CODE>\n",
    "You are given the use cases for a workflow graph along with dry runs.\n",
    "<USE_CASES>\n",
    "{use_cases}\n",
    "</USE_CASES>\n",
    "                                                   \n",
    "Below are documents which contain information about how to write test cases for the graph.\n",
    "<DOCUMENTS>\n",
    "{documents1}\n",
    "{documents2}\n",
    "</DOCUMENTS>\n",
    "                                                   \n",
    "You are supposed to write test cases for the graph in the <CODE> section, use the <JSON> section to understand the graph and the <USE_CASES> for generating test case inputs.\n",
    "                                              \n",
    "You are supposed to see the kind of tests that are being written in the <DOCUMENTS> section and write your own test cases in the same format.\n",
    "The tests should cover the following:\n",
    "Final response: The inputs are a prompt and an optional list of tools. The output is the final agent response.\n",
    "Trajectory: As before, the inputs are a prompt and an optional list of tools. The output is the list of tool calls\n",
    "Single step: As before, the inputs are a prompt and an optional list of tools. The output is the tool call.\n",
    "\n",
    "Use pytest paramterize when possible.\n",
    "Also include the code in the output at the top\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "cc8c6861",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_file =llm.invoke(TEST_GEN_PROMPT.format(\n",
    "    json_str=json_str,\n",
    "    code=code,\n",
    "    use_cases=use_cases,\n",
    "    documents1=fetch_documents(\"https://docs.smith.langchain.com/evaluation/tutorials/testing\"),\n",
    "    documents2=fetch_documents(\"https://docs.smith.langchain.com/evaluation/tutorials/agents\"),\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "917f8427",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "```python\n",
      "import pytest\n",
      "from langsmith import testing as t\n",
      "from typing_extensions import Annotated, TypedDict\n",
      "\n",
      "# Assuming the workflow app is imported as `app` from the compiled workflow in the <CODE> section\n",
      "# For example:\n",
      "# from my_workflow_module import app\n",
      "\n",
      "# Since the graph has nodes: __start__, supervisor, math_expert, research_expert, __end__\n",
      "# and the supervisor routes queries conditionally to math_expert or research_expert or ends directly,\n",
      "# we will write tests for:\n",
      "# - Final response correctness\n",
      "# - Trajectory of tool calls (which expert was called)\n",
      "# - Single step routing decision by supervisor\n",
      "\n",
      "# We define a TypedDict for structured response if needed\n",
      "class StructuredResponse(TypedDict):\n",
      "    numeric_answer: Annotated[float | None, ...]\n",
      "    text_answer: Annotated[str | None, ...]\n",
      "    reasoning: str\n",
      "\n",
      "# --- Final Response Tests ---\n",
      "\n",
      "@pytest.mark.langsmith\n",
      "@pytest.mark.parametrize(\n",
      "    \"query, expected_in_response\",\n",
      "    [\n",
      "        (\n",
      "            \"what's the combined headcount of the FAANG companies in 2024?\",\n",
      "            \"combined headcount\"\n",
      "        ),\n",
      "        (\n",
      "            \"What is 5 plus 7?\",\n",
      "            \"12\"\n",
      "        ),\n",
      "        (\n",
      "            \"Tell me something unrelated to math or research.\",\n",
      "            None  # Expect no tool usage, just a direct answer or no processing\n",
      "        ),\n",
      "    ],\n",
      ")\n",
      "def test_final_response_contains_expected_text(query: str, expected_in_response: str | None) -> None:\n",
      "    \"\"\"\n",
      "    Test that the final response contains expected content or handles no-tool queries gracefully.\n",
      "    \"\"\"\n",
      "    t.log_inputs({\"query\": query})\n",
      "    result = app.invoke({\n",
      "        \"messages\": [{\"role\": \"user\", \"content\": query}]\n",
      "    })\n",
      "    response_text = result[\"messages\"][-1][\"content\"]\n",
      "    t.log_outputs({\"response\": response_text})\n",
      "\n",
      "    if expected_in_response is not None:\n",
      "        assert expected_in_response.lower() in response_text.lower()\n",
      "    else:\n",
      "        # For no-tool queries, just check that the response is non-empty\n",
      "        assert response_text.strip() != \"\"\n",
      "\n",
      "# --- Trajectory Tests ---\n",
      "\n",
      "@pytest.mark.langsmith\n",
      "@pytest.mark.parametrize(\n",
      "    \"query, expected_trajectory\",\n",
      "    [\n",
      "        (\n",
      "            \"What is 5 plus 7?\",\n",
      "            [\"supervisor\", \"math_expert\", \"supervisor\"]\n",
      "        ),\n",
      "        (\n",
      "            \"what's the combined headcount of the FAANG companies in 2024?\",\n",
      "            [\"supervisor\", \"research_expert\", \"supervisor\"]\n",
      "        ),\n",
      "        (\n",
      "            \"Hello, how are you?\",\n",
      "            [\"supervisor\"]\n",
      "        ),\n",
      "    ],\n",
      ")\n",
      "def test_trajectory_of_tool_calls(query: str, expected_trajectory: list[str]) -> None:\n",
      "    \"\"\"\n",
      "    Test that the workflow routes queries correctly and returns through expected nodes.\n",
      "    \"\"\"\n",
      "    t.log_inputs({\"query\": query})\n",
      "    trajectory = []\n",
      "\n",
      "    # We simulate streaming or stepwise invocation by inspecting messages or logs.\n",
      "    # Since the app.invoke is synchronous here, we infer trajectory from tool calls and messages.\n",
      "\n",
      "    result = app.invoke({\n",
      "        \"messages\": [{\"role\": \"user\", \"content\": query}]\n",
      "    })\n",
      "\n",
      "    # The graph edges show supervisor routes to math_expert or research_expert conditionally,\n",
      "    # then back to supervisor, then ends.\n",
      "    # We track nodes by presence of tool calls or message types.\n",
      "\n",
      "    # Start with supervisor node always\n",
      "    trajectory.append(\"supervisor\")\n",
      "\n",
      "    # Check if math_expert or research_expert was called\n",
      "    tool_calls = []\n",
      "    for msg in result[\"messages\"]:\n",
      "        if \"tool_calls\" in msg and msg[\"tool_calls\"]:\n",
      "            tool_calls.extend([tc[\"name\"] for tc in msg[\"tool_calls\"]])\n",
      "\n",
      "    if \"math_expert\" in tool_calls:\n",
      "        trajectory.append(\"math_expert\")\n",
      "        trajectory.append(\"supervisor\")\n",
      "    elif \"research_expert\" in tool_calls:\n",
      "        trajectory.append(\"research_expert\")\n",
      "        trajectory.append(\"supervisor\")\n",
      "    # else no expert called, just supervisor\n",
      "\n",
      "    t.log_outputs({\"trajectory\": trajectory})\n",
      "    assert trajectory == expected_trajectory\n",
      "\n",
      "# --- Single Step Tests: Supervisor routing decision ---\n",
      "\n",
      "# We assume we can invoke the supervisor node directly for single step testing.\n",
      "# The supervisor node decides which expert to route to or to end.\n",
      "\n",
      "@pytest.mark.langsmith\n",
      "@pytest.mark.asyncio\n",
      "@pytest.mark.parametrize(\n",
      "    \"messages, expected_route\",\n",
      "    [\n",
      "        (\n",
      "            [{\"role\": \"user\", \"content\": \"Calculate 3 times 4\"}],\n",
      "            \"math_expert\"\n",
      "        ),\n",
      "        (\n",
      "            [{\"role\": \"user\", \"content\": \"Tell me about the latest news on Tesla\"}],\n",
      "            \"research_expert\"\n",
      "        ),\n",
      "        (\n",
      "            [{\"role\": \"user\", \"content\": \"Just saying hi\"}],\n",
      "            \"__end__\"\n",
      "        ),\n",
      "    ],\n",
      ")\n",
      "async def test_supervisor_routing(messages: list[dict], expected_route: str) -> None:\n",
      "    \"\"\"\n",
      "    Test that the supervisor routes to the correct expert or ends workflow.\n",
      "    \"\"\"\n",
      "    t.log_inputs({\"messages\": messages})\n",
      "\n",
      "    # Directly invoke the supervisor node\n",
      "    command = await app.nodes[\"supervisor\"].ainvoke({\"messages\": messages})\n",
      "\n",
      "    t.log_outputs({\"goto\": command.goto})\n",
      "\n",
      "    assert command.goto == expected_route\n",
      "\n",
      "# --- Additional test: math_expert correctness ---\n",
      "\n",
      "@pytest.mark.langsmith\n",
      "def test_math_expert_addition() -> None:\n",
      "    \"\"\"\n",
      "    Test that math_expert correctly adds two numbers.\n",
      "    \"\"\"\n",
      "    # Directly invoke math_expert with a simple addition query\n",
      "    query = \"Add 10 and 15\"\n",
      "    t.log_inputs({\"query\": query})\n",
      "\n",
      "    result = app.nodes[\"math_expert\"].invoke({\"messages\": [{\"role\": \"user\", \"content\": query}]})\n",
      "\n",
      "    t.log_outputs({\"response\": result[\"messages\"][0][\"content\"]})\n",
      "\n",
      "    # The response should contain the sum 25\n",
      "    assert \"25\" in result[\"messages\"][0][\"content\"]\n",
      "\n",
      "# --- Additional test: research_expert correctness ---\n",
      "\n",
      "@pytest.mark.langsmith\n",
      "def test_research_expert_web_search() -> None:\n",
      "    \"\"\"\n",
      "    Test that research_expert returns expected web search results.\n",
      "    \"\"\"\n",
      "    query = \"Headcount of Google in 2024\"\n",
      "    t.log_inputs({\"query\": query})\n",
      "\n",
      "    result = app.nodes[\"research_expert\"].invoke({\"messages\": [{\"role\": \"user\", \"content\": query}]})\n",
      "\n",
      "    t.log_outputs({\"response\": result[\"messages\"][0][\"content\"]})\n",
      "\n",
      "    # The response should mention Google or Alphabet\n",
      "    assert \"Google\" in result[\"messages\"][0][\"content\"] or \"Alphabet\" in result[\"messages\"][0][\"content\"]\n",
      "```\n",
      "---\n",
      "\n",
      "### Explanation\n",
      "\n",
      "- **Final response tests** check that the overall workflow returns a meaningful answer and that the answer contains expected keywords or numbers.\n",
      "- **Trajectory tests** verify the path taken through the graph nodes, ensuring the supervisor routes correctly and the experts are called as expected.\n",
      "- **Single step tests** directly invoke the supervisor node to test its routing logic in isolation.\n",
      "- Additional tests directly invoke the math_expert and research_expert nodes to verify their tool usage and output correctness.\n",
      "- `pytest.mark.langsmith` is used to integrate with LangSmith evaluation tooling.\n",
      "- `pytest.mark.parametrize` is used to cover multiple input cases efficiently.\n",
      "- `asyncio` is used for async node invocation tests.\n",
      "\n",
      "This test suite covers the main paths and logic of the workflow graph as described in the JSON, code, and use cases.\n"
     ]
    }
   ],
   "source": [
    "final_file.pretty_print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
