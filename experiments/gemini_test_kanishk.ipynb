{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b989112c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "import logging\n",
    "from pydantic import Field, BaseModel\n",
    "from langgraph.graph import MessagesState, StateGraph, START, END\n",
    "from typing import List\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage, ToolMessage\n",
    "from utils.fetch_docs import fetch_documents\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from code_reflection_agent import final_agent\n",
    "from langgraph.types import Command, interrupt\n",
    "from typing import  Literal, TypedDict\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Set up the logger\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,  # Set to DEBUG for detailed logs\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    "    handlers=[\n",
    "        # logging.FileHandler(\"scraper.log\"),  # Log to a file\n",
    "        logging.StreamHandler()  # Log to console\n",
    "    ]\n",
    ")\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "template = \"\"\"Your job is to get information from a user about what kind of agent they wish to build.\n",
    "\n",
    "You should get the following information from them:\n",
    "\n",
    "- What the objective of the agent is\n",
    "- Various usecases of the agent \n",
    "- Some examples of what the agent will be doing (Input and expected output pairs)\n",
    "\n",
    "If you are not able to discern this info, ask them to clarify, you can suggest and see if user confirms the suggestions.\n",
    "\n",
    "After you are able to discern all the information, call the tool AgentInstruction\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f4ea1b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentInstructions(BaseModel):\n",
    "    \"\"\"Instructions on how to build the Agent\"\"\"\n",
    "    objective: str = Field(description= \"What is the primary objective of the agent\")\n",
    "    usecases: List[str] = Field(description= \"What are the various responsibilities of the agent which it needs to fulfill\")\n",
    "    examples : str = Field(description= \"What are some examples of the usage of the agent (input query and expected output from the agent) ?\")\n",
    "\n",
    "class AgentBuilderState(MessagesState):\n",
    "    agent_instructions: AgentInstructions = Field(\"the requirement analysis generated by the model.\")\n",
    "    json_code: str = Field(\"The json code generated\")\n",
    "    python_code: str = Field(\"The Python code generated\")\n",
    "\n",
    "class ArchitectureEvaluationState(MessagesState):\n",
    "    agent_instructions: AgentInstructions = Field(\"the requirement analysis generated by the model.\")\n",
    "    url: str = Field(\"url of the agent architecture to evaluate against\")\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash-preview-05-20\", temperature=0)\n",
    "\n",
    "def requirement_analysis_node(state: AgentBuilderState) -> Command[Literal[\"requirement_analysis\", \"code_node\"]]:\n",
    "    llm_with_tool = llm.bind_tools([AgentInstructions])\n",
    "    response = llm_with_tool.invoke([SystemMessage(content=template)] + state[\"messages\"])\n",
    "    \n",
    "      # Construct the final answer from the arguments of the last tool call  \n",
    "    if len(response.tool_calls) == 0: # info is needed\n",
    "        value = interrupt(response.content)\n",
    "        return Command(goto=\"requirement_analysis_node\", update= {\"messages\": [response, HumanMessage(content=value)]} )\n",
    "        \n",
    "    agent_instructions = response.tool_calls[0]\n",
    "    agent_instructions = AgentInstructions(**agent_instructions[\"args\"])\n",
    "    return Command(goto=\"code_node\", update= {\"messages\": [response], \"agent_instructions\": agent_instructions } )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f50b4ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "CODE_GEN_PROMPT = PromptTemplate.from_template( \"\"\"\n",
    "You are an expert Python programmer specializing in AI agent development via the Langgraph and Langchain SDK . Your primary task is to generate compilable, logical, and complete Python code for a LangGraph state graph based on user input below. You must prioritize LLM-based implementations for relevant tasks and consider advanced graph architectures.\n",
    "\n",
    "**Input:**\n",
    "<INPUT>\n",
    "<OBJECTIVE>\n",
    "{objective}\n",
    "</OBJECTIVE>\n",
    "<USECASES>\n",
    "{usecases}\n",
    "</USECASES>\n",
    "<EXAMPLES>\n",
    "{examples}\n",
    "</EXAMPLES>\n",
    "</INPUT>\n",
    "---\n",
    "**Phase 1: Evaluating best architecture for the given INPUT**\n",
    "\n",
    "1.  **Identify Potential Architectures:** Consider if the described INPUT aligns with or would benefit from known advanced LangGraph architectures such as:\n",
    "    * **Plan and Execute**: Does the INPUT imply an agent which might need a planning step (e.g., breaking down a complex task) followed by the execution of those plans by one or more action nodes?\n",
    "    * **Agent Supervisor / Hierarchical Agent Teams**: Is the INPUT best served by a supervisor agent dispatching tasks to specialized worker agents, or a hierarchy of agents making decisions and delegating?\n",
    "    * **Multi-Agent Collaboration (e.g., Swarm Architecture)**: Does the problem benefit from multiple agents working in parallel or collaboratively, perhaps sharing insights or contributing to a common goal?\n",
    "    * **Reflection / Self-Correction (e.g., Self-Discover frameworks)**: Are there indications of iterative refinement, where results are evaluated and the process is adjusted?\n",
    "    * **Human in the Loop (HITL)**: Does the `description` of any node, or the overall process, imply a need for human review, approval, correction, or explicit input at specific stages (e.g., before executing a critical action, when confidence is low, or for subjective assessments)?\n",
    "\n",
    "2.  **Architectural Decision:**\n",
    "    * If you determine that one or more of these architectures are strongly applicable to the INPUT, choose to implement it.\n",
    "    * If no specific advanced architecture seems directly applicable for the given INPUT, proceed with a standard stateful graph construction based on the explicit langgraph nodes and edges.\n",
    "\n",
    "3.  **Initial Comment:** At the very beginning of your generated Python script, include a comment block stating:\n",
    "    * Which LangGraph architecture(s) (if any) you've identified and chosen to implement, with a brief justification based on your interpretation of the INPUT, provide dry runs of the usecases/examples.\n",
    "    * If you are proceeding with a standard graph, mention that.\n",
    "\n",
    "4. Generate a JSON representation of the architecture you have chosen, including:\n",
    "a.  `nodes`: A dictionary where each key is a unique node ID. The value for each node ID is an object containing:\n",
    "    * `id`: The node's identifier.\n",
    "    * `schema_info`: A string describing the structure of the `GraphState` (e.g., \"GraphState:\\n type: TypedDict\\n fields:\\n - name: input\\n type: str...\"). You will need to parse this to define the `GraphState` TypedDict.\n",
    "    * `input_schema`: The expected input schema for the node (typically \"GraphState\").\n",
    "    * `output_schema`: The schema of the output produced by the node (typically \"GraphState\", indicating a partial update).\n",
    "    * `description`: A natural language description of what the node does. This is crucial for determining implementation strategy and overall architecture.\n",
    "    * `function_name`: The suggested Python function name for this node.\n",
    "    * `code` (optional): A string containing Python code for the node's function. **Treat this `code` primarily as an illustration or a very basic version. Prioritize LLM-based solutions if the `description` suggests a more robust approach is needed.**\n",
    "\n",
    "b.  `edges`: A list of objects, each describing a directed edge in the graph. Each edge object contains:\n",
    "    * `source`: The ID of the source node (or \"__START__\" for the graph's entry point).\n",
    "    * `target`: The ID of the target node (or \"__END__\" for a graph termination point).\n",
    "    * `routing_conditions`: A natural language description of the condition under which this edge is taken, especially for conditional edges.\n",
    "    * `conditional`: A boolean flag, `true` if the edge is part of a conditional branch, `false` otherwise.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "** Phase 2: Graph Creation\n",
    "**Phase 2: Python Code Generation**\n",
    "\n",
    "Generate a single, self-contained, and compilable Python script that implements your chosen strategy.\n",
    "\n",
    "1.  **Imports:** Include all necessary Python libraries (e.g., `typing`, `langgraph.graph`, `langgraph.checkpoint.memory`, LLM client libraries like `langchain_openai`, `langchain_google_genai`, `langchain_core.pydantic_v1`, `langchain_core.tools`, `re`).\n",
    "\n",
    "2.  **State Definition (`GraphState`):**\n",
    "    *  Define a `GraphState` class using `MessagesState` (langgraph prebuilt class).\n",
    "\n",
    "3.  **Node Implementation (Python Functions):**\n",
    "    For each conceptual node in your chosen architecture (these may map directly to JSON you define):\n",
    "    * Create a Python function. This function must accept the `GraphState` and return a dictionary representing the partial update to the state.\n",
    "    * **Decision Logic for Implementation (Prioritize LLM, No Mock Data):**\n",
    "        * **Default to LLM-Based Solutions:** Your default stance should be to implement an **LLM-based solution** if the node's `description` (from JSON or your architectural design) suggests tasks like:\n",
    "            * Natural Language Understanding (NLU)\n",
    "            * Complex classification or routing\n",
    "            * Content generation or summarization\n",
    "            * Tool selection and usage\n",
    "            * Planning or complex decision-making.\n",
    "            * Any task where an LLM would provide more robust, flexible, or intelligent behavior than simple hardcoded logic.\n",
    "        * **Handling Provided `code`:** If `code` is present in the JSON for a node, treat it as a **low-priority hint or a simplistic example**. Do **not** simply copy it if an LLM approach is more appropriate for the described task.\n",
    "        * **Algorithmic Logic (Use Sparingly):** Only use purely algorithmic Python code (like from the `code` attribute or written new) if the node's task is genuinely simple, deterministic (e.g., basic data formatting, fixed calculation), *and* an LLM would offer no significant benefit for that specific, narrow function.\n",
    "        * **Functional LLM Calls:** When an LLM is used, instantiate a generic model (e.g., `llm = ChatOpenAI(model=\"gpt-3.5-turbo\")` or `llm = ChatGoogleGenerativeAI(model=\"gemini-pro\")`) and include a **functional, descriptive prompt** relevant to the node's task. Ensure the code for the LLM call is complete and not just a comment. Add a `TODO` comment for the user to specify API keys and potentially refine the model/prompt.\n",
    "        * **No Mock Data:** Generated functions must be logical and aim for completeness. **Avoid using mock data or overly simplistic placeholder logic** where an LLM or a proper algorithmic implementation is expected.\n",
    "        * **Structured Output & Tools:** If the task implies structured output from an LLM or the use of tools, define necessary Pydantic models and/or LangChain tools, and integrate them with the LLM call.\n",
    "            * Define a Pydantic model (e.g., `from langchain_core.pydantic_v1 import BaseModel, Field`) representing the desired structured output.\n",
    "            * If implementing an LLM call, configure it to use the Pydantic model for its output (e.g., with OpenAI's function calling/tool usage features, or by instructing the LLM to generate JSON conforming to the model).\n",
    "        * **Tool Definition and Usage:** If a node's `description` (or your architectural design) implies the LLM within that node needs to interact with external systems, perform specific actions, or fetch data (e.g., \"search customer database,\" \"get weather update\"):\n",
    "                * Define these capabilities as discrete LangChain tools using the `@tool` decorator (e.g., `from langchain_core.tools import tool`).\n",
    "                * **Crucially, each tool's internal Python function should be self-contained and directly perform its advertised action** (e.g., make a specific API call to an external service, run a local script, perform a calculation, retrieve data algorithmically). **Avoid embedding a *new, separate general-purpose LLM call within the tool's own implementation logic* unless the tool's explicit and documented purpose is to be a specialized, self-contained sub-agent (which is an advanced case).** The primary LLM within the graph node is responsible for *deciding to call* the tool and for interpreting its output.\n",
    "                * Bind these well-defined tools to the LLM instance operating within that graph node. The node's LLM will then intelligently decide when to call a tool and with what inputs.\n",
    "        * **Human in the Loop Nodes:** If you've designed a HITL step as a dedicated node, its function might primarily format data for human review and then process the subsequent human input (which would be added to the state, potentially by an external mechanism or a subsequent node). The graph might pause using an interruption mechanism tied to this node.\n",
    "        * **State Coherence:** Ensure variable assignments and updates within node functions are coherent with the `GraphState` definition and how state is managed in LangGraph.\n",
    "\n",
    "4.  **Graph Construction (`StatefulGraph`):**\n",
    "    * Instantiate `StatefulGraph(GraphState)`.\n",
    "    * Add each implemented node function to the graph using `graph.add_node(\"node_id\", node_function)`.\n",
    "    * Set the graph's entry point using `graph.add_edge(START, \"entry_node_id\")` where `\"entry_node_id\"` is the target of the edge originating from `\"__START__\"`.\n",
    "\n",
    "5.  **Edge Implementation:**\n",
    "    * Iterate through the `edges` list in the JSON.\n",
    "    * **Regular Edges:** If `conditional` is `false`:\n",
    "        * If `target` is `__END__`, use `graph.add_edge(source_node_id, END)`.\n",
    "        * Otherwise, use `graph.add_edge(source_node_id, target_node_id)`.\n",
    "    * **Conditional Edges:** If `conditional` is `true`:\n",
    "        * The `source` node of these conditional edges is expected to produce some output in the `GraphState` (e.g., an `intent` field) that determines the next path.\n",
    "        * Create a separate routing function (e.g., `def route_after_source_node(state: GraphState) -> str:`).\n",
    "        * This routing function must inspect the relevant fields in the `state` and return the string ID of the next node to execute, based on the logic described in the `routing_conditions` for each conditional edge originating from that source.\n",
    "        * Use `graph.add_conditional_edges(source_node_id, routing_function, {{ \"target_id_1\": \"target_id_1\", \"target_id_2\": \"target_id_2\", ... \"__END__\": END }})`. The keys in the dictionary are the possible return values from your routing function, and the values are the actual node IDs or `END`.\n",
    "\n",
    "6.  **Compilation:**\n",
    "    * Instantiate an `InMemoryCheckpointer`: `checkpointer = InMemoryCheckpointer()`.\n",
    "    * Compile the graph: `final_app = graph.compile(checkpointer=checkpointer)`. The compiled graph must be assigned to a variable named `final_app`.\n",
    "\n",
    "---\n",
    "**Phase 3: Required Keys/Credentials Identification**\n",
    "\n",
    "After generating the complete Python script, add a separate section at the end of your response, clearly titled:\n",
    "`## Required Keys and Credentials`\n",
    "\n",
    "In this section, list all environment variables or API keys a user would need to set for the generated code to execute successfully (e.g., `OPENAI_API_KEY`, `GOOGLE_API_KEY`, tool-specific keys). If no external keys are needed, state that.\n",
    "\n",
    "---\n",
    "**Important Considerations (General):**\n",
    "* The primary goal is **compilable, logical, and functionally plausible Python code** that intelligently interprets the JSON input.\n",
    "* Focus on creating a system that leverages LLMs effectively for tasks suited to them.\n",
    "* Ensure node functions correctly update and return relevant parts of the `GraphState`.\n",
    "* If the provided `code` in the JSON uses specific libraries (e.g., `re`), make sure the corresponding import is included at the top of the script.\n",
    "* Handle `__START__` and `__END__` correctly in edge definitions. `langgraph.graph.START` and `langgraph.graph.END` should be used.\n",
    "\n",
    "Please generate the Python code and the list of required keys now:\n",
    "\"\"\")\n",
    "\n",
    "def code_node(state: AgentBuilderState):\n",
    "    \"\"\"Produce the final python code\"\"\"\n",
    "    instructions: AgentInstructions = state[\"agent_instructions\"]\n",
    "    code_ouptut = llm.invoke([HumanMessage(content=CODE_GEN_PROMPT.format(\n",
    "        objective = instructions.objective, usecases = instructions.usecases, examples = instructions.examples\n",
    "        ))])\n",
    "    \n",
    "    # Return the JSON code as the output\n",
    "    return {\n",
    "        \"messages\": [AIMessage(content=\"Generated final python code!\")],\n",
    "        \"python_code\": code_ouptut.content,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4e33b565",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpointer = MemorySaver()\n",
    "workflow = StateGraph(AgentBuilderState)\n",
    "workflow.add_node(\"code_node\",code_node)\n",
    "workflow.add_node(\"requirement_analysis\", requirement_analysis_node)\n",
    "\n",
    "workflow.add_edge(\"code_node\", END)\n",
    "workflow.add_edge(START, \"requirement_analysis\")\n",
    "graph = workflow.compile(checkpointer=checkpointer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6ba59087",
   "metadata": {},
   "outputs": [],
   "source": [
    "query= \" need to create a worklow with the objective of managing my social media.  It should be able to tell me trends from social media for sports, get me all the relevant people I should contant for a spsonsoring a specific post,  suggest me content I should be posting, make content for me if I give it a description * Identifying social media trends in sports. *   Finding relevant people for sponsoring specific posts. *   Suggesting content to post. *   Generating content based on a description.  Examples: Given I ask it about trends in football, it goes through recent viral reels in football and tell me Q&A reels are trending If I tell it I want people who would be interested in a post about UCL football, it gives me current players like Dembele etc If I tell it I want to make a post about UCL football, it goes through what is trending and an agent that thinks about social media posts and tells me we should post a highlight reel\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a74c725e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the graph until the interrupt is hit.\n",
    "config = {\"configurable\": {\"thread_id\": \"some_id\"}}\n",
    "result = graph.invoke({\"messages\": HumanMessage(content=query)}, config=config) \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "601799ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      " need to create a worklow with the objective of managing my social media.  It should be able to tell me trends from social media for sports, get me all the relevant people I should contant for a spsonsoring a specific post,  suggest me content I should be posting, make content for me if I give it a description * Identifying social media trends in sports. *   Finding relevant people for sponsoring specific posts. *   Suggesting content to post. *   Generating content based on a description.  Examples: Given I ask it about trends in football, it goes through recent viral reels in football and tell me Q&A reels are trending If I tell it I want people who would be interested in a post about UCL football, it gives me current players like Dembele etc If I tell it I want to make a post about UCL football, it goes through what is trending and an agent that thinks about social media posts and tells me we should post a highlight reel\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  AgentInstructions (c25c75e0-f65c-4d92-89b7-8b7c32aafbbd)\n",
      " Call ID: c25c75e0-f65c-4d92-89b7-8b7c32aafbbd\n",
      "  Args:\n",
      "    examples: Given I ask it about trends in football, it goes through recent viral reels in football and tell me Q&A reels are trending If I tell it I want people who would be interested in a post about UCL football, it gives me current players like Dembele etc If I tell it I want to make a post about UCL football, it goes through what is trending and an agent that thinks about social media posts and tells me we should post a highlight reel\n",
      "    usecases: ['Identifying social media trends in sports.', 'Finding relevant people for sponsoring specific posts.', 'Suggesting content to post.', 'Generating content based on a description.']\n",
      "    objective: managing my social media\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Generated final python code!\n"
     ]
    }
   ],
   "source": [
    "for message in result[\"messages\"]:\n",
    "    message.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "36d0ef89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```python\n",
      "import operator\n",
      "from typing import TypedDict, Annotated, List, Optional\n",
      "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage\n",
      "from langgraph.graph import StateGraph, END, START\n",
      "from langgraph.graph.message import add_messages\n",
      "from langchain_openai import ChatOpenAI\n",
      "from langchain_core.pydantic_v1 import BaseModel, Field\n",
      "from langchain_core.tools import tool\n",
      "import json\n",
      "\n",
      "# --- Phase 1: Evaluating best architecture for the given INPUT ---\n",
      "\n",
      "\"\"\"\n",
      "LangGraph Architecture Chosen: Hierarchical Agent Team with Supervisor and Specialized Worker Agents.\n",
      "\n",
      "Justification:\n",
      "The user's objective of \"managing social media\" with distinct use cases like \"Identifying social media trends\", \"Finding relevant people for sponsoring\", \"Suggesting content\", and \"Generating content\" strongly suggests a need for specialized expertise. A single, monolithic agent would struggle to efficiently handle such diverse tasks.\n",
      "\n",
      "1.  **Supervisor Agent**: A central 'supervisor' node is ideal for interpreting the initial user query and routing it to the most appropriate specialized worker agent. This aligns with the \"Agent Supervisor\" pattern.\n",
      "    *   **Dry Run Example 1 (\"trends in football\"):** User input -> Supervisor (identifies 'identify_trends' intent) -> Trend Analyst Agent.\n",
      "    *   **Dry Run Example 2 (\"people interested in UCL football\"):** User input -> Supervisor (identifies 'find_sponsors' intent) -> Sponsor Finder Agent.\n",
      "    *   **Dry Run Example 3 (\"make a post about UCL football\"):** User input -> Supervisor (identifies 'suggest_content' intent) -> Content Suggester Agent. (If the user then asks to generate, it would be a new turn routed to Content Generator).\n",
      "\n",
      "2.  **Specialized Worker Agents**:\n",
      "    *   `Trend_Analyst_Agent`: Focuses solely on identifying trends.\n",
      "    *   `Sponsor_Finder_Agent`: Specializes in finding relevant people/sponsors.\n",
      "    *   `Content_Suggester_Agent`: Brainstorms and suggests content ideas.\n",
      "    *   `Content_Generator_Agent`: Crafts the actual social media post.\n",
      "    This modularity ensures each agent is expert in its domain, leading to more focused and effective responses.\n",
      "\n",
      "3.  **Multi-Agent Collaboration (Implicit/Potential)**: While not explicitly a \"swarm\" architecture, the `Content_Suggester_Agent` could implicitly collaborate by leveraging information that might typically come from a `Trend_Analyst_Agent` (e.g., \"goes through what is trending\"). For this implementation, the LLM within `Content_Suggester` will be prompted to consider trends, simulating this collaboration without explicit inter-agent calls within the graph, keeping the graph structure cleaner for routing. Tools could be used by agents to fetch external data, further enhancing their capabilities.\n",
      "\n",
      "This architecture provides clear separation of concerns, scalability, and maintainability, making it well-suited for managing diverse social media tasks.\n",
      "\"\"\"\n",
      "\n",
      "# JSON Representation of the Architecture\n",
      "ARCHITECTURE_JSON = {\n",
      "  \"nodes\": {\n",
      "    \"supervisor\": {\n",
      "      \"id\": \"supervisor\",\n",
      "      \"schema_info\": \"GraphState:\\n type: TypedDict\\n fields:\\n - name: messages\\n   type: Annotated[List[BaseMessage], add_messages]\\n - name: user_query\\n   type: str\\n - name: intent\\n   type: Optional[str]\\n - name: trends\\n   type: Optional[str]\\n - name: sponsors\\n   type: Optional[str]\\n - name: content_suggestion\\n   type: Optional[str]\\n - name: generated_content\\n   type: Optional[str]\\n - name: error\\n   type: Optional[str]\",\n",
      "      \"input_schema\": \"GraphState\",\n",
      "      \"output_schema\": \"GraphState\",\n",
      "      \"description\": \"Determines the user's intent (e.g., 'identify_trends', 'find_sponsors', 'suggest_content', 'generate_content') and routes the request to the appropriate specialized agent. Uses an LLM for intent classification.\",\n",
      "      \"function_name\": \"route_request\"\n",
      "    },\n",
      "    \"trend_analyst\": {\n",
      "      \"id\": \"trend_analyst\",\n",
      "      \"schema_info\": \"GraphState:\\n type: TypedDict\\n fields:\\n - name: messages\\n   type: Annotated[List[BaseMessage], add_messages]\\n - name: user_query\\n   type: str\\n - name: intent\\n   type: Optional[str]\\n - name: trends\\n   type: Optional[str]\\n - name: sponsors\\n   type: Optional[str]\\n - name: content_suggestion\\n   type: Optional[str]\\n - name: generated_content\\n   type: Optional[str]\\n - name: error\\n   type: Optional[str]\",\n",
      "      \"input_schema\": \"GraphState\",\n",
      "      \"output_schema\": \"GraphState\",\n",
      "      \"description\": \"Identifies social media trends based on the input query, focusing on sports or specific topics. It uses an LLM to simulate searching recent viral content and identifying patterns. It can leverage a 'search_social_media_trends' tool.\",\n",
      "      \"function_name\": \"identify_trends\"\n",
      "    },\n",
      "    \"sponsor_finder\": {\n",
      "      \"id\": \"sponsor_finder\",\n",
      "      \"schema_info\": \"GraphState:\\n type: TypedDict\\n fields:\\n - name: messages\\n   type: Annotated[List[BaseMessage], add_messages]\\n - name: user_query\\n   type: str\\n - name: intent\\n   type: Optional[str]\\n - name: trends\\n   type: Optional[str]\\n - name: sponsors\\n   type: Optional[str]\\n - name: content_suggestion\\n   type: Optional[str]\\n - name: generated_content\\n   type: Optional[str]\\n - name: error\\n   type: Optional[str]\",\n",
      "      \"input_schema\": \"GraphState\",\n",
      "      \"output_schema\": \"GraphState\",\n",
      "      \"description\": \"Finds relevant individuals or entities for sponsoring specific social media posts based on the post's topic. It uses an LLM to simulate searching for influencers or public figures. It can leverage a 'search_influencers' tool.\",\n",
      "      \"function_name\": \"find_sponsors\"\n",
      "    },\n",
      "    \"content_suggester\": {\n",
      "      \"id\": \"content_suggester\",\n",
      "      \"schema_info\": \"GraphState:\\n type: TypedDict\\n fields:\\n - name: messages\\n   type: Annotated[List[BaseMessage], add_messages]\\n - name: user_query\\n   type: str\\n - name: intent\\n   type: Optional[str]\\n - name: trends\\n   type: Optional[str]\\n - name: sponsors\\n   type: Optional[str]\\n - name: content_suggestion\\n   type: Optional[str]\\n - name: generated_content\\n   type: Optional[str]\\n - name: error\\n   type: Optional[str]\",\n",
      "      \"input_schema\": \"GraphState\",\n",
      "      \"output_schema\": \"GraphState\",\n",
      "      \"description\": \"Suggests high-level content ideas or formats for a social media post based on the topic and current trends. It uses an LLM to brainstorm and synthesize ideas. It might internally call the 'identify_trends' function or a similar tool to get trend data.\",\n",
      "      \"function_name\": \"suggest_content\"\n",
      "    },\n",
      "    \"content_generator\": {\n",
      "      \"id\": \"content_generator\",\n",
      "      \"schema_info\": \"GraphState:\\n type: TypedDict\\n fields:\\n - name: messages\\n   type: Annotated[List[BaseMessage], add_messages]\\n - name: user_query\\n   type: str\\n - name: intent\\n   type: Optional[str]\\n - name: trends\\n   type: Optional[str]\\n - name: sponsors\\n   type: Optional[str]\\n - name: content_suggestion\\n   type: Optional[str]\\n - name: generated_content\\n   type: Optional[str]\\n - name: error\\n   type: Optional[str]\",\n",
      "      \"input_schema\": \"GraphState\",\n",
      "      \"output_schema\": \"GraphState\",\n",
      "      \"description\": \"Generates detailed social media content (e.g., text, hashtags, call to action) based on a description or a content suggestion. It uses an LLM to craft engaging posts, ensuring they align with best practices for social media.\",\n",
      "      \"function_name\": \"generate_content\"\n",
      "    },\n",
      "    \"error_handler\": {\n",
      "      \"id\": \"error_handler\",\n",
      "      \"schema_info\": \"GraphState:\\n type: TypedDict\\n fields:\\n - name: messages\\n   type: Annotated[List[BaseMessage], add_messages]\\n - name: user_query\\n   type: str\\n - name: intent\\n   type: Optional[str]\\n - name: trends\\n   type: Optional[str]\\n - name: sponsors\\n   type: Optional[str]\\n - name: content_suggestion\\n   type: Optional[str]\\n - name: generated_content\\n   type: Optional[str]\\n - name: error\\n   type: Optional[str]\",\n",
      "      \"input_schema\": \"GraphState\",\n",
      "      \"output_schema\": \"GraphState\",\n",
      "      \"description\": \"Handles cases where the supervisor cannot determine a clear intent or an agent encounters an error, providing a helpful message to the user.\",\n",
      "      \"function_name\": \"handle_error\"\n",
      "    }\n",
      "  },\n",
      "  \"edges\": [\n",
      "    {\n",
      "      \"source\": \"__START__\",\n",
      "      \"target\": \"supervisor\",\n",
      "      \"routing_conditions\": \"Always starts with the supervisor to determine intent.\",\n",
      "      \"conditional\": false\n",
      "    },\n",
      "    {\n",
      "      \"source\": \"supervisor\",\n",
      "      \"target\": \"trend_analyst\",\n",
      "      \"routing_conditions\": \"If the supervisor determines the intent is 'identify_trends'.\",\n",
      "      \"conditional\": true\n",
      "    },\n",
      "    {\n",
      "      \"source\": \"supervisor\",\n",
      "      \"target\": \"sponsor_finder\",\n",
      "      \"routing_conditions\": \"If the supervisor determines the intent is 'find_sponsors'.\",\n",
      "      \"conditional\": true\n",
      "    },\n",
      "    {\n",
      "      \"source\": \"supervisor\",\n",
      "      \"target\": \"content_suggester\",\n",
      "      \"routing_conditions\": \"If the supervisor determines the intent is 'suggest_content'.\",\n",
      "      \"conditional\": true\n",
      "    },\n",
      "    {\n",
      "      \"source\": \"supervisor\",\n",
      "      \"target\": \"content_generator\",\n",
      "      \"routing_conditions\": \"If the supervisor determines the intent is 'generate_content'.\",\n",
      "      \"conditional\": true\n",
      "    },\n",
      "    {\n",
      "      \"source\": \"supervisor\",\n",
      "      \"target\": \"error_handler\",\n",
      "      \"routing_conditions\": \"If the supervisor cannot determine a clear intent ('unclear_intent').\",\n",
      "      \"conditional\": true\n",
      "    },\n",
      "    {\n",
      "      \"source\": \"trend_analyst\",\n",
      "      \"target\": \"__END__\",\n",
      "      \"routing_conditions\": \"After identifying trends, the process concludes.\",\n",
      "      \"conditional\": false\n",
      "    },\n",
      "    {\n",
      "      \"source\": \"sponsor_finder\",\n",
      "      \"target\": \"__END__\",\n",
      "      \"routing_conditions\": \"After finding sponsors, the process concludes.\",\n",
      "      \"conditional\": false\n",
      "    },\n",
      "    {\n",
      "      \"source\": \"content_suggester\",\n",
      "      \"target\": \"__END__\",\n",
      "      \"routing_conditions\": \"After suggesting content, the process concludes.\",\n",
      "      \"conditional\": false\n",
      "    },\n",
      "    {\n",
      "      \"source\": \"content_generator\",\n",
      "      \"target\": \"__END__\",\n",
      "      \"routing_conditions\": \"After generating content, the process concludes.\",\n",
      "      \"conditional\": false\n",
      "    },\n",
      "    {\n",
      "      \"source\": \"error_handler\",\n",
      "      \"target\": \"__END__\",\n",
      "      \"routing_conditions\": \"After handling an error, the process concludes.\",\n",
      "      \"conditional\": false\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "\n",
      "# --- Phase 2: Python Code Generation ---\n",
      "\n",
      "# 1. State Definition\n",
      "class GraphState(TypedDict):\n",
      "    \"\"\"\n",
      "    Represents the state of our graph.\n",
      "\n",
      "    Attributes:\n",
      "        messages: A list of messages passed between nodes.\n",
      "        user_query: The initial query from the user.\n",
      "        intent: The identified intent of the user's query.\n",
      "        trends: Information about social media trends.\n",
      "        sponsors: List of suggested sponsors.\n",
      "        content_suggestion: High-level content idea.\n",
      "        generated_content: The final generated social media content.\n",
      "        error: Any error message encountered.\n",
      "    \"\"\"\n",
      "    messages: Annotated[List[BaseMessage], add_messages]\n",
      "    user_query: str\n",
      "    intent: Optional[str]\n",
      "    trends: Optional[str]\n",
      "    sponsors: Optional[str]\n",
      "    content_suggestion: Optional[str]\n",
      "    generated_content: Optional[str]\n",
      "    error: Optional[str]\n",
      "\n",
      "# 2. Node Implementation (Python Functions)\n",
      "\n",
      "# Initialize LLM (TODO: User to specify API key and potentially model)\n",
      "# For OpenAI:\n",
      "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
      "# For Google GenAI:\n",
      "# from langchain_google_genai import ChatGoogleGenerativeAI\n",
      "# llm = ChatGoogleGenerativeAI(model=\"gemini-pro\", temperature=0)\n",
      "\n",
      "# Define Pydantic models for structured output\n",
      "class IntentClassifier(BaseModel):\n",
      "    \"\"\"Identifies the user's intent from a given query.\"\"\"\n",
      "    intent: str = Field(\n",
      "        description=\"The classified intent of the user's query. Must be one of: 'identify_trends', 'find_sponsors', 'suggest_content', 'generate_content', 'unclear_intent'.\"\n",
      "    )\n",
      "    topic: Optional[str] = Field(\n",
      "        description=\"The specific topic or subject mentioned in the user's query, if any. E.g., 'football', 'UCL football', 'AI in tech'.\"\n",
      "    )\n",
      "\n",
      "class TrendOutput(BaseModel):\n",
      "    \"\"\"Represents identified social media trends.\"\"\"\n",
      "    trends: str = Field(description=\"A summary of identified social media trends relevant to the topic.\")\n",
      "\n",
      "class SponsorOutput(BaseModel):\n",
      "    \"\"\"Represents suggested sponsors.\"\"\"\n",
      "    sponsors: str = Field(description=\"A list of relevant people or entities for sponsoring, with brief justification.\")\n",
      "\n",
      "class ContentSuggestionOutput(BaseModel):\n",
      "    \"\"\"Represents a high-level content suggestion.\"\"\"\n",
      "    suggestion: str = Field(description=\"A high-level content idea or format for a social media post.\")\n",
      "\n",
      "class GeneratedContentOutput(BaseModel):\n",
      "    \"\"\"Represents the generated social media content.\"\"\"\n",
      "    content: str = Field(description=\"The full generated social media post, including text, hashtags, and call to action.\")\n",
      "\n",
      "\n",
      "# Define Tools (simulated for demonstration)\n",
      "@tool\n",
      "def search_social_media_trends(query: str) -> str:\n",
      "    \"\"\"\n",
      "    Searches for recent viral social media trends related to a specific query.\n",
      "    Simulates fetching data from a social media analytics platform.\n",
      "    \"\"\"\n",
      "    if \"football\" in query.lower() or \"ucl\" in query.lower():\n",
      "        return \"Recent viral football reels show Q&A formats and short highlight compilations are highly trending. User engagement is high on interactive polls and 'behind-the-scenes' content.\"\n",
      "    elif \"basketball\" in query.lower():\n",
      "        return \"Basketball trends include player challenges, 'top 10 plays' compilations, and fan reaction videos.\"\n",
      "    else:\n",
      "        return f\"No specific trending information found for '{query}'. General trends include short-form video, interactive content, and authentic storytelling.\"\n",
      "\n",
      "@tool\n",
      "def search_influencers(topic: str) -> str:\n",
      "    \"\"\"\n",
      "    Searches for relevant influencers or public figures interested in a specific topic for sponsorship.\n",
      "    Simulates querying an influencer database.\n",
      "    \"\"\"\n",
      "    if \"ucl football\" in topic.lower() or \"football\" in topic.lower():\n",
      "        return \"For UCL football, relevant people include current players like Ousmane Dembele, Kylian MbappÃ©, Jude Bellingham, and football analysts like Jamie Carragher or Rio Ferdinand. Also, popular football fan accounts and sports journalists.\"\n",
      "    elif \"tech\" in topic.lower():\n",
      "        return \"For tech, consider tech reviewers like Marques Brownlee, AI researchers, or popular tech news outlets.\"\n",
      "    else:\n",
      "        return f\"Could not find specific influencers for '{topic}'. Consider general sports figures or content creators in related niches.\"\n",
      "\n",
      "\n",
      "# Bind tools to LLM for agents that might use them\n",
      "llm_with_tools = llm.bind_tools([search_social_media_trends, search_influencers])\n",
      "\n",
      "\n",
      "def route_request(state: GraphState) -> dict:\n",
      "    \"\"\"\n",
      "    Supervisor node: Determines the user's intent and routes the request.\n",
      "    \"\"\"\n",
      "    print(\"---SUPERVISOR: ROUTING REQUEST---\")\n",
      "    user_query = state[\"user_query\"]\n",
      "\n",
      "    # LLM for intent classification\n",
      "    intent_llm = llm.with_structured_output(IntentClassifier)\n",
      "    response = intent_llm.invoke(f\"\"\"\n",
      "    Classify the user's request into one of the following intents:\n",
      "    - 'identify_trends': User wants to know about social media trends.\n",
      "    - 'find_sponsors': User wants to find people for sponsoring posts.\n",
      "    - 'suggest_content': User wants ideas or suggestions for content to post.\n",
      "    - 'generate_content': User wants to generate actual content for a post.\n",
      "    - 'unclear_intent': If the intent is not clear from the above categories.\n",
      "\n",
      "    User query: {user_query}\n",
      "    \"\"\")\n",
      "\n",
      "    intent = response.intent\n",
      "    topic = response.topic if response.topic else user_query # Use user_query as fallback topic\n",
      "\n",
      "    print(f\"Identified Intent: {intent}, Topic: {topic}\")\n",
      "    return {\"intent\": intent, \"user_query\": topic} # Update user_query to be the extracted topic for subsequent nodes\n",
      "\n",
      "\n",
      "def identify_trends(state: GraphState) -> dict:\n",
      "    \"\"\"\n",
      "    Trend Analyst Agent: Identifies social media trends.\n",
      "    \"\"\"\n",
      "    print(\"---TREND ANALYST: IDENTIFYING TRENDS---\")\n",
      "    topic = state[\"user_query\"]\n",
      "    \n",
      "    # Use LLM with tool to simulate trend search\n",
      "    response = llm_with_tools.invoke(f\"\"\"\n",
      "    Identify social media trends related to the topic: '{topic}'.\n",
      "    Consider recent viral content and popular formats.\n",
      "    If a tool is available to search for trends, use it.\n",
      "    \"\"\")\n",
      "\n",
      "    # Check if the LLM decided to use a tool\n",
      "    tool_output = \"\"\n",
      "    if response.tool_calls:\n",
      "        for tool_call in response.tool_calls:\n",
      "            if tool_call.get(\"name\") == \"search_social_media_trends\":\n",
      "                tool_output = search_social_media_trends(tool_call[\"args\"][\"query\"])\n",
      "                print(f\"Tool 'search_social_media_trends' called with query: {tool_call['args']['query']}\")\n",
      "                print(f\"Tool output: {tool_output}\")\n",
      "                break\n",
      "    \n",
      "    if tool_output:\n",
      "        trends_summary = tool_output\n",
      "    else:\n",
      "        # Fallback to LLM generation if tool not used or no output\n",
      "        trends_llm = llm.with_structured_output(TrendOutput)\n",
      "        trends_response = trends_llm.invoke(f\"\"\"\n",
      "        Based on your knowledge, what are the current social media trends related to '{topic}'?\n",
      "        Focus on formats, engagement strategies, and popular themes.\n",
      "        \"\"\")\n",
      "        trends_summary = trends_response.trends\n",
      "\n",
      "    print(f\"Identified Trends: {trends_summary}\")\n",
      "    return {\"trends\": trends_summary, \"messages\": [AIMessage(content=f\"Trends for '{topic}': {trends_summary}\")]}\n",
      "\n",
      "\n",
      "def find_sponsors(state: GraphState) -> dict:\n",
      "    \"\"\"\n",
      "    Sponsor Finder Agent: Finds relevant people for sponsoring posts.\n",
      "    \"\"\"\n",
      "    print(\"---SPONSOR FINDER: FINDING SPONSORS---\")\n",
      "    topic = state[\"user_query\"]\n",
      "\n",
      "    # Use LLM with tool to simulate influencer search\n",
      "    response = llm_with_tools.invoke(f\"\"\"\n",
      "    Find relevant people or entities who would be interested in sponsoring a social media post about '{topic}'.\n",
      "    Consider influencers, public figures, and brands.\n",
      "    If a tool is available to search for influencers, use it.\n",
      "    \"\"\")\n",
      "\n",
      "    tool_output = \"\"\n",
      "    if response.tool_calls:\n",
      "        for tool_call in response.tool_calls:\n",
      "            if tool_call.get(\"name\") == \"search_influencers\":\n",
      "                tool_output = search_influencers(tool_call[\"args\"][\"topic\"])\n",
      "                print(f\"Tool 'search_influencers' called with topic: {tool_call['args']['topic']}\")\n",
      "                print(f\"Tool output: {tool_output}\")\n",
      "                break\n",
      "    \n",
      "    if tool_output:\n",
      "        sponsors_list = tool_output\n",
      "    else:\n",
      "        # Fallback to LLM generation if tool not used or no output\n",
      "        sponsors_llm = llm.with_structured_output(SponsorOutput)\n",
      "        sponsors_response = sponsors_llm.invoke(f\"\"\"\n",
      "        Based on your knowledge, suggest relevant people or entities for sponsoring a social media post about '{topic}'.\n",
      "        Provide a brief reason for each suggestion.\n",
      "        \"\"\")\n",
      "        sponsors_list = sponsors_response.sponsors\n",
      "\n",
      "    print(f\"Suggested Sponsors: {sponsors_list}\")\n",
      "    return {\"sponsors\": sponsors_list, \"messages\": [AIMessage(content=f\"Suggested sponsors for '{topic}': {sponsors_list}\")]}\n",
      "\n",
      "\n",
      "def suggest_content(state: GraphState) -> dict:\n",
      "    \"\"\"\n",
      "    Content Suggester Agent: Suggests high-level content ideas.\n",
      "    \"\"\"\n",
      "    print(\"---CONTENT SUGGESTER: SUGGESTING CONTENT---\")\n",
      "    topic = state[\"user_query\"]\n",
      "    \n",
      "    # The content suggester should consider trends. We can either call the trend tool directly\n",
      "    # or prompt the LLM to consider general trends. For simplicity, we'll prompt the LLM\n",
      "    # to consider trends as part of its general knowledge.\n",
      "    \n",
      "    content_suggester_llm = llm.with_structured_output(ContentSuggestionOutput)\n",
      "    suggestion_response = content_suggester_llm.invoke(f\"\"\"\n",
      "    Suggest a high-level content idea or format for a social media post about '{topic}'.\n",
      "    Consider current social media trends (e.g., short videos, interactive polls, Q&A, highlight reels, behind-the-scenes).\n",
      "    The suggestion should be concise and actionable.\n",
      "    \"\"\")\n",
      "    \n",
      "    suggestion = suggestion_response.suggestion\n",
      "    print(f\"Content Suggestion: {suggestion}\")\n",
      "    return {\"content_suggestion\": suggestion, \"messages\": [AIMessage(content=f\"Content suggestion for '{topic}': {suggestion}\")]}\n",
      "\n",
      "\n",
      "def generate_content(state: GraphState) -> dict:\n",
      "    \"\"\"\n",
      "    Content Generator Agent: Generates detailed social media content.\n",
      "    \"\"\"\n",
      "    print(\"---CONTENT GENERATOR: GENERATING CONTENT---\")\n",
      "    description = state[\"user_query\"] # This could be the original query or a content suggestion from previous step\n",
      "\n",
      "    content_generator_llm = llm.with_structured_output(GeneratedContentOutput)\n",
      "    generated_response = content_generator_llm.invoke(f\"\"\"\n",
      "    Generate a detailed social media post based on the following description/suggestion: '{description}'.\n",
      "    Include engaging text, relevant hashtags, and a call to action if appropriate.\n",
      "    The post should be suitable for platforms like Instagram, X (Twitter), or TikTok.\n",
      "    \"\"\")\n",
      "    \n",
      "    generated_text = generated_response.content\n",
      "    print(f\"Generated Content: {generated_text}\")\n",
      "    return {\"generated_content\": generated_text, \"messages\": [AIMessage(content=f\"Generated post for '{description}':\\n{generated_text}\")]}\n",
      "\n",
      "\n",
      "def handle_error(state: GraphState) -> dict:\n",
      "    \"\"\"\n",
      "    Error Handler: Provides a helpful message for unclear intents or errors.\n",
      "    \"\"\"\n",
      "    print(\"---ERROR HANDLER: HANDLING ERROR---\")\n",
      "    error_message = \"I'm sorry, I couldn't understand your request or encountered an issue. Please try rephrasing your query or specify if you want to identify trends, find sponsors, suggest content, or generate content.\"\n",
      "    print(f\"Error: {error_message}\")\n",
      "    return {\"error\": error_message, \"messages\": [AIMessage(content=error_message)]}\n",
      "\n",
      "\n",
      "# 3. Graph Construction (`StatefulGraph`)\n",
      "\n",
      "# Define the graph\n",
      "workflow = StateGraph(GraphState)\n",
      "\n",
      "# Add nodes\n",
      "workflow.add_node(\"supervisor\", route_request)\n",
      "workflow.add_node(\"trend_analyst\", identify_trends)\n",
      "workflow.add_node(\"sponsor_finder\", find_sponsors)\n",
      "workflow.add_node(\"content_suggester\", suggest_content)\n",
      "workflow.add_node(\"content_generator\", generate_content)\n",
      "workflow.add_node(\"error_handler\", handle_error)\n",
      "\n",
      "# Set the entry point\n",
      "workflow.add_edge(START, \"supervisor\")\n",
      "\n",
      "# Define the conditional routing function for the supervisor\n",
      "def route_to_agent(state: GraphState) -> str:\n",
      "    \"\"\"\n",
      "    Routes the request to the appropriate agent based on the identified intent.\n",
      "    \"\"\"\n",
      "    intent = state[\"intent\"]\n",
      "    if intent == \"identify_trends\":\n",
      "        return \"trend_analyst\"\n",
      "    elif intent == \"find_sponsors\":\n",
      "        return \"sponsor_finder\"\n",
      "    elif intent == \"suggest_content\":\n",
      "        return \"content_suggester\"\n",
      "    elif intent == \"generate_content\":\n",
      "        return \"content_generator\"\n",
      "    else:\n",
      "        return \"error_handler\"\n",
      "\n",
      "# Add conditional edges from the supervisor\n",
      "workflow.add_conditional_edges(\n",
      "    \"supervisor\",\n",
      "    route_to_agent,\n",
      "    {\n",
      "        \"trend_analyst\": \"trend_analyst\",\n",
      "        \"sponsor_finder\": \"sponsor_finder\",\n",
      "        \"content_suggester\": \"content_suggester\",\n",
      "        \"content_generator\": \"content_generator\",\n",
      "        \"error_handler\": \"error_handler\",\n",
      "    },\n",
      ")\n",
      "\n",
      "# Add direct edges from worker agents to END\n",
      "workflow.add_edge(\"trend_analyst\", END)\n",
      "workflow.add_edge(\"sponsor_finder\", END)\n",
      "workflow.add_edge(\"content_suggester\", END)\n",
      "workflow.add_edge(\"content_generator\", END)\n",
      "workflow.add_edge(\"error_handler\", END)\n",
      "\n",
      "# 4. Compilation\n",
      "from langgraph.checkpoint.memory import InMemoryCheckpointer\n",
      "\n",
      "checkpointer = InMemoryCheckpointer()\n",
      "final_app = workflow.compile(checkpointer=checkpointer)\n",
      "\n",
      "# Example Usage (Optional - for testing)\n",
      "if __name__ == \"__main__\":\n",
      "    print(\"--- Social Media Management Agent ---\")\n",
      "    print(\"Type 'exit' to quit.\")\n",
      "\n",
      "    while True:\n",
      "        user_input = input(\"\\nYour query: \")\n",
      "        if user_input.lower() == 'exit':\n",
      "            break\n",
      "\n",
      "        # Initial state for the graph run\n",
      "        initial_state = {\"messages\": [HumanMessage(content=user_input)], \"user_query\": user_input}\n",
      "\n",
      "        # Run the graph\n",
      "        # The output is an iterable, so we can iterate to see intermediate steps if needed\n",
      "        # For simplicity, we'll just get the final state\n",
      "        final_state = None\n",
      "        for s in final_app.stream(initial_state):\n",
      "            print(f\"Current state: {s}\")\n",
      "            final_state = s\n",
      "\n",
      "        # Print the final result based on the updated state\n",
      "        if final_state:\n",
      "            if final_state.get(\"trends\"):\n",
      "                print(f\"\\nResult: {final_state['trends']}\")\n",
      "            elif final_state.get(\"sponsors\"):\n",
      "                print(f\"\\nResult: {final_state['sponsors']}\")\n",
      "            elif final_state.get(\"content_suggestion\"):\n",
      "                print(f\"\\nResult: {final_state['content_suggestion']}\")\n",
      "            elif final_state.get(\"generated_content\"):\n",
      "                print(f\"\\nResult:\\n{final_state['generated_content']}\")\n",
      "            elif final_state.get(\"error\"):\n",
      "                print(f\"\\nResult: {final_state['error']}\")\n",
      "            else:\n",
      "                print(\"\\nNo specific output generated, check graph state.\")\n",
      "                print(final_state)\n",
      "\n",
      "```\n",
      "\n",
      "## Required Keys and Credentials\n",
      "\n",
      "For the generated Python code to execute successfully, you will need to set the following environment variables:\n",
      "\n",
      "*   **`OPENAI_API_KEY`**: This key is required if you are using `ChatOpenAI` as your LLM provider. You can obtain it from the OpenAI platform.\n",
      "\n",
      "    *   **How to set (Linux/macOS):**\n",
      "        ```bash\n",
      "        export OPENAI_API_KEY=\"your_openai_api_key_here\"\n",
      "        ```\n",
      "    *   **How to set (Windows Command Prompt):**\n",
      "        ```cmd\n",
      "        set OPENAI_API_KEY=\"your_openai_api_key_here\"\n",
      "        ```\n",
      "    *   **How to set (Windows PowerShell):**\n",
      "        ```powershell\n",
      "        $env:OPENAI_API_KEY=\"your_openai_api_key_here\"\n",
      "        ```\n",
      "\n",
      "If you choose to use `ChatGoogleGenerativeAI` (commented out in the code), you would instead need:\n",
      "\n",
      "*   **`GOOGLE_API_KEY`**: This key is required for Google's Generative AI models.\n",
      "\n",
      "    *   **How to set (Linux/macOS):**\n",
      "        ```bash\n",
      "        export GOOGLE_API_KEY=\"your_google_api_key_here\"\n",
      "        ```\n",
      "    *   **How to set (Windows Command Prompt):**\n",
      "        ```cmd\n",
      "        set GOOGLE_API_KEY=\"your_google_api_key_here\"\n",
      "        ```\n",
      "    *   **How to set (Windows PowerShell):**\n",
      "        ```powershell\n",
      "        $env:GOOGLE_API_KEY=\"your_google_api_key_here\"\n",
      "        ```\n",
      "\n",
      "**Note:** The `search_social_media_trends` and `search_influencers` tools are currently simulated (i.e., they contain hardcoded logic for demonstration purposes). In a real-world application, these tools would require their own API keys or credentials to connect to actual external services (e.g., social media analytics APIs, influencer databases). These are not included in the current `Required Keys` list as their implementations are mock.\n"
     ]
    }
   ],
   "source": [
    "print(result[\"python_code\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "30db85fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "This example demonstrates how to implement basic tool calling using `langgraph` with `ToolNode` and `tools_condition`.\n",
      "\n",
      "**Key Concepts:**\n",
      "\n",
      "1.  **`StateGraph`**: Defines the state and nodes of our graph.\n",
      "2.  **`Annotated` and `add_messages`**: Used to manage the conversation history in the graph's state.\n",
      "3.  **LangChain Tools**: Standard LangChain tools defined using the `@tool` decorator.\n",
      "4.  **`ChatOpenAI` with `bind_tools`**: An LLM capable of understanding and generating tool calls.\n",
      "5.  **`ToolNode`**: A built-in LangGraph node that executes tool calls present in the state.\n",
      "6.  **`tools_condition`**: A helper function from `langgraph.prebuilt` that checks if the last message from the LLM contains tool calls. This is crucial for routing.\n",
      "7.  **Graph Flow**:\n",
      "    *   Start at the `llm_node`.\n",
      "    *   After the `llm_node`, use `tools_condition` to decide:\n",
      "        *   If tool calls are present, go to `tool_node`.\n",
      "        *   If no tool calls (final answer), end the graph (`END`).\n",
      "    *   After the `tool_node`, always go back to the `llm_node` so the LLM can process the tool results and formulate a final answer or decide on further actions.\n",
      "\n",
      "---\n",
      "\n",
      "```python\n",
      "import os\n",
      "from typing import Annotated, List, Tuple, TypedDict, Union\n",
      "\n",
      "from langchain_core.messages import (\n",
      "    AIMessage,\n",
      "    BaseMessage,\n",
      "    HumanMessage,\n",
      "    ToolMessage,\n",
      ")\n",
      "from langchain_core.tools import tool\n",
      "from langchain_openai import ChatOpenAI\n",
      "\n",
      "from langgraph.graph import END, StateGraph\n",
      "from langgraph.prebuilt import ToolNode, tools_condition\n",
      "\n",
      "# Set your OpenAI API key\n",
      "# os.environ[\"OPENAI_API_KEY\"] = \"YOUR_OPENAI_API_KEY\"\n",
      "# Ensure you have the API key set as an environment variable or uncomment and set it directly.\n",
      "\n",
      "# 1. Define the Graph State\n",
      "# This defines the input and output types for our graph.\n",
      "# 'messages' will hold the conversation history.\n",
      "class AgentState(TypedDict):\n",
      "    messages: Annotated[List[BaseMessage], lambda x, y: x + y]\n",
      "\n",
      "# 2. Define LangChain Tools\n",
      "# These are the functions our LLM can call.\n",
      "\n",
      "@tool\n",
      "def add(a: int, b: int) -> int:\n",
      "    \"\"\"Adds two integers together.\"\"\"\n",
      "    print(f\"Executing tool: add({a}, {b})\")\n",
      "    return a + b\n",
      "\n",
      "@tool\n",
      "def multiply(a: int, b: int) -> int:\n",
      "    \"\"\"Multiplies two integers together.\"\"\"\n",
      "    print(f\"Executing tool: multiply({a}, {b})\")\n",
      "    return a * b\n",
      "\n",
      "@tool\n",
      "def get_current_time() -> str:\n",
      "    \"\"\"Returns the current time as a string.\"\"\"\n",
      "    import datetime\n",
      "    print(\"Executing tool: get_current_time()\")\n",
      "    return datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
      "\n",
      "# List of all tools available to the LLM\n",
      "tools = [add, multiply, get_current_time]\n",
      "\n",
      "# 3. Initialize the LLM\n",
      "# We use ChatOpenAI and bind our tools to it.\n",
      "# This allows the LLM to know about and generate calls to these tools.\n",
      "llm = ChatOpenAI(model=\"gpt-4o\", temperature=0).bind_tools(tools)\n",
      "\n",
      "# 4. Define the LLM Node\n",
      "# This function represents the LLM's turn in the conversation.\n",
      "# It takes the current state (messages) and returns the LLM's response.\n",
      "def call_llm(state: AgentState) -> AgentState:\n",
      "    messages = state[\"messages\"]\n",
      "    print(f\"\\n--- LLM Node: Processing {len(messages)} messages ---\")\n",
      "    response = llm.invoke(messages)\n",
      "    print(f\"LLM Response: {response}\")\n",
      "    return {\"messages\": [response]}\n",
      "\n",
      "# 5. Build the LangGraph\n",
      "graph_builder = StateGraph(AgentState)\n",
      "\n",
      "# Add the LLM node\n",
      "graph_builder.add_node(\"llm_node\", call_llm)\n",
      "\n",
      "# Add the ToolNode. This node automatically executes any tool calls\n",
      "# found in the last message of the state.\n",
      "# It then adds the ToolMessage (result) to the state.\n",
      "tool_node = ToolNode(tools)\n",
      "graph_builder.add_node(\"tool_node\", tool_node)\n",
      "\n",
      "# Set the entry point of the graph\n",
      "graph_builder.set_entry_point(\"llm_node\")\n",
      "\n",
      "# Define the conditional edge from the LLM node\n",
      "# If the LLM's response contains tool calls, go to 'tool_node'.\n",
      "# Otherwise (it's a final answer), end the graph.\n",
      "graph_builder.add_conditional_edges(\n",
      "    \"llm_node\",\n",
      "    tools_condition,\n",
      "    {\n",
      "        \"tools\": \"tool_node\",  # If tools are called, go to tool_node\n",
      "        \"__end__\": END,        # If no tools, end the graph\n",
      "    },\n",
      ")\n",
      "\n",
      "# Define the edge from the tool node\n",
      "# After executing tools, always go back to the LLM node.\n",
      "# This allows the LLM to see the tool results and formulate a final answer.\n",
      "graph_builder.add_edge(\"tool_node\", \"llm_node\")\n",
      "\n",
      "# Compile the graph\n",
      "app = graph_builder.compile()\n",
      "\n",
      "# 6. Test the Graph\n",
      "\n",
      "print(\"\\n--- Test Case 1: Simple question (no tool call) ---\")\n",
      "for s in app.stream({\"messages\": [HumanMessage(content=\"Hello, how are you?\")]}):\n",
      "    print(s)\n",
      "    if \"__end__\" in s:\n",
      "        final_message = s[\"__end__\"][\"messages\"][-1]\n",
      "        print(f\"\\nFinal Answer: {final_message.content}\")\n",
      "\n",
      "print(\"\\n--- Test Case 2: Single tool call (add) ---\")\n",
      "for s in app.stream({\"messages\": [HumanMessage(content=\"What is 15 + 7?\")]}):\n",
      "    print(s)\n",
      "    if \"__end__\" in s:\n",
      "        final_message = s[\"__end__\"][\"messages\"][-1]\n",
      "        print(f\"\\nFinal Answer: {final_message.content}\")\n",
      "\n",
      "print(\"\\n--- Test Case 3: Multiple tool calls (sequential logic) ---\")\n",
      "for s in app.stream({\"messages\": [HumanMessage(content=\"What is 10 + 5, and then multiply that result by 3?\")]}):\n",
      "    print(s)\n",
      "    if \"__end__\" in s:\n",
      "        final_message = s[\"__end__\"][\"messages\"][-1]\n",
      "        print(f\"\\nFinal Answer: {final_message.content}\")\n",
      "\n",
      "print(\"\\n--- Test Case 4: Tool with no arguments ---\")\n",
      "for s in app.stream({\"messages\": [HumanMessage(content=\"What is the current time?\")]}):\n",
      "    print(s)\n",
      "    if \"__end__\" in s:\n",
      "        final_message = s[\"__end__\"][\"messages\"][-1]\n",
      "        print(f\"\\nFinal Answer: {final_message.content}\")\n",
      "\n",
      "print(\"\\n--- Test Case 5: Multiple tool calls (parallel execution if LLM requests) ---\")\n",
      "# Note: The LLM might choose to call these sequentially or in parallel.\n",
      "# ToolNode will execute all tool calls it receives in one go.\n",
      "for s in app.stream({\"messages\": [HumanMessage(content=\"What is 20 * 5 and also what is 100 + 25?\")]}):\n",
      "    print(s)\n",
      "    if \"__end__\" in s:\n",
      "        final_message = s[\"__end__\"][\"messages\"][-1]\n",
      "        print(f\"\\nFinal Answer: {final_message.content}\")\n",
      "\n",
      "```\n",
      "\n",
      "**Explanation of the Flow:**\n",
      "\n",
      "1.  **User Input (`HumanMessage`)**: The graph starts with a `HumanMessage` in the `messages` state.\n",
      "2.  **`llm_node`**:\n",
      "    *   The `call_llm` function is executed.\n",
      "    *   It takes the `messages` from the state and invokes the `llm`.\n",
      "    *   The `llm` (which is `bind_tools`'d) decides whether to respond directly or call a tool.\n",
      "    *   The LLM's `AIMessage` (which might contain `tool_calls`) is added to the state.\n",
      "3.  **Conditional Edge (`tools_condition`)**:\n",
      "    *   After `llm_node` completes, `tools_condition` is evaluated.\n",
      "    *   If the `AIMessage` from the LLM contains `tool_calls`:\n",
      "        *   The graph transitions to the `tool_node`.\n",
      "    *   If the `AIMessage` does *not* contain `tool_calls` (meaning it's a direct answer):\n",
      "        *   The graph transitions to `END`.\n",
      "4.  **`tool_node`**:\n",
      "    *   If the graph goes to `tool_node`, the `ToolNode` automatically inspects the last `AIMessage` in the state.\n",
      "    *   It extracts any `tool_calls` and executes the corresponding Python functions (`add`, `multiply`, `get_current_time`).\n",
      "    *   The results of these tool executions are then added to the state as `ToolMessage` objects.\n",
      "5.  **Edge from `tool_node` to `llm_node`**:\n",
      "    *   After `tool_node` finishes, the graph always transitions back to `llm_node`.\n",
      "    *   This is crucial because the LLM needs to see the `ToolMessage` (the results of the tool calls) to formulate a coherent final answer to the user or decide if further tool calls are needed.\n",
      "6.  **Looping**: Steps 2-5 can repeat multiple times if the LLM needs to perform a sequence of tool calls to answer a complex query (e.g., \"add X and Y, then multiply the result by Z\").\n",
      "7.  **`END`**: The graph terminates when the `llm_node` produces an `AIMessage` that does not contain any `tool_calls`, indicating a final answer.\n",
      "\n",
      "This setup provides a robust and flexible way to integrate LLMs with external tools within a structured conversational flow.\n"
     ]
    }
   ],
   "source": [
    "response = llm.invoke([HumanMessage(content=\"implement basic tool calling via langgraph and langchain, use ToolNode and tools_condition\")])\n",
    "\n",
    "response.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5034b621",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(graph.invoke(Command(resume=\"Edited text\"), config=config)) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
