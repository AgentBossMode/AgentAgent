{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://python.langchain.com/docs/integrations/vectorstores/neo4jvector/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"bolt://localhost:7687\"\n",
    "username = \"neo4j\"\n",
    "password = \"password\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_neo4j import Neo4jVector\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "import os\n",
    "from langchain_openai import AzureOpenAIEmbeddings\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "embeddings = AzureOpenAIEmbeddings(\n",
    "openai_api_type=\"azure\",\n",
    "openai_api_version=os.environ[\"OPENAI_API_EMBEDDING_VERSION\"],\n",
    "openai_api_key=os.environ[\"OPENAI_API_EMBEDDING_KEY\"],\n",
    "azure_endpoint=os.environ[\"AZURE_OPENAI_EMBEDDING_ENDPOINT\"],\n",
    "deployment=os.environ[\"AZURE_OPENAI_EMBEDDING_DEPLOYMENT\"],\n",
    "model=os.environ[\"AZURE_OPENAI_EMBEDDING_MODEL\"],\n",
    "validate_base_url=True,\n",
    ")\n",
    "\n",
    "\n",
    "vector_store = Neo4jVector.from_existing_index(\n",
    "    embeddings,\n",
    "    url=url,\n",
    "    username=username,\n",
    "    password=password,\n",
    "    index_name=\"neo4j\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://langchain-ai.github.io/langgraph/how-tos/react-agent-from-scratch'}, page_content='## Use ReAct agentÂ¶\\n\\nNow that we have created our react agent, let\\'s actually put it to the test!\\n\\n    \\n    \\n    # Helper function for formatting the stream nicely\\n    def print_stream(stream):\\n        for s in stream:\\n            message = s[\"messages\"][-1]\\n            if isinstance(message, tuple):\\n                print(message)\\n            else:\\n                message.pretty_print()\\n    \\n    \\n    inputs = {\"messages\": [(\"user\", \"what is the weather in sf\")]}\\n    print_stream(graph.stream(inputs, stream_mode=\"values\"))\\n    \\n    \\n    \\n    ================================\\x1b[1m Human Message \\x1b[0m=================================\\n    \\n    what is the weather in sf\\n    ==================================\\x1b[1m Ai Message \\x1b[0m==================================\\n    Tool Calls:\\n      get_weather (call_azW0cQ4XjWWj0IAkWAxq9nLB)\\n     Call ID: call_azW0cQ4XjWWj0IAkWAxq9nLB\\n      Args:\\n        location: San Francisco\\n    =================================\\x1b[1m Tool Message \\x1b[0m=================================\\n    Name: get_weather\\n    \\n    \"It\\'s sunny in San Francisco, but you better look out if you\\'re a Gemini \\\\ud83d\\\\ude08.\"\\n    ==================================\\x1b[1m Ai Message \\x1b[0m==================================\\n    \\n    The weather in San Francisco is sunny! However, it seems there\\'s a playful warning for Geminis. Enjoy the sunshine!\\n    \\n\\nPerfect! The graph correctly calls the `get_weather` tool and responds to the\\nuser after receiving the information from the tool.\\n\\nWas this page helpful?\\n\\nThanks for your feedback!\\n\\nThanks for your feedback! Please help us improve this page by adding to the\\ndiscussion below.\\n\\n## Comments'),\n",
       " Document(metadata={'source': 'https://langchain-ai.github.io/langgraph/how-tos/create-react-agent'}, page_content='[ ](https://github.com/langchain-ai/langgraph/edit/main/docs/docs/how-\\ntos/create-react-agent.ipynb \"Edit this page\")\\n\\n# How to use the pre-built ReAct agentÂ¶\\n\\nPrerequisites\\n\\nThis guide assumes familiarity with the following:\\n\\n  * [ Agent Architectures ](https://langchain-ai.github.io/langgraph/concepts/agentic_concepts/)\\n  * [ Chat Models ](https://python.langchain.com/docs/concepts/chat_models/)\\n  * [ Tools ](https://python.langchain.com/docs/concepts/tools/)\\n\\nIn this how-to we\\'ll create a simple [ReAct](https://arxiv.org/abs/2210.03629)\\nagent app that can check the weather. The app consists of an agent (LLM) and\\ntools. As we interact with the app, we will first call the agent (LLM) to\\ndecide if we should use tools. Then we will run a loop:\\n\\n  1. If the agent said to take an action (i.e. call tool), we\\'ll run the tools and pass the results back to the agent\\n  2. If the agent did not ask to run tools, we will finish (respond to the user)\\n\\nPrebuilt Agent\\n\\nPlease note that here will we use [a prebuilt agent](https://langchain-\\nai.github.io/langgraph/reference/prebuilt/#langgraph.prebuilt.chat_agent_executor.create_react_agent).\\nOne of the big benefits of LangGraph is that you can easily create your own\\nagent architectures. So while it\\'s fine to start here to build an agent\\nquickly, we would strongly recommend learning how to build your own agent so\\nthat you can take full advantage of LangGraph.\\n\\n## SetupÂ¶\\n\\nFirst let\\'s install the required packages and set our API keys\\n\\n    \\n    \\n    %%capture --no-stderr\\n    %pip install -U langgraph langchain-openai\\n    \\n    \\n    \\n    import getpass\\n    import os\\n    \\n    \\n    def _set_env(var: str):\\n        if not os.environ.get(var):\\n            os.environ[var] = getpass.getpass(f\"{var}: \")\\n    \\n    \\n    _set_env(\"OPENAI_API_KEY\")\\n    \\n\\nSet up [LangSmith](https://smith.langchain.com) for LangGraph development\\n\\nSign up for LangSmith to quickly spot issues and improve the performance of\\nyour LangGraph projects. LangSmith lets you use trace data to debug, test, and\\nmonitor your LLM apps built with LangGraph â€” read more about how to get\\nstarted [here](https://docs.smith.langchain.com).\\n\\n## CodeÂ¶\\n\\n    \\n    \\n    # First we initialize the model we want to use.\\n    from langchain_openai import ChatOpenAI\\n    \\n    model = ChatOpenAI(model=\"gpt-4o\", temperature=0)\\n    \\n    \\n    # For this tutorial we will use custom tool that returns pre-defined values for weather in two cities (NYC & SF)\\n    \\n    from typing import Literal\\n    \\n    from langchain_core.tools import tool\\n    \\n    \\n    @tool\\n    def get_weather(city: Literal[\"nyc\", \"sf\"]):\\n        \"\"\"Use this to get weather information.\"\"\"\\n        if city == \"nyc\":\\n            return \"It might be cloudy in nyc\"\\n        elif city == \"sf\":\\n            return \"It\\'s always sunny in sf\"\\n        else:\\n            raise AssertionError(\"Unknown city\")\\n    \\n    \\n    tools = [get_weather]\\n    \\n    \\n    # Define the graph\\n    \\n    from langgraph.prebuilt import create_react_agent\\n    \\n    graph = create_react_agent(model, tools=tools)\\n    \\n\\nAPI Reference: [ChatOpenAI](https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html) | [tool](https://python.langchain.com/api_reference/core/tools/langchain_core.tools.convert.tool.html) | [create_react_agent](https://langchain-ai.github.io/langgraph/reference/prebuilt/#langgraph.prebuilt.chat_agent_executor.create_react_agent)\\n\\n## UsageÂ¶\\n\\nFirst, let\\'s visualize the graph we just created\\n\\n    \\n    \\n    from IPython.display import Image, display\\n    \\n    display(Image(graph.get_graph().draw_mermaid_png()))'),\n",
       " Document(metadata={'source': 'https://langchain-ai.github.io/langgraph/how-tos/react-agent-from-scratch'}, page_content='[ ](https://github.com/langchain-ai/langgraph/edit/main/docs/docs/how-\\ntos/react-agent-from-scratch.ipynb \"Edit this page\")\\n\\n# How to create a ReAct agent from scratchÂ¶\\n\\nPrerequisites\\n\\nThis guide assumes familiarity with the following:\\n\\n  * [Tool calling agent](../../concepts/agentic_concepts/#tool-calling-agent)\\n  * [Chat Models](https://python.langchain.com/docs/concepts/chat_models/)\\n  * [Messages](https://python.langchain.com/docs/concepts/messages/)\\n  * [LangGraph Glossary](../../concepts/low_level/)\\n\\nUsing the prebuilt ReAct agent\\n[create_react_agent](../../reference/prebuilt/#langgraph.prebuilt.chat_agent_executor.create_react_agent)\\nis a great way to get started, but sometimes you might want more control and\\ncustomization. In those cases, you can create a custom ReAct agent. This guide\\nshows how to implement ReAct agent from scratch using LangGraph.\\n\\n## SetupÂ¶\\n\\nFirst, let\\'s install the required packages and set our API keys:\\n\\n    \\n    \\n    %%capture --no-stderr\\n    %pip install -U langgraph langchain-openai\\n    \\n    \\n    \\n    import getpass\\n    import os\\n    \\n    \\n    def _set_env(var: str):\\n        if not os.environ.get(var):\\n            os.environ[var] = getpass.getpass(f\"{var}: \")\\n    \\n    \\n    _set_env(\"OPENAI_API_KEY\")\\n    \\n\\nSet up [LangSmith](https://smith.langchain.com) for better debugging\\n\\nSign up for LangSmith to quickly spot issues and improve the performance of\\nyour LangGraph projects. LangSmith lets you use trace data to debug, test, and\\nmonitor your LLM aps built with LangGraph â€” read more about how to get started\\nin the [docs](https://docs.smith.langchain.com).\\n\\n## Create ReAct agentÂ¶\\n\\nNow that you have installed the required packages and set your environment\\nvariables, we can code our ReAct agent!\\n\\n### Define graph stateÂ¶\\n\\nWe are going to define the most basic ReAct state in this example, which will\\njust contain a list of messages.\\n\\nFor your specific use case, feel free to add any other state keys that you\\nneed.\\n\\n    \\n    \\n    from typing import (\\n        Annotated,\\n        Sequence,\\n        TypedDict,\\n    )\\n    from langchain_core.messages import BaseMessage\\n    from langgraph.graph.message import add_messages\\n    \\n    \\n    class AgentState(TypedDict):\\n        \"\"\"The state of the agent.\"\"\"\\n    \\n        # add_messages is a reducer\\n        # See https://langchain-ai.github.io/langgraph/concepts/low_level/#reducers\\n        messages: Annotated[Sequence[BaseMessage], add_messages]\\n    \\n\\nAPI Reference: [BaseMessage](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.base.BaseMessage.html) | [add_messages](https://langchain-ai.github.io/langgraph/reference/graphs/#langgraph.graph.message.add_messages)\\n\\n### Define model and toolsÂ¶\\n\\nNext, let\\'s define the tools and model we will use for our example.\\n\\n    \\n    \\n    from langchain_openai import ChatOpenAI\\n    from langchain_core.tools import tool\\n    \\n    model = ChatOpenAI(model=\"gpt-4o-mini\")\\n    \\n    \\n    @tool\\n    def get_weather(location: str):\\n        \"\"\"Call to get the weather from a specific location.\"\"\"\\n        # This is a placeholder for the actual implementation\\n        # Don\\'t let the LLM know this though ðŸ˜Š\\n        if any([city in location.lower() for city in [\"sf\", \"san francisco\"]]):\\n            return \"It\\'s sunny in San Francisco, but you better look out if you\\'re a Gemini ðŸ˜ˆ.\"\\n        else:\\n            return f\"I am not sure what the weather is in {location}\"\\n    \\n    \\n    tools = [get_weather]\\n    \\n    model = model.bind_tools(tools)\\n    \\n\\nAPI Reference: [ChatOpenAI](https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html) | [tool](https://python.langchain.com/api_reference/core/tools/langchain_core.tools.convert.tool.html)\\n\\n### Define nodes and edgesÂ¶\\n\\nNext let\\'s define our nodes and edges. In our basic ReAct agent there are only\\ntwo nodes, one for calling the model and one for using tools, however you can\\nmodify this basic structure to work better for your use case. The tool node we\\ndefine here is a simplified version of the prebuilt\\n[`ToolNode`](https://langchain-ai.github.io/langgraph/how-tos/tool-calling/),\\nwhich has some additional features.\\n\\nPerhaps you want to add a node for [adding structured\\noutput](https://langchain-ai.github.io/langgraph/how-tos/react-agent-\\nstructured-output/) or a node for executing some external action (sending an\\nemail, adding a calendar event, etc.). Maybe you just want to change the way\\nthe `call_model` node works and how `should_continue` decides whether to call\\ntools - the possibilities are endless and LangGraph makes it easy to customize\\nthis basic structure for your specific use case.\\n\\n    \\n    \\n    import json\\n    from langchain_core.messages import ToolMessage, SystemMessage\\n    from langchain_core.runnables import RunnableConfig\\n    \\n    tools_by_name = {tool.name: tool for tool in tools}\\n    \\n    \\n    # Define our tool node\\n    def tool_node(state: AgentState):\\n        outputs = []\\n        for tool_call in state[\"messages\"][-1].tool_calls:\\n            tool_result = tools_by_name[tool_call[\"name\"]].invoke(tool_call[\"args\"])\\n            outputs.append(\\n                ToolMessage(\\n                    content=json.dumps(tool_result),\\n                    name=tool_call[\"name\"],\\n                    tool_call_id=tool_call[\"id\"],\\n                )\\n            )\\n        return {\"messages\": outputs}\\n    \\n    \\n    # Define the node that calls the model\\n    def call_model(\\n        state: AgentState,\\n        config: RunnableConfig,\\n    ):\\n        # this is similar to customizing the create_react_agent with \\'prompt\\' parameter, but is more flexible\\n        system_prompt = SystemMessage(\\n            \"You are a helpful AI assistant, please respond to the users query to the best of your ability!\"\\n        )\\n        response = model.invoke([system_prompt] + state[\"messages\"], config)\\n        # We return a list, because this will get added to the existing list\\n        return {\"messages\": [response]}\\n    \\n    \\n    # Define the conditional edge that determines whether to continue or not\\n    def should_continue(state: AgentState):\\n        messages = state[\"messages\"]\\n        last_message = messages[-1]\\n        # If there is no function call, then we finish\\n        if not last_message.tool_calls:\\n            return \"end\"\\n        # Otherwise if there is, we continue\\n        else:\\n            return \"continue\"'),\n",
       " Document(metadata={'source': 'https://langchain-ai.github.io/langgraph/how-tos/react-agent-from-scratch-functional'}, page_content='[ ](https://github.com/langchain-ai/langgraph/edit/main/docs/docs/how-\\ntos/react-agent-from-scratch-functional.ipynb \"Edit this page\")\\n\\n# How to create a ReAct agent from scratch (Functional API)Â¶\\n\\nPrerequisites\\n\\nThis guide assumes familiarity with the following:\\n\\n  * [Chat Models](https://python.langchain.com/docs/concepts/chat_models)\\n  * [Messages](https://python.langchain.com/docs/concepts/messages)\\n  * [Tool Calling](https://python.langchain.com/docs/concepts/tool_calling/)\\n  * [Entrypoints](../../concepts/functional_api/#entrypoint) and [Tasks](../../concepts/functional_api/#task)\\n\\nThis guide demonstrates how to implement a ReAct agent using the LangGraph\\n[Functional API](../../concepts/functional_api).\\n\\nThe ReAct agent is a [tool-calling\\nagent](../../concepts/agentic_concepts/#tool-calling-agent) that operates as\\nfollows:\\n\\n  1. Queries are issued to a chat model;\\n  2. If the model generates no [tool calls](../../concepts/agentic_concepts/#tool-calling), we return the model response.\\n  3. If the model generates tool calls, we execute the tool calls with available tools, append them as [tool messages](https://python.langchain.com/docs/concepts/messages/) to our message list, and repeat the process.\\n\\nThis is a simple and versatile set-up that can be extended with memory, human-\\nin-the-loop capabilities, and other features. See the dedicated [how-to\\nguides](../../how-tos/#prebuilt-react-agent) for examples.\\n\\n## SetupÂ¶\\n\\nFirst, let\\'s install the required packages and set our API keys:\\n\\n    \\n    \\n    %%capture --no-stderr\\n    %pip install -U langgraph langchain-openai\\n    \\n    \\n    \\n    import getpass\\n    import os\\n    \\n    \\n    def _set_env(var: str):\\n        if not os.environ.get(var):\\n            os.environ[var] = getpass.getpass(f\"{var}: \")\\n    \\n    \\n    _set_env(\"OPENAI_API_KEY\")\\n    \\n\\nSet up [LangSmith](https://smith.langchain.com) for better debugging\\n\\nSign up for LangSmith to quickly spot issues and improve the performance of\\nyour LangGraph projects. LangSmith lets you use trace data to debug, test, and\\nmonitor your LLM aps built with LangGraph â€” read more about how to get started\\nin the [docs](https://docs.smith.langchain.com).\\n\\n## Create ReAct agentÂ¶\\n\\nNow that you have installed the required packages and set your environment\\nvariables, we can create our agent.\\n\\n### Define model and toolsÂ¶\\n\\nLet\\'s first define the tools and model we will use for our example. Here we\\nwill use a single place-holder tool that gets a description of the weather for\\na location.\\n\\nWe will use an\\n[OpenAI](https://python.langchain.com/docs/integrations/providers/openai/)\\nchat model for this example, but any model [supporting tool-\\ncalling](https://python.langchain.com/docs/integrations/chat/) will suffice.\\n\\n    \\n    \\n    from langchain_openai import ChatOpenAI\\n    from langchain_core.tools import tool\\n    \\n    model = ChatOpenAI(model=\"gpt-4o-mini\")\\n    \\n    \\n    @tool\\n    def get_weather(location: str):\\n        \"\"\"Call to get the weather from a specific location.\"\"\"\\n        # This is a placeholder for the actual implementation\\n        if any([city in location.lower() for city in [\"sf\", \"san francisco\"]]):\\n            return \"It\\'s sunny!\"\\n        elif \"boston\" in location.lower():\\n            return \"It\\'s rainy!\"\\n        else:\\n            return f\"I am not sure what the weather is in {location}\"\\n    \\n    \\n    tools = [get_weather]\\n    \\n\\nAPI Reference: [ChatOpenAI](https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html) | [tool](https://python.langchain.com/api_reference/core/tools/langchain_core.tools.convert.tool.html)\\n\\n### Define tasksÂ¶\\n\\nWe next define the [tasks](../../concepts/functional_api/#task) we will\\nexecute. Here there are two different tasks:\\n\\n  1. **Call model** : We want to query our chat model with a list of messages.\\n  2. **Call tool** : If our model generates tool calls, we want to execute them.\\n\\n    \\n    \\n    from langchain_core.messages import ToolMessage\\n    from langgraph.func import entrypoint, task\\n    \\n    tools_by_name = {tool.name: tool for tool in tools}\\n    \\n    \\n    @task\\n    def call_model(messages):\\n        \"\"\"Call model with a sequence of messages.\"\"\"\\n        response = model.bind_tools(tools).invoke(messages)\\n        return response\\n    \\n    \\n    @task\\n    def call_tool(tool_call):\\n        tool = tools_by_name[tool_call[\"name\"]]\\n        observation = tool.invoke(tool_call[\"args\"])\\n        return ToolMessage(content=observation, tool_call_id=tool_call[\"id\"])\\n    \\n\\nAPI Reference: [ToolMessage](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.tool.ToolMessage.html) | [entrypoint](https://langchain-ai.github.io/langgraph/reference/func/#langgraph.func.entrypoint) | [task](https://langchain-ai.github.io/langgraph/reference/func/#langgraph.func.task)\\n\\n### Define entrypointÂ¶\\n\\nOur [entrypoint](../../concepts/functional_api/#entrypoint) will handle the\\norchestration of these two tasks. As described above, when our `call_model`\\ntask generates tool calls, the `call_tool` task will generate responses for\\neach. We append all messages to a single messages list.\\n\\nTip\\n\\nNote that because tasks return future-like objects, the below implementation\\nexecutes tools in parallel.\\n\\n    \\n    \\n    from langgraph.graph.message import add_messages\\n    \\n    \\n    @entrypoint()\\n    def agent(messages):\\n        llm_response = call_model(messages).result()\\n        while True:\\n            if not llm_response.tool_calls:\\n                break\\n    \\n            # Execute tools\\n            tool_result_futures = [\\n                call_tool(tool_call) for tool_call in llm_response.tool_calls\\n            ]\\n            tool_results = [fut.result() for fut in tool_result_futures]\\n    \\n            # Append to message list\\n            messages = add_messages(messages, [llm_response, *tool_results])\\n    \\n            # Call model again\\n            llm_response = call_model(messages).result()\\n    \\n        return llm_response\\n    \\n\\nAPI Reference: [add_messages](https://langchain-\\nai.github.io/langgraph/reference/graphs/#langgraph.graph.message.add_messages)\\n\\n## UsageÂ¶\\n\\nTo use our agent, we invoke it with a messages list. Based on our\\nimplementation, these can be LangChain\\n[message](https://python.langchain.com/docs/concepts/messages/) objects or\\nOpenAI-style dicts:')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_store.similarity_search(\"generate react-agent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vector_store.as_retriever()\n",
    "from langchain.tools.retriever import create_retriever_tool\n",
    "\n",
    "retriever_tool = create_retriever_tool(\n",
    "    retriever,\n",
    "    \"retrieve_blog_posts\",\n",
    "    \"Search and return information about Lilian Weng blog posts on LLM agents, prompt engineering, and adversarial attacks on LLMs.\",\n",
    ")\n",
    "\n",
    "tools = [retriever_tool]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "from langchain import hub\n",
    "from langchain_core.messages import BaseMessage, HumanMessage, SystemMessage\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "from langgraph.prebuilt import tools_condition\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "\n",
    "### Edges\n",
    "\n",
    "\n",
    "def grade_documents(state) -> Literal[\"generate\", \"rewrite\"]:\n",
    "    \"\"\"\n",
    "    Determines whether the retrieved documents are relevant to the question.\n",
    "\n",
    "    Args:\n",
    "        state (messages): The current state\n",
    "\n",
    "    Returns:\n",
    "        str: A decision for whether the documents are relevant or not\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---CHECK RELEVANCE---\")\n",
    "\n",
    "    # Data model\n",
    "    class grade(BaseModel):\n",
    "        \"\"\"Binary score for relevance check.\"\"\"\n",
    "\n",
    "        binary_score: str = Field(description=\"Relevance score 'yes' or 'no'\")\n",
    "\n",
    "    # LLM\n",
    "    model = ChatOpenAI(temperature=0, model=\"gpt-4o-mini\", streaming=True)\n",
    "\n",
    "    # LLM with tool and validation\n",
    "    llm_with_tool = model.with_structured_output(grade)\n",
    "\n",
    "    # Prompt\n",
    "    prompt = PromptTemplate(\n",
    "        template=\"\"\"You are a grader assessing relevance of a retrieved document to a user question. \\n \n",
    "        Here is the retrieved document: \\n\\n {context} \\n\\n\n",
    "        Here is the user question: {question} \\n\n",
    "        If the document contains keyword(s) or semantic meaning related to the user question, grade it as relevant. \\n\n",
    "        Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question.\"\"\",\n",
    "        input_variables=[\"context\", \"question\"],\n",
    "    )\n",
    "\n",
    "    # Chain\n",
    "    chain = prompt | llm_with_tool\n",
    "\n",
    "    messages = state[\"messages\"]\n",
    "    last_message = messages[-1]\n",
    "\n",
    "    question = messages[0].content\n",
    "    docs = last_message.content\n",
    "\n",
    "    scored_result = chain.invoke({\"question\": question, \"context\": docs})\n",
    "\n",
    "    score = scored_result.binary_score\n",
    "\n",
    "    if score == \"yes\":\n",
    "        print(\"---DECISION: DOCS RELEVANT---\")\n",
    "        return \"generate\"\n",
    "\n",
    "    else:\n",
    "        print(\"---DECISION: DOCS NOT RELEVANT---\")\n",
    "        print(score)\n",
    "        return \"rewrite\"\n",
    "\n",
    "\n",
    "### Nodes\n",
    "planner_prompt = \"\"\"\n",
    "<Task>\n",
    "You will help plan the steps to implement a LangGraph application based on the user's request. \n",
    "</Task>\n",
    "\n",
    "<Instructions>\n",
    "1. Reflect on the user's request and the project scope\n",
    "2. Use the retrieval tool to get access to the LangGraph documentation.\n",
    "3. Do not hallucinate or generate langgraph code on your own, only trust the accessed information. \n",
    "</Instructions>\n",
    "\"\"\"\n",
    "\n",
    "def agent(state):\n",
    "    \"\"\"\n",
    "    Invokes the agent model to generate a response based on the current state. Given\n",
    "    the question, it will decide to retrieve using the retriever tool, or simply end.\n",
    "\n",
    "    Args:\n",
    "        state (messages): The current state\n",
    "\n",
    "    Returns:\n",
    "        dict: The updated state with the agent response appended to messages\n",
    "    \"\"\"\n",
    "    print(\"---CALL AGENT---\")\n",
    "    messages = state[\"messages\"]\n",
    "    model = ChatOpenAI(temperature=0, streaming=True, model=\"gpt-4o-mini\")\n",
    "    model = model.bind_tools(tools)\n",
    "    response = model.invoke([SystemMessage(content=planner_prompt)]  + messages)\n",
    "    # We return a list, because this will get added to the existing list\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "\n",
    "def rewrite(state):\n",
    "    \"\"\"\n",
    "    Transform the query to produce a better question.\n",
    "\n",
    "    Args:\n",
    "        state (messages): The current state\n",
    "\n",
    "    Returns:\n",
    "        dict: The updated state with re-phrased question\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---TRANSFORM QUERY---\")\n",
    "    messages = state[\"messages\"]\n",
    "    question = messages[0].content\n",
    "\n",
    "    msg = [\n",
    "        HumanMessage(\n",
    "            content=f\"\"\" \\n \n",
    "    Look at the input and try to reason about the underlying semantic intent / meaning. \\n \n",
    "    Here is the initial question:\n",
    "    \\n ------- \\n\n",
    "    {question} \n",
    "    \\n ------- \\n\n",
    "    Formulate an improved question: \"\"\",\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    # Grader\n",
    "    model = ChatOpenAI(temperature=0, model=\"gpt-4o-mini\", streaming=True)\n",
    "    response = model.invoke(msg)\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "\n",
    "\n",
    "def generate(state):\n",
    "    \"\"\"\n",
    "    Generate answer\n",
    "\n",
    "    Args:\n",
    "        state (messages): The current state\n",
    "\n",
    "    Returns:\n",
    "         dict: The updated state with re-phrased question\n",
    "    \"\"\"\n",
    "    print(\"---GENERATE---\")\n",
    "    messages = state[\"messages\"]\n",
    "    question = messages[0].content\n",
    "    last_message = messages[-1]\n",
    "\n",
    "    docs = last_message.content\n",
    "\n",
    "    # Prompt\n",
    "    \n",
    "    prompt = PromptTemplate.from_template(\n",
    "\"\"\"Your job is to generate python code for the user query using langgraph sdk.\n",
    "1. Do not hallucinate when generating code.\n",
    "2. Only refer to the context shared with you\n",
    "\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\n",
    "<query>\n",
    "{question}\n",
    "<query>\n",
    "\"\"\")\n",
    "\n",
    "    # LLM\n",
    "    llm = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0, streaming=True)\n",
    "\n",
    "    # Post-processing\n",
    "    def format_docs(docs):\n",
    "        return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "    # Chain\n",
    "    rag_chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "    # Run\n",
    "    response = rag_chain.invoke({\"context\": docs, \"question\": question})\n",
    "    return {\"messages\": [response]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import END, StateGraph, START, MessagesState\n",
    "from langgraph.prebuilt import ToolNode\n",
    "\n",
    "# Define a new graph\n",
    "workflow = StateGraph(MessagesState)\n",
    "\n",
    "# Define the nodes we will cycle between\n",
    "workflow.add_node(\"agent\", agent)  # agent\n",
    "retrieve = ToolNode([retriever_tool])\n",
    "workflow.add_node(\"retrieve\", retrieve)  # retrieval\n",
    "workflow.add_node(\"rewrite\", rewrite)  # Re-writing the question\n",
    "workflow.add_node(\n",
    "    \"generate\", generate\n",
    ")  # Generating a response after we know the documents are relevant\n",
    "# Call agent node to decide to retrieve or not\n",
    "workflow.add_edge(START, \"agent\")\n",
    "\n",
    "# Decide whether to retrieve\n",
    "workflow.add_conditional_edges(\n",
    "    \"agent\",\n",
    "    # Assess agent decision\n",
    "    tools_condition,\n",
    "    {\n",
    "        # Translate the condition outputs to nodes in our graph\n",
    "        \"tools\": \"retrieve\",\n",
    "        END: END,\n",
    "    },\n",
    ")\n",
    "\n",
    "# Edges taken after the `action` node is called.\n",
    "workflow.add_conditional_edges(\n",
    "    \"retrieve\",\n",
    "    # Assess agent decision\n",
    "    grade_documents,\n",
    ")\n",
    "workflow.add_edge(\"generate\", END)\n",
    "workflow.add_edge(\"rewrite\", \"agent\")\n",
    "\n",
    "# Compile\n",
    "graph = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "try:\n",
    "    display(Image(graph.get_graph(xray=True).draw_mermaid_png()))\n",
    "except Exception:\n",
    "    # This requires some extra dependencies and is optional\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---CALL AGENT---\n",
      "\"Output from node 'agent':\"\n",
      "'---'\n",
      "{ 'messages': [ AIMessage(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': 'call_m8jtKFV1fsmy1DtViFomA57D', 'function': {'arguments': '{\"query\":\"LangGraph Web Research STORM\"}', 'name': 'retrieve_blog_posts'}, 'type': 'function'}]}, response_metadata={'finish_reason': 'tool_calls', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_86d0290411'}, id='run-a1fe0109-cab8-4807-9786-d4c56e65e83c-0', tool_calls=[{'name': 'retrieve_blog_posts', 'args': {'query': 'LangGraph Web Research STORM'}, 'id': 'call_m8jtKFV1fsmy1DtViFomA57D', 'type': 'tool_call'}])]}\n",
      "'\\n---\\n'\n",
      "---CHECK RELEVANCE---\n",
      "---DECISION: DOCS RELEVANT---\n",
      "\"Output from node 'retrieve':\"\n",
      "'---'\n",
      "{ 'messages': [ ToolMessage(content='## Final FlowÂ¶\\n\\nNow it\\'s time to string everything together. We will have 6 main stages in\\nsequence: . 1\\\\. Generate the initial outline + perspectives 2\\\\. Batch converse\\nwith each perspective to expand the content for the article 3\\\\. Refine the\\noutline based on the conversations 4\\\\. Index the reference docs from the\\nconversations 5\\\\. Write the individual sections of the article 6\\\\. Write the\\nfinal wiki\\n\\nThe state tracks the outputs of each stage.\\n\\n    \\n    \\n    class ResearchState(TypedDict):\\n        topic: str\\n        outline: Outline\\n        editors: List[Editor]\\n        interview_results: List[InterviewState]\\n        # The final sections output\\n        sections: List[WikiSection]\\n        article: str\\n    \\n    \\n    \\n    import asyncio\\n    \\n    \\n    async def initialize_research(state: ResearchState):\\n        topic = state[\"topic\"]\\n        coros = (\\n            generate_outline_direct.ainvoke({\"topic\": topic}),\\n            survey_subjects.ainvoke(topic),\\n        )\\n        results = await asyncio.gather(*coros)\\n        return {\\n            **state,\\n            \"outline\": results[0],\\n            \"editors\": results[1].editors,\\n        }\\n    \\n    \\n    async def conduct_interviews(state: ResearchState):\\n        topic = state[\"topic\"]\\n        initial_states = [\\n            {\\n                \"editor\": editor,\\n                \"messages\": [\\n                    AIMessage(\\n                        content=f\"So you said you were writing an article on {topic}?\",\\n                        name=\"Subject_Matter_Expert\",\\n                    )\\n                ],\\n            }\\n            for editor in state[\"editors\"]\\n        ]\\n        # We call in to the sub-graph here to parallelize the interviews\\n        interview_results = await interview_graph.abatch(initial_states)\\n    \\n        return {\\n            **state,\\n            \"interview_results\": interview_results,\\n        }\\n    \\n    \\n    def format_conversation(interview_state):\\n        messages = interview_state[\"messages\"]\\n        convo = \"\\\\n\".join(f\"{m.name}: {m.content}\" for m in messages)\\n        return f\\'Conversation with {interview_state[\"editor\"].name}\\\\n\\\\n\\' + convo\\n    \\n    \\n    async def refine_outline(state: ResearchState):\\n        convos = \"\\\\n\\\\n\".join(\\n            [\\n                format_conversation(interview_state)\\n                for interview_state in state[\"interview_results\"]\\n            ]\\n        )\\n    \\n        updated_outline = await refine_outline_chain.ainvoke(\\n            {\\n                \"topic\": state[\"topic\"],\\n                \"old_outline\": state[\"outline\"].as_str,\\n                \"conversations\": convos,\\n            }\\n        )\\n        return {**state, \"outline\": updated_outline}\\n    \\n    \\n    async def index_references(state: ResearchState):\\n        all_docs = []\\n        for interview_state in state[\"interview_results\"]:\\n            reference_docs = [\\n                Document(page_content=v, metadata={\"source\": k})\\n                for k, v in interview_state[\"references\"].items()\\n            ]\\n            all_docs.extend(reference_docs)\\n        await vectorstore.aadd_documents(all_docs)\\n        return state\\n    \\n    \\n    async def write_sections(state: ResearchState):\\n        outline = state[\"outline\"]\\n        sections = await section_writer.abatch(\\n            [\\n                {\\n                    \"outline\": refined_outline.as_str,\\n                    \"section\": section.section_title,\\n                    \"topic\": state[\"topic\"],\\n                }\\n                for section in outline.sections\\n            ]\\n        )\\n        return {\\n            **state,\\n            \"sections\": sections,\\n        }\\n    \\n    \\n    async def write_article(state: ResearchState):\\n        topic = state[\"topic\"]\\n        sections = state[\"sections\"]\\n        draft = \"\\\\n\\\\n\".join([section.as_str for section in sections])\\n        article = await writer.ainvoke({\"topic\": topic, \"draft\": draft})\\n        return {\\n            **state,\\n            \"article\": article,\\n        }\\n    \\n\\n#### Create the graphÂ¶\\n\\n    \\n    \\n    from langgraph.checkpoint.memory import MemorySaver\\n    \\n    builder_of_storm = StateGraph(ResearchState)\\n    \\n    nodes = [\\n        (\"init_research\", initialize_research),\\n        (\"conduct_interviews\", conduct_interviews),\\n        (\"refine_outline\", refine_outline),\\n        (\"index_references\", index_references),\\n        (\"write_sections\", write_sections),\\n        (\"write_article\", write_article),\\n    ]\\n    for i in range(len(nodes)):\\n        name, node = nodes[i]\\n        builder_of_storm.add_node(name, node, retry=RetryPolicy(max_attempts=3))\\n        if i > 0:\\n            builder_of_storm.add_edge(nodes[i - 1][0], name)\\n    \\n    builder_of_storm.add_edge(START, nodes[0][0])\\n    builder_of_storm.add_edge(nodes[-1][0], END)\\n    storm = builder_of_storm.compile(checkpointer=MemorySaver())\\n    \\n\\nAPI Reference: [MemorySaver](https://langchain-\\nai.github.io/langgraph/reference/checkpoints/#langgraph.checkpoint.memory.MemorySaver)\\n\\n    \\n    \\n    from IPython.display import Image, display\\n    \\n    try:\\n        display(Image(storm.get_graph().draw_mermaid_png()))\\n    except Exception:\\n        # This requires some extra dependencies and is optional\\n        pass\\n\\nconfig = {\"configurable\": {\"thread_id\": \"my-thread\"}}\\n    async for step in storm.astream(\\n        {\\n            \"topic\": \"Groq, NVIDIA, Llamma.cpp and the future of LLM Inference\",\\n        },\\n        config,\\n    ):\\n        name = next(iter(step))\\n        print(name)\\n        print(\"-- \", str(step[name])[:300])\\n    \\n    \\n    \\n    init_research\\n    --  {\\'topic\\': \\'Groq, NVIDIA, Llamma.cpp and the future of LLM Inference\\', \\'outline\\': Outline(page_title=\\'Groq, NVIDIA, Llamma.cpp and the future of LLM Inference\\', sections=[Section(section_title=\\'Introduction\\', description=\\'Overview of Groq, NVIDIA, Llamma.cpp, and their significance in the field of La\\n    conduct_interviews\\n    --  {\\'topic\\': \\'Groq, NVIDIA, Llamma.cpp and the future of LLM Inference\\', \\'outline\\': Outline(page_title=\\'Groq, NVIDIA, Llamma.cpp and the future of LLM Inference\\', sections=[Section(section_title=\\'Introduction\\', description=\\'Overview of Groq, NVIDIA, Llamma.cpp, and their significance in the field of La\\n    refine_outline\\n    --  {\\'topic\\': \\'Groq, NVIDIA, Llamma.cpp and the future of LLM Inference\\', \\'outline\\': Outline(page_title=\\'Groq, NVIDIA, Llamma.cpp and the Future of LLM Inference\\', sections=[Section(section_title=\\'Introduction\\', description=\\'An overview of the significance and roles of Groq, NVIDIA, and Llamma.cpp in th\\n    index_references\\n    --  {\\'topic\\': \\'Groq, NVIDIA, Llamma.cpp and the future of LLM Inference\\', \\'outline\\': Outline(page_title=\\'Groq, NVIDIA, Llamma.cpp and the Future of LLM Inference\\', sections=[Section(section_title=\\'Introduction\\', description=\\'An overview of the significance and roles of Groq, NVIDIA, and Llamma.cpp in th\\n    write_sections\\n    --  {\\'topic\\': \\'Groq, NVIDIA, Llamma.cpp and the future of LLM Inference\\', \\'outline\\': Outline(page_title=\\'Groq, NVIDIA, Llamma.cpp and the Future of LLM Inference\\', sections=[Section(section_title=\\'Introduction\\', description=\\'An overview of the significance and roles of Groq, NVIDIA, and Llamma.cpp in th\\n    write_article\\n    --  {\\'topic\\': \\'Groq, NVIDIA, Llamma.cpp and the future of LLM Inference\\', \\'outline\\': Outline(page_title=\\'Groq, NVIDIA, Llamma.cpp and the Future of LLM Inference\\', sections=[Section(section_title=\\'Introduction\\', description=\\'An overview of the significance and roles of Groq, NVIDIA, and Llamma.cpp in th\\n    __end__\\n    --  {\\'topic\\': \\'Groq, NVIDIA, Llamma.cpp and the future of LLM Inference\\', \\'outline\\': Outline(page_title=\\'Groq, NVIDIA, Llamma.cpp and the Future of LLM Inference\\', sections=[Section(section_title=\\'Introduction\\', description=\\'An overview of the significance and roles of Groq, NVIDIA, and Llamma.cpp in th\\n    \\n    \\n    \\n    checkpoint = storm.get_state(config)\\n    article = checkpoint.values[\"article\"]\\n    \\n\\n## Render the WikiÂ¶\\n\\nNow we can render the final wiki page!\\n\\n    \\n    \\n    from IPython.display import Markdown\\n    \\n    # We will down-header the sections to create less confusion in this notebook\\n    Markdown(article.replace(\"\\\\n#\", \"\\\\n##\"))\\n    \\n\\n# Large Language Model (LLM) Inference TechnologiesÂ¶\\n\\n### ContentsÂ¶\\n\\n  1. Introduction\\n  2. Groq\\'s Advancements in LLM Inference\\n  3. NVIDIA\\'s Contributions to LLM Inference\\n  4. Hardware Innovations\\n  5. Software Solutions\\n  6. Research and Development\\n  7. Llamma.cpp: Accelerating LLM Inference\\n  8. The Future of LLM Inference\\n  9. References\\n\\n### IntroductionÂ¶\\n\\nThe advent of million-plus token context window language models, such as\\nGemini 1.5, has significantly advanced the field of artificial intelligence,\\nparticularly in natural language processing (NLP). These models have expanded\\nthe capabilities of machine learning in understanding and generating text over\\nvastly larger contexts than previously possible. This leap in technology has\\npaved the way for transformative applications across various domains,\\nincluding the integration into Retrieval-Augmented Generation (RAG) systems to\\nproduce more accurate and contextually rich responses.\\n\\n### Groq\\'s Advancements in LLM InferenceÂ¶\\n\\nGroq has introduced the Groq Linear Processor Unit (LPU), a purpose-built\\nhardware architecture for LLM inference. This innovation positions Groq as a\\nleader in efficient and high-performance LLM processing by optimizing the\\nhardware specifically for LLM tasks. The Groq LPU dramatically reduces latency\\nand increases the throughput of LLM inferences, facilitating advancements in a\\nwide range of applications, from natural language processing to broader\\nartificial intelligence technologies[1].\\n\\n### NVIDIA\\'s Contributions to LLM InferenceÂ¶\\n\\nNVIDIA has played a pivotal role in advancing LLM inference through its GPUs,\\noptimized for AI and machine learning workloads, and specialized software\\nframeworks. The company\\'s GPU architecture and software solutions, such as the\\nCUDA Deep Neural Network library (cuDNN) and the TensorRT inference optimizer,\\nare designed to accelerate computational processes and improve LLM\\nperformance. NVIDIA\\'s active participation in research and development further\\nunderscores its commitment to enhancing the capabilities of LLMs[1].\\n\\n#### Hardware InnovationsÂ¶\\n\\nNVIDIA\\'s GPU architecture facilitates high throughput and parallel processing\\nfor LLM inference tasks, significantly reducing inference time and enabling\\ncomplex models to be used in real-time applications.\\n\\n#### Software SolutionsÂ¶\\n\\nNVIDIA\\'s suite of software tools, including cuDNN and TensorRT, optimizes LLM\\nperformance on its hardware, streamlining the deployment of LLMs by improving\\ntheir efficiency and reducing latency.\\n\\n#### Research and DevelopmentÂ¶\\n\\nNVIDIA collaborates with academic and industry partners to develop new\\ntechniques and models that push the boundaries of LLM technology, aiming to\\nmake LLMs more powerful and applicable across a broader range of tasks.\\n\\n### Llamma.cpp: Accelerating LLM InferenceÂ¶\\n\\nLlamma.cpp is a framework developed to enhance the speed and efficiency of LLM\\ninference. By integrating specialized hardware, such as Groq\\'s LPU, and\\noptimizing for parallel processing, Llamma.cpp significantly accelerates\\ncomputation times and reduces energy consumption. The framework supports\\nmillion-plus token context window models, enabling applications requiring deep\\ncontextual understanding and extensive knowledge retrieval[1][2].\\n\\n### The Future of LLM InferenceÂ¶\\n\\n[ ](https://github.com/langchain-\\nai/langgraph/edit/main/docs/docs/concepts/high_level.md \"Edit this page\")\\n\\n# Why LangGraph?Â¶\\n\\n## LLM applicationsÂ¶\\n\\nLLMs make it possible to embed intelligence into a new class of applications.\\nThere are many patterns for building applications that use LLMs. Workflows\\nhave scaffolding of predefined code paths around LLM calls. LLMs can direct\\nthe control flow through these predefined code paths, which some consider to\\nbe an \"agentic system\". In other cases, it\\'s possible to remove this\\nscaffolding, creating autonomous agents that can\\n[plan](https://huyenchip.com/2025/01/07/agents.html), take actions via [tool\\ncalls](https://python.langchain.com/docs/concepts/tool_calling/), and directly\\nrespond [to the feedback from their own\\nactions](https://research.google/blog/react-synergizing-reasoning-and-acting-\\nin-language-models/) with further actions.\\n\\n![Agent Workflow](../img/agent_workflow.png)\\n\\n## What LangGraph providesÂ¶\\n\\nLangGraph provides low-level supporting infrastructure that sits underneath\\n_any_ workflow or agent. It does not abstract prompts or architecture, and\\nprovides three central benefits:\\n\\n### PersistenceÂ¶\\n\\nLangGraph has a [persistence layer](https://langchain-\\nai.github.io/langgraph/concepts/persistence/), which offers a number of\\nbenefits:\\n\\n  * [Memory](https://langchain-ai.github.io/langgraph/concepts/memory/): LangGraph persists arbitrary aspects of your application\\'s state, supporting memory of conversations and other updates within and across user interactions;\\n  * [Human-in-the-loop](https://langchain-ai.github.io/langgraph/concepts/human_in_the_loop/): Because state is checkpointed, execution can be interrupted and resumed, allowing for decisions, validation, and corrections via human input.\\n\\n### StreamingÂ¶\\n\\nLangGraph also provides support for [streaming](../../how-tos/#streaming)\\nworkflow / agent state to the user (or developer) over the course of\\nexecution. LangGraph supports streaming of both events ([such as feedback from\\na tool call](../../how-tos/streaming/#updates)) and [tokens from LLM\\ncalls](../../how-tos/streaming-tokens/) embedded in an application.\\n\\n### Debugging and DeploymentÂ¶\\n\\nLangGraph provides an easy onramp for testing, debugging, and deploying\\napplications via [LangGraph Platform](https://langchain-\\nai.github.io/langgraph/concepts/langgraph_platform/). This includes\\n[Studio](https://langchain-ai.github.io/langgraph/concepts/langgraph_studio/),\\nan IDE that enables visualization, interaction, and debugging of workflows or\\nagents. This also includes numerous [options](https://langchain-\\nai.github.io/langgraph/tutorials/deployment/) for deployment.\\n\\nWas this page helpful?\\n\\nThanks for your feedback!\\n\\nThanks for your feedback! Please help us improve this page by adding to the\\ndiscussion below.\\n\\n## Comments\\n\\n[ ](https://github.com/langchain-\\nai/langgraph/edit/main/docs/docs/concepts/high_level.md \"Edit this page\")\\n\\n# Why LangGraph?Â¶\\n\\n## LLM applicationsÂ¶\\n\\nLLMs make it possible to embed intelligence into a new class of applications.\\nThere are many patterns for building applications that use LLMs. Workflows\\nhave scaffolding of predefined code paths around LLM calls. LLMs can direct\\nthe control flow through these predefined code paths, which some consider to\\nbe an \"agentic system\". In other cases, it\\'s possible to remove this\\nscaffolding, creating autonomous agents that can\\n[plan](https://huyenchip.com/2025/01/07/agents.html), take actions via [tool\\ncalls](https://python.langchain.com/docs/concepts/tool_calling/), and directly\\nrespond [to the feedback from their own\\nactions](https://research.google/blog/react-synergizing-reasoning-and-acting-\\nin-language-models/) with further actions.\\n\\n![Agent Workflow](../img/agent_workflow.png)\\n\\n## What LangGraph providesÂ¶\\n\\nLangGraph provides low-level supporting infrastructure that sits underneath\\n_any_ workflow or agent. It does not abstract prompts or architecture, and\\nprovides three central benefits:\\n\\n### PersistenceÂ¶\\n\\nLangGraph has a [persistence layer](https://langchain-\\nai.github.io/langgraph/concepts/persistence/), which offers a number of\\nbenefits:\\n\\n  * [Memory](https://langchain-ai.github.io/langgraph/concepts/memory/): LangGraph persists arbitrary aspects of your application\\'s state, supporting memory of conversations and other updates within and across user interactions;\\n  * [Human-in-the-loop](https://langchain-ai.github.io/langgraph/concepts/human_in_the_loop/): Because state is checkpointed, execution can be interrupted and resumed, allowing for decisions, validation, and corrections via human input.\\n\\n### StreamingÂ¶\\n\\nLangGraph also provides support for [streaming](../../how-tos/#streaming)\\nworkflow / agent state to the user (or developer) over the course of\\nexecution. LangGraph supports streaming of both events ([such as feedback from\\na tool call](../../how-tos/streaming/#updates)) and [tokens from LLM\\ncalls](../../how-tos/streaming-tokens/) embedded in an application.\\n\\n### Debugging and DeploymentÂ¶\\n\\nLangGraph provides an easy onramp for testing, debugging, and deploying\\napplications via [LangGraph Platform](https://langchain-\\nai.github.io/langgraph/concepts/langgraph_platform/). This includes\\n[Studio](https://langchain-ai.github.io/langgraph/concepts/langgraph_studio/),\\nan IDE that enables visualization, interaction, and debugging of workflows or\\nagents. This also includes numerous [options](https://langchain-\\nai.github.io/langgraph/tutorials/deployment/) for deployment.\\n\\nWas this page helpful?\\n\\nThanks for your feedback!\\n\\nThanks for your feedback! Please help us improve this page by adding to the\\ndiscussion below.\\n\\n## Comments\\n\\n', name='retrieve_blog_posts', id='5c13f4ca-8dd5-4b39-9f47-43ad49cea0ac', tool_call_id='call_m8jtKFV1fsmy1DtViFomA57D')]}\n",
      "'\\n---\\n'\n",
      "---GENERATE---\n",
      "\"Output from node 'generate':\"\n",
      "'---'\n",
      "{ 'messages': [ 'Here is the LangGraph code for conducting web research using '\n",
      "                'the STORM framework:\\n'\n",
      "                '\\n'\n",
      "                '```python\\n'\n",
      "                'from langgraph.checkpoint.memory import MemorySaver\\n'\n",
      "                'import asyncio\\n'\n",
      "                '\\n'\n",
      "                '# Define the ResearchState TypedDict\\n'\n",
      "                'class ResearchState(TypedDict):\\n'\n",
      "                '    topic: str\\n'\n",
      "                '    outline: Outline\\n'\n",
      "                '    editors: List[Editor]\\n'\n",
      "                '    interview_results: List[InterviewState]\\n'\n",
      "                '    sections: List[WikiSection]\\n'\n",
      "                '    article: str\\n'\n",
      "                '\\n'\n",
      "                '# Initialize research\\n'\n",
      "                'async def initialize_research(state: ResearchState):\\n'\n",
      "                '    topic = state[\"topic\"]\\n'\n",
      "                '    coros = (\\n'\n",
      "                '        generate_outline_direct.ainvoke({\"topic\": topic}),\\n'\n",
      "                '        survey_subjects.ainvoke(topic),\\n'\n",
      "                '    )\\n'\n",
      "                '    results = await asyncio.gather(*coros)\\n'\n",
      "                '    return {\\n'\n",
      "                '        **state,\\n'\n",
      "                '        \"outline\": results[0],\\n'\n",
      "                '        \"editors\": results[1].editors,\\n'\n",
      "                '    }\\n'\n",
      "                '\\n'\n",
      "                '# Conduct interviews\\n'\n",
      "                'async def conduct_interviews(state: ResearchState):\\n'\n",
      "                '    topic = state[\"topic\"]\\n'\n",
      "                '    initial_states = [\\n'\n",
      "                '        {\\n'\n",
      "                '            \"editor\": editor,\\n'\n",
      "                '            \"messages\": [\\n'\n",
      "                '                AIMessage(\\n'\n",
      "                '                    content=f\"So you said you were writing an '\n",
      "                'article on {topic}?\",\\n'\n",
      "                '                    name=\"Subject_Matter_Expert\",\\n'\n",
      "                '                )\\n'\n",
      "                '            ],\\n'\n",
      "                '        }\\n'\n",
      "                '        for editor in state[\"editors\"]\\n'\n",
      "                '    ]\\n'\n",
      "                '    interview_results = await '\n",
      "                'interview_graph.abatch(initial_states)\\n'\n",
      "                '    return {\\n'\n",
      "                '        **state,\\n'\n",
      "                '        \"interview_results\": interview_results,\\n'\n",
      "                '    }\\n'\n",
      "                '\\n'\n",
      "                '# Format conversation\\n'\n",
      "                'def format_conversation(interview_state):\\n'\n",
      "                '    messages = interview_state[\"messages\"]\\n'\n",
      "                '    convo = \"\\\\n\".join(f\"{m.name}: {m.content}\" for m in '\n",
      "                'messages)\\n'\n",
      "                \"    return f'Conversation with \"\n",
      "                '{interview_state[\"editor\"].name}\\\\n\\\\n\\' + convo\\n'\n",
      "                '\\n'\n",
      "                '# Refine outline\\n'\n",
      "                'async def refine_outline(state: ResearchState):\\n'\n",
      "                '    convos = \"\\\\n\\\\n\".join(\\n'\n",
      "                '        [\\n'\n",
      "                '            format_conversation(interview_state)\\n'\n",
      "                '            for interview_state in '\n",
      "                'state[\"interview_results\"]\\n'\n",
      "                '        ]\\n'\n",
      "                '    )\\n'\n",
      "                '    updated_outline = await refine_outline_chain.ainvoke(\\n'\n",
      "                '        {\\n'\n",
      "                '            \"topic\": state[\"topic\"],\\n'\n",
      "                '            \"old_outline\": state[\"outline\"].as_str,\\n'\n",
      "                '            \"conversations\": convos,\\n'\n",
      "                '        }\\n'\n",
      "                '    )\\n'\n",
      "                '    return {**state, \"outline\": updated_outline}\\n'\n",
      "                '\\n'\n",
      "                '# Index references\\n'\n",
      "                'async def index_references(state: ResearchState):\\n'\n",
      "                '    all_docs = []\\n'\n",
      "                '    for interview_state in state[\"interview_results\"]:\\n'\n",
      "                '        reference_docs = [\\n'\n",
      "                '            Document(page_content=v, metadata={\"source\": k})\\n'\n",
      "                '            for k, v in '\n",
      "                'interview_state[\"references\"].items()\\n'\n",
      "                '        ]\\n'\n",
      "                '        all_docs.extend(reference_docs)\\n'\n",
      "                '    await vectorstore.aadd_documents(all_docs)\\n'\n",
      "                '    return state\\n'\n",
      "                '\\n'\n",
      "                '# Write sections\\n'\n",
      "                'async def write_sections(state: ResearchState):\\n'\n",
      "                '    outline = state[\"outline\"]\\n'\n",
      "                '    sections = await section_writer.abatch(\\n'\n",
      "                '        [\\n'\n",
      "                '            {\\n'\n",
      "                '                \"outline\": outline.as_str,\\n'\n",
      "                '                \"section\": section.section_title,\\n'\n",
      "                '                \"topic\": state[\"topic\"],\\n'\n",
      "                '            }\\n'\n",
      "                '            for section in outline.sections\\n'\n",
      "                '        ]\\n'\n",
      "                '    )\\n'\n",
      "                '    return {\\n'\n",
      "                '        **state,\\n'\n",
      "                '        \"sections\": sections,\\n'\n",
      "                '    }\\n'\n",
      "                '\\n'\n",
      "                '# Write article\\n'\n",
      "                'async def write_article(state: ResearchState):\\n'\n",
      "                '    topic = state[\"topic\"]\\n'\n",
      "                '    sections = state[\"sections\"]\\n'\n",
      "                '    draft = \"\\\\n\\\\n\".join([section.as_str for section in '\n",
      "                'sections])\\n'\n",
      "                '    article = await writer.ainvoke({\"topic\": topic, \"draft\": '\n",
      "                'draft})\\n'\n",
      "                '    return {\\n'\n",
      "                '        **state,\\n'\n",
      "                '        \"article\": article,\\n'\n",
      "                '    }\\n'\n",
      "                '\\n'\n",
      "                '# Create the graph\\n'\n",
      "                'builder_of_storm = StateGraph(ResearchState)\\n'\n",
      "                '\\n'\n",
      "                'nodes = [\\n'\n",
      "                '    (\"init_research\", initialize_research),\\n'\n",
      "                '    (\"conduct_interviews\", conduct_interviews),\\n'\n",
      "                '    (\"refine_outline\", refine_outline),\\n'\n",
      "                '    (\"index_references\", index_references),\\n'\n",
      "                '    (\"write_sections\", write_sections),\\n'\n",
      "                '    (\"write_article\", write_article),\\n'\n",
      "                ']\\n'\n",
      "                '\\n'\n",
      "                'for i in range(len(nodes)):\\n'\n",
      "                '    name, node = nodes[i]\\n'\n",
      "                '    builder_of_storm.add_node(name, node, '\n",
      "                'retry=RetryPolicy(max_attempts=3))\\n'\n",
      "                '    if i > 0:\\n'\n",
      "                '        builder_of_storm.add_edge(nodes[i - 1][0], name)\\n'\n",
      "                '\\n'\n",
      "                'builder_of_storm.add_edge(START, nodes[0][0])\\n'\n",
      "                'builder_of_storm.add_edge(nodes[-1][0], END)\\n'\n",
      "                'storm = builder_of_storm.compile(checkpointer=MemorySaver())\\n'\n",
      "                '\\n'\n",
      "                '# Execute the graph\\n'\n",
      "                'config = {\"configurable\": {\"thread_id\": \"my-thread\"}}\\n'\n",
      "                'async for step in storm.astream(\\n'\n",
      "                '    {\\n'\n",
      "                '        \"topic\": \"Groq, NVIDIA, Llamma.cpp and the future of '\n",
      "                'LLM Inference\",\\n'\n",
      "                '    },\\n'\n",
      "                '    config,\\n'\n",
      "                '):\\n'\n",
      "                '    name = next(iter(step))\\n'\n",
      "                '    print(name)\\n'\n",
      "                '    print(\"-- \", str(step[name])[:300])\\n'\n",
      "                '\\n'\n",
      "                '# Render the final wiki\\n'\n",
      "                'from IPython.display import Markdown\\n'\n",
      "                '\\n'\n",
      "                'checkpoint = storm.get_state(config)\\n'\n",
      "                'article = checkpoint.values[\"article\"]\\n'\n",
      "                'Markdown(article.replace(\"\\\\n#\", \"\\\\n##\"))\\n'\n",
      "                '```\\n'\n",
      "                '\\n'\n",
      "                'This code outlines the process of conducting web research '\n",
      "                'using the LangGraph SDK, following the defined stages of the '\n",
      "                'research workflow.']}\n",
      "'\\n---\\n'\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "\n",
    "inputs = {\n",
    "    \"messages\": [\n",
    "        (\"user\", \"Write Langgraph code Web Research (STORM)\"),\n",
    "    ]\n",
    "}\n",
    "for output in graph.stream(inputs):\n",
    "    for key, value in output.items():\n",
    "        pprint.pprint(f\"Output from node '{key}':\")\n",
    "        pprint.pprint(\"---\")\n",
    "        pprint.pprint(value, indent=2, width=80, depth=None)\n",
    "    pprint.pprint(\"\\n---\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---CALL AGENT---\n",
      "\"Output from node 'agent':\"\n",
      "'---'\n",
      "{ 'messages': [ AIMessage(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': 'call_GRXUxnlroNsRTFFQXUNpkWIo', 'function': {'arguments': '{\"query\":\"Plan-And-Execute Agent\"}', 'name': 'retrieve_blog_posts'}, 'type': 'function'}]}, response_metadata={'finish_reason': 'tool_calls', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_86d0290411'}, id='run-1fe4e5a6-27fb-4a43-9672-bb42add7155d-0', tool_calls=[{'name': 'retrieve_blog_posts', 'args': {'query': 'Plan-And-Execute Agent'}, 'id': 'call_GRXUxnlroNsRTFFQXUNpkWIo', 'type': 'tool_call'}])]}\n",
      "'\\n---\\n'\n",
      "---CHECK RELEVANCE---\n",
      "---DECISION: DOCS RELEVANT---\n",
      "\"Output from node 'retrieve':\"\n",
      "'---'\n",
      "{ 'messages': [ ToolMessage(content='![plan-and-execute\\n\\n[ ](https://github.com/langchain-\\nai/langgraph/edit/main/docs/docs/tutorials/plan-and-execute/plan-and-\\nexecute.ipynb \"Edit this page\")\\n\\n# Plan-and-ExecuteÂ¶\\n\\nThis notebook shows how to create a \"plan-and-execute\" style agent. This is\\nheavily inspired by the [Plan-and-Solve](https://arxiv.org/abs/2305.04091)\\npaper as well as the [Baby-AGI](https://github.com/yoheinakajima/babyagi)\\nproject.\\n\\nThe core idea is to first come up with a multi-step plan, and then go through\\nthat plan one item at a time. After accomplishing a particular task, you can\\nthen revisit the plan and modify as appropriate.\\n\\nThe general computational graph looks like the following:\\n\\n## Define the StateÂ¶\\n\\nLet\\'s now start by defining the state the track for this agent.\\n\\nFirst, we will need to track the current plan. Let\\'s represent that as a list\\nof strings.\\n\\nNext, we should track previously executed steps. Let\\'s represent that as a\\nlist of tuples (these tuples will contain the step and then the result)\\n\\nFinally, we need to have some state to represent the final response as well as\\nthe original input.\\n\\n    \\n    \\n    import operator\\n    from typing import Annotated, List, Tuple\\n    from typing_extensions import TypedDict\\n    \\n    \\n    class PlanExecute(TypedDict):\\n        input: str\\n        plan: List[str]\\n        past_steps: Annotated[List[Tuple], operator.add]\\n        response: str\\n    \\n\\n## Planning StepÂ¶\\n\\nLet\\'s now think about creating the planning step. This will use function\\ncalling to create a plan.\\n\\nUsing Pydantic with LangChain\\n\\nThis notebook uses Pydantic v2 `BaseModel`, which requires `langchain-core >=\\n0.3`. Using `langchain-core < 0.3` will result in errors due to mixing of\\nPydantic v1 and v2 `BaseModels`.\\n\\n    \\n    \\n    from pydantic import BaseModel, Field\\n    \\n    \\n    class Plan(BaseModel):\\n        \"\"\"Plan to follow in future\"\"\"\\n    \\n        steps: List[str] = Field(\\n            description=\"different steps to follow, should be in sorted order\"\\n        )\\n    \\n    \\n    \\n    from langchain_core.prompts import ChatPromptTemplate\\n    \\n    planner_prompt = ChatPromptTemplate.from_messages(\\n        [\\n            (\\n                \"system\",\\n                \"\"\"For the given objective, come up with a simple step by step plan. \\\\\\n    This plan should involve individual tasks, that if executed correctly will yield the correct answer. Do not add any superfluous steps. \\\\\\n    The result of the final step should be the final answer. Make sure that each step has all the information needed - do not skip steps.\"\"\",\\n            ),\\n            (\"placeholder\", \"{messages}\"),\\n        ]\\n    )\\n    planner = planner_prompt | ChatOpenAI(\\n        model=\"gpt-4o\", temperature=0\\n    ).with_structured_output(Plan)\\n    \\n\\nAPI Reference:\\n[ChatPromptTemplate](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html)\\n\\n    \\n    \\n    planner.invoke(\\n        {\\n            \"messages\": [\\n                (\"user\", \"what is the hometown of the current Australia open winner?\")\\n            ]\\n        }\\n    )\\n    \\n    \\n    \\n    Plan(steps=[\\'Identify the current winner of the Australia Open.\\', \\'Find the hometown of the identified winner.\\'])\\n    \\n\\n## Re-Plan StepÂ¶\\n\\nNow, let\\'s create a step that re-does the plan based on the result of the\\nprevious step.\\n\\n    \\n    \\n    from typing import Union\\n    \\n    \\n    class Response(BaseModel):\\n        \"\"\"Response to user.\"\"\"\\n    \\n        response: str\\n    \\n    \\n    class Act(BaseModel):\\n        \"\"\"Action to perform.\"\"\"\\n    \\n        action: Union[Response, Plan] = Field(\\n            description=\"Action to perform. If you want to respond to user, use Response. \"\\n            \"If you need to further use tools to get the answer, use Plan.\"\\n        )\\n    \\n    \\n    replanner_prompt = ChatPromptTemplate.from_template(\\n        \"\"\"For the given objective, come up with a simple step by step plan. \\\\\\n    This plan should involve individual tasks, that if executed correctly will yield the correct answer. Do not add any superfluous steps. \\\\\\n    The result of the final step should be the final answer. Make sure that each step has all the information needed - do not skip steps.\\n    \\n    Your objective was this:\\n    {input}\\n    \\n    Your original plan was this:\\n    {plan}\\n    \\n    You have currently done the follow steps:\\n    {past_steps}\\n    \\n    Update your plan accordingly. If no more steps are needed and you can return to the user, then respond with that. Otherwise, fill out the plan. Only add steps to the plan that still NEED to be done. Do not return previously done steps as part of the plan.\"\"\"\\n    )\\n    \\n    \\n    replanner = replanner_prompt | ChatOpenAI(\\n        model=\"gpt-4o\", temperature=0\\n    ).with_structured_output(Act)\\n    \\n\\n## Create the GraphÂ¶\\n\\nWe can now create the graph!\\n\\n    \\n    \\n    from typing import Literal\\n    from langgraph.graph import END\\n    \\n    \\n    async def execute_step(state: PlanExecute):\\n        plan = state[\"plan\"]\\n        plan_str = \"\\\\n\".join(f\"{i+1}. {step}\" for i, step in enumerate(plan))\\n        task = plan[0]\\n        task_formatted = f\"\"\"For the following plan:\\n    {plan_str}\\\\n\\\\nYou are tasked with executing step {1}, {task}.\"\"\"\\n        agent_response = await agent_executor.ainvoke(\\n            {\"messages\": [(\"user\", task_formatted)]}\\n        )\\n        return {\\n            \"past_steps\": [(task, agent_response[\"messages\"][-1].content)],\\n        }\\n    \\n    \\n    async def plan_step(state: PlanExecute):\\n        plan = await planner.ainvoke({\"messages\": [(\"user\", state[\"input\"])]})\\n        return {\"plan\": plan.steps}\\n    \\n    \\n    async def replan_step(state: PlanExecute):\\n        output = await replanner.ainvoke(state)\\n        if isinstance(output.action, Response):\\n            return {\"response\": output.action.response}\\n        else:\\n            return {\"plan\": output.action.steps}\\n    \\n    \\n    def should_end(state: PlanExecute):\\n        if \"response\" in state and state[\"response\"]:\\n            return END\\n        else:\\n            return \"agent\"\\n    \\n\\nAPI Reference: [END](https://langchain-\\nai.github.io/langgraph/reference/constants/#langgraph.constants.END)\\n\\n    \\n    \\n    from langgraph.graph import StateGraph, START\\n    \\n    workflow = StateGraph(PlanExecute)\\n    \\n    # Add the plan node\\n    workflow.add_node(\"planner\", plan_step)\\n    \\n    # Add the execution step\\n    workflow.add_node(\"agent\", execute_step)\\n    \\n    # Add a replan node\\n    workflow.add_node(\"replan\", replan_step)\\n    \\n    workflow.add_edge(START, \"planner\")\\n    \\n    # From plan we go to agent\\n    workflow.add_edge(\"planner\", \"agent\")\\n    \\n    # From agent, we replan\\n    workflow.add_edge(\"agent\", \"replan\")\\n    \\n    workflow.add_conditional_edges(\\n        \"replan\",\\n        # Next, we pass in the function that will determine which node is called next.\\n        should_end,\\n        [\"agent\", END],\\n    )\\n    \\n    # Finally, we compile it!\\n    # This compiles it into a LangChain Runnable,\\n    # meaning you can use it as you would any other runnable\\n    app = workflow.compile()\\n    \\n\\nAPI Reference: [StateGraph](https://langchain-ai.github.io/langgraph/reference/graphs/#langgraph.graph.state.StateGraph) | [START](https://langchain-ai.github.io/langgraph/reference/constants/#langgraph.constants.START)\\n    \\n    \\n    from IPython.display import Image, display\\n    \\n    display(Image(app.get_graph(xray=True).draw_mermaid_png()))\\n\\nThis compares to a typical [ReAct](https://arxiv.org/abs/2210.03629) style\\nagent where you think one step at a time. The advantages of this \"plan-and-\\nexecute\" style agent are:\\n\\n  1. Explicit long term planning (which even really strong LLMs can struggle with)\\n  2. Ability to use smaller/weaker models for the execution step, only using larger/better models for the planning step\\n\\nThe following walkthrough demonstrates how to do so in LangGraph. The\\nresulting agent will leave a trace like the following example:\\n([link](https://smith.langchain.com/public/d46e24d3-dda6-44d5-9550-b618fca4e0d4/r)).\\n\\n## SetupÂ¶\\n\\nFirst, we need to install the packages required.\\n\\n    \\n    \\n    %%capture --no-stderr\\n    %pip install --quiet -U langgraph langchain-community langchain-openai tavily-python\\n    \\n\\nNext, we need to set API keys for OpenAI (the LLM we will use) and Tavily (the\\nsearch tool we will use)\\n\\n    \\n    \\n    import getpass\\n    import os\\n    \\n    \\n    def _set_env(var: str):\\n        if not os.environ.get(var):\\n            os.environ[var] = getpass.getpass(f\"{var}: \")\\n    \\n    \\n    _set_env(\"OPENAI_API_KEY\")\\n    _set_env(\"TAVILY_API_KEY\")\\n    \\n\\nSet up [LangSmith](https://smith.langchain.com) for LangGraph development\\n\\nSign up for LangSmith to quickly spot issues and improve the performance of\\nyour LangGraph projects. LangSmith lets you use trace data to debug, test, and\\nmonitor your LLM apps built with LangGraph â€” read more about how to get\\nstarted [here](https://docs.smith.langchain.com).\\n\\n## Define ToolsÂ¶\\n\\nWe will first define the tools we want to use. For this simple example, we\\nwill use a built-in search tool via Tavily. However, it is really easy to\\ncreate your own tools - see documentation\\n[here](https://python.langchain.com/docs/how_to/custom_tools) on how to do\\nthat.\\n\\n    \\n    \\n    from langchain_community.tools.tavily_search import TavilySearchResults\\n    \\n    tools = [TavilySearchResults(max_results=3)]\\n    \\n\\nAPI Reference:\\n[TavilySearchResults](https://python.langchain.com/api_reference/community/tools/langchain_community.tools.tavily_search.tool.TavilySearchResults.html)\\n\\n## Define our Execution AgentÂ¶\\n\\nNow we will create the execution agent we want to use to execute tasks. Note\\nthat for this example, we will be using the same execution agent for each\\ntask, but this doesn\\'t HAVE to be the case.\\n\\n    \\n    \\n    from langchain import hub\\n    from langchain_openai import ChatOpenAI\\n    \\n    from langgraph.prebuilt import create_react_agent\\n    \\n    # Choose the LLM that will drive the agent\\n    llm = ChatOpenAI(model=\"gpt-4-turbo-preview\")\\n    prompt = \"You are a helpful assistant.\"\\n    agent_executor = create_react_agent(llm, tools, prompt=prompt)', name='retrieve_blog_posts', id='d1d5c97b-f3b2-4547-b0c5-c39bc3ec14d9', tool_call_id='call_GRXUxnlroNsRTFFQXUNpkWIo')]}\n",
      "'\\n---\\n'\n",
      "---GENERATE---\n",
      "\"Output from node 'generate':\"\n",
      "'---'\n",
      "{ 'messages': [ \"Here's how you can implement a Plan-And-Execute agent using \"\n",
      "                'the LangGraph SDK based on the provided context:\\n'\n",
      "                '\\n'\n",
      "                '```python\\n'\n",
      "                'import operator\\n'\n",
      "                'from typing import Annotated, List, Tuple\\n'\n",
      "                'from typing_extensions import TypedDict\\n'\n",
      "                'from pydantic import BaseModel, Field\\n'\n",
      "                'from langchain_core.prompts import ChatPromptTemplate\\n'\n",
      "                'from langgraph.graph import StateGraph, START, END\\n'\n",
      "                'from langchain_openai import ChatOpenAI\\n'\n",
      "                'from langchain_community.tools.tavily_search import '\n",
      "                'TavilySearchResults\\n'\n",
      "                'from langgraph.prebuilt import create_react_agent\\n'\n",
      "                '\\n'\n",
      "                '# Define the State\\n'\n",
      "                'class PlanExecute(TypedDict):\\n'\n",
      "                '    input: str\\n'\n",
      "                '    plan: List[str]\\n'\n",
      "                '    past_steps: Annotated[List[Tuple], operator.add]\\n'\n",
      "                '    response: str\\n'\n",
      "                '\\n'\n",
      "                '# Planning Step\\n'\n",
      "                'class Plan(BaseModel):\\n'\n",
      "                '    steps: List[str] = Field(description=\"different steps to '\n",
      "                'follow, should be in sorted order\")\\n'\n",
      "                '\\n'\n",
      "                'planner_prompt = ChatPromptTemplate.from_messages(\\n'\n",
      "                '    [\\n'\n",
      "                '        (\\n'\n",
      "                '            \"system\",\\n'\n",
      "                '            \"\"\"For the given objective, come up with a simple '\n",
      "                'step by step plan. \\\\\\n'\n",
      "                'This plan should involve individual tasks, that if executed '\n",
      "                'correctly will yield the correct answer. Do not add any '\n",
      "                'superfluous steps. \\\\\\n'\n",
      "                'The result of the final step should be the final answer. Make '\n",
      "                'sure that each step has all the information needed - do not '\n",
      "                'skip steps.\"\"\",\\n'\n",
      "                '        ),\\n'\n",
      "                '        (\"placeholder\", \"{messages}\"),\\n'\n",
      "                '    ]\\n'\n",
      "                ')\\n'\n",
      "                'planner = planner_prompt | ChatOpenAI(model=\"gpt-4o\", '\n",
      "                'temperature=0).with_structured_output(Plan)\\n'\n",
      "                '\\n'\n",
      "                '# Re-Plan Step\\n'\n",
      "                'class Response(BaseModel):\\n'\n",
      "                '    response: str\\n'\n",
      "                '\\n'\n",
      "                'class Act(BaseModel):\\n'\n",
      "                '    action: Union[Response, Plan] = Field(\\n'\n",
      "                '        description=\"Action to perform. If you want to '\n",
      "                'respond to user, use Response. \"\\n'\n",
      "                '        \"If you need to further use tools to get the answer, '\n",
      "                'use Plan.\"\\n'\n",
      "                '    )\\n'\n",
      "                '\\n'\n",
      "                'replanner_prompt = ChatPromptTemplate.from_template(\\n'\n",
      "                '    \"\"\"For the given objective, come up with a simple step by '\n",
      "                'step plan. \\\\\\n'\n",
      "                'This plan should involve individual tasks, that if executed '\n",
      "                'correctly will yield the correct answer. Do not add any '\n",
      "                'superfluous steps. \\\\\\n'\n",
      "                'The result of the final step should be the final answer. Make '\n",
      "                'sure that each step has all the information needed - do not '\n",
      "                'skip steps.\\n'\n",
      "                '\\n'\n",
      "                'Your objective was this:\\n'\n",
      "                '{input}\\n'\n",
      "                '\\n'\n",
      "                'Your original plan was this:\\n'\n",
      "                '{plan}\\n'\n",
      "                '\\n'\n",
      "                'You have currently done the follow steps:\\n'\n",
      "                '{past_steps}\\n'\n",
      "                '\\n'\n",
      "                'Update your plan accordingly. If no more steps are needed and '\n",
      "                'you can return to the user, then respond with that. '\n",
      "                'Otherwise, fill out the plan. Only add steps to the plan that '\n",
      "                'still NEED to be done. Do not return previously done steps as '\n",
      "                'part of the plan.\"\"\"\\n'\n",
      "                ')\\n'\n",
      "                'replanner = replanner_prompt | ChatOpenAI(model=\"gpt-4o\", '\n",
      "                'temperature=0).with_structured_output(Act)\\n'\n",
      "                '\\n'\n",
      "                '# Define Tools\\n'\n",
      "                'tools = [TavilySearchResults(max_results=3)]\\n'\n",
      "                '\\n'\n",
      "                '# Define Execution Agent\\n'\n",
      "                'llm = ChatOpenAI(model=\"gpt-4-turbo-preview\")\\n'\n",
      "                'prompt = \"You are a helpful assistant.\"\\n'\n",
      "                'agent_executor = create_react_agent(llm, tools, '\n",
      "                'prompt=prompt)\\n'\n",
      "                '\\n'\n",
      "                '# Create the Graph\\n'\n",
      "                'async def execute_step(state: PlanExecute):\\n'\n",
      "                '    plan = state[\"plan\"]\\n'\n",
      "                '    plan_str = \"\\\\n\".join(f\"{i+1}. {step}\" for i, step in '\n",
      "                'enumerate(plan))\\n'\n",
      "                '    task = plan[0]\\n'\n",
      "                '    task_formatted = f\"\"\"For the following plan:\\n'\n",
      "                '{plan_str}\\\\n\\\\nYou are tasked with executing step {1}, '\n",
      "                '{task}.\"\"\"\\n'\n",
      "                '    agent_response = await '\n",
      "                'agent_executor.ainvoke({\"messages\": [(\"user\", '\n",
      "                'task_formatted)]})\\n'\n",
      "                '    return {\\n'\n",
      "                '        \"past_steps\": [(task, '\n",
      "                'agent_response[\"messages\"][-1].content)],\\n'\n",
      "                '    }\\n'\n",
      "                '\\n'\n",
      "                'async def plan_step(state: PlanExecute):\\n'\n",
      "                '    plan = await planner.ainvoke({\"messages\": [(\"user\", '\n",
      "                'state[\"input\"])]})\\n'\n",
      "                '    return {\"plan\": plan.steps}\\n'\n",
      "                '\\n'\n",
      "                'async def replan_step(state: PlanExecute):\\n'\n",
      "                '    output = await replanner.ainvoke(state)\\n'\n",
      "                '    if isinstance(output.action, Response):\\n'\n",
      "                '        return {\"response\": output.action.response}\\n'\n",
      "                '    else:\\n'\n",
      "                '        return {\"plan\": output.action.steps}\\n'\n",
      "                '\\n'\n",
      "                'def should_end(state: PlanExecute):\\n'\n",
      "                '    if \"response\" in state and state[\"response\"]:\\n'\n",
      "                '        return END\\n'\n",
      "                '    else:\\n'\n",
      "                '        return \"agent\"\\n'\n",
      "                '\\n'\n",
      "                'workflow = StateGraph(PlanExecute)\\n'\n",
      "                'workflow.add_node(\"planner\", plan_step)\\n'\n",
      "                'workflow.add_node(\"agent\", execute_step)\\n'\n",
      "                'workflow.add_node(\"replan\", replan_step)\\n'\n",
      "                'workflow.add_edge(START, \"planner\")\\n'\n",
      "                'workflow.add_edge(\"planner\", \"agent\")\\n'\n",
      "                'workflow.add_edge(\"agent\", \"replan\")\\n'\n",
      "                'workflow.add_conditional_edges(\"replan\", should_end, '\n",
      "                '[\"agent\", END])\\n'\n",
      "                '\\n'\n",
      "                '# Compile the workflow\\n'\n",
      "                'app = workflow.compile()\\n'\n",
      "                '```\\n'\n",
      "                '\\n'\n",
      "                'This code sets up a Plan-And-Execute agent that can create a '\n",
      "                'plan based on user input, execute the steps of that plan, and '\n",
      "                're-plan if necessary. Make sure to have the required packages '\n",
      "                'installed and API keys set up as mentioned in the context.']}\n",
      "'\\n---\\n'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "inputs = {\n",
    "    \"messages\": [\n",
    "        (\"user\", \"Implement a Plan-And-Execute Agent\"),\n",
    "    ]\n",
    "}\n",
    "for output in graph.stream(inputs):\n",
    "    for key, value in output.items():\n",
    "        pprint.pprint(f\"Output from node '{key}':\")\n",
    "        pprint.pprint(\"---\")\n",
    "        pprint.pprint(value, indent=2, width=80, depth=None)\n",
    "    pprint.pprint(\"\\n---\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---CALL AGENT---\n",
      "\"Output from node 'agent':\"\n",
      "'---'\n",
      "{ 'messages': [ AIMessage(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': 'call_AgQ2xywXeISNl3hgbDpoZ0Vg', 'function': {'arguments': '{\"query\":\"supervisor agent\"}', 'name': 'retrieve_blog_posts'}, 'type': 'function'}]}, response_metadata={'finish_reason': 'tool_calls', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_86d0290411'}, id='run-f5d3c2ec-dac6-456f-bf75-e7e855927b62-0', tool_calls=[{'name': 'retrieve_blog_posts', 'args': {'query': 'supervisor agent'}, 'id': 'call_AgQ2xywXeISNl3hgbDpoZ0Vg', 'type': 'tool_call'}])]}\n",
      "'\\n---\\n'\n",
      "---CHECK RELEVANCE---\n",
      "---DECISION: DOCS RELEVANT---\n",
      "\"Output from node 'retrieve':\"\n",
      "'---'\n",
      "{ 'messages': [ ToolMessage(content='[ ](https://github.com/langchain-\\nai/langgraph/edit/main/docs/docs/tutorials/multi_agent/agent_supervisor.ipynb\\n\"Edit this page\")\\n\\n# Multi-agent supervisorÂ¶\\n\\nThe [previous example](../multi-agent-collaboration) routed messages\\nautomatically based on the output of the initial researcher agent.\\n\\nWe can also choose to use an [LLM to orchestrate](https://langchain-\\nai.github.io/langgraph/concepts/multi_agent/#supervisor) the different agents.\\n\\nBelow, we will create an agent group, with an agent supervisor to help\\ndelegate tasks.\\n\\n[ ](https://github.com/langchain-\\nai/langgraph/edit/main/docs/docs/tutorials/multi_agent/hierarchical_agent_teams.ipynb\\n\"Edit this page\")\\n\\n# Hierarchical Agent TeamsÂ¶\\n\\nIn our previous example ([Agent Supervisor](../agent_supervisor)), we\\nintroduced the concept of a single [supervisor node](https://langchain-\\nai.github.io/langgraph/concepts/multi_agent/#supervisor) to route work between\\ndifferent worker nodes.\\n\\nBut what if the job for a single worker becomes too complex? What if the\\nnumber of workers becomes too large?\\n\\nFor some applications, the system may be more effective if work is distributed\\n_hierarchically_.\\n\\nYou can do this by composing different subgraphs and creating a top-level\\nsupervisor, along with mid-level supervisors.\\n\\nTo do this, let\\'s build a simple research assistant! The graph will look\\nsomething like the following:\\n\\nCheck out this [tutorial](https://langchain-\\nai.github.io/langgraph/tutorials/multi_agent/agent_supervisor/) for an example\\nof supervisor multi-agent architecture.\\n\\n### Supervisor (tool-calling)Â¶\\n\\nIn this variant of the supervisor architecture, we define individual agents as\\n**tools** and use a tool-calling LLM in the supervisor node. This can be\\nimplemented as a [ReAct](../agentic_concepts/#react-implementation)-style\\nagent with two nodes â€” an LLM node (supervisor) and a tool-calling node that\\nexecutes tools (agents in this case).\\n\\n    \\n    \\n    from typing import Annotated\\n    from langchain_openai import ChatOpenAI\\n    from langgraph.prebuilt import InjectedState, create_react_agent\\n    \\n    model = ChatOpenAI()\\n    \\n    # this is the agent function that will be called as tool\\n    # notice that you can pass the state to the tool via InjectedState annotation\\n    def agent_1(state: Annotated[dict, InjectedState]):\\n        # you can pass relevant parts of the state to the LLM (e.g., state[\"messages\"])\\n        # and add any additional logic (different models, custom prompts, structured output, etc.)\\n        response = model.invoke(...)\\n        # return the LLM response as a string (expected tool response format)\\n        # this will be automatically turned to ToolMessage\\n        # by the prebuilt create_react_agent (supervisor)\\n        return response.content\\n    \\n    def agent_2(state: Annotated[dict, InjectedState]):\\n        response = model.invoke(...)\\n        return response.content\\n    \\n    tools = [agent_1, agent_2]\\n    # the simplest way to build a supervisor w/ tool-calling is to use prebuilt ReAct agent graph\\n    # that consists of a tool-calling LLM node (i.e. supervisor) and a tool-executing node\\n    supervisor = create_react_agent(model, tools)\\n    \\n\\nAPI Reference: [ChatOpenAI](https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html) | [InjectedState](https://langchain-ai.github.io/langgraph/reference/prebuilt/#langgraph.prebuilt.tool_node.InjectedState) | [create_react_agent](https://langchain-ai.github.io/langgraph/reference/prebuilt/#langgraph.prebuilt.chat_agent_executor.create_react_agent)\\n\\n### HierarchicalÂ¶\\n\\nAs you add more agents to your system, it might become too hard for the\\nsupervisor to manage all of them. The supervisor might start making poor\\ndecisions about which agent to call next, or the context might become too\\ncomplex for a single supervisor to keep track of. In other words, you end up\\nwith the same problems that motivated the multi-agent architecture in the\\nfirst place.\\n\\nTo address this, you can design your system _hierarchically_. For example, you\\ncan create separate, specialized teams of agents managed by individual\\nsupervisors, and a top-level supervisor to manage the teams.\\n\\n    \\n    \\n    from typing import Literal\\n    from langchain_openai import ChatOpenAI\\n    from langgraph.graph import StateGraph, MessagesState, START, END\\n    from langgraph.types import Command\\n    model = ChatOpenAI()\\n    \\n    # define team 1 (same as the single supervisor example above)\\n    \\n    def team_1_supervisor(state: MessagesState) -> Command[Literal[\"team_1_agent_1\", \"team_1_agent_2\", END]]:\\n        response = model.invoke(...)\\n        return Command(goto=response[\"next_agent\"])\\n    \\n    def team_1_agent_1(state: MessagesState) -> Command[Literal[\"team_1_supervisor\"]]:\\n        response = model.invoke(...)\\n        return Command(goto=\"team_1_supervisor\", update={\"messages\": [response]})\\n    \\n    def team_1_agent_2(state: MessagesState) -> Command[Literal[\"team_1_supervisor\"]]:\\n        response = model.invoke(...)\\n        return Command(goto=\"team_1_supervisor\", update={\"messages\": [response]})\\n    \\n    team_1_builder = StateGraph(Team1State)\\n    team_1_builder.add_node(team_1_supervisor)\\n    team_1_builder.add_node(team_1_agent_1)\\n    team_1_builder.add_node(team_1_agent_2)\\n    team_1_builder.add_edge(START, \"team_1_supervisor\")\\n    team_1_graph = team_1_builder.compile()\\n    \\n    # define team 2 (same as the single supervisor example above)\\n    class Team2State(MessagesState):\\n        next: Literal[\"team_2_agent_1\", \"team_2_agent_2\", \"__end__\"]\\n    \\n    def team_2_supervisor(state: Team2State):\\n        ...\\n    \\n    def team_2_agent_1(state: Team2State):\\n        ...\\n    \\n    def team_2_agent_2(state: Team2State):\\n        ...\\n    \\n    team_2_builder = StateGraph(Team2State)\\n    ...\\n    team_2_graph = team_2_builder.compile()\\n    \\n    \\n    # define top-level supervisor\\n    \\n    builder = StateGraph(MessagesState)\\n    def top_level_supervisor(state: MessagesState) -> Command[Literal[\"team_1_graph\", \"team_2_graph\", END]]:\\n        # you can pass relevant parts of the state to the LLM (e.g., state[\"messages\"])\\n        # to determine which team to call next. a common pattern is to call the model\\n        # with a structured output (e.g. force it to return an output with a \"next_team\" field)\\n        response = model.invoke(...)\\n        # route to one of the teams or exit based on the supervisor\\'s decision\\n        # if the supervisor returns \"__end__\", the graph will finish execution\\n        return Command(goto=response[\"next_team\"])\\n    \\n    builder = StateGraph(MessagesState)\\n    builder.add_node(top_level_supervisor)\\n    builder.add_node(\"team_1_graph\", team_1_graph)\\n    builder.add_node(\"team_2_graph\", team_2_graph)\\n    builder.add_edge(START, \"top_level_supervisor\")\\n    builder.add_edge(\"team_1_graph\", \"top_level_supervisor\")\\n    builder.add_edge(\"team_2_graph\", \"top_level_supervisor\")\\n    graph = builder.compile()\\n    \\n\\nAPI Reference: [ChatOpenAI](https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html) | [StateGraph](https://langchain-ai.github.io/langgraph/reference/graphs/#langgraph.graph.state.StateGraph) | [START](https://langchain-ai.github.io/langgraph/reference/constants/#langgraph.constants.START) | [END](https://langchain-ai.github.io/langgraph/reference/constants/#langgraph.constants.END) | [Command](https://langchain-ai.github.io/langgraph/reference/types/#langgraph.types.Command)\\n\\n### Custom multi-agent workflowÂ¶\\n\\ndef call_tools(state):\\n        ...\\n        commands = [tools_by_name[tool_call[\"name\"]].invoke(tool_call) for tool_call in tool_calls]\\n        return commands\\n    \\n\\nLet\\'s now take a closer look at the different multi-agent architectures.\\n\\n### NetworkÂ¶\\n\\nIn this architecture, agents are defined as graph nodes. Each agent can\\ncommunicate with every other agent (many-to-many connections) and can decide\\nwhich agent to call next. This architecture is good for problems that do not\\nhave a clear hierarchy of agents or a specific sequence in which agents should\\nbe called.\\n\\n    \\n    \\n    from typing import Literal\\n    from langchain_openai import ChatOpenAI\\n    from langgraph.types import Command\\n    from langgraph.graph import StateGraph, MessagesState, START, END\\n    \\n    model = ChatOpenAI()\\n    \\n    def agent_1(state: MessagesState) -> Command[Literal[\"agent_2\", \"agent_3\", END]]:\\n        # you can pass relevant parts of the state to the LLM (e.g., state[\"messages\"])\\n        # to determine which agent to call next. a common pattern is to call the model\\n        # with a structured output (e.g. force it to return an output with a \"next_agent\" field)\\n        response = model.invoke(...)\\n        # route to one of the agents or exit based on the LLM\\'s decision\\n        # if the LLM returns \"__end__\", the graph will finish execution\\n        return Command(\\n            goto=response[\"next_agent\"],\\n            update={\"messages\": [response[\"content\"]]},\\n        )\\n    \\n    def agent_2(state: MessagesState) -> Command[Literal[\"agent_1\", \"agent_3\", END]]:\\n        response = model.invoke(...)\\n        return Command(\\n            goto=response[\"next_agent\"],\\n            update={\"messages\": [response[\"content\"]]},\\n        )\\n    \\n    def agent_3(state: MessagesState) -> Command[Literal[\"agent_1\", \"agent_2\", END]]:\\n        ...\\n        return Command(\\n            goto=response[\"next_agent\"],\\n            update={\"messages\": [response[\"content\"]]},\\n        )\\n    \\n    builder = StateGraph(MessagesState)\\n    builder.add_node(agent_1)\\n    builder.add_node(agent_2)\\n    builder.add_node(agent_3)\\n    \\n    builder.add_edge(START, \"agent_1\")\\n    network = builder.compile()\\n    \\n\\nAPI Reference: [ChatOpenAI](https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html) | [Command](https://langchain-ai.github.io/langgraph/reference/types/#langgraph.types.Command) | [StateGraph](https://langchain-ai.github.io/langgraph/reference/graphs/#langgraph.graph.state.StateGraph) | [START](https://langchain-ai.github.io/langgraph/reference/constants/#langgraph.constants.START) | [END](https://langchain-ai.github.io/langgraph/reference/constants/#langgraph.constants.END)\\n\\n### SupervisorÂ¶\\n\\nIn this architecture, we define agents as nodes and add a supervisor node\\n(LLM) that decides which agent nodes should be called next. We use\\n[`Command`](../low_level/#command) to route execution to the appropriate agent\\nnode based on supervisor\\'s decision. This architecture also lends itself well\\nto running multiple agents in parallel or using [map-reduce](../../how-\\ntos/map-reduce/) pattern.\\n\\n    \\n    \\n    from typing import Literal\\n    from langchain_openai import ChatOpenAI\\n    from langgraph.types import Command\\n    from langgraph.graph import StateGraph, MessagesState, START, END\\n    \\n    model = ChatOpenAI()\\n    \\n    def supervisor(state: MessagesState) -> Command[Literal[\"agent_1\", \"agent_2\", END]]:\\n        # you can pass relevant parts of the state to the LLM (e.g., state[\"messages\"])\\n        # to determine which agent to call next. a common pattern is to call the model\\n        # with a structured output (e.g. force it to return an output with a \"next_agent\" field)\\n        response = model.invoke(...)\\n        # route to one of the agents or exit based on the supervisor\\'s decision\\n        # if the supervisor returns \"__end__\", the graph will finish execution\\n        return Command(goto=response[\"next_agent\"])\\n    \\n    def agent_1(state: MessagesState) -> Command[Literal[\"supervisor\"]]:\\n        # you can pass relevant parts of the state to the LLM (e.g., state[\"messages\"])\\n        # and add any additional logic (different models, custom prompts, structured output, etc.)\\n        response = model.invoke(...)\\n        return Command(\\n            goto=\"supervisor\",\\n            update={\"messages\": [response]},\\n        )\\n    \\n    def agent_2(state: MessagesState) -> Command[Literal[\"supervisor\"]]:\\n        response = model.invoke(...)\\n        return Command(\\n            goto=\"supervisor\",\\n            update={\"messages\": [response]},\\n        )\\n    \\n    builder = StateGraph(MessagesState)\\n    builder.add_node(supervisor)\\n    builder.add_node(agent_1)\\n    builder.add_node(agent_2)\\n    \\n    builder.add_edge(START, \"supervisor\")\\n    \\n    supervisor = builder.compile()\\n    \\n\\nAPI Reference: [ChatOpenAI](https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html) | [Command](https://langchain-ai.github.io/langgraph/reference/types/#langgraph.types.Command) | [StateGraph](https://langchain-ai.github.io/langgraph/reference/graphs/#langgraph.graph.state.StateGraph) | [START](https://langchain-ai.github.io/langgraph/reference/constants/#langgraph.constants.START) | [END](https://langchain-ai.github.io/langgraph/reference/constants/#langgraph.constants.END)\\n\\nCheck out this [tutorial](https://langchain-\\nai.github.io/langgraph/tutorials/multi_agent/agent_supervisor/) for an example\\nof supervisor multi-agent architecture.\\n\\n### Supervisor (tool-calling)Â¶\\n\\nIn this variant of the supervisor architecture, we define individual agents as\\n**tools** and use a tool-calling LLM in the supervisor node. This can be\\nimplemented as a [ReAct](../agentic_concepts/#react-implementation)-style\\nagent with two nodes â€” an LLM node (supervisor) and a tool-calling node that\\nexecutes tools (agents in this case).', name='retrieve_blog_posts', id='19e70009-90a8-4fe9-9b73-e880669b933b', tool_call_id='call_AgQ2xywXeISNl3hgbDpoZ0Vg')]}\n",
      "'\\n---\\n'\n",
      "---GENERATE---\n",
      "\"Output from node 'generate':\"\n",
      "'---'\n",
      "{ 'messages': [ 'Here is a Python implementation of a supervisor agent using '\n",
      "                'the LangGraph SDK:\\n'\n",
      "                '\\n'\n",
      "                '```python\\n'\n",
      "                'from typing import Literal\\n'\n",
      "                'from langchain_openai import ChatOpenAI\\n'\n",
      "                'from langgraph.types import Command\\n'\n",
      "                'from langgraph.graph import StateGraph, MessagesState, START, '\n",
      "                'END\\n'\n",
      "                '\\n'\n",
      "                '# Initialize the model\\n'\n",
      "                'model = ChatOpenAI()\\n'\n",
      "                '\\n'\n",
      "                '# Define the supervisor function\\n'\n",
      "                'def supervisor(state: MessagesState) -> '\n",
      "                'Command[Literal[\"agent_1\", \"agent_2\", END]]:\\n'\n",
      "                '    # Use the model to determine which agent to call next\\n'\n",
      "                '    response = model.invoke(...)\\n'\n",
      "                '    # Route to one of the agents or exit based on the '\n",
      "                \"supervisor's decision\\n\"\n",
      "                '    return Command(goto=response[\"next_agent\"])\\n'\n",
      "                '\\n'\n",
      "                '# Define agent 1\\n'\n",
      "                'def agent_1(state: MessagesState) -> '\n",
      "                'Command[Literal[\"supervisor\"]]:\\n'\n",
      "                '    response = model.invoke(...)\\n'\n",
      "                '    return Command(\\n'\n",
      "                '        goto=\"supervisor\",\\n'\n",
      "                '        update={\"messages\": [response]},\\n'\n",
      "                '    )\\n'\n",
      "                '\\n'\n",
      "                '# Define agent 2\\n'\n",
      "                'def agent_2(state: MessagesState) -> '\n",
      "                'Command[Literal[\"supervisor\"]]:\\n'\n",
      "                '    response = model.invoke(...)\\n'\n",
      "                '    return Command(\\n'\n",
      "                '        goto=\"supervisor\",\\n'\n",
      "                '        update={\"messages\": [response]},\\n'\n",
      "                '    )\\n'\n",
      "                '\\n'\n",
      "                '# Build the state graph\\n'\n",
      "                'builder = StateGraph(MessagesState)\\n'\n",
      "                'builder.add_node(supervisor)\\n'\n",
      "                'builder.add_node(agent_1)\\n'\n",
      "                'builder.add_node(agent_2)\\n'\n",
      "                '\\n'\n",
      "                '# Define the starting point\\n'\n",
      "                'builder.add_edge(START, \"supervisor\")\\n'\n",
      "                '\\n'\n",
      "                '# Compile the graph\\n'\n",
      "                'supervisor_graph = builder.compile()\\n'\n",
      "                '```\\n'\n",
      "                '\\n'\n",
      "                'In this implementation, the `supervisor` function decides '\n",
      "                \"which agent to call next based on the model's response. Each \"\n",
      "                'agent then returns control back to the supervisor after '\n",
      "                'processing.']}\n",
      "'\\n---\\n'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "inputs = {\n",
    "    \"messages\": [\n",
    "        (\"user\", \"Implement a supervisor agent\"),\n",
    "    ]\n",
    "}\n",
    "for output in graph.stream(inputs):\n",
    "    for key, value in output.items():\n",
    "        pprint.pprint(f\"Output from node '{key}':\")\n",
    "        pprint.pprint(\"---\")\n",
    "        pprint.pprint(value, indent=2, width=80, depth=None)\n",
    "    pprint.pprint(\"\\n---\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
