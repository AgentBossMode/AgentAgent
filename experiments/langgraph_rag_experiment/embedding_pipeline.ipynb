{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_openai import AzureOpenAIEmbeddings\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "embeddings = AzureOpenAIEmbeddings(\n",
    "openai_api_type=\"azure\",\n",
    "openai_api_version=os.environ[\"OPENAI_API_EMBEDDING_VERSION\"],\n",
    "openai_api_key=os.environ[\"OPENAI_API_EMBEDDING_KEY\"],\n",
    "azure_endpoint=os.environ[\"AZURE_OPENAI_EMBEDDING_ENDPOINT\"],\n",
    "deployment=os.environ[\"AZURE_OPENAI_EMBEDDING_DEPLOYMENT\"],\n",
    "model=os.environ[\"AZURE_OPENAI_EMBEDDING_MODEL\"],\n",
    "validate_base_url=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_neo4j import Neo4jVector\n",
    "\n",
    "url = \"bolt://localhost:7687\"\n",
    "username = \"neo4j\"\n",
    "password = \"password\"\n",
    "\n",
    "\n",
    "vector_store = Neo4jVector.from_documents(\n",
    "    [], embeddings, url=url, username=username, password=password\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "# Set up the logger\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,  # Set to DEBUG for detailed logs\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    "    handlers=[\n",
    "        # logging.FileHandler(\"scraper.log\"),  # Log to a file\n",
    "        logging.StreamHandler()  # Log to console\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: html2text in d:\\agentagent\\agentagent\\myenv\\lib\\site-packages (2024.2.26)\n"
     ]
    }
   ],
   "source": [
    "!pip install html2text\n",
    "import html2text\n",
    "import aiohttp\n",
    "import asyncio\n",
    "from bs4 import BeautifulSoup\n",
    "from langchain_core.documents import Document\n",
    "import json\n",
    "\n",
    "async def parse_url_to_doc(url: str) -> str:\n",
    "    try:\n",
    "        timeout = aiohttp.ClientTimeout(total=10)  # Set timeout to 3 seconds\n",
    "        async with aiohttp.ClientSession() as session:\n",
    "            async with session.get(url, timeout= timeout) as response:\n",
    "                if response.status != 200:\n",
    "                    logger.warning(f\"Failed to fetch {url}: Status code {response.status}\")\n",
    "                    return None\n",
    "                html_content = await response.text()\n",
    "                soup = BeautifulSoup(html_content, 'html.parser')\n",
    "                target_div = soup.find('div', class_= \"theme-doc-markdown markdown\") #langchain\n",
    "\n",
    "                if not target_div:\n",
    "                    target_div = soup.find('article') #langraph\n",
    "\n",
    "                if not target_div:\n",
    "                    return None\n",
    "\n",
    "                return Document(page_content=html2text.html2text(str(target_div)), metadata={\"source\": url})\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in parsing {url}: {e}\")\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-04 01:28:30,394 - WARNING - Failed to fetch https://langchain-ai.github.io/langgraph/branching: Status code 404\n",
      "2025-04-04 01:28:30,402 - WARNING - Failed to fetch https://langchain-ai.github.io/langgraph/autogen-integration-functional: Status code 404\n",
      "2025-04-04 01:28:30,416 - WARNING - Failed to fetch https://langchain-ai.github.io/langgraph/command: Status code 404\n",
      "2025-04-04 01:28:30,418 - WARNING - Failed to fetch https://langchain-ai.github.io/langgraph/async: Status code 404\n",
      "2025-04-04 01:28:30,419 - WARNING - Failed to fetch https://langchain-ai.github.io/langgraph/agentic_concepts: Status code 404\n",
      "2025-04-04 01:28:30,420 - WARNING - Failed to fetch https://langchain-ai.github.io/langgraph/breakpoints: Status code 404\n",
      "2025-04-04 01:28:30,422 - WARNING - Failed to fetch https://langchain-ai.github.io/langgraph/autogen-integration: Status code 404\n",
      "2025-04-04 01:28:35,773 - WARNING - Failed to fetch https://langchain-ai.github.io/langgraph/create-react-agent-memory: Status code 404\n",
      "2025-04-04 01:28:35,782 - WARNING - Failed to fetch https://langchain-ai.github.io/langgraph/cross-thread-persistence: Status code 404\n",
      "2025-04-04 01:28:35,784 - WARNING - Failed to fetch https://langchain-ai.github.io/langgraph/create-react-agent-system-prompt: Status code 404\n",
      "2025-04-04 01:28:35,795 - WARNING - Failed to fetch https://langchain-ai.github.io/langgraph/cross-thread-persistence-functional: Status code 404\n",
      "2025-04-04 01:28:35,800 - WARNING - Failed to fetch https://langchain-ai.github.io/langgraph/configuration: Status code 404\n",
      "2025-04-04 01:28:35,801 - WARNING - Failed to fetch https://langchain-ai.github.io/langgraph/create-react-agent-structured-output: Status code 404\n",
      "2025-04-04 01:28:35,803 - WARNING - Failed to fetch https://langchain-ai.github.io/langgraph/create-react-agent: Status code 404\n",
      "2025-04-04 01:28:35,805 - WARNING - Failed to fetch https://langchain-ai.github.io/langgraph/create-react-agent-hitl: Status code 404\n",
      "2025-04-04 01:28:36,430 - WARNING - Failed to fetch https://langchain-ai.github.io/langgraph/faq: Status code 404\n",
      "2025-04-04 01:28:36,445 - WARNING - Failed to fetch https://langchain-ai.github.io/langgraph/durable_execution: Status code 404\n",
      "2025-04-04 01:28:36,448 - WARNING - Failed to fetch https://langchain-ai.github.io/langgraph/disable-streaming: Status code 404\n",
      "2025-04-04 01:28:36,449 - WARNING - Failed to fetch https://langchain-ai.github.io/langgraph/functional_api: Status code 404\n",
      "2025-04-04 01:28:36,452 - WARNING - Failed to fetch https://langchain-ai.github.io/langgraph/deployment: Status code 404\n",
      "2025-04-04 01:28:36,454 - WARNING - Failed to fetch https://langchain-ai.github.io/langgraph/high_level: Status code 404\n",
      "2025-04-04 01:28:51,061 - WARNING - Failed to fetch https://langchain-ai.github.io/langgraph/human_in_the_loop: Status code 404\n",
      "2025-04-04 01:28:51,064 - WARNING - Failed to fetch https://langchain-ai.github.io/langgraph/low_level: Status code 404\n",
      "2025-04-04 01:28:51,101 - WARNING - Failed to fetch https://langchain-ai.github.io/langgraph/many-tools: Status code 404\n",
      "2025-04-04 01:28:51,106 - WARNING - Failed to fetch https://langchain-ai.github.io/langgraph/map-reduce: Status code 404\n",
      "2025-04-04 01:28:51,107 - WARNING - Failed to fetch https://langchain-ai.github.io/langgraph/input_output_schema: Status code 404\n",
      "2025-04-04 01:28:51,118 - WARNING - Failed to fetch https://langchain-ai.github.io/langgraph/introduction: Status code 404\n",
      "2025-04-04 01:28:51,722 - WARNING - Failed to fetch https://langchain-ai.github.io/langgraph/memory/manage-conversation-history: Status code 404\n",
      "2025-04-04 01:28:51,743 - WARNING - Failed to fetch https://langchain-ai.github.io/langgraph/multi_agent: Status code 404\n",
      "2025-04-04 01:28:51,747 - WARNING - Failed to fetch https://langchain-ai.github.io/langgraph/node-retries: Status code 404\n",
      "2025-04-04 01:28:51,750 - WARNING - Failed to fetch https://langchain-ai.github.io/langgraph/memory: Status code 404\n",
      "2025-04-04 01:28:51,752 - WARNING - Failed to fetch https://langchain-ai.github.io/langgraph/multi-agent-multi-turn-convo: Status code 404\n",
      "2025-04-04 01:28:51,756 - WARNING - Failed to fetch https://langchain-ai.github.io/langgraph/multi-agent-network: Status code 404\n",
      "2025-04-04 01:28:51,758 - WARNING - Failed to fetch https://langchain-ai.github.io/langgraph/pass_private_state: Status code 404\n",
      "2025-04-04 01:28:51,760 - WARNING - Failed to fetch https://langchain-ai.github.io/langgraph/multi-agent-network-functional: Status code 404\n",
      "2025-04-04 01:28:51,763 - WARNING - Failed to fetch https://langchain-ai.github.io/langgraph/multi-agent-multi-turn-convo-functional: Status code 404\n",
      "2025-04-04 01:28:51,766 - WARNING - Failed to fetch https://langchain-ai.github.io/langgraph/persistence: Status code 404\n",
      "2025-04-04 01:28:51,870 - WARNING - Failed to fetch https://langchain-ai.github.io/langgraph/react-agent-from-scratch-functional: Status code 404\n",
      "2025-04-04 01:28:51,873 - WARNING - Failed to fetch https://langchain-ai.github.io/langgraph/persistence-functional: Status code 404\n",
      "2025-04-04 01:28:51,874 - WARNING - Failed to fetch https://langchain-ai.github.io/langgraph/pregel: Status code 404\n",
      "2025-04-04 01:28:51,876 - WARNING - Failed to fetch https://langchain-ai.github.io/langgraph/persistence_mongodb: Status code 404\n",
      "2025-04-04 01:28:51,879 - WARNING - Failed to fetch https://langchain-ai.github.io/langgraph/persistence_redis: Status code 404\n",
      "2025-04-04 01:28:51,890 - WARNING - Failed to fetch https://langchain-ai.github.io/langgraph/persistence_postgres: Status code 404\n",
      "2025-04-04 01:28:51,892 - WARNING - Failed to fetch https://langchain-ai.github.io/langgraph/react-agent-from-scratch: Status code 404\n",
      "2025-04-04 01:28:51,899 - WARNING - Failed to fetch https://langchain-ai.github.io/langgraph/recursion-limit: Status code 404\n",
      "2025-04-04 01:28:55,697 - WARNING - Failed to fetch https://langchain-ai.github.io/langgraph/self-discover/self-discover: Status code 404\n",
      "2025-04-04 01:28:55,702 - WARNING - Failed to fetch https://langchain-ai.github.io/langgraph/review-tool-calls-functional: Status code 404\n",
      "2025-04-04 01:28:55,703 - WARNING - Failed to fetch https://langchain-ai.github.io/langgraph/sql-agent: Status code 404\n",
      "2025-04-04 01:28:55,705 - WARNING - Failed to fetch https://langchain-ai.github.io/langgraph/run-id-langsmith: Status code 404\n",
      "2025-04-04 01:28:55,709 - WARNING - Failed to fetch https://langchain-ai.github.io/langgraph/return-when-recursion-limit-hits: Status code 404\n",
      "2025-04-04 01:28:55,712 - WARNING - Failed to fetch https://langchain-ai.github.io/langgraph/state-reducers: Status code 404\n",
      "2025-04-04 01:28:55,714 - WARNING - Failed to fetch https://langchain-ai.github.io/langgraph/sequence: Status code 404\n",
      "2025-04-04 01:28:57,082 - WARNING - Failed to fetch https://langchain-ai.github.io/langgraph/streaming: Status code 404\n",
      "2025-04-04 01:28:57,085 - WARNING - Failed to fetch https://langchain-ai.github.io/langgraph/streaming-specific-nodes: Status code 404\n",
      "2025-04-04 01:28:57,101 - WARNING - Failed to fetch https://langchain-ai.github.io/langgraph/subgraph-persistence: Status code 404\n",
      "2025-04-04 01:28:57,106 - WARNING - Failed to fetch https://langchain-ai.github.io/langgraph/streaming-subgraphs: Status code 404\n",
      "2025-04-04 01:28:57,109 - WARNING - Failed to fetch https://langchain-ai.github.io/langgraph/subgraphs-manage-state: Status code 404\n",
      "2025-04-04 01:28:57,112 - WARNING - Failed to fetch https://langchain-ai.github.io/langgraph/time-travel: Status code 404\n",
      "2025-04-04 01:28:57,118 - WARNING - Failed to fetch https://langchain-ai.github.io/langgraph/subgraph-transform-state: Status code 404\n",
      "2025-04-04 01:28:57,120 - WARNING - Failed to fetch https://langchain-ai.github.io/langgraph/streaming-events-from-within-tools: Status code 404\n",
      "2025-04-04 01:28:57,125 - WARNING - Failed to fetch https://langchain-ai.github.io/langgraph/streaming-tokens: Status code 404\n",
      "2025-04-04 01:28:57,627 - WARNING - Failed to fetch https://langchain-ai.github.io/langgraph/tutorials/agent_supervisor: Status code 404\n",
      "2025-04-04 01:28:58,700 - WARNING - Failed to fetch https://langchain-ai.github.io/langgraph/tutorials/hierarchical_agent_teams: Status code 404\n",
      "2025-04-04 01:28:58,883 - WARNING - Failed to fetch https://langchain-ai.github.io/langgraph/tutorials/chatbot-simulation-evaluation: Status code 404\n",
      "2025-04-04 01:29:04,236 - WARNING - Failed to fetch https://langchain-ai.github.io/langgraph/tutorials/langsmith-agent-simulation-evaluation: Status code 404\n",
      "2025-04-04 01:29:04,461 - WARNING - Failed to fetch https://langchain-ai.github.io/langgraph/tutorials/langgraph-platform: Status code 404\n",
      "2025-04-04 01:29:04,502 - WARNING - Failed to fetch https://langchain-ai.github.io/langgraph/tutorials/multi_agent: Status code 404\n",
      "2025-04-04 01:29:14,196 - WARNING - Failed to fetch https://langchain-ai.github.io/langgraph/v0-human-in-the-loop: Status code 404\n",
      "2025-04-04 01:29:18,785 - WARNING - Failed to fetch https://langchain-ai.github.io/langgraph/workflows: Status code 404\n",
      "2025-04-04 01:29:18,788 - WARNING - Failed to fetch https://langchain-ai.github.io/langgraph/visualization: Status code 404\n"
     ]
    }
   ],
   "source": [
    "with open('webscraping/visited_urls_langgraph.json', 'r') as f:\n",
    "    urls = json.load(f)\n",
    "\n",
    "    # Generator function to yield 10 items at a time\n",
    "    def batch_iterator(iterable, batch_size):\n",
    "        for i in range(0, len(iterable), batch_size):\n",
    "            yield iterable[i:i + batch_size]\n",
    "\n",
    "    batch_size = 10\n",
    "\n",
    "    # Create the iterator\n",
    "    iterator = batch_iterator(urls, batch_size)\n",
    "\n",
    "    documents = []\n",
    "\n",
    "    # Use the iterator\n",
    "    for batch in iterator:\n",
    "        tasks = [parse_url_to_doc(url) for url in batch]\n",
    "        results = await asyncio.gather(*tasks)\n",
    "        documents.append(results)\n",
    "    documents = [item for sublist in documents for item in sublist]\n",
    "    documents =  [item for item in documents if item != None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9895"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(encoding_name=\"cl100k_base\",\n",
    "    chunk_size=1500, chunk_overlap=200\n",
    ")\n",
    "doc_splits = text_splitter.split_documents(documents)\n",
    "\n",
    "len(doc_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10263075\n"
     ]
    }
   ],
   "source": [
    "doc_len=0\n",
    "import tiktoken\n",
    "for doc in doc_splits:\n",
    "    encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    num_tokens = len(encoding.encode(doc.page_content))\n",
    "    doc_len += num_tokens\n",
    "    \n",
    "print(doc_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-04 02:21:36,194 - INFO - HTTP Request: POST https://kanis-m8htxgs7-eastus.openai.azure.com/openai/deployments/text-embedding-3-small/embeddings?api-version=2023-05-15 \"HTTP/1.1 200 OK\"\n",
      "2025-04-04 02:23:03,367 - INFO - HTTP Request: POST https://kanis-m8htxgs7-eastus.openai.azure.com/openai/deployments/text-embedding-3-small/embeddings?api-version=2023-05-15 \"HTTP/1.1 200 OK\"\n",
      "2025-04-04 02:23:43,528 - INFO - HTTP Request: POST https://kanis-m8htxgs7-eastus.openai.azure.com/openai/deployments/text-embedding-3-small/embeddings?api-version=2023-05-15 \"HTTP/1.1 200 OK\"\n",
      "2025-04-04 02:24:32,915 - INFO - HTTP Request: POST https://kanis-m8htxgs7-eastus.openai.azure.com/openai/deployments/text-embedding-3-small/embeddings?api-version=2023-05-15 \"HTTP/1.1 200 OK\"\n",
      "2025-04-04 02:25:13,115 - INFO - HTTP Request: POST https://kanis-m8htxgs7-eastus.openai.azure.com/openai/deployments/text-embedding-3-small/embeddings?api-version=2023-05-15 \"HTTP/1.1 200 OK\"\n",
      "2025-04-04 02:26:02,574 - INFO - HTTP Request: POST https://kanis-m8htxgs7-eastus.openai.azure.com/openai/deployments/text-embedding-3-small/embeddings?api-version=2023-05-15 \"HTTP/1.1 200 OK\"\n",
      "2025-04-04 02:27:35,499 - INFO - HTTP Request: POST https://kanis-m8htxgs7-eastus.openai.azure.com/openai/deployments/text-embedding-3-small/embeddings?api-version=2023-05-15 \"HTTP/1.1 200 OK\"\n",
      "2025-04-04 02:28:18,782 - INFO - HTTP Request: POST https://kanis-m8htxgs7-eastus.openai.azure.com/openai/deployments/text-embedding-3-small/embeddings?api-version=2023-05-15 \"HTTP/1.1 200 OK\"\n",
      "2025-04-04 02:29:34,290 - INFO - HTTP Request: POST https://kanis-m8htxgs7-eastus.openai.azure.com/openai/deployments/text-embedding-3-small/embeddings?api-version=2023-05-15 \"HTTP/1.1 200 OK\"\n",
      "2025-04-04 02:31:57,856 - INFO - HTTP Request: POST https://kanis-m8htxgs7-eastus.openai.azure.com/openai/deployments/text-embedding-3-small/embeddings?api-version=2023-05-15 \"HTTP/1.1 200 OK\"\n",
      "2025-04-04 02:32:25,060 - INFO - HTTP Request: POST https://kanis-m8htxgs7-eastus.openai.azure.com/openai/deployments/text-embedding-3-small/embeddings?api-version=2023-05-15 \"HTTP/1.1 200 OK\"\n",
      "2025-04-04 02:33:00,532 - INFO - HTTP Request: POST https://kanis-m8htxgs7-eastus.openai.azure.com/openai/deployments/text-embedding-3-small/embeddings?api-version=2023-05-15 \"HTTP/1.1 200 OK\"\n",
      "2025-04-04 02:33:45,036 - INFO - HTTP Request: POST https://kanis-m8htxgs7-eastus.openai.azure.com/openai/deployments/text-embedding-3-small/embeddings?api-version=2023-05-15 \"HTTP/1.1 200 OK\"\n",
      "2025-04-04 02:35:16,293 - INFO - HTTP Request: POST https://kanis-m8htxgs7-eastus.openai.azure.com/openai/deployments/text-embedding-3-small/embeddings?api-version=2023-05-15 \"HTTP/1.1 200 OK\"\n",
      "2025-04-04 02:36:33,950 - INFO - HTTP Request: POST https://kanis-m8htxgs7-eastus.openai.azure.com/openai/deployments/text-embedding-3-small/embeddings?api-version=2023-05-15 \"HTTP/1.1 200 OK\"\n",
      "2025-04-04 02:37:11,235 - INFO - HTTP Request: POST https://kanis-m8htxgs7-eastus.openai.azure.com/openai/deployments/text-embedding-3-small/embeddings?api-version=2023-05-15 \"HTTP/1.1 200 OK\"\n",
      "2025-04-04 02:37:52,140 - INFO - HTTP Request: POST https://kanis-m8htxgs7-eastus.openai.azure.com/openai/deployments/text-embedding-3-small/embeddings?api-version=2023-05-15 \"HTTP/1.1 200 OK\"\n",
      "2025-04-04 02:38:31,381 - INFO - HTTP Request: POST https://kanis-m8htxgs7-eastus.openai.azure.com/openai/deployments/text-embedding-3-small/embeddings?api-version=2023-05-15 \"HTTP/1.1 200 OK\"\n",
      "2025-04-04 02:39:05,841 - INFO - HTTP Request: POST https://kanis-m8htxgs7-eastus.openai.azure.com/openai/deployments/text-embedding-3-small/embeddings?api-version=2023-05-15 \"HTTP/1.1 200 OK\"\n",
      "2025-04-04 02:39:41,915 - INFO - HTTP Request: POST https://kanis-m8htxgs7-eastus.openai.azure.com/openai/deployments/text-embedding-3-small/embeddings?api-version=2023-05-15 \"HTTP/1.1 200 OK\"\n",
      "2025-04-04 02:40:41,848 - INFO - HTTP Request: POST https://kanis-m8htxgs7-eastus.openai.azure.com/openai/deployments/text-embedding-3-small/embeddings?api-version=2023-05-15 \"HTTP/1.1 200 OK\"\n",
      "2025-04-04 02:41:30,316 - INFO - HTTP Request: POST https://kanis-m8htxgs7-eastus.openai.azure.com/openai/deployments/text-embedding-3-small/embeddings?api-version=2023-05-15 \"HTTP/1.1 200 OK\"\n",
      "2025-04-04 02:42:40,661 - INFO - HTTP Request: POST https://kanis-m8htxgs7-eastus.openai.azure.com/openai/deployments/text-embedding-3-small/embeddings?api-version=2023-05-15 \"HTTP/1.1 200 OK\"\n",
      "2025-04-04 02:43:38,732 - INFO - HTTP Request: POST https://kanis-m8htxgs7-eastus.openai.azure.com/openai/deployments/text-embedding-3-small/embeddings?api-version=2023-05-15 \"HTTP/1.1 200 OK\"\n",
      "2025-04-04 02:44:11,506 - INFO - HTTP Request: POST https://kanis-m8htxgs7-eastus.openai.azure.com/openai/deployments/text-embedding-3-small/embeddings?api-version=2023-05-15 \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "iterator = batch_iterator(doc_splits, 400)\n",
    "\n",
    "for doc in iterator:\n",
    "    try:\n",
    "        vector_store.add_documents(doc)\n",
    "    except Exception as ex:\n",
    "        logger.info(f\"{ex} caught, adding a 62 second sleep and retrying again\")\n",
    "        time.sleep(62)\n",
    "        vector_store.add_documents(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-04 18:52:29,890 - INFO - HTTP Request: POST https://kanis-m8htxgs7-eastus.openai.azure.com/openai/deployments/text-embedding-3-small/embeddings?api-version=2023-05-15 \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(Document(metadata={'source': 'https://langchain-ai.github.io/langgraph/tutorials/workflows'}, page_content='[ ](https://github.com/langchain-\\nai/langgraph/edit/main/docs/docs/tutorials/workflows/index.md \"Edit this\\npage\")\\n\\n# Workflows and Agents¶\\n\\nThis guide reviews common patterns for agentic systems. In describing these\\nsystems, it can be useful to make a distinction between \"workflows\" and\\n\"agents\". One way to think about this difference is nicely explained in\\n[Anthropic\\'s](https://python.langchain.com/docs/integrations/providers/anthropic/)\\n`Building Effective Agents` blog post:\\n\\n> Workflows are systems where LLMs and tools are orchestrated through\\n> predefined code paths. Agents, on the other hand, are systems where LLMs\\n> dynamically direct their own processes and tool usage, maintaining control\\n> over how they accomplish tasks.\\n\\nHere is a simple way to visualize these differences:\\n\\n![Agent Workflow](../../concepts/img/agent_workflow.png)\\n\\nWhen building agents and workflows, LangGraph offers a number of benefits\\nincluding persistence, streaming, and support for debugging as well as\\ndeployment.\\n\\n## Set up¶\\n\\nYou can use [any chat\\nmodel](https://python.langchain.com/docs/integrations/chat/) that supports\\nstructured outputs and tool calling. Below, we show the process of installing\\nthe packages, setting API keys, and testing structured outputs / tool calling\\nfor Anthropic.\\n\\nInstall dependencies\\n\\n    \\n    \\n    pip install langchain_core langchain-anthropic langgraph \\n    \\n\\nInitialize an LLM\\n\\n    \\n    \\n    import os\\n    import getpass\\n    \\n    from langchain_anthropic import ChatAnthropic\\n    \\n    def _set_env(var: str):\\n        if not os.environ.get(var):\\n            os.environ[var] = getpass.getpass(f\"{var}: \")\\n    \\n    \\n    _set_env(\"ANTHROPIC_API_KEY\")\\n    \\n    llm = ChatAnthropic(model=\"claude-3-5-sonnet-latest\")\\n    \\n\\nAPI Reference:\\n[ChatAnthropic](https://python.langchain.com/api_reference/anthropic/chat_models/langchain_anthropic.chat_models.ChatAnthropic.html)\\n\\n## Building Blocks: The Augmented LLM¶\\n\\nLLM have augmentations that support building workflows and agents. These\\ninclude [structured\\noutputs](https://python.langchain.com/docs/concepts/structured_outputs/) and\\n[tool calling](https://python.langchain.com/docs/concepts/tool_calling/), as\\nshown in this image from the Anthropic blog on `Building Effective Agents`:\\n\\n![augmented_llm.png](img/augmented_llm.png)\\n\\n    \\n    \\n    # Schema for structured output\\n    from pydantic import BaseModel, Field\\n    \\n    class SearchQuery(BaseModel):\\n        search_query: str = Field(None, description=\"Query that is optimized web search.\")\\n        justification: str = Field(\\n            None, description=\"Why this query is relevant to the user\\'s request.\"\\n        )\\n    \\n    \\n    # Augment the LLM with schema for structured output\\n    structured_llm = llm.with_structured_output(SearchQuery)\\n    \\n    # Invoke the augmented LLM\\n    output = structured_llm.invoke(\"How does Calcium CT score relate to high cholesterol?\")\\n    \\n    # Define a tool\\n    def multiply(a: int, b: int) -> int:\\n        return a * b\\n    \\n    # Augment the LLM with tools\\n    llm_with_tools = llm.bind_tools([multiply])\\n    \\n    # Invoke the LLM with input that triggers the tool call\\n    msg = llm_with_tools.invoke(\"What is 2 times 3?\")\\n    \\n    # Get the tool call\\n    msg.tool_calls\\n    \\n\\n## Prompt chaining¶\\n\\nIn prompt chaining, each LLM call processes the output of the previous one.\\n\\nAs noted in the Anthropic blog on `Building Effective Agents`:\\n\\n> Prompt chaining decomposes a task into a sequence of steps, where each LLM\\n> call processes the output of the previous one. You can add programmatic\\n> checks (see \"gate” in the diagram below) on any intermediate steps to ensure\\n> that the process is still on track.\\n>\\n> When to use this workflow: This workflow is ideal for situations where the\\n> task can be easily and cleanly decomposed into fixed subtasks. The main goal\\n> is to trade off latency for higher accuracy, by making each LLM call an\\n> easier task.\\n\\n![prompt_chain.png](img/prompt_chain.png)\\n\\nGraph APIFunctional API'),\n",
       "  0.6855273246765137),\n",
       " (Document(metadata={'source': 'https://langchain-ai.github.io/langgraph/how-tos/autogen-langgraph-platform'}, page_content='[ ](https://github.com/langchain-ai/langgraph/edit/main/docs/docs/how-\\ntos/autogen-langgraph-platform.ipynb \"Edit this page\")\\n\\n# How to use LangGraph Platform to deploy CrewAI, AutoGen, and other\\nframeworks¶\\n\\n[LangGraph Platform](https://langchain-\\nai.github.io/langgraph/concepts/langgraph_platform/) provides infrastructure\\nfor deploying agents. This integrates seamlessly with LangGraph, but can also\\nwork with other frameworks. The way to make this work is to wrap the agent in\\na single LangGraph node, and have that be the entire graph.\\n\\nDoing so will allow you to deploy to LangGraph Platform, and allows you to get\\na lot of the [benefits](https://langchain-\\nai.github.io/langgraph/concepts/langgraph_platform/). You get horizontally\\nscalable infrastructure, a task queue to handle bursty operations, a\\npersistence layer to power short term memory, and long term memory support.\\n\\nIn this guide we show how to do this with an AutoGen agent, but this method\\nshould work for agents defined in other frameworks like CrewAI, LlamaIndex,\\nand others as well.\\n\\n## Setup¶\\n\\n    \\n    \\n    %pip install autogen langgraph\\n    \\n    \\n    \\n    import getpass\\n    import os\\n    \\n    \\n    def _set_env(var: str):\\n        if not os.environ.get(var):\\n            os.environ[var] = getpass.getpass(f\"{var}: \")\\n    \\n    \\n    _set_env(\"OPENAI_API_KEY\")\\n    \\n\\n## Define autogen agent¶\\n\\nHere we define our AutoGen agent. From\\n<https://github.com/microsoft/autogen/blob/0.2/notebook/agentchat_web_info.ipynb>\\n\\n    \\n    \\n    import autogen\\n    import os\\n    \\n    config_list = [{\"model\": \"gpt-4o\", \"api_key\": os.environ[\"OPENAI_API_KEY\"]}]\\n    \\n    llm_config = {\\n        \"timeout\": 600,\\n        \"cache_seed\": 42,\\n        \"config_list\": config_list,\\n        \"temperature\": 0,\\n    }\\n    \\n    autogen_agent = autogen.AssistantAgent(\\n        name=\"assistant\",\\n        llm_config=llm_config,\\n    )\\n    \\n    user_proxy = autogen.UserProxyAgent(\\n        name=\"user_proxy\",\\n        human_input_mode=\"NEVER\",\\n        max_consecutive_auto_reply=10,\\n        is_termination_msg=lambda x: x.get(\"content\", \"\").rstrip().endswith(\"TERMINATE\"),\\n        code_execution_config={\\n            \"work_dir\": \"web\",\\n            \"use_docker\": False,\\n        },  # Please set use_docker=True if docker is available to run the generated code. Using docker is safer than running the generated code directly.\\n        llm_config=llm_config,\\n        system_message=\"Reply TERMINATE if the task has been solved at full satisfaction. Otherwise, reply CONTINUE, or the reason why the task is not solved yet.\",\\n    )\\n    \\n\\n## Wrap in LangGraph¶\\n\\nWe now wrap the AutoGen agent in a single LangGraph node, and make that the\\nentire graph. The main thing this involves is defining an Input and Output\\nschema for the node, which you would need to do if deploying this manually, so\\nit\\'s no extra work\\n\\n    \\n    \\n    from langgraph.graph import StateGraph, MessagesState\\n    \\n    \\n    def call_autogen_agent(state: MessagesState):\\n        last_message = state[\"messages\"][-1]\\n        response = user_proxy.initiate_chat(autogen_agent, message=last_message.content)\\n        # get the final response from the agent\\n        content = response.chat_history[-1][\"content\"]\\n        return {\"messages\": {\"role\": \"assistant\", \"content\": content}}\\n    \\n    \\n    graph = StateGraph(MessagesState)\\n    graph.add_node(call_autogen_agent)\\n    graph.set_entry_point(\"call_autogen_agent\")\\n    graph = graph.compile()\\n    \\n\\nAPI Reference: [StateGraph](https://langchain-\\nai.github.io/langgraph/reference/graphs/#langgraph.graph.state.StateGraph)\\n\\n## Deploy with LangGraph Platform¶\\n\\nYou can now deploy this as you normally would with LangGraph Platform. See\\n[these instructions](https://langchain-\\nai.github.io/langgraph/concepts/deployment_options/) for more details.\\n\\nWas this page helpful?\\n\\nThanks for your feedback!\\n\\nThanks for your feedback! Please help us improve this page by adding to the\\ndiscussion below.\\n\\n## Comments\\n\\n'),\n",
       "  0.6788091659545898),\n",
       " (Document(metadata={'source': 'https://langchain-ai.github.io/langgraph/concepts/multi_agent'}, page_content='[ ](https://github.com/langchain-\\nai/langgraph/edit/main/docs/docs/concepts/multi_agent.md \"Edit this page\")\\n\\n# Multi-agent Systems¶\\n\\nAn [agent](../agentic_concepts/#agent-architectures) is _a system that uses an\\nLLM to decide the control flow of an application_. As you develop these\\nsystems, they might grow more complex over time, making them harder to manage\\nand scale. For example, you might run into the following problems:\\n\\n  * agent has too many tools at its disposal and makes poor decisions about which tool to call next\\n  * context grows too complex for a single agent to keep track of\\n  * there is a need for multiple specialization areas in the system (e.g. planner, researcher, math expert, etc.)\\n\\nTo tackle these, you might consider breaking your application into multiple\\nsmaller, independent agents and composing them into a **multi-agent system**.\\nThese independent agents can be as simple as a prompt and an LLM call, or as\\ncomplex as a [ReAct](../agentic_concepts/#react-implementation) agent (and\\nmore!).\\n\\nThe primary benefits of using multi-agent systems are:\\n\\n  * **Modularity** : Separate agents make it easier to develop, test, and maintain agentic systems.\\n  * **Specialization** : You can create expert agents focused on specific domains, which helps with the overall system performance.\\n  * **Control** : You can explicitly control how agents communicate (as opposed to relying on function calling).\\n\\n## Multi-agent architectures¶\\n\\n![](../img/multi_agent/architectures.png)\\n\\nThere are several ways to connect agents in a multi-agent system:\\n\\n  * **Network** : each agent can communicate with [every other agent](https://langchain-ai.github.io/langgraph/tutorials/multi_agent/multi-agent-collaboration/). Any agent can decide which other agent to call next.\\n  * **Supervisor** : each agent communicates with a single [supervisor](https://langchain-ai.github.io/langgraph/tutorials/multi_agent/agent_supervisor/) agent. Supervisor agent makes decisions on which agent should be called next.\\n  * **Supervisor (tool-calling)** : this is a special case of supervisor architecture. Individual agents can be represented as tools. In this case, a supervisor agent uses a tool-calling LLM to decide which of the agent tools to call, as well as the arguments to pass to those agents.\\n  * **Hierarchical** : you can define a multi-agent system with [a supervisor of supervisors](https://langchain-ai.github.io/langgraph/tutorials/multi_agent/hierarchical_agent_teams/). This is a generalization of the supervisor architecture and allows for more complex control flows.\\n  * **Custom multi-agent workflow** : each agent communicates with only a subset of agents. Parts of the flow are deterministic, and only some agents can decide which other agents to call next.\\n\\n### Handoffs¶\\n\\nIn multi-agent architectures, agents can be represented as graph nodes. Each\\nagent node executes its step(s) and decides whether to finish execution or\\nroute to another agent, including potentially routing to itself (e.g., running\\nin a loop). A common pattern in multi-agent interactions is handoffs, where\\none agent hands off control to another. Handoffs allow you to specify:\\n\\n  * **destination** : target agent to navigate to (e.g., name of the node to go to)\\n  * **payload** : information to pass to that agent (e.g., state update)\\n\\nTo implement handoffs in LangGraph, agent nodes can return\\n[`Command`](../low_level/#command) object that allows you to combine both\\ncontrol flow and state updates:\\n\\n    \\n    \\n    def agent(state) -> Command[Literal[\"agent\", \"another_agent\"]]:\\n        # the condition for routing/halting can be anything, e.g. LLM tool call / structured output, etc.\\n        goto = get_next_agent(...)  # \\'agent\\' / \\'another_agent\\'\\n        return Command(\\n            # Specify which agent to call next\\n            goto=goto,\\n            # Update the graph state\\n            update={\"my_state_key\": \"my_state_value\"}\\n        )\\n    \\n\\nIn a more complex scenario where each agent node is itself a graph (i.e., a\\n[subgraph](../low_level/#subgraphs)), a node in one of the agent subgraphs\\nmight want to navigate to a different agent. For example, if you have two\\nagents, `alice` and `bob` (subgraph nodes in a parent graph), and `alice`\\nneeds to navigate to `bob`, you can set `graph=Command.PARENT` in the\\n`Command` object:\\n\\n    \\n    \\n    def some_node_inside_alice(state):\\n        return Command(\\n            goto=\"bob\",\\n            update={\"my_state_key\": \"my_state_value\"},\\n            # specify which graph to navigate to (defaults to the current graph)\\n            graph=Command.PARENT,\\n        )\\n    \\n\\nNote\\n\\nIf you need to support visualization for subgraphs communicating using\\n`Command(graph=Command.PARENT)` you would need to wrap them in a node function\\nwith `Command` annotation, e.g. instead of this:\\n\\n    \\n    \\n    builder.add_node(alice)\\n    \\n\\nyou would need to do this:\\n\\n    \\n    \\n    def call_alice(state) -> Command[Literal[\"bob\"]]:\\n        return alice.invoke(state)\\n    \\n    builder.add_node(\"alice\", call_alice)\\n    \\n\\n#### Handoffs as tools¶\\n\\nOne of the most common agent types is a ReAct-style tool-calling agents. For\\nthose types of agents, a common pattern is wrapping a handoff in a tool call,\\ne.g.:\\n\\n    \\n    \\n    def transfer_to_bob(state):\\n        \"\"\"Transfer to bob.\"\"\"\\n        return Command(\\n            goto=\"bob\",\\n            update={\"my_state_key\": \"my_state_value\"},\\n            graph=Command.PARENT,\\n        )\\n    \\n\\nThis is a special case of updating the graph state from tools where, in\\naddition to the state update, the control flow is included as well.\\n\\nImportant\\n\\nIf you want to use tools that return `Command`, you can either use prebuilt\\n[`create_react_agent`](../../reference/prebuilt/#langgraph.prebuilt.chat_agent_executor.create_react_agent)\\n/\\n[`ToolNode`](../../reference/prebuilt/#langgraph.prebuilt.tool_node.ToolNode)\\ncomponents, or implement your own tool-executing node that collects `Command`\\nobjects returned by the tools and returns a list of them, e.g.:\\n\\n    \\n    \\n    def call_tools(state):\\n        ...\\n        commands = [tools_by_name[tool_call[\"name\"]].invoke(tool_call) for tool_call in tool_calls]\\n        return commands\\n    \\n\\nLet\\'s now take a closer look at the different multi-agent architectures.\\n\\n### Network¶\\n\\nIn this architecture, agents are defined as graph nodes. Each agent can\\ncommunicate with every other agent (many-to-many connections) and can decide\\nwhich agent to call next. This architecture is good for problems that do not\\nhave a clear hierarchy of agents or a specific sequence in which agents should\\nbe called.\\n\\n    \\n    \\n    from typing import Literal\\n    from langchain_openai import ChatOpenAI\\n    from langgraph.types import Command\\n    from langgraph.graph import StateGraph, MessagesState, START, END\\n    \\n    model = ChatOpenAI()\\n    \\n    def agent_1(state: MessagesState) -> Command[Literal[\"agent_2\", \"agent_3\", END]]:\\n        # you can pass relevant parts of the state to the LLM (e.g., state[\"messages\"])\\n        # to determine which agent to call next. a common pattern is to call the model\\n        # with a structured output (e.g. force it to return an output with a \"next_agent\" field)\\n        response = model.invoke(...)\\n        # route to one of the agents or exit based on the LLM\\'s decision\\n        # if the LLM returns \"__end__\", the graph will finish execution\\n        return Command(\\n            goto=response[\"next_agent\"],\\n            update={\"messages\": [response[\"content\"]]},\\n        )\\n    \\n    def agent_2(state: MessagesState) -> Command[Literal[\"agent_1\", \"agent_3\", END]]:\\n        response = model.invoke(...)\\n        return Command(\\n            goto=response[\"next_agent\"],\\n            update={\"messages\": [response[\"content\"]]},\\n        )\\n    \\n    def agent_3(state: MessagesState) -> Command[Literal[\"agent_1\", \"agent_2\", END]]:\\n        ...\\n        return Command(\\n            goto=response[\"next_agent\"],\\n            update={\"messages\": [response[\"content\"]]},\\n        )\\n    \\n    builder = StateGraph(MessagesState)\\n    builder.add_node(agent_1)\\n    builder.add_node(agent_2)\\n    builder.add_node(agent_3)\\n    \\n    builder.add_edge(START, \"agent_1\")\\n    network = builder.compile()\\n    \\n\\nAPI Reference: [ChatOpenAI](https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html) | [Command](https://langchain-ai.github.io/langgraph/reference/types/#langgraph.types.Command) | [StateGraph](https://langchain-ai.github.io/langgraph/reference/graphs/#langgraph.graph.state.StateGraph) | [START](https://langchain-ai.github.io/langgraph/reference/constants/#langgraph.constants.START) | [END](https://langchain-ai.github.io/langgraph/reference/constants/#langgraph.constants.END)\\n\\n### Supervisor¶\\n\\nIn this architecture, we define agents as nodes and add a supervisor node\\n(LLM) that decides which agent nodes should be called next. We use\\n[`Command`](../low_level/#command) to route execution to the appropriate agent\\nnode based on supervisor\\'s decision. This architecture also lends itself well\\nto running multiple agents in parallel or using [map-reduce](../../how-\\ntos/map-reduce/) pattern.\\n\\n    \\n    \\n    from typing import Literal\\n    from langchain_openai import ChatOpenAI\\n    from langgraph.types import Command\\n    from langgraph.graph import StateGraph, MessagesState, START, END\\n    \\n    model = ChatOpenAI()\\n    \\n    def supervisor(state: MessagesState) -> Command[Literal[\"agent_1\", \"agent_2\", END]]:\\n        # you can pass relevant parts of the state to the LLM (e.g., state[\"messages\"])\\n        # to determine which agent to call next. a common pattern is to call the model\\n        # with a structured output (e.g. force it to return an output with a \"next_agent\" field)\\n        response = model.invoke(...)\\n        # route to one of the agents or exit based on the supervisor\\'s decision\\n        # if the supervisor returns \"__end__\", the graph will finish execution\\n        return Command(goto=response[\"next_agent\"])\\n    \\n    def agent_1(state: MessagesState) -> Command[Literal[\"supervisor\"]]:\\n        # you can pass relevant parts of the state to the LLM (e.g., state[\"messages\"])\\n        # and add any additional logic (different models, custom prompts, structured output, etc.)\\n        response = model.invoke(...)\\n        return Command(\\n            goto=\"supervisor\",\\n            update={\"messages\": [response]},\\n        )\\n    \\n    def agent_2(state: MessagesState) -> Command[Literal[\"supervisor\"]]:\\n        response = model.invoke(...)\\n        return Command(\\n            goto=\"supervisor\",\\n            update={\"messages\": [response]},\\n        )\\n    \\n    builder = StateGraph(MessagesState)\\n    builder.add_node(supervisor)\\n    builder.add_node(agent_1)\\n    builder.add_node(agent_2)\\n    \\n    builder.add_edge(START, \"supervisor\")\\n    \\n    supervisor = builder.compile()\\n    \\n\\nAPI Reference: [ChatOpenAI](https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html) | [Command](https://langchain-ai.github.io/langgraph/reference/types/#langgraph.types.Command) | [StateGraph](https://langchain-ai.github.io/langgraph/reference/graphs/#langgraph.graph.state.StateGraph) | [START](https://langchain-ai.github.io/langgraph/reference/constants/#langgraph.constants.START) | [END](https://langchain-ai.github.io/langgraph/reference/constants/#langgraph.constants.END)\\n\\nCheck out this [tutorial](https://langchain-\\nai.github.io/langgraph/tutorials/multi_agent/agent_supervisor/) for an example\\nof supervisor multi-agent architecture.\\n\\n### Supervisor (tool-calling)¶\\n\\nIn this variant of the supervisor architecture, we define individual agents as\\n**tools** and use a tool-calling LLM in the supervisor node. This can be\\nimplemented as a [ReAct](../agentic_concepts/#react-implementation)-style\\nagent with two nodes — an LLM node (supervisor) and a tool-calling node that\\nexecutes tools (agents in this case).\\n\\n    \\n    \\n    from typing import Annotated\\n    from langchain_openai import ChatOpenAI\\n    from langgraph.prebuilt import InjectedState, create_react_agent\\n    \\n    model = ChatOpenAI()\\n    \\n    # this is the agent function that will be called as tool\\n    # notice that you can pass the state to the tool via InjectedState annotation\\n    def agent_1(state: Annotated[dict, InjectedState]):\\n        # you can pass relevant parts of the state to the LLM (e.g., state[\"messages\"])\\n        # and add any additional logic (different models, custom prompts, structured output, etc.)\\n        response = model.invoke(...)\\n        # return the LLM response as a string (expected tool response format)\\n        # this will be automatically turned to ToolMessage\\n        # by the prebuilt create_react_agent (supervisor)\\n        return response.content\\n    \\n    def agent_2(state: Annotated[dict, InjectedState]):\\n        response = model.invoke(...)\\n        return response.content\\n    \\n    tools = [agent_1, agent_2]\\n    # the simplest way to build a supervisor w/ tool-calling is to use prebuilt ReAct agent graph\\n    # that consists of a tool-calling LLM node (i.e. supervisor) and a tool-executing node\\n    supervisor = create_react_agent(model, tools)\\n    \\n\\nAPI Reference: [ChatOpenAI](https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html) | [InjectedState](https://langchain-ai.github.io/langgraph/reference/prebuilt/#langgraph.prebuilt.tool_node.InjectedState) | [create_react_agent](https://langchain-ai.github.io/langgraph/reference/prebuilt/#langgraph.prebuilt.chat_agent_executor.create_react_agent)\\n\\n### Hierarchical¶\\n\\nAs you add more agents to your system, it might become too hard for the\\nsupervisor to manage all of them. The supervisor might start making poor\\ndecisions about which agent to call next, or the context might become too\\ncomplex for a single supervisor to keep track of. In other words, you end up\\nwith the same problems that motivated the multi-agent architecture in the\\nfirst place.\\n\\nTo address this, you can design your system _hierarchically_. For example, you\\ncan create separate, specialized teams of agents managed by individual\\nsupervisors, and a top-level supervisor to manage the teams.\\n\\n    \\n    \\n    from typing import Literal\\n    from langchain_openai import ChatOpenAI\\n    from langgraph.graph import StateGraph, MessagesState, START, END\\n    from langgraph.types import Command\\n    model = ChatOpenAI()\\n    \\n    # define team 1 (same as the single supervisor example above)\\n    \\n    def team_1_supervisor(state: MessagesState) -> Command[Literal[\"team_1_agent_1\", \"team_1_agent_2\", END]]:\\n        response = model.invoke(...)\\n        return Command(goto=response[\"next_agent\"])\\n    \\n    def team_1_agent_1(state: MessagesState) -> Command[Literal[\"team_1_supervisor\"]]:\\n        response = model.invoke(...)\\n        return Command(goto=\"team_1_supervisor\", update={\"messages\": [response]})\\n    \\n    def team_1_agent_2(state: MessagesState) -> Command[Literal[\"team_1_supervisor\"]]:\\n        response = model.invoke(...)\\n        return Command(goto=\"team_1_supervisor\", update={\"messages\": [response]})\\n    \\n    team_1_builder = StateGraph(Team1State)\\n    team_1_builder.add_node(team_1_supervisor)\\n    team_1_builder.add_node(team_1_agent_1)\\n    team_1_builder.add_node(team_1_agent_2)\\n    team_1_builder.add_edge(START, \"team_1_supervisor\")\\n    team_1_graph = team_1_builder.compile()\\n    \\n    # define team 2 (same as the single supervisor example above)\\n    class Team2State(MessagesState):\\n        next: Literal[\"team_2_agent_1\", \"team_2_agent_2\", \"__end__\"]\\n    \\n    def team_2_supervisor(state: Team2State):\\n        ...\\n    \\n    def team_2_agent_1(state: Team2State):\\n        ...\\n    \\n    def team_2_agent_2(state: Team2State):\\n        ...\\n    \\n    team_2_builder = StateGraph(Team2State)\\n    ...\\n    team_2_graph = team_2_builder.compile()\\n    \\n    \\n    # define top-level supervisor\\n    \\n    builder = StateGraph(MessagesState)\\n    def top_level_supervisor(state: MessagesState) -> Command[Literal[\"team_1_graph\", \"team_2_graph\", END]]:\\n        # you can pass relevant parts of the state to the LLM (e.g., state[\"messages\"])\\n        # to determine which team to call next. a common pattern is to call the model\\n        # with a structured output (e.g. force it to return an output with a \"next_team\" field)\\n        response = model.invoke(...)\\n        # route to one of the teams or exit based on the supervisor\\'s decision\\n        # if the supervisor returns \"__end__\", the graph will finish execution\\n        return Command(goto=response[\"next_team\"])\\n    \\n    builder = StateGraph(MessagesState)\\n    builder.add_node(top_level_supervisor)\\n    builder.add_node(\"team_1_graph\", team_1_graph)\\n    builder.add_node(\"team_2_graph\", team_2_graph)\\n    builder.add_edge(START, \"top_level_supervisor\")\\n    builder.add_edge(\"team_1_graph\", \"top_level_supervisor\")\\n    builder.add_edge(\"team_2_graph\", \"top_level_supervisor\")\\n    graph = builder.compile()\\n    \\n\\nAPI Reference: [ChatOpenAI](https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html) | [StateGraph](https://langchain-ai.github.io/langgraph/reference/graphs/#langgraph.graph.state.StateGraph) | [START](https://langchain-ai.github.io/langgraph/reference/constants/#langgraph.constants.START) | [END](https://langchain-ai.github.io/langgraph/reference/constants/#langgraph.constants.END) | [Command](https://langchain-ai.github.io/langgraph/reference/types/#langgraph.types.Command)\\n\\n### Custom multi-agent workflow¶\\n\\nIn this architecture we add individual agents as graph nodes and define the\\norder in which agents are called ahead of time, in a custom workflow. In\\nLangGraph the workflow can be defined in two ways:\\n\\n  * **Explicit control flow (normal edges)** : LangGraph allows you to explicitly define the control flow of your application (i.e. the sequence of how agents communicate) explicitly, via [normal graph edges](../low_level/#normal-edges). This is the most deterministic variant of this architecture above — we always know which agent will be called next ahead of time.\\n\\n  * **Dynamic control flow (Command)** : in LangGraph you can allow LLMs to decide parts of your application control flow. This can be achieved by using [`Command`](../low_level/#command). A special case of this is a supervisor tool-calling architecture. In that case, the tool-calling LLM powering the supervisor agent will make decisions about the order in which the tools (agents) are being called.\\n\\n    \\n    \\n    from langchain_openai import ChatOpenAI\\n    from langgraph.graph import StateGraph, MessagesState, START\\n    \\n    model = ChatOpenAI()\\n    \\n    def agent_1(state: MessagesState):\\n        response = model.invoke(...)\\n        return {\"messages\": [response]}\\n    \\n    def agent_2(state: MessagesState):\\n        response = model.invoke(...)\\n        return {\"messages\": [response]}\\n    \\n    builder = StateGraph(MessagesState)\\n    builder.add_node(agent_1)\\n    builder.add_node(agent_2)\\n    # define the flow explicitly\\n    builder.add_edge(START, \"agent_1\")\\n    builder.add_edge(\"agent_1\", \"agent_2\")\\n    \\n\\nAPI Reference: [ChatOpenAI](https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html) | [StateGraph](https://langchain-ai.github.io/langgraph/reference/graphs/#langgraph.graph.state.StateGraph) | [START](https://langchain-ai.github.io/langgraph/reference/constants/#langgraph.constants.START)\\n\\n## Communication between agents¶\\n\\nThe most important thing when building multi-agent systems is figuring out how\\nthe agents communicate. There are a few different considerations:\\n\\n  * Do agents communicate **via graph state or via tool calls**?\\n  * What if two agents have **different state schemas**?\\n  * How to communicate over a **shared message list**?\\n\\n### Graph state vs tool calls¶\\n\\nWhat is the \"payload\" that is being passed around between agents? In most of\\nthe architectures discussed above the agents communicate via the [graph\\nstate](../low_level/#state). In the case of the supervisor with tool-calling,\\nthe payloads are tool call arguments.\\n\\n![](../img/multi_agent/request.png)\\n\\n#### Graph state¶\\n\\nTo communicate via graph state, individual agents need to be defined as [graph\\nnodes](../low_level/#nodes). These can be added as functions or as entire\\n[subgraphs](../low_level/#subgraphs). At each step of the graph execution,\\nagent node receives the current state of the graph, executes the agent code\\nand then passes the updated state to the next nodes.\\n\\nTypically agent nodes share a single [state schema](../low_level/#schema).\\nHowever, you might want to design agent nodes with different state schemas.\\n\\n### Different state schemas¶\\n\\nAn agent might need to have a different state schema from the rest of the\\nagents. For example, a search agent might only need to keep track of queries\\nand retrieved documents. There are two ways to achieve this in LangGraph:\\n\\n  * Define [subgraph](../low_level/#subgraphs) agents with a separate state schema. If there are no shared state keys (channels) between the subgraph and the parent graph, it’s important to [add input / output transformations](https://langchain-ai.github.io/langgraph/how-tos/subgraph-transform-state/) so that the parent graph knows how to communicate with the subgraphs.\\n  * Define agent node functions with a [private input state schema](https://langchain-ai.github.io/langgraph/how-tos/pass_private_state/) that is distinct from the overall graph state schema. This allows passing information that is only needed for executing that particular agent.\\n\\n### Shared message list¶\\n\\nThe most common way for the agents to communicate is via a shared state\\nchannel, typically a list of messages. This assumes that there is always at\\nleast a single channel (key) in the state that is shared by the agents. When\\ncommunicating via a shared message list there is an additional consideration:\\nshould the agents share the full history of their thought process or only the\\nfinal result?\\n\\n![](../img/multi_agent/response.png)\\n\\n#### Share full history¶\\n\\nAgents can **share the full history** of their thought process (i.e.\\n\"scratchpad\") with all other agents. This \"scratchpad\" would typically look\\nlike a [list of messages](../low_level/#why-use-messages). The benefit of\\nsharing full thought process is that it might help other agents make better\\ndecisions and improve reasoning ability for the system as a whole. The\\ndownside is that as the number of agents and their complexity grows, the\\n\"scratchpad\" will grow quickly and might require additional strategies for\\n[memory management](../memory/#managing-long-conversation-history).\\n\\n#### Share final result¶\\n\\nAgents can have their own private \"scratchpad\" and only **share the final\\nresult** with the rest of the agents. This approach might work better for\\nsystems with many agents or agents that are more complex. In this case, you\\nwould need to define agents with different state schemas\\n\\nFor agents called as tools, the supervisor determines the inputs based on the\\ntool schema. Additionally, LangGraph allows [passing state](https://langchain-\\nai.github.io/langgraph/how-tos/pass-run-time-values-to-tools/#pass-graph-\\nstate-to-tools) to individual tools at runtime, so subordinate agents can\\naccess parent state, if needed.\\n\\nWas this page helpful?\\n\\nThanks for your feedback!\\n\\nThanks for your feedback! Please help us improve this page by adding to the\\ndiscussion below.\\n\\n## Comments\\n\\n'),\n",
       "  0.6785445213317871),\n",
       " (Document(metadata={'source': 'https://langchain-ai.github.io/langgraph/how-tos/multi-agent-network'}, page_content='[ ](https://github.com/langchain-ai/langgraph/edit/main/docs/docs/how-\\ntos/multi-agent-network.ipynb \"Edit this page\")\\n\\n# How to build a multi-agent network¶\\n\\nPrerequisites\\n\\nThis guide assumes familiarity with the following:\\n\\n  * [How to implement handoffs between agents](../agent-handoffs)\\n  * [Multi-agent systems](../../concepts/multi_agent)\\n  * [Command](../../concepts/low_level/#command)\\n  * [LangGraph Glossary](../../concepts/low_level/)\\n\\nIn this how-to guide we will demonstrate how to implement a [multi-agent\\nnetwork](../../concepts/multi_agent#network) architecture where each agent can\\ncommunicate with every other agent (many-to-many connections) and can decide\\nwhich agent to call next. Individual agents will be defined as graph nodes.\\n\\nTo implement communication between the agents, we will be using\\n[handoffs](../agent-handoffs):\\n\\n    \\n    \\n    def agent(state) -> Command[Literal[\"agent\", \"another_agent\"]]:\\n        # the condition for routing/halting can be anything, e.g. LLM tool call / structured output, etc.\\n        goto = get_next_agent(...)  # \\'agent\\' / \\'another_agent\\'\\n        return Command(\\n            # Specify which agent to call next\\n            goto=goto,\\n            # Update the graph state\\n            update={\"my_state_key\": \"my_state_value\"}\\n        )\\n    \\n\\n## Setup¶\\n\\nFirst, let\\'s install the required packages\\n\\n    \\n    \\n    %%capture --no-stderr\\n    %pip install -U langgraph langchain-anthropic\\n    \\n    \\n    \\n    import getpass\\n    import os\\n    \\n    \\n    def _set_env(var: str):\\n        if not os.environ.get(var):\\n            os.environ[var] = getpass.getpass(f\"{var}: \")\\n    \\n    \\n    _set_env(\"ANTHROPIC_API_KEY\")\\n    \\n    \\n    \\n    ANTHROPIC_API_KEY:  ········\\n    \\n\\nSet up [LangSmith](https://smith.langchain.com) for LangGraph development\\n\\nSign up for LangSmith to quickly spot issues and improve the performance of\\nyour LangGraph projects. LangSmith lets you use trace data to debug, test, and\\nmonitor your LLM apps built with LangGraph — read more about how to get\\nstarted [here](https://docs.smith.langchain.com).\\n\\n## Using a custom agent implementation¶\\n\\nIn this example we will build a team of travel assistant agents that can\\ncommunicate with each other via handoffs.\\n\\nWe will create 2 agents:\\n\\n  * `travel_advisor`: can help with travel destination recommendations. Can ask `hotel_advisor` for help.\\n  * `hotel_advisor`: can help with hotel recommendations. Can ask `travel_advisor` for help.\\n\\nThis is a fully-connected network - every agent can talk to any other agent.\\n\\nEach agent will have a corresponding node function that can conditionally\\nreturn a `Command` object (the handoff). The node function will use an LLM\\nwith a system prompt and a tool that lets it signal when it needs to hand off\\nto another agent. If the LLM responds with the tool calls, we will return a\\n`Command(goto=<other_agent>)`.\\n\\n> **Note** : while we\\'re using tools for the LLM to signal that it needs a\\n> handoff, the condition for the handoff can be anything: a specific response\\n> text from the LLM, structured output from the LLM, any other custom logic,\\n> etc.\\n\\nNow, let\\'s define our agent nodes and graph!'),\n",
       "  0.6780953407287598),\n",
       " (Document(metadata={'source': 'https://langchain-ai.github.io/langgraph/how-tos/multi-agent-multi-turn-convo'}, page_content='[ ](https://github.com/langchain-ai/langgraph/edit/main/docs/docs/how-\\ntos/multi-agent-multi-turn-convo.ipynb \"Edit this page\")\\n\\n# How to add multi-turn conversation in a multi-agent application¶\\n\\nPrerequisites\\n\\nThis guide assumes familiarity with the following:\\n\\n  * [How to implement handoffs between agents](../agent-handoffs)\\n  * [Multi-agent systems](../../concepts/multi_agent)\\n  * [Human-in-the-loop](../../concepts/human_in_the_loop)\\n  * [Command](../../concepts/low_level/#command)\\n  * [LangGraph Glossary](../../concepts/low_level/)\\n\\nIn this how-to guide, we’ll build an application that allows an end-user to\\nengage in a _multi-turn conversation_ with one or more agents. We\\'ll create a\\nnode that uses an\\n[`interrupt`](../../reference/types/#langgraph.types.interrupt) to collect\\nuser input and routes back to the **active** agent.\\n\\nThe agents will be implemented as nodes in a graph that executes agent steps\\nand determines the next action:\\n\\n  1. **Wait for user input** to continue the conversation, or \\n  2. **Route to another agent** (or back to itself, such as in a loop) via a [**handoff**](../../concepts/multi_agent/#handoffs).\\n\\n    \\n    \\n    def human(state: MessagesState) -> Command[Literal[\"agent\", \"another_agent\"]]:\\n        \"\"\"A node for collecting user input.\"\"\"\\n        user_input = interrupt(value=\"Ready for user input.\")\\n    \\n        # Determine the active agent.\\n        active_agent = ...\\n    \\n        ...\\n        return Command(\\n            update={\\n                \"messages\": [{\\n                    \"role\": \"human\",\\n                    \"content\": user_input,\\n                }]\\n            },\\n            goto=active_agent\\n        )\\n    \\n    def agent(state) -> Command[Literal[\"agent\", \"another_agent\", \"human\"]]:\\n        # The condition for routing/halting can be anything, e.g. LLM tool call / structured output, etc.\\n        goto = get_next_agent(...)  # \\'agent\\' / \\'another_agent\\'\\n        if goto:\\n            return Command(goto=goto, update={\"my_state_key\": \"my_state_value\"})\\n        else:\\n            return Command(goto=\"human\") # Go to human node\\n    \\n\\n## Setup¶\\n\\nFirst, let\\'s install the required packages\\n\\n    \\n    \\n    %%capture --no-stderr\\n    %pip install -U langgraph langchain-anthropic\\n    \\n    \\n    \\n    import getpass\\n    import os\\n    \\n    \\n    def _set_env(var: str):\\n        if not os.environ.get(var):\\n            os.environ[var] = getpass.getpass(f\"{var}: \")\\n    \\n    \\n    _set_env(\"ANTHROPIC_API_KEY\")\\n    \\n    \\n    \\n    ANTHROPIC_API_KEY:  ········\\n    \\n\\nSet up [LangSmith](https://smith.langchain.com) for LangGraph development\\n\\nSign up for LangSmith to quickly spot issues and improve the performance of\\nyour LangGraph projects. LangSmith lets you use trace data to debug, test, and\\nmonitor your LLM apps built with LangGraph — read more about how to get\\nstarted [here](https://docs.smith.langchain.com).\\n\\n## Define agents¶\\n\\nIn this example, we will build a team of travel assistant agents that can\\ncommunicate with each other via handoffs.\\n\\nWe will create 2 agents:\\n\\n  * `travel_advisor`: can help with travel destination recommendations. Can ask `hotel_advisor` for help.\\n  * `hotel_advisor`: can help with hotel recommendations. Can ask `travel_advisor` for help.\\n\\nWe will be using prebuilt\\n[`create_react_agent`](../../reference/prebuilt/#langgraph.prebuilt.chat_agent_executor.create_react_agent)\\nfor the agents - each agent will have tools specific to its area of expertise\\nas well as a special [tool for handoffs](../agent-handoffs#implementing-\\nhandoffs-using-tools) to another agent.\\n\\nFirst, let\\'s define the tools we\\'ll be using:\\n\\n    \\n    \\n    import random\\n    from typing import Annotated, Literal\\n    \\n    from langchain_core.tools import tool\\n    from langchain_core.tools.base import InjectedToolCallId\\n    from langgraph.prebuilt import InjectedState\\n    \\n    \\n    @tool\\n    def get_travel_recommendations():\\n        \"\"\"Get recommendation for travel destinations\"\"\"\\n        return random.choice([\"aruba\", \"turks and caicos\"])\\n    \\n    \\n    @tool\\n    def get_hotel_recommendations(location: Literal[\"aruba\", \"turks and caicos\"]):\\n        \"\"\"Get hotel recommendations for a given destination.\"\"\"\\n        return {\\n            \"aruba\": [\\n                \"The Ritz-Carlton, Aruba (Palm Beach)\"\\n                \"Bucuti & Tara Beach Resort (Eagle Beach)\"\\n            ],\\n            \"turks and caicos\": [\"Grace Bay Club\", \"COMO Parrot Cay\"],\\n        }[location]\\n    \\n    \\n    def make_handoff_tool(*, agent_name: str):\\n        \"\"\"Create a tool that can return handoff via a Command\"\"\"\\n        tool_name = f\"transfer_to_{agent_name}\"\\n    \\n        @tool(tool_name)\\n        def handoff_to_agent(\\n            state: Annotated[dict, InjectedState],\\n            tool_call_id: Annotated[str, InjectedToolCallId],\\n        ):\\n            \"\"\"Ask another agent for help.\"\"\"\\n            tool_message = {\\n                \"role\": \"tool\",\\n                \"content\": f\"Successfully transferred to {agent_name}\",\\n                \"name\": tool_name,\\n                \"tool_call_id\": tool_call_id,\\n            }\\n            return Command(\\n                # navigate to another agent node in the PARENT graph\\n                goto=agent_name,\\n                graph=Command.PARENT,\\n                # This is the state update that the agent `agent_name` will see when it is invoked.\\n                # We\\'re passing agent\\'s FULL internal message history AND adding a tool message to make sure\\n                # the resulting chat history is valid.\\n                update={\"messages\": state[\"messages\"] + [tool_message]},\\n            )\\n    \\n        return handoff_to_agent\\n    \\n\\nAPI Reference: [tool](https://python.langchain.com/api_reference/core/tools/langchain_core.tools.convert.tool.html) | [InjectedToolCallId](https://python.langchain.com/api_reference/core/tools/langchain_core.tools.base.InjectedToolCallId.html) | [InjectedState](https://langchain-ai.github.io/langgraph/reference/prebuilt/#langgraph.prebuilt.tool_node.InjectedState)\\n\\nLet\\'s now create our agents using the the prebuilt\\n[`create_react_agent`](../../reference/prebuilt/#langgraph.prebuilt.chat_agent_executor.create_react_agent).\\nWe\\'ll also define a dedicated `human` node with an\\n[`interrupt`](../../reference/types/#langgraph.types.interrupt) \\\\-- we will\\nroute to this node after the final response from the agents. Note that to do\\nso we\\'re wrapping each agent invocation in a separate node function that\\nreturns `Command(goto=\"human\", ...)`.'),\n",
       "  0.6780586242675781),\n",
       " (Document(metadata={'source': 'https://langchain-ai.github.io/langgraph/how-tos/multi-agent-network-functional'}, page_content='[ ](https://github.com/langchain-ai/langgraph/edit/main/docs/docs/how-\\ntos/multi-agent-network-functional.ipynb \"Edit this page\")\\n\\n# How to build a multi-agent network (functional API)¶\\n\\nPrerequisites\\n\\nThis guide assumes familiarity with the following:\\n\\n  * [Multi-agent systems](../../concepts/multi_agent)\\n  * [Functional API](../../concepts/functional_api)\\n  * [Command](../../concepts/low_level/#command)\\n  * [LangGraph Glossary](../../concepts/low_level/)\\n\\nIn this how-to guide we will demonstrate how to implement a [multi-agent\\nnetwork](../../concepts/multi_agent#network) architecture where each agent can\\ncommunicate with every other agent (many-to-many connections) and can decide\\nwhich agent to call next. We will be using [functional\\nAPI](../../concepts/functional_api) — individual agents will be defined as\\ntasks and the agent handoffs will be defined in the main\\n[entrypoint()](../../reference/func/#langgraph.func.entrypoint):\\n\\n    \\n    \\n    from langgraph.func import entrypoint\\n    from langgraph.prebuilt import create_react_agent\\n    from langchain_core.tools import tool\\n    \\n    \\n    # Define a tool to signal intent to hand off to a different agent\\n    @tool(return_direct=True)\\n    def transfer_to_hotel_advisor():\\n        \"\"\"Ask hotel advisor agent for help.\"\"\"\\n        return \"Successfully transferred to hotel advisor\"\\n    \\n    \\n    # define an agent\\n    travel_advisor_tools = [transfer_to_hotel_advisor, ...]\\n    travel_advisor = create_react_agent(model, travel_advisor_tools)\\n    \\n    \\n    # define a task that calls an agent\\n    @task\\n    def call_travel_advisor(messages):\\n        response = travel_advisor.invoke({\"messages\": messages})\\n        return response[\"messages\"]\\n    \\n    \\n    # define the multi-agent network workflow\\n    @entrypoint()\\n    def workflow(messages):\\n        call_active_agent = call_travel_advisor\\n        while True:\\n            agent_messages = call_active_agent(messages).result()\\n            messages = messages + agent_messages\\n            call_active_agent = get_next_agent(messages)\\n        return messages\\n    \\n\\nAPI Reference: [entrypoint](https://langchain-ai.github.io/langgraph/reference/func/#langgraph.func.entrypoint) | [create_react_agent](https://langchain-ai.github.io/langgraph/reference/prebuilt/#langgraph.prebuilt.chat_agent_executor.create_react_agent) | [tool](https://python.langchain.com/api_reference/core/tools/langchain_core.tools.convert.tool.html)\\n\\n## Setup¶\\n\\nFirst, let\\'s install the required packages\\n\\n    \\n    \\n    %%capture --no-stderr\\n    %pip install -U langgraph langchain-anthropic\\n    \\n    \\n    \\n    import getpass\\n    import os\\n    \\n    \\n    def _set_env(var: str):\\n        if not os.environ.get(var):\\n            os.environ[var] = getpass.getpass(f\"{var}: \")\\n    \\n    \\n    _set_env(\"ANTHROPIC_API_KEY\")\\n    \\n    \\n    \\n    ANTHROPIC_API_KEY:  ········\\n    \\n\\nSet up [LangSmith](https://smith.langchain.com) for LangGraph development\\n\\nSign up for LangSmith to quickly spot issues and improve the performance of\\nyour LangGraph projects. LangSmith lets you use trace data to debug, test, and\\nmonitor your LLM apps built with LangGraph — read more about how to get\\nstarted [here](https://docs.smith.langchain.com).\\n\\n## Travel agent example¶\\n\\nIn this example we will build a team of travel assistant agents that can\\ncommunicate with each other.\\n\\nWe will create 2 agents:\\n\\n  * `travel_advisor`: can help with travel destination recommendations. Can ask `hotel_advisor` for help.\\n  * `hotel_advisor`: can help with hotel recommendations. Can ask `travel_advisor` for help.\\n\\nThis is a fully-connected network - every agent can talk to any other agent.\\n\\nFirst, let\\'s create some of the tools that the agents will be using:\\n\\n    \\n    \\n    import random\\n    from typing_extensions import Literal\\n    from langchain_core.tools import tool\\n    \\n    \\n    @tool\\n    def get_travel_recommendations():\\n        \"\"\"Get recommendation for travel destinations\"\"\"\\n        return random.choice([\"aruba\", \"turks and caicos\"])\\n    \\n    \\n    @tool\\n    def get_hotel_recommendations(location: Literal[\"aruba\", \"turks and caicos\"]):\\n        \"\"\"Get hotel recommendations for a given destination.\"\"\"\\n        return {\\n            \"aruba\": [\\n                \"The Ritz-Carlton, Aruba (Palm Beach)\"\\n                \"Bucuti & Tara Beach Resort (Eagle Beach)\"\\n            ],\\n            \"turks and caicos\": [\"Grace Bay Club\", \"COMO Parrot Cay\"],\\n        }[location]\\n    \\n    \\n    @tool(return_direct=True)\\n    def transfer_to_hotel_advisor():\\n        \"\"\"Ask hotel advisor agent for help.\"\"\"\\n        return \"Successfully transferred to hotel advisor\"\\n    \\n    \\n    @tool(return_direct=True)\\n    def transfer_to_travel_advisor():\\n        \"\"\"Ask travel advisor agent for help.\"\"\"\\n        return \"Successfully transferred to travel advisor\"\\n    \\n\\nAPI Reference:\\n[tool](https://python.langchain.com/api_reference/core/tools/langchain_core.tools.convert.tool.html)\\n\\nTransfer tools\\n\\nYou might have noticed that we\\'re using `@tool(return_direct=True)` in the\\ntransfer tools. This is done so that individual agents (e.g.,\\n`travel_advisor`) can exit the ReAct loop early once these tools are called.\\nThis is the desired behavior, as we want to detect when the agent calls this\\ntool and hand control off _immediately_ to a different agent.\\n\\n**NOTE** : This is meant to work with the prebuilt\\n[`create_react_agent`](../../reference/prebuilt/#langgraph.prebuilt.chat_agent_executor.create_react_agent)\\n\\\\-- if you are building a custom agent, make sure to manually add logic for\\nhandling early exit for tools that are marked with `return_direct`.\\n\\nNow let\\'s define our agent tasks and combine them into a single multi-agent\\nnetwork workflow:'),\n",
       "  0.6780490875244141),\n",
       " (Document(metadata={'source': 'https://langchain-ai.github.io/langgraph/how-tos/human_in_the_loop/time-travel'}, page_content='[ ](https://github.com/langchain-ai/langgraph/edit/main/docs/docs/how-\\ntos/human_in_the_loop/time-travel.ipynb \"Edit this page\")\\n\\n# How to view and update past graph state¶\\n\\nPrerequisites\\n\\nThis guide assumes familiarity with the following concepts:\\n\\n  * [Time Travel](../../../concepts/time-travel)\\n  * [Breakpoints](../../../concepts/breakpoints)\\n  * [LangGraph Glossary](../../../concepts/low_level)\\n\\nOnce you start [checkpointing](../../persistence) your graphs, you can easily\\n**get** or **update** the state of the agent at any point in time. This\\npermits a few things:\\n\\n  1. You can surface a state during an interrupt to a user to let them accept an action.\\n  2. You can **rewind** the graph to reproduce or avoid issues.\\n  3. You can **modify** the state to embed your agent into a larger system, or to let the user better control its actions.\\n\\nThe key methods used for this functionality are:\\n\\n  * [get_state](https://langchain-ai.github.io/langgraph/reference/graphs/#langgraph.graph.graph.CompiledGraph.get_state): fetch the values from the target config\\n  * [update_state](https://langchain-ai.github.io/langgraph/reference/graphs/#langgraph.graph.graph.CompiledGraph.update_state): apply the given values to the target state\\n\\n**Note:** this requires passing in a checkpointer.\\n\\nBelow is a quick example.\\n\\n## Setup¶\\n\\nFirst we need to install the packages required\\n\\n    \\n    \\n    %%capture --no-stderr\\n    %pip install --quiet -U langgraph langchain_openai\\n    \\n\\nNext, we need to set API keys for OpenAI (the LLM we will use)\\n\\n    \\n    \\n    import getpass\\n    import os\\n    \\n    \\n    def _set_env(var: str):\\n        if not os.environ.get(var):\\n            os.environ[var] = getpass.getpass(f\"{var}: \")\\n    \\n    \\n    _set_env(\"OPENAI_API_KEY\")\\n    \\n    \\n    \\n    ANTHROPIC_API_KEY:  ········\\n    \\n\\nSet up [LangSmith](https://smith.langchain.com) for LangGraph development\\n\\nSign up for LangSmith to quickly spot issues and improve the performance of\\nyour LangGraph projects. LangSmith lets you use trace data to debug, test, and\\nmonitor your LLM apps built with LangGraph — read more about how to get\\nstarted [here](https://docs.smith.langchain.com).\\n\\n## Build the agent¶\\n\\nWe can now build the agent. We will build a relatively simple ReAct-style\\nagent that does tool calling. We will use Anthropic\\'s models and fake tools\\n(just for demo purposes).\\n\\n    \\n    \\n    # Set up the tool\\n    from langchain_openai import ChatOpenAI\\n    from langchain_core.tools import tool\\n    from langgraph.graph import MessagesState, START\\n    from langgraph.prebuilt import ToolNode\\n    from langgraph.graph import END, StateGraph\\n    from langgraph.checkpoint.memory import MemorySaver\\n    \\n    \\n    @tool\\n    def play_song_on_spotify(song: str):\\n        \"\"\"Play a song on Spotify\"\"\"\\n        # Call the spotify API ...\\n        return f\"Successfully played {song} on Spotify!\"\\n    \\n    \\n    @tool\\n    def play_song_on_apple(song: str):\\n        \"\"\"Play a song on Apple Music\"\"\"\\n        # Call the apple music API ...\\n        return f\"Successfully played {song} on Apple Music!\"\\n    \\n    \\n    tools = [play_song_on_apple, play_song_on_spotify]\\n    tool_node = ToolNode(tools)\\n    \\n    # Set up the model\\n    \\n    model = ChatOpenAI(model=\"gpt-4o-mini\")\\n    model = model.bind_tools(tools, parallel_tool_calls=False)\\n    \\n    \\n    # Define nodes and conditional edges\\n    \\n    \\n    # Define the function that determines whether to continue or not\\n    def should_continue(state):\\n        messages = state[\"messages\"]\\n        last_message = messages[-1]\\n        # If there is no function call, then we finish\\n        if not last_message.tool_calls:\\n            return \"end\"\\n        # Otherwise if there is, we continue\\n        else:\\n            return \"continue\"\\n    \\n    \\n    # Define the function that calls the model\\n    def call_model(state):\\n        messages = state[\"messages\"]\\n        response = model.invoke(messages)\\n        # We return a list, because this will get added to the existing list\\n        return {\"messages\": [response]}\\n    \\n    \\n    # Define a new graph\\n    workflow = StateGraph(MessagesState)\\n    \\n    # Define the two nodes we will cycle between\\n    workflow.add_node(\"agent\", call_model)\\n    workflow.add_node(\"action\", tool_node)\\n    \\n    # Set the entrypoint as `agent`\\n    # This means that this node is the first one called\\n    workflow.add_edge(START, \"agent\")\\n    \\n    # We now add a conditional edge\\n    workflow.add_conditional_edges(\\n        # First, we define the start node. We use `agent`.\\n        # This means these are the edges taken after the `agent` node is called.\\n        \"agent\",\\n        # Next, we pass in the function that will determine which node is called next.\\n        should_continue,\\n        # Finally we pass in a mapping.\\n        # The keys are strings, and the values are other nodes.\\n        # END is a special node marking that the graph should finish.\\n        # What will happen is we will call `should_continue`, and then the output of that\\n        # will be matched against the keys in this mapping.\\n        # Based on which one it matches, that node will then be called.\\n        {\\n            # If `tools`, then we call the tool node.\\n            \"continue\": \"action\",\\n            # Otherwise we finish.\\n            \"end\": END,\\n        },\\n    )\\n    \\n    # We now add a normal edge from `tools` to `agent`.\\n    # This means that after `tools` is called, `agent` node is called next.\\n    workflow.add_edge(\"action\", \"agent\")\\n    \\n    # Set up memory\\n    memory = MemorySaver()\\n    \\n    # Finally, we compile it!\\n    # This compiles it into a LangChain Runnable,\\n    # meaning you can use it as you would any other runnable\\n    \\n    # We add in `interrupt_before=[\"action\"]`\\n    # This will add a breakpoint before the `action` node is called\\n    app = workflow.compile(checkpointer=memory)'),\n",
       "  0.676915168762207),\n",
       " (Document(metadata={'source': 'https://langchain-ai.github.io/langgraph/how-tos/react-agent-from-scratch'}, page_content='[ ](https://github.com/langchain-ai/langgraph/edit/main/docs/docs/how-\\ntos/react-agent-from-scratch.ipynb \"Edit this page\")\\n\\n# How to create a ReAct agent from scratch¶\\n\\nPrerequisites\\n\\nThis guide assumes familiarity with the following:\\n\\n  * [Tool calling agent](../../concepts/agentic_concepts/#tool-calling-agent)\\n  * [Chat Models](https://python.langchain.com/docs/concepts/chat_models/)\\n  * [Messages](https://python.langchain.com/docs/concepts/messages/)\\n  * [LangGraph Glossary](../../concepts/low_level/)\\n\\nUsing the prebuilt ReAct agent\\n[create_react_agent](../../reference/prebuilt/#langgraph.prebuilt.chat_agent_executor.create_react_agent)\\nis a great way to get started, but sometimes you might want more control and\\ncustomization. In those cases, you can create a custom ReAct agent. This guide\\nshows how to implement ReAct agent from scratch using LangGraph.\\n\\n## Setup¶\\n\\nFirst, let\\'s install the required packages and set our API keys:\\n\\n    \\n    \\n    %%capture --no-stderr\\n    %pip install -U langgraph langchain-openai\\n    \\n    \\n    \\n    import getpass\\n    import os\\n    \\n    \\n    def _set_env(var: str):\\n        if not os.environ.get(var):\\n            os.environ[var] = getpass.getpass(f\"{var}: \")\\n    \\n    \\n    _set_env(\"OPENAI_API_KEY\")\\n    \\n\\nSet up [LangSmith](https://smith.langchain.com) for better debugging\\n\\nSign up for LangSmith to quickly spot issues and improve the performance of\\nyour LangGraph projects. LangSmith lets you use trace data to debug, test, and\\nmonitor your LLM aps built with LangGraph — read more about how to get started\\nin the [docs](https://docs.smith.langchain.com).\\n\\n## Create ReAct agent¶\\n\\nNow that you have installed the required packages and set your environment\\nvariables, we can code our ReAct agent!\\n\\n### Define graph state¶\\n\\nWe are going to define the most basic ReAct state in this example, which will\\njust contain a list of messages.\\n\\nFor your specific use case, feel free to add any other state keys that you\\nneed.\\n\\n    \\n    \\n    from typing import (\\n        Annotated,\\n        Sequence,\\n        TypedDict,\\n    )\\n    from langchain_core.messages import BaseMessage\\n    from langgraph.graph.message import add_messages\\n    \\n    \\n    class AgentState(TypedDict):\\n        \"\"\"The state of the agent.\"\"\"\\n    \\n        # add_messages is a reducer\\n        # See https://langchain-ai.github.io/langgraph/concepts/low_level/#reducers\\n        messages: Annotated[Sequence[BaseMessage], add_messages]\\n    \\n\\nAPI Reference: [BaseMessage](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.base.BaseMessage.html) | [add_messages](https://langchain-ai.github.io/langgraph/reference/graphs/#langgraph.graph.message.add_messages)\\n\\n### Define model and tools¶\\n\\nNext, let\\'s define the tools and model we will use for our example.\\n\\n    \\n    \\n    from langchain_openai import ChatOpenAI\\n    from langchain_core.tools import tool\\n    \\n    model = ChatOpenAI(model=\"gpt-4o-mini\")\\n    \\n    \\n    @tool\\n    def get_weather(location: str):\\n        \"\"\"Call to get the weather from a specific location.\"\"\"\\n        # This is a placeholder for the actual implementation\\n        # Don\\'t let the LLM know this though 😊\\n        if any([city in location.lower() for city in [\"sf\", \"san francisco\"]]):\\n            return \"It\\'s sunny in San Francisco, but you better look out if you\\'re a Gemini 😈.\"\\n        else:\\n            return f\"I am not sure what the weather is in {location}\"\\n    \\n    \\n    tools = [get_weather]\\n    \\n    model = model.bind_tools(tools)\\n    \\n\\nAPI Reference: [ChatOpenAI](https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html) | [tool](https://python.langchain.com/api_reference/core/tools/langchain_core.tools.convert.tool.html)\\n\\n### Define nodes and edges¶\\n\\nNext let\\'s define our nodes and edges. In our basic ReAct agent there are only\\ntwo nodes, one for calling the model and one for using tools, however you can\\nmodify this basic structure to work better for your use case. The tool node we\\ndefine here is a simplified version of the prebuilt\\n[`ToolNode`](https://langchain-ai.github.io/langgraph/how-tos/tool-calling/),\\nwhich has some additional features.\\n\\nPerhaps you want to add a node for [adding structured\\noutput](https://langchain-ai.github.io/langgraph/how-tos/react-agent-\\nstructured-output/) or a node for executing some external action (sending an\\nemail, adding a calendar event, etc.). Maybe you just want to change the way\\nthe `call_model` node works and how `should_continue` decides whether to call\\ntools - the possibilities are endless and LangGraph makes it easy to customize\\nthis basic structure for your specific use case.\\n\\n    \\n    \\n    import json\\n    from langchain_core.messages import ToolMessage, SystemMessage\\n    from langchain_core.runnables import RunnableConfig\\n    \\n    tools_by_name = {tool.name: tool for tool in tools}\\n    \\n    \\n    # Define our tool node\\n    def tool_node(state: AgentState):\\n        outputs = []\\n        for tool_call in state[\"messages\"][-1].tool_calls:\\n            tool_result = tools_by_name[tool_call[\"name\"]].invoke(tool_call[\"args\"])\\n            outputs.append(\\n                ToolMessage(\\n                    content=json.dumps(tool_result),\\n                    name=tool_call[\"name\"],\\n                    tool_call_id=tool_call[\"id\"],\\n                )\\n            )\\n        return {\"messages\": outputs}\\n    \\n    \\n    # Define the node that calls the model\\n    def call_model(\\n        state: AgentState,\\n        config: RunnableConfig,\\n    ):\\n        # this is similar to customizing the create_react_agent with \\'prompt\\' parameter, but is more flexible\\n        system_prompt = SystemMessage(\\n            \"You are a helpful AI assistant, please respond to the users query to the best of your ability!\"\\n        )\\n        response = model.invoke([system_prompt] + state[\"messages\"], config)\\n        # We return a list, because this will get added to the existing list\\n        return {\"messages\": [response]}\\n    \\n    \\n    # Define the conditional edge that determines whether to continue or not\\n    def should_continue(state: AgentState):\\n        messages = state[\"messages\"]\\n        last_message = messages[-1]\\n        # If there is no function call, then we finish\\n        if not last_message.tool_calls:\\n            return \"end\"\\n        # Otherwise if there is, we continue\\n        else:\\n            return \"continue\"'),\n",
       "  0.6748700141906738),\n",
       " (Document(metadata={'source': 'https://langchain-ai.github.io/langgraph/concepts/agentic_concepts'}, page_content='[ ](https://github.com/langchain-\\nai/langgraph/edit/main/docs/docs/concepts/agentic_concepts.md \"Edit this\\npage\")\\n\\n# Agent architectures¶\\n\\nMany LLM applications implement a particular control flow of steps before and\\n/ or after LLM calls. As an example, [RAG](https://github.com/langchain-\\nai/rag-from-scratch) performs retrieval of documents relevant to a user\\nquestion, and passes those documents to an LLM in order to ground the model\\'s\\nresponse in the provided document context.\\n\\nInstead of hard-coding a fixed control flow, we sometimes want LLM systems\\nthat can pick their own control flow to solve more complex problems! This is\\none definition of an [agent](https://blog.langchain.dev/what-is-an-agent/):\\n_an agent is a system that uses an LLM to decide the control flow of an\\napplication._ There are many ways that an LLM can control application:\\n\\n  * An LLM can route between two potential paths\\n  * An LLM can decide which of many tools to call\\n  * An LLM can decide whether the generated answer is sufficient or more work is needed\\n\\nAs a result, there are many different types of [agent\\narchitectures](https://blog.langchain.dev/what-is-a-cognitive-architecture/),\\nwhich give an LLM varying levels of control.\\n\\n![Agent Types](../img/agent_types.png)\\n\\n## Router¶\\n\\nA router allows an LLM to select a single step from a specified set of\\noptions. This is an agent architecture that exhibits a relatively limited\\nlevel of control because the LLM usually focuses on making a single decision\\nand produces a specific output from a limited set of pre-defined options.\\nRouters typically employ a few different concepts to achieve this.\\n\\n### Structured Output¶\\n\\nStructured outputs with LLMs work by providing a specific format or schema\\nthat the LLM should follow in its response. This is similar to tool calling,\\nbut more general. While tool calling typically involves selecting and using\\npredefined functions, structured outputs can be used for any type of formatted\\nresponse. Common methods to achieve structured outputs include:\\n\\n  1. Prompt engineering: Instructing the LLM to respond in a specific format via the system prompt.\\n  2. Output parsers: Using post-processing to extract structured data from LLM responses.\\n  3. Tool calling: Leveraging built-in tool calling capabilities of some LLMs to generate structured outputs.\\n\\nStructured outputs are crucial for routing as they ensure the LLM\\'s decision\\ncan be reliably interpreted and acted upon by the system. Learn more about\\n[structured outputs in this how-to\\nguide](https://python.langchain.com/docs/how_to/structured_output/).\\n\\n## Tool calling agent¶\\n\\nWhile a router allows an LLM to make a single decision, more complex agent\\narchitectures expand the LLM\\'s control in two key ways:\\n\\n  1. Multi-step decision making: The LLM can make a series of decisions, one after another, instead of just one.\\n  2. Tool access: The LLM can choose from and use a variety of tools to accomplish tasks.\\n\\n[ReAct](https://arxiv.org/abs/2210.03629) is a popular general purpose agent\\narchitecture that combines these expansions, integrating three core concepts.\\n\\n  1. `Tool calling`: Allowing the LLM to select and use various tools as needed.\\n  2. `Memory`: Enabling the agent to retain and use information from previous steps.\\n  3. `Planning`: Empowering the LLM to create and follow multi-step plans to achieve goals.\\n\\nThis architecture allows for more complex and flexible agent behaviors, going\\nbeyond simple routing to enable dynamic problem-solving with multiple steps.\\nYou can use it with\\n[`create_react_agent`](../../reference/prebuilt/#langgraph.prebuilt.chat_agent_executor.create_react_agent).\\n\\n### Tool calling¶\\n\\nTools are useful whenever you want an agent to interact with external systems.\\nExternal systems (e.g., APIs) often require a particular input schema or\\npayload, rather than natural language. When we bind an API, for example, as a\\ntool, we give the model awareness of the required input schema. The model will\\nchoose to call a tool based upon the natural language input from the user and\\nit will return an output that adheres to the tool\\'s required schema.\\n\\n[Many LLM providers support tool\\ncalling](https://python.langchain.com/docs/integrations/chat/) and [tool\\ncalling interface](https://blog.langchain.dev/improving-core-tool-interfaces-\\nand-docs-in-langchain/) in LangChain is simple: you can simply pass any Python\\n`function` into `ChatModel.bind_tools(function)`.\\n\\n![Tools](../img/tool_call.png)\\n\\n### Memory¶\\n\\nMemory is crucial for agents, enabling them to retain and utilize information\\nacross multiple steps of problem-solving. It operates on different scales:\\n\\n  1. Short-term memory: Allows the agent to access information acquired during earlier steps in a sequence.\\n  2. Long-term memory: Enables the agent to recall information from previous interactions, such as past messages in a conversation.\\n\\nLangGraph provides full control over memory implementation:\\n\\n  * [`State`](../low_level/#state): User-defined schema specifying the exact structure of memory to retain.\\n  * [`Checkpointers`](../persistence/): Mechanism to store state at every step across different interactions.\\n\\nThis flexible approach allows you to tailor the memory system to your specific\\nagent architecture needs. For a practical guide on adding memory to your\\ngraph, see [this tutorial](../../how-tos/persistence/).\\n\\nEffective memory management enhances an agent\\'s ability to maintain context,\\nlearn from past experiences, and make more informed decisions over time.\\n\\n### Planning¶\\n\\nIn the ReAct architecture, an LLM is called repeatedly in a while-loop. At\\neach step the agent decides which tools to call, and what the inputs to those\\ntools should be. Those tools are then executed, and the outputs are fed back\\ninto the LLM as observations. The while-loop terminates when the agent decides\\nit has enough information to solve the user request and it is not worth\\ncalling any more tools.\\n\\n### ReAct implementation¶\\n\\nThere are several differences between [this](https://arxiv.org/abs/2210.03629)\\npaper and the pre-built\\n[`create_react_agent`](../../reference/prebuilt/#langgraph.prebuilt.chat_agent_executor.create_react_agent)\\nimplementation:\\n\\n  * First, we use tool-calling to have LLMs call tools, whereas the paper used prompting + parsing of raw output. This is because tool calling did not exist when the paper was written, but is generally better and more reliable.\\n  * Second, we use messages to prompt the LLM, whereas the paper used string formatting. This is because at the time of writing, LLMs didn\\'t even expose a message-based interface, whereas now that\\'s the only interface they expose.\\n  * Third, the paper required all inputs to the tools to be a single string. This was largely due to LLMs not being super capable at the time, and only really being able to generate a single input. Our implementation allows for using tools that require multiple inputs.\\n  * Fourth, the paper only looks at calling a single tool at the time, largely due to limitations in LLMs performance at the time. Our implementation allows for calling multiple tools at a time.\\n  * Finally, the paper asked the LLM to explicitly generate a \"Thought\" step before deciding which tools to call. This is the \"Reasoning\" part of \"ReAct\". Our implementation does not do this by default, largely because LLMs have gotten much better and that is not as necessary. Of course, if you wish to prompt it do so, you certainly can.\\n\\n## Custom agent architectures¶\\n\\nWhile routers and tool-calling agents (like ReAct) are common, [customizing\\nagent architectures](https://blog.langchain.dev/why-you-should-outsource-your-\\nagentic-infrastructure-but-own-your-cognitive-architecture/) often leads to\\nbetter performance for specific tasks. LangGraph offers several powerful\\nfeatures for building tailored agent systems:\\n\\n### Human-in-the-loop¶\\n\\nHuman involvement can significantly enhance agent reliability, especially for\\nsensitive tasks. This can involve:\\n\\n  * Approving specific actions\\n  * Providing feedback to update the agent\\'s state\\n  * Offering guidance in complex decision-making processes\\n\\nHuman-in-the-loop patterns are crucial when full automation isn\\'t feasible or\\ndesirable. Learn more in our [human-in-the-loop guide](../human_in_the_loop/).\\n\\n### Parallelization¶\\n\\nParallel processing is vital for efficient multi-agent systems and complex\\ntasks. LangGraph supports parallelization through its\\n[Send](../low_level/#send) API, enabling:\\n\\n  * Concurrent processing of multiple states\\n  * Implementation of map-reduce-like operations\\n  * Efficient handling of independent subtasks\\n\\nFor practical implementation, see our [map-reduce tutorial](../../how-tos/map-\\nreduce/).\\n\\n### Subgraphs¶\\n\\n[Subgraphs](../low_level/#subgraphs) are essential for managing complex agent\\narchitectures, particularly in [multi-agent systems](../multi_agent/). They\\nallow:\\n\\n  * Isolated state management for individual agents\\n  * Hierarchical organization of agent teams\\n  * Controlled communication between agents and the main system\\n\\nSubgraphs communicate with the parent graph through overlapping keys in the\\nstate schema. This enables flexible, modular agent design. For implementation\\ndetails, refer to our [subgraph how-to guide](../../how-tos/subgraph/).\\n\\n### Reflection¶\\n\\nReflection mechanisms can significantly improve agent reliability by:\\n\\n  1. Evaluating task completion and correctness\\n  2. Providing feedback for iterative improvement\\n  3. Enabling self-correction and learning\\n\\nWhile often LLM-based, reflection can also use deterministic methods. For\\ninstance, in coding tasks, compilation errors can serve as feedback. This\\napproach is demonstrated in [this video using LangGraph for self-corrective\\ncode generation](https://www.youtube.com/watch?v=MvNdgmM7uyc).\\n\\nBy leveraging these features, LangGraph enables the creation of sophisticated,\\ntask-specific agent architectures that can handle complex workflows,\\ncollaborate effectively, and continuously improve their performance.\\n\\nWas this page helpful?\\n\\nThanks for your feedback!\\n\\nThanks for your feedback! Please help us improve this page by adding to the\\ndiscussion below.\\n\\n## Comments\\n\\n'),\n",
       "  0.6729421615600586),\n",
       " (Document(metadata={'source': 'https://langchain-ai.github.io/langgraph/prebuilt'}, page_content='[ ](https://github.com/langchain-ai/langgraph/edit/main/docs/docs/prebuilt.md\\n\"Edit this page\")\\n\\n# 🚀 Prebuilt Agents¶\\n\\nLangGraph includes a prebuilt React agent. For more information on how to use\\nit, check out our [how-to guides](https://langchain-\\nai.github.io/langgraph/how-tos/#prebuilt-react-agent).\\n\\nIf you’re looking for other prebuilt libraries, explore the community-built\\noptions below. These libraries can extend LangGraph\\'s functionality in various\\nways.\\n\\n## 📚 Available Libraries¶\\n\\nName | GitHub URL | Description | Weekly Downloads | Stars  \\n---|---|---|---|---  \\n**langchain-mcp-adapters** | [langchain-ai/langchain-mcp-adapters](https://github.com/langchain-ai/langchain-mcp-adapters) | Make Anthropic Model Context Protocol (MCP) tools compatible with LangGraph agents. | 27447 | ![GitHub stars](https://img.shields.io/github/stars/langchain-ai/langchain-mcp-adapters?style=social)  \\n**trustcall** | [hinthornw/trustcall](https://github.com/hinthornw/trustcall) | Tenacious tool calling built on LangGraph. | 15989 | ![GitHub stars](https://img.shields.io/github/stars/hinthornw/trustcall?style=social)  \\n**langgraph-supervisor** | [langchain-ai/langgraph-supervisor-py](https://github.com/langchain-ai/langgraph-supervisor-py) | Build supervisor multi-agent systems with LangGraph. | 13755 | ![GitHub stars](https://img.shields.io/github/stars/langchain-ai/langgraph-supervisor-py?style=social)  \\n**langmem** | [langchain-ai/langmem](https://github.com/langchain-ai/langmem) | Build agents that learn and adapt from interactions over time. | 6393 | ![GitHub stars](https://img.shields.io/github/stars/langchain-ai/langmem?style=social)  \\n**langgraph-swarm** | [langchain-ai/langgraph-swarm-py](https://github.com/langchain-ai/langgraph-swarm-py) | Build swarm-style multi-agent systems using LangGraph. | 4416 | ![GitHub stars](https://img.shields.io/github/stars/langchain-ai/langgraph-swarm-py?style=social)  \\n**open-deep-research** | [langchain-ai/open_deep_research](https://github.com/langchain-ai/open_deep_research) | Open source assistant for iterative web research and report writing. | 1438 | ![GitHub stars](https://img.shields.io/github/stars/langchain-ai/open_deep_research?style=social)  \\n**ai-data-science-team** | [business-science/ai-data-science-team](https://github.com/business-science/ai-data-science-team) | An AI-powered data science team of agents to help you perform common data science tasks 10X faster. | 601 | ![GitHub stars](https://img.shields.io/github/stars/business-science/ai-data-science-team?style=social)  \\n**langgraph-codeact** | [langchain-ai/langgraph-codeact](https://github.com/langchain-ai/langgraph-codeact) | LangGraph implementation of CodeAct agent that generates and executes code instead of tool calling. | 569 | ![GitHub stars](https://img.shields.io/github/stars/langchain-ai/langgraph-codeact?style=social)  \\n**langgraph-reflection** | [langchain-ai/langgraph-reflection](https://github.com/langchain-ai/langgraph-reflection) | LangGraph agent that runs a reflection step. | 465 | ![GitHub stars](https://img.shields.io/github/stars/langchain-ai/langgraph-reflection?style=social)  \\n**langgraph-bigtool** | [langchain-ai/langgraph-bigtool](https://github.com/langchain-ai/langgraph-bigtool) | Build LangGraph agents with large numbers of tools. | 328 | ![GitHub stars](https://img.shields.io/github/stars/langchain-ai/langgraph-bigtool?style=social)  \\n**nodeology** | [xyin-anl/Nodeology](https://github.com/xyin-anl/Nodeology) | Enable researcher to build scientific workflows easily with simplified interface. | 48 | ![GitHub stars](https://img.shields.io/github/stars/xyin-anl/Nodeology?style=social)  \\n**delve-taxonomy-generator** | [andrestorres123/delve](https://github.com/andrestorres123/delve) | A taxonomy generator for unstructured data | 47 | ![GitHub stars](https://img.shields.io/github/stars/andrestorres123/delve?style=social)  \\n**breeze-agent** | [andrestorres123/breeze-agent](https://github.com/andrestorres123/breeze-agent) | A streamlined research system built inspired on STORM and built on LangGraph. | 33 | ![GitHub stars](https://img.shields.io/github/stars/andrestorres123/breeze-agent?style=social)  \\n  \\n## ✨ Contributing Your Library¶\\n\\nHave you built an awesome open-source library using LangGraph? We\\'d love to\\nfeature your project on the official LangGraph documentation pages! 🏆\\n\\nTo share your project, simply open a Pull Request adding an entry for your\\npackage in our [packages.yml](https://github.com/langchain-\\nai/langgraph/blob/main/docs/_scripts/third_party_page/packages.yml) file.\\n\\n**Guidelines**\\n\\n  * Your repo must be distributed as an installable package (e.g., PyPI for Python, npm for JavaScript/TypeScript, etc.) 📦\\n  * The repo should either use the Graph API (exposing a `StateGraph` instance) or the Functional API (exposing an `entrypoint`).\\n  * The package must include documentation (e.g., a `README.md` or docs site) explaining how to use it.\\n\\nWe\\'ll review your contribution and merge it in!\\n\\nThanks for contributing! 🚀\\n\\nWas this page helpful?\\n\\nThanks for your feedback!\\n\\nThanks for your feedback! Please help us improve this page by adding to the\\ndiscussion below.\\n\\n## Comments'),\n",
       "  0.6722707748413086)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_store.similarity_search_with_relevance_scores(\"build a swarm agent\", k=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
