{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_openai import AzureOpenAIEmbeddings\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "embeddings = AzureOpenAIEmbeddings(\n",
    "openai_api_type=\"azure\",\n",
    "openai_api_version=os.environ[\"OPENAI_API_EMBEDDING_VERSION\"],\n",
    "openai_api_key=os.environ[\"OPENAI_API_EMBEDDING_KEY\"],\n",
    "azure_endpoint=os.environ[\"AZURE_OPENAI_EMBEDDING_ENDPOINT\"],\n",
    "deployment=os.environ[\"AZURE_OPENAI_EMBEDDING_DEPLOYMENT\"],\n",
    "model=os.environ[\"AZURE_OPENAI_EMBEDDING_MODEL\"],\n",
    "validate_base_url=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_pinecone import PineconeVectorStore\n",
    "from pinecone import Pinecone,ServerlessSpec\n",
    "import time\n",
    "\n",
    "pc = Pinecone(api_key=os.environ.get(\"PINECONE_API_KEY\"))\n",
    "vector_store = PineconeVectorStore(index=pc.Index(\"langstuffindex\"), embedding=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-05 15:18:19,663 - INFO - HTTP Request: POST https://kanis-m8htxgs7-eastus.openai.azure.com/openai/deployments/text-embedding-3-small/embeddings?api-version=2023-05-15 \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "# SKIP this, only for local DB loading deprecated. \n",
    "from langchain_neo4j import Neo4jVector\n",
    "\n",
    "url = \"bolt://localhost:7687\"\n",
    "username = \"neo4j\"\n",
    "password = \"password\"\n",
    "\n",
    "\n",
    "vector_store = Neo4jVector.from_existing_index(\n",
    "    embeddings,\n",
    "    url=url,\n",
    "    username=username,\n",
    "    password=password,\n",
    "    index_name=\"neo4j\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "# Set up the logger\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,  # Set to DEBUG for detailed logs\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    "    handlers=[\n",
    "        # logging.FileHandler(\"scraper.log\"),  # Log to a file\n",
    "        logging.StreamHandler()  # Log to console\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: html2text in d:\\agentagent\\agentagent\\myenv\\lib\\site-packages (2024.2.26)\n"
     ]
    }
   ],
   "source": [
    "!pip install html2text\n",
    "import html2text\n",
    "import aiohttp\n",
    "import asyncio\n",
    "from bs4 import BeautifulSoup\n",
    "from langchain_core.documents import Document\n",
    "import json\n",
    "\n",
    "async def parse_url_to_doc(url: str) -> str:\n",
    "    try:\n",
    "        timeout = aiohttp.ClientTimeout(total=10)  # Set timeout to 3 seconds\n",
    "        async with aiohttp.ClientSession() as session:\n",
    "            async with session.get(url, timeout= timeout) as response:\n",
    "                if response.status != 200:\n",
    "                    logger.warning(f\"Failed to fetch {url}: Status code {response.status}\")\n",
    "                    return None\n",
    "                html_content = await response.text()\n",
    "                soup = BeautifulSoup(html_content, 'html.parser')\n",
    "                target_div = soup.find('div', class_= \"theme-doc-markdown markdown\") #langchain\n",
    "\n",
    "                if not target_div:\n",
    "                    target_div = soup.find('article') #langraph\n",
    "                \n",
    "                if not target_div:\n",
    "                    target_div = soup.find(\"div\", id=\"content-area\")\n",
    "                \n",
    "                if not target_div:\n",
    "                    target_div = soup.find(\"textarea\", id=\"read-only-cursor-text-area\")\n",
    "\n",
    "                if not target_div:\n",
    "                    return None\n",
    "\n",
    "                return Document(page_content=html2text.html2text(str(target_div)), metadata={\"source\": url})\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in parsing {url}: {e}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='Concepts\n",
      "\n",
      "# Tools\n",
      "\n",
      "Enable LLMs to perform actions through your server\n",
      "\n",
      "Tools are a powerful primitive in the Model Context Protocol (MCP) that enable\n",
      "servers to expose executable functionality to clients. Through tools, LLMs can\n",
      "interact with external systems, perform computations, and take actions in the\n",
      "real world.\n",
      "\n",
      "Tools are designed to be **model-controlled** , meaning that tools are exposed\n",
      "from servers to clients with the intention of the AI model being able to\n",
      "automatically invoke them (with a human in the loop to grant approval).\n",
      "\n",
      "##\n",
      "\n",
      "â€‹\n",
      "\n",
      "Overview\n",
      "\n",
      "Tools in MCP allow servers to expose executable functions that can be invoked\n",
      "by clients and used by LLMs to perform actions. Key aspects of tools include:\n",
      "\n",
      "  * **Discovery** : Clients can list available tools through the `tools/list` endpoint\n",
      "  * **Invocation** : Tools are called using the `tools/call` endpoint, where servers perform the requested operation and return results\n",
      "  * **Flexibility** : Tools can range from simple calculations to complex API interactions\n",
      "\n",
      "Like [resources](/docs/concepts/resources), tools are identified by unique\n",
      "names and can include descriptions to guide their usage. However, unlike\n",
      "resources, tools represent dynamic operations that can modify state or\n",
      "interact with external systems.\n",
      "\n",
      "##\n",
      "\n",
      "â€‹\n",
      "\n",
      "Tool definition structure\n",
      "\n",
      "Each tool is defined with the following structure:\n",
      "\n",
      "    \n",
      "    \n",
      "    {\n",
      "      name: string;          // Unique identifier for the tool\n",
      "      description?: string;  // Human-readable description\n",
      "      inputSchema: {         // JSON Schema for the tool's parameters\n",
      "        type: \"object\",\n",
      "        properties: { ... }  // Tool-specific parameters\n",
      "      }\n",
      "    }\n",
      "    \n",
      "\n",
      "##\n",
      "\n",
      "â€‹\n",
      "\n",
      "Implementing tools\n",
      "\n",
      "Hereâ€™s an example of implementing a basic tool in an MCP server:\n",
      "\n",
      "  * TypeScript\n",
      "  * Python\n",
      "\n",
      "    \n",
      "    \n",
      "    const server = new Server({\n",
      "      name: \"example-server\",\n",
      "      version: \"1.0.0\"\n",
      "    }, {\n",
      "      capabilities: {\n",
      "        tools: {}\n",
      "      }\n",
      "    });\n",
      "    \n",
      "    // Define available tools\n",
      "    server.setRequestHandler(ListToolsRequestSchema, async () => {\n",
      "      return {\n",
      "        tools: [{\n",
      "          name: \"calculate_sum\",\n",
      "          description: \"Add two numbers together\",\n",
      "          inputSchema: {\n",
      "            type: \"object\",\n",
      "            properties: {\n",
      "              a: { type: \"number\" },\n",
      "              b: { type: \"number\" }\n",
      "            },\n",
      "            required: [\"a\", \"b\"]\n",
      "          }\n",
      "        }]\n",
      "      };\n",
      "    });\n",
      "    \n",
      "    // Handle tool execution\n",
      "    server.setRequestHandler(CallToolRequestSchema, async (request) => {\n",
      "      if (request.params.name === \"calculate_sum\") {\n",
      "        const { a, b } = request.params.arguments;\n",
      "        return {\n",
      "          content: [\n",
      "            {\n",
      "              type: \"text\",\n",
      "              text: String(a + b)\n",
      "            }\n",
      "          ]\n",
      "        };\n",
      "      }\n",
      "      throw new Error(\"Tool not found\");\n",
      "    });\n",
      "    \n",
      "\n",
      "##\n",
      "\n",
      "â€‹\n",
      "\n",
      "Example tool patterns\n",
      "\n",
      "Here are some examples of types of tools that a server could provide:\n",
      "\n",
      "###\n",
      "\n",
      "â€‹\n",
      "\n",
      "System operations\n",
      "\n",
      "Tools that interact with the local system:\n",
      "\n",
      "    \n",
      "    \n",
      "    {\n",
      "      name: \"execute_command\",\n",
      "      description: \"Run a shell command\",\n",
      "      inputSchema: {\n",
      "        type: \"object\",\n",
      "        properties: {\n",
      "          command: { type: \"string\" },\n",
      "          args: { type: \"array\", items: { type: \"string\" } }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "    \n",
      "\n",
      "###\n",
      "\n",
      "â€‹\n",
      "\n",
      "API integrations\n",
      "\n",
      "Tools that wrap external APIs:\n",
      "\n",
      "    \n",
      "    \n",
      "    {\n",
      "      name: \"github_create_issue\",\n",
      "      description: \"Create a GitHub issue\",\n",
      "      inputSchema: {\n",
      "        type: \"object\",\n",
      "        properties: {\n",
      "          title: { type: \"string\" },\n",
      "          body: { type: \"string\" },\n",
      "          labels: { type: \"array\", items: { type: \"string\" } }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "    \n",
      "\n",
      "###\n",
      "\n",
      "â€‹\n",
      "\n",
      "Data processing\n",
      "\n",
      "Tools that transform or analyze data:\n",
      "\n",
      "    \n",
      "    \n",
      "    {\n",
      "      name: \"analyze_csv\",\n",
      "      description: \"Analyze a CSV file\",\n",
      "      inputSchema: {\n",
      "        type: \"object\",\n",
      "        properties: {\n",
      "          filepath: { type: \"string\" },\n",
      "          operations: {\n",
      "            type: \"array\",\n",
      "            items: {\n",
      "              enum: [\"sum\", \"average\", \"count\"]\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "    \n",
      "\n",
      "##\n",
      "\n",
      "â€‹\n",
      "\n",
      "Best practices\n",
      "\n",
      "When implementing tools:\n",
      "\n",
      "  1. Provide clear, descriptive names and descriptions\n",
      "  2. Use detailed JSON Schema definitions for parameters\n",
      "  3. Include examples in tool descriptions to demonstrate how the model should use them\n",
      "  4. Implement proper error handling and validation\n",
      "  5. Use progress reporting for long operations\n",
      "  6. Keep tool operations focused and atomic\n",
      "  7. Document expected return value structures\n",
      "  8. Implement proper timeouts\n",
      "  9. Consider rate limiting for resource-intensive operations\n",
      "  10. Log tool usage for debugging and monitoring\n",
      "\n",
      "##\n",
      "\n",
      "â€‹\n",
      "\n",
      "Security considerations\n",
      "\n",
      "When exposing tools:\n",
      "\n",
      "###\n",
      "\n",
      "â€‹\n",
      "\n",
      "Input validation\n",
      "\n",
      "  * Validate all parameters against the schema\n",
      "  * Sanitize file paths and system commands\n",
      "  * Validate URLs and external identifiers\n",
      "  * Check parameter sizes and ranges\n",
      "  * Prevent command injection\n",
      "\n",
      "###\n",
      "\n",
      "â€‹\n",
      "\n",
      "Access control\n",
      "\n",
      "  * Implement authentication where needed\n",
      "  * Use appropriate authorization checks\n",
      "  * Audit tool usage\n",
      "  * Rate limit requests\n",
      "  * Monitor for abuse\n",
      "\n",
      "###\n",
      "\n",
      "â€‹\n",
      "\n",
      "Error handling\n",
      "\n",
      "  * Donâ€™t expose internal errors to clients\n",
      "  * Log security-relevant errors\n",
      "  * Handle timeouts appropriately\n",
      "  * Clean up resources after errors\n",
      "  * Validate return values\n",
      "\n",
      "##\n",
      "\n",
      "â€‹\n",
      "\n",
      "Tool discovery and updates\n",
      "\n",
      "MCP supports dynamic tool discovery:\n",
      "\n",
      "  1. Clients can list available tools at any time\n",
      "  2. Servers can notify clients when tools change using `notifications/tools/list_changed`\n",
      "  3. Tools can be added or removed during runtime\n",
      "  4. Tool definitions can be updated (though this should be done carefully)\n",
      "\n",
      "##\n",
      "\n",
      "â€‹\n",
      "\n",
      "Error handling\n",
      "\n",
      "Tool errors should be reported within the result object, not as MCP protocol-\n",
      "level errors. This allows the LLM to see and potentially handle the error.\n",
      "When a tool encounters an error:\n",
      "\n",
      "  1. Set `isError` to `true` in the result\n",
      "  2. Include error details in the `content` array\n",
      "\n",
      "Hereâ€™s an example of proper error handling for tools:\n",
      "\n",
      "  * TypeScript\n",
      "  * Python\n",
      "\n",
      "    \n",
      "    \n",
      "    try {\n",
      "      // Tool operation\n",
      "      const result = performOperation();\n",
      "      return {\n",
      "        content: [\n",
      "          {\n",
      "            type: \"text\",\n",
      "            text: `Operation successful: ${result}`\n",
      "          }\n",
      "        ]\n",
      "      };\n",
      "    } catch (error) {\n",
      "      return {\n",
      "        isError: true,\n",
      "        content: [\n",
      "          {\n",
      "            type: \"text\",\n",
      "            text: `Error: ${error.message}`\n",
      "          }\n",
      "        ]\n",
      "      };\n",
      "    }\n",
      "    \n",
      "\n",
      "This approach allows the LLM to see that an error occurred and potentially\n",
      "take corrective action or request human intervention.\n",
      "\n",
      "##\n",
      "\n",
      "â€‹\n",
      "\n",
      "Testing tools\n",
      "\n",
      "A comprehensive testing strategy for MCP tools should cover:\n",
      "\n",
      "  * **Functional testing** : Verify tools execute correctly with valid inputs and handle invalid inputs appropriately\n",
      "  * **Integration testing** : Test tool interaction with external systems using both real and mocked dependencies\n",
      "  * **Security testing** : Validate authentication, authorization, input sanitization, and rate limiting\n",
      "  * **Performance testing** : Check behavior under load, timeout handling, and resource cleanup\n",
      "  * **Error handling** : Ensure tools properly report errors through the MCP protocol and clean up resources\n",
      "\n",
      "Was this page helpful?\n",
      "\n",
      "YesNo\n",
      "\n",
      "[Prompts](/docs/concepts/prompts)[Sampling](/docs/concepts/sampling)\n",
      "\n",
      "[github](https://github.com/modelcontextprotocol)\n",
      "\n",
      "' metadata={'source': 'https://modelcontextprotocol.io/docs/concepts/tools'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kanis\\AppData\\Local\\Temp\\ipykernel_1420\\3532148005.py:1: RuntimeWarning: coroutine 'parse_url_to_doc' was never awaited\n",
      "  doc = await parse_url_to_doc(\"https://modelcontextprotocol.io/docs/concepts/tools\")\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    }
   ],
   "source": [
    "doc = await parse_url_to_doc(\"https://modelcontextprotocol.io/docs/concepts/tools\")\n",
    "\n",
    "print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-06 14:42:18,987 - WARNING - Failed to fetch https://python.langchain.com/docs/expression_language/streaming: Status code 404\n",
      "2025-04-06 14:42:19,214 - WARNING - Failed to fetch https://python.langchain.com/docs/expression_language/how_to/inspect: Status code 404\n",
      "2025-04-06 14:42:19,718 - WARNING - Failed to fetch https://python.langchain.com/docs/expression_language/how_to/decorator: Status code 404\n"
     ]
    }
   ],
   "source": [
    "with open('../../webscraping/visited_urls_langchain.json', 'r') as f:\n",
    "    urls = json.load(f)\n",
    "\n",
    "    # Generator function to yield 10 items at a time\n",
    "    def batch_iterator(iterable, batch_size):\n",
    "        for i in range(0, len(iterable), batch_size):\n",
    "            yield iterable[i:i + batch_size]\n",
    "\n",
    "    batch_size = 10\n",
    "\n",
    "    # Create the iterator\n",
    "    iterator = batch_iterator(urls, batch_size)\n",
    "\n",
    "    documents = []\n",
    "\n",
    "    # Use the iterator\n",
    "    for batch in iterator:\n",
    "        tasks = [parse_url_to_doc(url) for url in batch]\n",
    "        results = await asyncio.gather(*tasks)\n",
    "        documents.append(results)\n",
    "    documents = [item for sublist in documents for item in sublist]\n",
    "    documents =  [item for item in documents if item != None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1800"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(encoding_name=\"cl100k_base\",\n",
    "    chunk_size=1500, chunk_overlap=100\n",
    ")\n",
    "doc_splits = text_splitter.split_documents(documents)\n",
    "\n",
    "len(doc_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1911028\n"
     ]
    }
   ],
   "source": [
    "doc_len=0\n",
    "import tiktoken\n",
    "for doc in doc_splits:\n",
    "    encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    num_tokens = len(encoding.encode(doc.page_content))\n",
    "    doc_len += num_tokens\n",
    "    \n",
    "print(doc_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-06 14:44:28,979 - INFO - HTTP Request: POST https://kanis-m8htxgs7-eastus.openai.azure.com/openai/deployments/text-embedding-3-small/embeddings?api-version=2023-05-15 \"HTTP/1.1 200 OK\"\n",
      "2025-04-06 14:44:54,298 - INFO - HTTP Request: POST https://kanis-m8htxgs7-eastus.openai.azure.com/openai/deployments/text-embedding-3-small/embeddings?api-version=2023-05-15 \"HTTP/1.1 200 OK\"\n",
      "2025-04-06 14:45:16,562 - INFO - HTTP Request: POST https://kanis-m8htxgs7-eastus.openai.azure.com/openai/deployments/text-embedding-3-small/embeddings?api-version=2023-05-15 \"HTTP/1.1 200 OK\"\n",
      "2025-04-06 14:45:47,629 - INFO - HTTP Request: POST https://kanis-m8htxgs7-eastus.openai.azure.com/openai/deployments/text-embedding-3-small/embeddings?api-version=2023-05-15 \"HTTP/1.1 200 OK\"\n",
      "2025-04-06 14:46:18,875 - INFO - HTTP Request: POST https://kanis-m8htxgs7-eastus.openai.azure.com/openai/deployments/text-embedding-3-small/embeddings?api-version=2023-05-15 \"HTTP/1.1 200 OK\"\n",
      "2025-04-06 14:46:53,795 - INFO - HTTP Request: POST https://kanis-m8htxgs7-eastus.openai.azure.com/openai/deployments/text-embedding-3-small/embeddings?api-version=2023-05-15 \"HTTP/1.1 200 OK\"\n",
      "2025-04-06 14:47:21,002 - INFO - HTTP Request: POST https://kanis-m8htxgs7-eastus.openai.azure.com/openai/deployments/text-embedding-3-small/embeddings?api-version=2023-05-15 \"HTTP/1.1 200 OK\"\n",
      "2025-04-06 14:47:45,208 - INFO - HTTP Request: POST https://kanis-m8htxgs7-eastus.openai.azure.com/openai/deployments/text-embedding-3-small/embeddings?api-version=2023-05-15 \"HTTP/1.1 200 OK\"\n",
      "2025-04-06 14:48:04,849 - INFO - HTTP Request: POST https://kanis-m8htxgs7-eastus.openai.azure.com/openai/deployments/text-embedding-3-small/embeddings?api-version=2023-05-15 \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "iterator = batch_iterator(doc_splits, 220)\n",
    "\n",
    "for doc in iterator:\n",
    "    try:\n",
    "        vector_store.add_documents(doc)\n",
    "    except Exception as ex:\n",
    "        logger.info(f\"{ex} caught, adding a 62 second sleep and retrying again\")\n",
    "        time.sleep(62)\n",
    "        vector_store.add_documents(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-06 13:58:57,310 - INFO - HTTP Request: POST https://kanis-m8htxgs7-eastus.openai.azure.com/openai/deployments/text-embedding-3-small/embeddings?api-version=2023-05-15 \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(Document(id='0dc04ca9-5f69-48f7-81eb-d1ed937a78a9', metadata={'source': 'https://github.com/langchain-ai/langgraph-swarm-py/blob/main/README.md'}, page_content='# ðŸ¤– LangGraph Multi-Agent Swarm\\n\\nA Python library for creating swarm-style multi-agent systems using\\n[LangGraph](https://github.com/langchain-ai/langgraph). A swarm is a type of\\n[multi-agent](https://langchain-ai.github.io/langgraph/concepts/multi_agent)\\narchitecture where agents dynamically hand off control to one another based on\\ntheir specializations. The system remembers which agent was last active,\\nensuring that on subsequent interactions, the conversation resumes with that\\nagent.\\n\\n[![Swarm](/langchain-ai/langgraph-swarm-\\npy/raw/main/static/img/swarm.png)](/langchain-ai/langgraph-swarm-\\npy/blob/main/static/img/swarm.png)\\n\\n## Features\\n\\n  * ðŸ¤– **Multi-agent collaboration** \\\\- Enable specialized agents to work together and hand off context to each other\\n  * ðŸ› ï¸ **Customizable handoff tools** \\\\- Built-in tools for communication between agents\\n\\nThis library is built on top of [LangGraph](https://github.com/langchain-\\nai/langgraph), a powerful framework for building agent applications, and comes\\nwith out-of-box support for [streaming](https://langchain-\\nai.github.io/langgraph/how-tos/#streaming), [short-term and long-term\\nmemory](https://langchain-ai.github.io/langgraph/concepts/memory/) and [human-\\nin-the-loop](https://langchain-\\nai.github.io/langgraph/concepts/human_in_the_loop/)\\n\\n## Installation\\n\\n    \\n    \\n    pip install langgraph-swarm\\n\\n## Quickstart'),\n",
       "  0.859215915),\n",
       " (Document(id='78c58646-8c66-4841-a1ee-d2e56f624f73', metadata={'source': 'https://github.com/langchain-ai/langgraph-swarm-py/blob/main/README.md'}, page_content='from typing_extensions import TypedDict, Annotated\\n    \\n    from langchain_core.messages import AnyMessage\\n    from langgraph.graph import StateGraph, add_messages\\n    from langgraph_swarm import SwarmState\\n    \\n    class AliceState(TypedDict):\\n        alice_messages: Annotated[list[AnyMessage], add_messages]\\n    \\n    # see this guide to learn how you can implement a custom tool-calling agent\\n    # https://langchain-ai.github.io/langgraph/how-tos/react-agent-from-scratch/\\n    alice = (\\n        StateGraph(AliceState)\\n        .add_node(\"model\", ...)\\n        .add_node(\"tools\", ...)\\n        .add_edge(...)\\n        ...\\n        .compile()\\n    )\\n    \\n    # wrapper calling the agent\\n    def call_alice(state: SwarmState):\\n        # you can put any input transformation from parent state -> agent state\\n        # for example, you can invoke \"alice\" with \"task_description\" populated by the LLM\\n        response = alice.invoke({\"alice_messages\": state[\"messages\"]})\\n        # you can put any output transformation from agent state -> parent state\\n        return {\"messages\": response[\"alice_messages\"]}\\n    \\n    def call_bob(state: SwarmState):\\n        ...\\n\\nThen, you can create the swarm manually in the following way:\\n\\n    \\n    \\n    from langgraph_swarm import add_active_agent_router\\n    \\n    workflow = (\\n        StateGraph(SwarmState)\\n        .add_node(\"Alice\", call_alice, destinations=(\"Bob\",))\\n        .add_node(\"Bob\", call_bob, destinations=(\"Alice\",))\\n    )\\n    # this is the router that enables us to keep track of the last active agent\\n    workflow = add_active_agent_router(\\n        builder=workflow,\\n        route_to=[\"Alice\", \"Bob\"],\\n        default_active_agent=\"Alice\",\\n    )\\n    \\n    # compile the workflow\\n    app = workflow.compile()'),\n",
       "  0.8211761415000001),\n",
       " (Document(id='5b4835ac-8968-4229-932f-b65c611a43d7', metadata={'source': 'https://github.com/langchain-ai/langgraph-swarm-py/blob/main/README.md'}, page_content='## Installation\\n\\n    \\n    \\n    pip install langgraph-swarm\\n\\n## Quickstart\\n\\n    \\n    \\n    pip install langgraph-swarm langchain-openai\\n    \\n    export OPENAI_API_KEY=<your_api_key>\\n    \\n    \\n    from langchain_openai import ChatOpenAI\\n    \\n    from langgraph.checkpoint.memory import InMemorySaver\\n    from langgraph.prebuilt import create_react_agent\\n    from langgraph_swarm import create_handoff_tool, create_swarm\\n    \\n    model = ChatOpenAI(model=\"gpt-4o\")\\n    \\n    def add(a: int, b: int) -> int:\\n        \"\"\"Add two numbers\"\"\"\\n        return a + b\\n    \\n    alice = create_react_agent(\\n        model,\\n        [add, create_handoff_tool(agent_name=\"Bob\")],\\n        prompt=\"You are Alice, an addition expert.\",\\n        name=\"Alice\",\\n    )\\n    \\n    bob = create_react_agent(\\n        model,\\n        [create_handoff_tool(agent_name=\"Alice\", description=\"Transfer to Alice, she can help with math\")],\\n        prompt=\"You are Bob, you speak like a pirate.\",\\n        name=\"Bob\",\\n    )\\n    \\n    checkpointer = InMemorySaver()\\n    workflow = create_swarm(\\n        [alice, bob],\\n        default_active_agent=\"Alice\"\\n    )\\n    app = workflow.compile(checkpointer=checkpointer)\\n    \\n    config = {\"configurable\": {\"thread_id\": \"1\"}}\\n    turn_1 = app.invoke(\\n        {\"messages\": [{\"role\": \"user\", \"content\": \"i\\'d like to speak to Bob\"}]},\\n        config,\\n    )\\n    print(turn_1)\\n    turn_2 = app.invoke(\\n        {\"messages\": [{\"role\": \"user\", \"content\": \"what\\'s 5 + 7?\"}]},\\n        config,\\n    )\\n    print(turn_2)\\n\\n## Memory'),\n",
       "  0.781640291),\n",
       " (Document(id='80340cab-86e0-4c9d-beb7-402a536986fb', metadata={'source': 'https://github.com/langchain-ai/langgraph-swarm-py/blob/main/README.md'}, page_content='## Memory\\n\\nYou can add [short-term](https://langchain-ai.github.io/langgraph/how-\\ntos/persistence/) and [long-term](https://langchain-\\nai.github.io/langgraph/how-tos/cross-thread-persistence/)\\n[memory](https://langchain-ai.github.io/langgraph/concepts/memory/) to your\\nswarm multi-agent system. Since `create_swarm()` returns an instance of\\n`StateGraph` that needs to be compiled before use, you can directly pass a\\n[checkpointer](https://langchain-\\nai.github.io/langgraph/reference/checkpoints/#langgraph.checkpoint.base.BaseCheckpointSaver)\\nor a [store](https://langchain-\\nai.github.io/langgraph/reference/store/#langgraph.store.base.BaseStore)\\ninstance to the `.compile()` method:\\n\\n    \\n    \\n    from langgraph.checkpoint.memory import InMemorySaver\\n    from langgraph.store.memory import InMemoryStore\\n    \\n    # short-term memory\\n    checkpointer = InMemorySaver()\\n    # long-term memory\\n    store = InMemoryStore()\\n    \\n    model = ...\\n    alice = ...\\n    bob = ...\\n    \\n    workflow = create_swarm(\\n        [alice, bob],\\n        default_active_agent=\"Alice\"\\n    )\\n    \\n    # Compile with checkpointer/store\\n    app = workflow.compile(\\n        checkpointer=checkpointer,\\n        store=store\\n    )\\n\\nImportant\\n\\nAdding [short-term memory](https://langchain-\\nai.github.io/langgraph/concepts/persistence/) is crucial for maintaining\\nconversation state across multiple interactions. Without it, the swarm would\\n\"forget\" which agent was last active and lose the conversation history. Make\\nsure to always compile the swarm with a checkpointer if you plan to use it in\\nmulti-turn conversations; e.g., `workflow.compile(checkpointer=checkpointer)`.\\n\\n## How to customize'),\n",
       "  0.7767106295),\n",
       " (Document(id='2c4a71fe-9d1e-4e5e-956f-a3528eada928', metadata={'source': 'https://github.com/langchain-ai/langchain-mcp-adapters/blob/main/README.md'}, page_content='If you want to run a LangGraph agent that uses MCP tools in a LangGraph API\\nserver, you can use the following setup:\\n\\n    \\n    \\n    # graph.py\\n    from contextlib import asynccontextmanager\\n    from langchain_mcp_adapters.client import MultiServerMCPClient\\n    from langgraph.prebuilt import create_react_agent\\n    from langchain_anthropic import ChatAnthropic\\n    \\n    model = ChatAnthropic(model=\"claude-3-5-sonnet-latest\")\\n    \\n    @asynccontextmanager\\n    async def make_graph():\\n        async with MultiServerMCPClient(\\n            {\\n                \"math\": {\\n                    \"command\": \"python\",\\n                    # Make sure to update to the full absolute path to your math_server.py file\\n                    \"args\": [\"/path/to/math_server.py\"],\\n                    \"transport\": \"stdio\",\\n                },\\n                \"weather\": {\\n                    # make sure you start your weather server on port 8000\\n                    \"url\": \"http://localhost:8000/sse\",\\n                    \"transport\": \"sse\",\\n                }\\n            }\\n        ) as client:\\n            agent = create_react_agent(model, client.get_tools())\\n            yield agent\\n\\nIn your [`langgraph.json`](https://langchain-\\nai.github.io/langgraph/cloud/reference/cli/#configuration-file) make sure to\\nspecify `make_graph` as your graph entrypoint:\\n\\n    \\n    \\n    {\\n      \"dependencies\": [\".\"],\\n      \"graphs\": {\\n        \"agent\": \"./graph.py:make_graph\"\\n      }\\n    }'),\n",
       "  0.7738840579999999),\n",
       " (Document(id='09736236-0a90-4fbe-8380-cda05090bf27', metadata={'source': 'https://github.com/langchain-ai/langgraph-supervisor-py/blob/main/README.md'}, page_content='pip install langgraph-supervisor langchain-openai\\n    \\n    export OPENAI_API_KEY=<your_api_key>\\n    \\n    \\n    from langgraph.prebuilt import create_react_agent\\n    from langgraph_supervisor import create_supervisor\\n    \\n    from langchain_openai import ChatOpenAI\\n    \\n    from langgraph.func import entrypoint, task\\n    from langgraph.graph import add_messages\\n    \\n    model = ChatOpenAI(model=\"gpt-4o\")\\n    \\n    # Create specialized agents\\n    \\n    # Functional API - Agent 1 (Joke Generator)\\n    @task\\n    def generate_joke(messages):\\n        \"\"\"First LLM call to generate initial joke\"\"\"\\n        system_message = {\\n            \"role\": \"system\", \\n            \"content\": \"Write a short joke\"\\n        }\\n        msg = model.invoke(\\n            [system_message] + messages\\n        )\\n        return msg\\n    \\n    @entrypoint()\\n    def joke_agent(state):\\n        joke = generate_joke(state[\\'messages\\']).result()\\n        messages = add_messages(state[\"messages\"], [joke])\\n        return {\"messages\": messages}\\n    \\n    joke_agent.name = \"joke_agent\"\\n    \\n    # Graph API - Agent 2 (Research Expert)\\n    def web_search(query: str) -> str:\\n        \"\"\"Search the web for information.\"\"\"\\n        return (\\n            \"Here are the headcounts for each of the FAANG companies in 2024:\\\\n\"\\n            \"1. **Facebook (Meta)**: 67,317 employees.\\\\n\"\\n            \"2. **Apple**: 164,000 employees.\\\\n\"\\n            \"3. **Amazon**: 1,551,000 employees.\\\\n\"'),\n",
       "  0.773836136),\n",
       " (Document(id='5f3c1936-dfd9-4a43-ba29-ab1f1e70fe4f', metadata={'source': 'https://github.com/langchain-ai/langgraph-swarm-py/blob/main/README.md'}, page_content=\"Important\\n\\nIf you are implementing custom handoff tools that return `Command`, you need\\nto ensure that:  \\n(1) your agent has a tool-calling node that can handle tools returning\\n`Command` (like LangGraph's prebuilt [`ToolNode`](https://langchain-\\nai.github.io/langgraph/reference/prebuilt/#langgraph.prebuilt.tool_node.ToolNode))  \\n(2) both the swarm graph and the next agent graph have the [state\\nschema](https://langchain-ai.github.io/langgraph/concepts/low_level#schema)\\ncontaining the keys you want to update in `Command.update`\\n\\n### Customizing agent implementation\\n\\nBy default, individual agents are expected to communicate over a single\\n`messages` key that is shared by all agents and the overall multi-agent swarm\\ngraph. This means that messages from **all** of the agents will be combined\\ninto a single, shared list of messages. This might not be desirable if you\\ndon't want to expose an agent's internal history of messages. To change this,\\nyou can customize the agent by taking the following steps:\\n\\n  1. use custom [state schema](https://langchain-ai.github.io/langgraph/concepts/low_level#schema) with a different key for messages, for example `alice_messages`\\n  2. write a wrapper that converts the parent graph state to the child agent state and back (see this [how-to](https://langchain-ai.github.io/langgraph/how-tos/subgraph-transform-state/) guide)\"),\n",
       "  0.772803098),\n",
       " (Document(id='403df725-59b8-44e1-a2c5-f265de94f96e', metadata={'source': 'https://github.com/langchain-ai/langgraph-supervisor-py/blob/main/README.md'}, page_content=\"# ðŸ¤– LangGraph Multi-Agent Supervisor\\n\\nA Python library for creating hierarchical multi-agent systems using\\n[LangGraph](https://github.com/langchain-ai/langgraph). Hierarchical systems\\nare a type of [multi-agent](https://langchain-\\nai.github.io/langgraph/concepts/multi_agent) architecture where specialized\\nagents are coordinated by a central **supervisor** agent. The supervisor\\ncontrols all communication flow and task delegation, making decisions about\\nwhich agent to invoke based on the current context and task requirements.\\n\\n## Features\\n\\n  * ðŸ¤– **Create a supervisor agent** to orchestrate multiple specialized agents\\n  * ðŸ› ï¸ **Tool-based agent handoff mechanism** for communication between agents\\n  * ðŸ“ **Flexible message history management** for conversation control\\n\\nThis library is built on top of [LangGraph](https://github.com/langchain-\\nai/langgraph), a powerful framework for building agent applications, and comes\\nwith out-of-box support for [streaming](https://langchain-\\nai.github.io/langgraph/how-tos/#streaming), [short-term and long-term\\nmemory](https://langchain-ai.github.io/langgraph/concepts/memory/) and [human-\\nin-the-loop](https://langchain-\\nai.github.io/langgraph/concepts/human_in_the_loop/)\\n\\n## Installation\\n\\n    \\n    \\n    pip install langgraph-supervisor\\n\\nNote\\n\\nLangGraph Supervisor requires Python >= 3.10\\n\\n## Quickstart\\n\\nHere's a simple example of a supervisor managing two specialized agents:\\n\\n[![Supervisor Architecture](/langchain-ai/langgraph-supervisor-\\npy/raw/main/static/img/supervisor.png)](/langchain-ai/langgraph-supervisor-\\npy/blob/main/static/img/supervisor.png)\"),\n",
       "  0.76259616),\n",
       " (Document(id='464b19df-a5b8-47c7-9824-4b75e35c0188', metadata={'source': 'https://github.com/langchain-ai/langgraph-bigtool/blob/main/README.md'}, page_content='pip install langgraph-bigtool \"langchain[openai]\"\\n    \\n    export OPENAI_API_KEY=<your_api_key>\\n    \\n    \\n    import math\\n    import types\\n    import uuid\\n    \\n    from langchain.chat_models import init_chat_model\\n    from langchain.embeddings import init_embeddings\\n    from langgraph.store.memory import InMemoryStore\\n    \\n    from langgraph_bigtool import create_agent\\n    from langgraph_bigtool.utils import (\\n        convert_positional_only_function_to_tool\\n    )\\n    \\n    # Collect functions from `math` built-in\\n    all_tools = []\\n    for function_name in dir(math):\\n        function = getattr(math, function_name)\\n        if not isinstance(\\n            function, types.BuiltinFunctionType\\n        ):\\n            continue\\n        # This is an idiosyncrasy of the `math` library\\n        if tool := convert_positional_only_function_to_tool(\\n            function\\n        ):\\n            all_tools.append(tool)\\n    \\n    # Create registry of tools. This is a dict mapping\\n    # identifiers to tool instances.\\n    tool_registry = {\\n        str(uuid.uuid4()): tool\\n        for tool in all_tools\\n    }\\n    \\n    # Index tool names and descriptions in the LangGraph\\n    # Store. Here we use a simple in-memory store.\\n    embeddings = init_embeddings(\"openai:text-embedding-3-small\")\\n    \\n    store = InMemoryStore(\\n        index={\\n            \"embed\": embeddings,\\n            \"dims\": 1536,\\n            \"fields\": [\"description\"],\\n        }\\n    )\\n    for tool_id, tool in tool_registry.items():\\n        store.put(\\n            (\"tools\",),\\n            tool_id,\\n            {\\n                \"description\": f\"{tool.name}: {tool.description}\",'),\n",
       "  0.7570081355),\n",
       " (Document(id='c7e110dd-3291-4e64-9d8c-47cbb01c7fce', metadata={'source': 'https://github.com/langchain-ai/langgraph-bigtool/blob/main/README.md'}, page_content=\"## Installation\\n\\n    \\n    \\n    pip install langgraph-bigtool\\n\\n## Quickstart\\n\\nWe demonstrate `langgraph-bigtool` by equipping an agent with all functions\\nfrom Python's built-in `math` library.\\n\\nNote\\n\\nThis includes about 50 tools. Some LLMs can handle this number of tools\\ntogether in a single invocation without issue. This example is for\\ndemonstration purposes.\"),\n",
       "  0.7527805865)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_store.similarity_search_with_relevance_scores(\"implement a langgraph swarm agent\", k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
