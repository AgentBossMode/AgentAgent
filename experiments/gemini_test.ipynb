{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fce52156",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "import logging\n",
    "from bs4 import BeautifulSoup\n",
    "import html2text\n",
    "import httpx\n",
    "import yaml\n",
    "import json\n",
    "from pydantic import Field, BaseModel\n",
    "from langgraph.graph import MessagesState, StateGraph, START, END\n",
    "from typing import List\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage, ToolMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.types import Send\n",
    "import operator\n",
    "from typing import Annotated\n",
    "from typing import NamedTuple\n",
    "from utils.fetch_docs import fetch_documents\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from code_reflection_agent import final_agent\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Set up the logger\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,  # Set to DEBUG for detailed logs\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    "    handlers=[\n",
    "        # logging.FileHandler(\"scraper.log\"),  # Log to a file\n",
    "        logging.StreamHandler()  # Log to console\n",
    "    ]\n",
    ")\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "template = \"\"\"Your job is to get information from a user about what kind of agent they wish to build.\n",
    "\n",
    "You should get the following information from them:\n",
    "\n",
    "- What the objective of the agent is\n",
    "- Various usecases of the agent \n",
    "- Some examples of what the agent will be doing (Input and expected output pairs)\n",
    "\n",
    "If you are not able to discern this info, ask them to clarify! Do not attempt to wildly guess.\n",
    "\n",
    "After you are able to discern all the information, call the tool AgentInstruction\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "919d49de",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentInstructions(BaseModel):\n",
    "    \"\"\"Instructions on how to build the Agent\"\"\"\n",
    "    objective: str = Field(description= \"What is the primary objective of the agent\")\n",
    "    usecases: List[str] = Field(description= \"What are the various responsibilities of the agent which it needs to fulfill\")\n",
    "    examples : str = Field(description= \"What are some examples of the usage of the agent (input query and expected output from the agent) ?\")\n",
    "\n",
    "class AgentBuilderState(MessagesState):\n",
    "    agent_instructions: AgentInstructions = Field(\"the requirement analysis generated by the model.\")\n",
    "    json_code: str = Field(\"The json code generated\")\n",
    "    python_code: str = Field(\"The Python code generated\")\n",
    "\n",
    "class ArchitectureEvaluationState(MessagesState):\n",
    "    agent_instructions: AgentInstructions = Field(\"the requirement analysis generated by the model.\")\n",
    "    url: str = Field(\"url of the agent architecture to evaluate against\")\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash-preview-05-20\", temperature=0)\n",
    "\n",
    "def requirement_analysis_node(state: AgentBuilderState):\n",
    "    \n",
    "    llm_with_tool = llm.bind_tools([AgentInstructions])\n",
    "    response = llm_with_tool.invoke([SystemMessage(content=template)] + state[\"messages\"])\n",
    "    \n",
    "      # Construct the final answer from the arguments of the last tool call  \n",
    "    if len(response.tool_calls) == 0:\n",
    "        return {\"messages\": [response]}\n",
    "    \n",
    "    agent_instructions = response.tool_calls[0]\n",
    "    agent_instructions = AgentInstructions(**agent_instructions[\"args\"])\n",
    "    \n",
    "    return {\"messages\": [response], \"agent_instructions\": agent_instructions}\n",
    "\n",
    "\n",
    "def route_state(state: AgentBuilderState):\n",
    "    messages = state[\"messages\"]\n",
    "    if isinstance(messages[-1], AIMessage) and messages[-1].tool_calls:\n",
    "        return \"agent_kernel_builder\"\n",
    "    elif not isinstance(messages[-1], HumanMessage):\n",
    "        return END\n",
    "    return \"requirement_analysis\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5f87e970",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "AGENT_KERNEL_PROMPT = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "        Task Overview:\n",
    "        Design a langgraph StateGraph object implementing the the best architecture for the given set of requirements, tailored to fulfill the user requirements defined below\n",
    "\n",
    "        <Requirements>\n",
    "        Objectives: {objective}\n",
    "        usecases: {responsibilities}\n",
    "        examples: {examples}\n",
    "        </Requirements>\n",
    "\n",
    "        \n",
    "        Expected Output:\n",
    "        Your task is to produce a compiled StateGraph object.\n",
    "\n",
    "        Guidelines for Code Generation:\n",
    "        - Accuracy: Avoid hallucinations or speculative assumptions when writing code. Refer exclusively to the provided documentation.\n",
    "        - Understanding: Thoroughly comprehend the architecture and examples of code.\n",
    "        - Customization: Generate code tailored specifically to meet the user requirements.\n",
    "\n",
    "    \"\"\")\n",
    "\n",
    "def agent_kernel_builder(state: AgentBuilderState):\n",
    "    \"\"\"Build the agent kernel using the best architecture.\"\"\"\n",
    "    agent_instructions : AgentInstructions = state[\"agent_instructions\"]\n",
    "    langgraph_glossary_url = \"https://langchain-ai.github.io/langgraph/concepts/low_level/\"\n",
    "    # agent_architecture_report.name\n",
    "    #agent_architecture_report.highlights\n",
    "    #agent_architecture_report.justification\n",
    "    #agent_architecture_report.tailored_design\n",
    "    print(\"reached here\")\n",
    "    response =  llm.invoke([HumanMessage(content=AGENT_KERNEL_PROMPT.format(\n",
    "        objective=agent_instructions.objective,\n",
    "        responsibilities=agent_instructions.usecases,\n",
    "        examples = agent_instructions.examples,\n",
    "        # langgraph_glossary=fetch_documents(langgraph_glossary_url),\n",
    "        ))])\n",
    "    \n",
    "    # Return the generated agent kernel as the output\n",
    "    return {\n",
    "        \"messages\": [AIMessage(content=\"Generated agent kernel code!\")],\n",
    "        \"python_code\": response.content,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3ec67a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "CODE_TO_JSON_PROMPT = PromptTemplate.from_template(\"\"\"\n",
    "You are tasked with converting the following stategraph comPilation code into a JSON. \n",
    "\n",
    "Contextual documents for understanding code:\n",
    "{documents}\n",
    "\n",
    "The Input code is as follows:\n",
    "{code_snippet}\n",
    "\n",
    "OUTPUT: Explaination and JSON. Do not include any code blocks. Seperate the JSON and explaination blocks and ensure that there is an explaination for each line of JSON produced but keep the blocks seperated.\n",
    "Each Output JSON will have a nodes sections containing all the nodes and an edges section\n",
    "\n",
    "Please follow:\n",
    "1. Produce the explaination first and then the JSON after it. DO not produce the JSON first. \n",
    "2. For any conditional edges, please include all the nodes that the source of a conditional edge can reach as part of the explaination.\n",
    "3. Any Edge entry in the JSON can only be conditional(mention conditional: true) if the source for that edge acts as a source for multiple edges. If you cannot point to atleast 2 targets for 1 source, then that source will not have any conditional edges\n",
    "4. A source can have any number of targets. Please write the explaination for each source node to target node edge\n",
    "5. Please ensure that the JSON starts with __START__ node and __END__ node with the correct edges from and to them\n",
    "6. Ensure all elements in the nodes sections of the output json contain the following fields: Schema_info, input_schema, output_schema, description, function_name. Please do not return any entries in the nodes without these fields and these fields can't be empty\n",
    "7. Ensure all elements in the edges sections of the output json contain the following fields: source, target, routing_conditions, conditional. Please do not return any entries in the edges without these fields and they can't be empty\n",
    "8. Every node should be a part of atleast one edge, Please ensure this is followed\n",
    "9. Attach the code snippet for each node aswell that. Please extract it from the Input code\n",
    "\n",
    "\n",
    "Example output JSON for a node:\n",
    "    \"code_node\":{{\n",
    "        \"schema_info\": /\"/\"/\"CodeWriterState:\n",
    "      type: TypedDict\n",
    "      fields:\n",
    "      - name: user_query\n",
    "        type: str\n",
    "      - name: execution_result\n",
    "        type: str/\"/\"/\",\n",
    "    \"input_schema\": \"CodeWriterState\",\n",
    "    \"output_schema\":\"RequiremenCodeWriterStatetAnalysisState\",\n",
    "    \"description\":\"This node analyzes the user_query, if the query is to write a code, it will make a tool call to run the proposed code. This node returns command object\",\n",
    "    \"function_name\": \"code_step\"\n",
    "    }}\n",
    "\n",
    "Example output JSON for an edge:\n",
    "edge:{{ source: \"abc\", target: \"cde\", routing_condition: \"if abc made a tool call then go to cde\", \"conditional\": true}}\n",
    "edge:{{ source: \"abc\", target: \"xyz\", routing_condition: \"if abc made an interupt to a human then go to xyz\", \"conditional\": true}}\n",
    "edge:{{ source: \"xyz\", target: \"_END_\", routing_condition: \"no nodes to go after xyz, we have our final output for this path\", \"conditional\": false}}s         \n",
    "\"\"\")\n",
    "def code_to_json_node(state: AgentBuilderState):\n",
    "    \"\"\"Convert the generated code to JSON.\"\"\"\n",
    "    langgraph_glossary_url = \"https://langchain-ai.github.io/langgraph/concepts/low_level/\"\n",
    "    json_code_ouptut = llm.invoke([HumanMessage(content=CODE_TO_JSON_PROMPT.format(\n",
    "        code_snippet=state[\"python_code\"],\n",
    "        documents = fetch_documents(langgraph_glossary_url),\n",
    "        ))])\n",
    "    \n",
    "    # Return the JSON code as the output\n",
    "    return {\n",
    "        \"messages\": [AIMessage(content=\"Generated JSON code!\")],\n",
    "        \"json_code\": json_code_ouptut.content,\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "69f75bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "JSON_CODE_COMBINE_PROMPT = PromptTemplate.from_template(\"\"\"\n",
    "You are tasked with verifying and updating the provided JSON that represents the nodes and edges of the langgraph to ensure it is correct with respect to the input code.\n",
    "\n",
    "Contextual Documents for Understanding Code:\n",
    "{documents}\n",
    "\n",
    "Input Code:\n",
    "<Input code>{code_snippet}</Input code>\n",
    "JSON:\n",
    "<JSON>{node_json}</JSON>\n",
    "\n",
    "OUTPUT: JSON\n",
    "Your task is divided into two parts:\n",
    "- Validation: Verify that the JSON adheres to the rules outlined below.\n",
    "- Correction and Augmentation: If the JSON is incorrect or incomplete, update it with a clear justification for every change, and include the code snippet for each node extracted from the input code.\n",
    "\n",
    "Rules for Validation and Update:\n",
    "- Conditional Edges:- An edge can be marked as conditional: true only if its source acts as a source for multiple edges (i.e., at least two targets).\n",
    "- If the source does not meet this condition, then it cannot have conditional edges.\n",
    "\n",
    "- Edge Targets:- Each source can have any number of targets. This flexibility must be maintained.\n",
    "\n",
    "- Start and End Nodes:- The JSON must begin with the __START__ node and conclude with the __END__ node, with correct edges to and from them. Edges into the END node can also be conditional if they meet the above mentioned conditions      \n",
    "\n",
    "- Node Structure:- Each node entry in the nodes section of the JSON must include the following non-empty fields:- schema_info\n",
    "- id\n",
    "- schema_info\n",
    "- input_schema\n",
    "- output_schema\n",
    "- description\n",
    "- function_name\n",
    "\n",
    "\n",
    "- Edge Structure:- Each edge entry in the edges section of the JSON must include the following non-empty fields:- source\n",
    "- target\n",
    "- routing_conditions\n",
    "- conditional\n",
    "\n",
    "\n",
    "- Node-Edge Relationship:- Every node must be part of at least one edge. Ensure this relationship is consistently followed.\n",
    "\n",
    "- Node Code Snippets:- Attach a code field to every node in the JSON, extracted directly from the input code.\n",
    "\n",
    "- Schema Requirements:- The final JSON must conform to the schema provided below:\n",
    "\n",
    "\n",
    "Schema for JSON. Ensure the following schema is followed. No field should be missing for any node or edge:\n",
    "- Node Example:\n",
    "\n",
    "{{\n",
    "  \"code_node\": {{\n",
    "    \"id\" : \"<id of the node, similar to name>\"\n",
    "    \"schema_info\": \"<define the class structure of the state of this node. Also provide a name>\",\n",
    "    \"input_schema\": \"<input state object name>\",\n",
    "    \"output_schema\": \"<output state object name>\",\n",
    "    \"description\": \"<description>\",\n",
    "    \"function_name\": \"<function_name>\",\n",
    "    \"code\": \"<python_code>\"\n",
    "  }}\n",
    "}}\n",
    "\n",
    "\n",
    "- Edge Example:\n",
    "\n",
    "{{\n",
    "  \"edge\": {{\n",
    "    \"source\": \"<source_node>\",\n",
    "    \"target\": \"<target_node>\",\n",
    "    \"routing_condition\": \"<routing_condition>\",\n",
    "    \"conditional\": true/false\n",
    "  }}\n",
    "}}\n",
    "\n",
    "\n",
    "                                            \n",
    "Key Instructions:\n",
    "- Do not update any pre-existing field of the JSON unless you have an extremely strong justification for doing so.\n",
    "- Clearly document the reasoning behind any additions, updates, or modifications to the JSON. Justifications should draw inspiration from the contextual documents mentioned earlier.\n",
    "- Ensure conditional edges strictly adhere to the rules outlined above.\n",
    "- Input_Schema and output_schemas can only have value None in JSON for START and END nodes. Please follow this without fail\n",
    "- Include a code field for each code_node entry, with the exact code that corresponds to the node in the input code.\n",
    "\n",
    "\n",
    "\"\"\")\n",
    "\n",
    "def json_better_node(state: AgentBuilderState):\n",
    "    \"\"\"Add code to the json flow\"\"\"\n",
    "    langgraph_glossary_url = \"https://langchain-ai.github.io/langgraph/concepts/low_level/\"\n",
    "    json_code_ouptut = llm.invoke([HumanMessage(content=JSON_CODE_COMBINE_PROMPT.format(\n",
    "        code_snippet=state[\"python_code\"],\n",
    "        documents = fetch_documents(langgraph_glossary_url),\n",
    "        node_json = state[\"json_code\"]\n",
    "        ))])\n",
    "    \n",
    "    # Return the JSON code as the output\n",
    "    return {\n",
    "        \"messages\": [AIMessage(content=\"Generated updated JSON code!\")],\n",
    "        \"json_code\": json_code_ouptut.content,\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5e6ed40a",
   "metadata": {},
   "outputs": [],
   "source": [
    "CODE_GEN_PROMPT = PromptTemplate.from_template( \"\"\"\n",
    "You are an expert Python programmer specializing in LangGraph and AI agent development. Your primary task is to generate compilable Python code for a LangGraph state graph based on a JSON description that will be provided. You must carefully analyze the JSON to determine node functionalities, state management, and graph connectivity.\n",
    "\n",
    "**Input:**\n",
    "You will receive a JSON object with two main keys:\n",
    "1.  `nodes`: A dictionary where each key is a unique node ID. The value for each node ID is an object containing:\n",
    "    * `id`: The node's identifier.\n",
    "    * `schema_info`: A string describing the structure of the `GraphState` (e.g., \"GraphState:\\n type: TypedDict\\n fields:\\n - name: input\\n type: str...\"). You will need to parse this to define the `GraphState` TypedDict.\n",
    "    * `input_schema`: The expected input schema for the node (typically \"GraphState\").\n",
    "    * `output_schema`: The schema of the output produced by the node (typically \"GraphState\", indicating a partial update).\n",
    "    * `description`: A natural language description of what the node does. This is crucial for determining implementation strategy.\n",
    "    * `function_name`: The suggested Python function name for this node.\n",
    "    * `code` (optional): A string containing Python code for the node's function. This might be a complete implementation or a simplified example.\n",
    "\n",
    "2.  `edges`: A list of objects, each describing a directed edge in the graph. Each edge object contains:\n",
    "    * `source`: The ID of the source node (or \"__START__\" for the graph's entry point).\n",
    "    * `target`: The ID of the target node (or \"__END__\" for a graph termination point).\n",
    "    * `routing_conditions`: A natural language description of the condition under which this edge is taken, especially for conditional edges.\n",
    "    * `conditional`: A boolean flag, `true` if the edge is part of a conditional branch, `false` otherwise.\n",
    "\n",
    "**Output Requirements:**\n",
    "Generate a single, self-contained, and compilable Python script that performs the following:\n",
    "\n",
    "1.  **Imports:** Include all necessary Python libraries (e.g., `typing` for `TypedDict`, `Optional`, `Dict`, `Any`; `langgraph.graph.StatefulGraph`, `langgraph.graph.START`, `langgraph.graph.END`; `langgraph.checkpoint.memory.InMemoryCheckpointer`; potentially `langchain_openai`, `langchain_core.pydantic_v1` for Pydantic models if LLM with structured output is needed; `re` if used in provided code snippets).\n",
    "\n",
    "2.  **State Definition (`GraphState`):**\n",
    "    * Parse the `schema_info` string from one of the node descriptions (assume they are consistent; if not, use the first one encountered).\n",
    "    * Define a `GraphState` class using `typing.TypedDict` that accurately reflects the fields and their types as specified in `schema_info`. Pay close attention to `Optional` types and nested structures if any.\n",
    "\n",
    "3.  **Node Implementation (Python Functions):**\n",
    "    For each node defined in the `nodes` section of the JSON:\n",
    "    * Create a Python function with the `function_name` provided. This function must accept the `GraphState` (the TypedDict you defined) as its sole argument and return a dictionary representing the partial update to the state.\n",
    "    * **Decision Logic for Implementation Strategy (CRITICAL):**\n",
    "        * **Simple Algorithmic Logic:** If the `description` indicates a straightforward task and the provided `code` snippet is a complete, purely algorithmic Python function (e.g., string manipulation, regex-based parsing, simple dictionary operations) that fully implements this description without requiring advanced natural language understanding or complex inference, then directly use or adapt this provided `code` for the node's function.\n",
    "        * **LLM Requirement:** If the `description` implies complex tasks such as:\n",
    "            * Natural Language Understanding (NLU) (e.g., \"robust intent classification\", \"understanding user queries\")\n",
    "            * Complex decision-making based on unstructured input\n",
    "            * Text generation or summarization\n",
    "            * Or if the `description` explicitly states \"this function would typically use an LLM\" or similar.\n",
    "            Then, you MUST implement the node to utilize a Large Language Model (LLM).\n",
    "            * If `code` is provided but is clearly a simplistic placeholder for what should be an LLM-driven task (e.g., basic keyword matching for intent classification when robust understanding is needed, as seen in the example's `classify_intent` node), you should replace or augment this simplistic code with an actual LLM integration.\n",
    "            * When an LLM is required:\n",
    "                * Include the necessary import for a common LLM client (e.g., `from langchain_openai import ChatOpenAI`).\n",
    "                * In the node function, you can either instantiate a generic model (e.g., `llm = ChatOpenAI(model=\"gpt-3.5-turbo\") # TODO: Replace with specific model and API key if needed`) or provide clear comment placeholders (e.g., `# Initialize LLM here (e.g., llm = ChatOpenAI(...))` followed by `# response = llm.invoke(...) # Adapt prompt and parsing based on node's task`).\n",
    "                * The LLM should be prompted (even if schematically) to perform the task outlined in the node's `description`.\n",
    "        * **Structured Output from LLM:** If the node's `description` or its purpose (e.g., extracting specific entities, parsing data into a predefined schema beyond simple `GraphState` fields) suggests that an LLM needs to produce a structured output (e.g., JSON, specific Pydantic model):\n",
    "            * Define a Pydantic model (e.g., `from langchain_core.pydantic_v1 import BaseModel, Field`) representing the desired structured output.\n",
    "            * If implementing an LLM call, configure it to use the Pydantic model for its output (e.g., with OpenAI's function calling/tool usage features, or by instructing the LLM to generate JSON conforming to the model).\n",
    "        * **Tools for LLM:** If the `description` implies the node needs to interact with external systems, call specific APIs, or choose from a set of defined capabilities (e.g., \"search customer database,\" \"fetch external data,\" \"invoke a specific service\"):\n",
    "            * Define appropriate LangChain tools (e.g., using `@tool` from `langchain_core.tools`).\n",
    "            * If implementing an LLM call, bind these tools to the LLM. The LLM's role would be to decide which tool(s) to call based on the current state and input. For this generation task, you might define placeholder tools if the specifics are not in the JSON.\n",
    "\n",
    "4.  **Graph Construction (`StatefulGraph`):**\n",
    "    * Instantiate `StatefulGraph(GraphState)`.\n",
    "    * Add each implemented node function to the graph using `graph.add_node(\"node_id\", node_function)`.\n",
    "    * Set the graph's entry point using `graph.add_edge(START, \"entry_node_id\")` where `\"entry_node_id\"` is the target of the edge originating from `\"__START__\"`.\n",
    "\n",
    "5.  **Edge Implementation:**\n",
    "    * Iterate through the `edges` list in the JSON.\n",
    "    * **Regular Edges:** If `conditional` is `false`:\n",
    "        * If `target` is `__END__`, use `graph.add_edge(source_node_id, END)`.\n",
    "        * Otherwise, use `graph.add_edge(source_node_id, target_node_id)`.\n",
    "    * **Conditional Edges:** If `conditional` is `true`:\n",
    "        * The `source` node of these conditional edges is expected to produce some output in the `GraphState` (e.g., an `intent` field) that determines the next path.\n",
    "        * Create a separate routing function (e.g., `def route_after_source_node(state: GraphState) -> str:`).\n",
    "        * This routing function must inspect the relevant fields in the `state` and return the string ID of the next node to execute, based on the logic described in the `routing_conditions` for each conditional edge originating from that source.\n",
    "        * Use `graph.add_conditional_edges(source_node_id, routing_function, {{ \"target_id_1\": \"target_id_1\", \"target_id_2\": \"target_id_2\", ... \"__END__\": END }})`. The keys in the dictionary are the possible return values from your routing function, and the values are the actual node IDs or `END`.\n",
    "\n",
    "6.  **Compilation:**\n",
    "    * Instantiate an `InMemoryCheckpointer`: `checkpointer = InMemoryCheckpointer()`.\n",
    "    * Compile the graph: `final_app = graph.compile(checkpointer=checkpointer)`. The compiled graph must be assigned to a variable named `final_app`.\n",
    "\n",
    "**Important Considerations:**\n",
    "* The generated Python code must be complete, runnable, and accurately reflect the graph structure and logic described in the JSON.\n",
    "* Ensure all node functions correctly update and return the relevant parts of the `GraphState`.\n",
    "* If the provided `code` in the JSON uses specific libraries (e.g., `re`), make sure the corresponding import is included at the top of the script.\n",
    "* Handle `__START__` and `__END__` correctly in edge definitions. `langgraph.graph.START` and `langgraph.graph.END` should be used.\n",
    "\n",
    "Here is the JSON input:\n",
    "<json>\n",
    "{code_json}\n",
    "</json>\n",
    "\n",
    "Please generate the Python code now:\n",
    "\"\"\")\n",
    "\n",
    "def code_node(state: AgentBuilderState):\n",
    "    \"\"\"Produce the final python code\"\"\"\n",
    "    code_ouptut = llm.invoke([HumanMessage(content=CODE_GEN_PROMPT.format(\n",
    "        code_json = state[\"json_code\"]\n",
    "        ))])\n",
    "    \n",
    "    # Return the JSON code as the output\n",
    "    return {\n",
    "        \"messages\": [AIMessage(content=\"Generated final python code!\")],\n",
    "        \"python_code\": code_ouptut.content,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a752111",
   "metadata": {},
   "outputs": [],
   "source": [
    "CODE_GEN_PROMPT = PromptTemplate.from_template( \"\"\"\n",
    "You are an expert Python programmer specializing in LangGraph and AI agent development. Your primary task is to generate compilable, logical, and complete Python code for a LangGraph state graph based on a JSON description. You must prioritize LLM-based implementations for relevant tasks and consider advanced graph architectures.\n",
    "\n",
    "**Input:**\n",
    "You will receive a JSON object with two main keys:\n",
    "1.  `nodes`: A dictionary where each key is a unique node ID. The value for each node ID is an object containing:\n",
    "    * `id`: The node's identifier.\n",
    "    * `schema_info`: A string describing the structure of the `GraphState` (e.g., \"GraphState:\\n type: TypedDict\\n fields:\\n - name: input\\n type: str...\"). You will need to parse this to define the `GraphState` TypedDict.\n",
    "    * `input_schema`: The expected input schema for the node (typically \"GraphState\").\n",
    "    * `output_schema`: The schema of the output produced by the node (typically \"GraphState\", indicating a partial update).\n",
    "    * `description`: A natural language description of what the node does. This is crucial for determining implementation strategy and overall architecture.\n",
    "    * `function_name`: The suggested Python function name for this node.\n",
    "    * `code` (optional): A string containing Python code for the node's function. **Treat this `code` primarily as an illustration or a very basic version. Prioritize LLM-based solutions if the `description` suggests a more robust approach is needed.**\n",
    "\n",
    "2.  `edges`: A list of objects, each describing a directed edge in the graph. Each edge object contains:\n",
    "    * `source`: The ID of the source node (or \"__START__\" for the graph's entry point).\n",
    "    * `target`: The ID of the target node (or \"__END__\" for a graph termination point).\n",
    "    * `routing_conditions`: A natural language description of the condition under which this edge is taken, especially for conditional edges.\n",
    "    * `conditional`: A boolean flag, `true` if the edge is part of a conditional branch, `false` otherwise.\n",
    "\n",
    "---\n",
    "**Phase 1: Graph Architecture Analysis & Strategy**\n",
    "\n",
    "Before generating any code, analyze the overall workflow implied by the `nodes` and `edges` in the JSON.\n",
    "1.  **Identify Potential Architectures:** Consider if the described system aligns with or would benefit from known advanced LangGraph architectures such as:\n",
    "    * **Plan and Execute**: Does the graph imply a planning step (e.g., breaking down a complex task) followed by the execution of those plans by one or more action nodes?\n",
    "    * **Agent Supervisor / Hierarchical Agent Teams**: Do the nodes and their conditional routing suggest a supervisory agent dispatching tasks to specialized worker agents, or a hierarchy of agents making decisions and delegating?\n",
    "    * **Multi-Agent Collaboration (e.g., Swarm Architecture)**: Does the problem benefit from multiple agents working in parallel or collaboratively, perhaps sharing insights or contributing to a common goal?\n",
    "    * **Reflection / Self-Correction (e.g., Self-Discover frameworks)**: Are there indications of iterative refinement, where results are evaluated and the process is adjusted?\n",
    "    * **Human in the Loop (HITL)**: Does the `description` of any node, or the overall process, imply a need for human review, approval, correction, or explicit input at specific stages (e.g., before executing a critical action, when confidence is low, or for subjective assessments)?\n",
    "\n",
    "2.  **Architectural Decision:**\n",
    "    * If you determine that one or more of these architectures are strongly applicable and would create a more robust or intelligent system based on the JSON's intent, choose to implement it.\n",
    "    * This may require you to define additional coordinating nodes or logic (e.g., a dedicated 'planner' LLM-agent, a 'supervisor' LLM-agent that routes tasks) that are not explicitly listed as individual nodes in the input JSON but are integral to the chosen architecture.\n",
    "    * If no specific advanced architecture seems directly applicable or sufficiently justified by the input JSON, proceed with a standard stateful graph construction based on the explicit nodes and edges.\n",
    "\n",
    "3.  **Initial Comment:** At the very beginning of your generated Python script, include a comment block stating:\n",
    "    * Which LangGraph architecture(s) (if any) you've identified and chosen to implement, with a brief justification based on your interpretation of the input JSON.\n",
    "    * If you are proceeding with a standard graph, mention that.\n",
    "\n",
    "---\n",
    "**Phase 2: Python Code Generation**\n",
    "\n",
    "Generate a single, self-contained, and compilable Python script that implements your chosen strategy.\n",
    "\n",
    "1.  **Imports:** Include all necessary Python libraries (e.g., `typing`, `langgraph.graph`, `langgraph.checkpoint.memory`, LLM client libraries like `langchain_openai`, `langchain_google_genai`, `langchain_core.pydantic_v1`, `langchain_core.tools`, `re`).\n",
    "\n",
    "2.  **State Definition (`GraphState`):**\n",
    "    * Parse the `schema_info` string to define a `GraphState` class using `typing.TypedDict`.\n",
    "\n",
    "3.  **Node Implementation (Python Functions):**\n",
    "    For each conceptual node in your chosen architecture (these may map directly to JSON nodes or be new architectural nodes you define):\n",
    "    * Create a Python function. This function must accept the `GraphState` and return a dictionary representing the partial update to the state.\n",
    "    * **Decision Logic for Implementation (Prioritize LLM, No Mock Data):**\n",
    "        * **Default to LLM-Based Solutions:** Your default stance should be to implement an **LLM-based solution** if the node's `description` (from JSON or your architectural design) suggests tasks like:\n",
    "            * Natural Language Understanding (NLU)\n",
    "            * Complex classification or routing\n",
    "            * Content generation or summarization\n",
    "            * Tool selection and usage\n",
    "            * Planning or complex decision-making.\n",
    "            * Any task where an LLM would provide more robust, flexible, or intelligent behavior than simple hardcoded logic.\n",
    "        * **Handling Provided `code`:** If `code` is present in the JSON for a node, treat it as a **low-priority hint or a simplistic example**. Do **not** simply copy it if an LLM approach is more appropriate for the described task.\n",
    "        * **Algorithmic Logic (Use Sparingly):** Only use purely algorithmic Python code (like from the `code` attribute or written new) if the node's task is genuinely simple, deterministic (e.g., basic data formatting, fixed calculation), *and* an LLM would offer no significant benefit for that specific, narrow function.\n",
    "        * **Functional LLM Calls:** When an LLM is used, instantiate a generic model (e.g., `llm = ChatOpenAI(model=\"gpt-3.5-turbo\")` or `llm = ChatGoogleGenerativeAI(model=\"gemini-pro\")`) and include a **functional, descriptive prompt** relevant to the node's task. Ensure the code for the LLM call is complete and not just a comment. Add a `TODO` comment for the user to specify API keys and potentially refine the model/prompt.\n",
    "        * **No Mock Data:** Generated functions must be logical and aim for completeness. **Avoid using mock data or overly simplistic placeholder logic** where an LLM or a proper algorithmic implementation is expected.\n",
    "        * **Structured Output & Tools:** If the task implies structured output from an LLM or the use of tools, define necessary Pydantic models and/or LangChain tools, and integrate them with the LLM call.\n",
    "            * Define a Pydantic model (e.g., `from langchain_core.pydantic_v1 import BaseModel, Field`) representing the desired structured output.\n",
    "            * If implementing an LLM call, configure it to use the Pydantic model for its output (e.g., with OpenAI's function calling/tool usage features, or by instructing the LLM to generate JSON conforming to the model).\n",
    "        * **Tool Definition and Usage:** If a node's `description` (or your architectural design) implies the LLM within that node needs to interact with external systems, perform specific actions, or fetch data (e.g., \"search customer database,\" \"get weather update\"):\n",
    "                * Define these capabilities as discrete LangChain tools using the `@tool` decorator (e.g., `from langchain_core.tools import tool`).\n",
    "                * **Crucially, each tool's internal Python function should be self-contained and directly perform its advertised action** (e.g., make a specific API call to an external service, run a local script, perform a calculation, retrieve data algorithmically). **Avoid embedding a *new, separate general-purpose LLM call within the tool's own implementation logic* unless the tool's explicit and documented purpose is to be a specialized, self-contained sub-agent (which is an advanced case).** The primary LLM within the graph node is responsible for *deciding to call* the tool and for interpreting its output.\n",
    "                * Bind these well-defined tools to the LLM instance operating within that graph node. The node's LLM will then intelligently decide when to call a tool and with what inputs.\n",
    "        * **Human in the Loop Nodes:** If you've designed a HITL step as a dedicated node, its function might primarily format data for human review and then process the subsequent human input (which would be added to the state, potentially by an external mechanism or a subsequent node). The graph might pause using an interruption mechanism tied to this node.\n",
    "        * **State Coherence:** Ensure variable assignments and updates within node functions are coherent with the `GraphState` definition and how state is managed in LangGraph.\n",
    "\n",
    "4.  **Graph Construction (`StatefulGraph`):**\n",
    "    * Instantiate `StatefulGraph(GraphState)`.\n",
    "    * Add each implemented node function to the graph using `graph.add_node(\"node_id\", node_function)`.\n",
    "    * Set the graph's entry point using `graph.add_edge(START, \"entry_node_id\")` where `\"entry_node_id\"` is the target of the edge originating from `\"__START__\"`.\n",
    "\n",
    "5.  **Edge Implementation:**\n",
    "    * Iterate through the `edges` list in the JSON.\n",
    "    * **Regular Edges:** If `conditional` is `false`:\n",
    "        * If `target` is `__END__`, use `graph.add_edge(source_node_id, END)`.\n",
    "        * Otherwise, use `graph.add_edge(source_node_id, target_node_id)`.\n",
    "    * **Conditional Edges:** If `conditional` is `true`:\n",
    "        * The `source` node of these conditional edges is expected to produce some output in the `GraphState` (e.g., an `intent` field) that determines the next path.\n",
    "        * Create a separate routing function (e.g., `def route_after_source_node(state: GraphState) -> str:`).\n",
    "        * This routing function must inspect the relevant fields in the `state` and return the string ID of the next node to execute, based on the logic described in the `routing_conditions` for each conditional edge originating from that source.\n",
    "        * Use `graph.add_conditional_edges(source_node_id, routing_function, {{ \"target_id_1\": \"target_id_1\", \"target_id_2\": \"target_id_2\", ... \"__END__\": END }})`. The keys in the dictionary are the possible return values from your routing function, and the values are the actual node IDs or `END`.\n",
    "\n",
    "6.  **Compilation:**\n",
    "    * Instantiate an `InMemoryCheckpointer`: `checkpointer = InMemoryCheckpointer()`.\n",
    "    * Compile the graph: `final_app = graph.compile(checkpointer=checkpointer)`. The compiled graph must be assigned to a variable named `final_app`.\n",
    "\n",
    "---\n",
    "**Phase 3: Required Keys/Credentials Identification**\n",
    "\n",
    "After generating the complete Python script, add a separate section at the end of your response, clearly titled:\n",
    "`## Required Keys and Credentials`\n",
    "\n",
    "In this section, list all environment variables or API keys a user would need to set for the generated code to execute successfully (e.g., `OPENAI_API_KEY`, `GOOGLE_API_KEY`, tool-specific keys). If no external keys are needed, state that.\n",
    "\n",
    "---\n",
    "**Important Considerations (General):**\n",
    "* The primary goal is **compilable, logical, and functionally plausible Python code** that intelligently interprets the JSON input.\n",
    "* Focus on creating a system that leverages LLMs effectively for tasks suited to them.\n",
    "* Ensure node functions correctly update and return relevant parts of the `GraphState`.\n",
    "* If the provided `code` in the JSON uses specific libraries (e.g., `re`), make sure the corresponding import is included at the top of the script.\n",
    "* Handle `__START__` and `__END__` correctly in edge definitions. `langgraph.graph.START` and `langgraph.graph.END` should be used.\n",
    "\n",
    "Here is the JSON input:\n",
    "<json>\n",
    "{code_json}\n",
    "</json>\n",
    "\n",
    "Please generate the Python code and the list of required keys now:\n",
    "\"\"\")\n",
    "\n",
    "def code_node(state: AgentBuilderState):\n",
    "    \"\"\"Produce the final python code\"\"\"\n",
    "    code_ouptut = llm.invoke([HumanMessage(content=CODE_GEN_PROMPT.format(\n",
    "        code_json = state[\"json_code\"]\n",
    "        ))])\n",
    "    \n",
    "    # Return the JSON code as the output\n",
    "    return {\n",
    "        \"messages\": [AIMessage(content=\"Generated final python code!\")],\n",
    "        \"python_code\": code_ouptut.content,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "35028943",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User (q/Q to quit): I need to create a worklow with the objective of managing my social media.  It should be able to tell me trends from social media for sports, get me all the relevant people I should contant for a spsonsoring a specific post,  suggest me content I should be posting, make content for me if I give it a description * Identifying social media trends in sports. *   Finding relevant people for sponsoring specific posts. *   Suggesting content to post. *   Generating content based on a description.  Examples: Given I ask it about trends in football, it goes through recent viral reels in football and tell me Q&A reels are trending If I tell it I want people who would be interested in a post about UCL football, it gives me current players like Dembele etc If I tell it I want to make a post about UCL football, it goes through what is trending and an agent that thinks about social media posts and tells me we should post a highlight reel\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  AgentInstructions (4a1d150c-6843-453d-a3bb-d12be73c3880)\n",
      " Call ID: 4a1d150c-6843-453d-a3bb-d12be73c3880\n",
      "  Args:\n",
      "    examples: Given I ask it about trends in football, it goes through recent viral reels in football and tell me Q&A reels are trending If I tell it I want people who would be interested in a post about UCL football, it gives me current players like Dembele etc If I tell it I want to make a post about UCL football, it goes through what is trending and an agent that thinks about social media posts and tells me we should post a highlight reel\n",
      "    objective: managing my social media\n",
      "    usecases: ['Identifying social media trends in sports.', 'Finding relevant people for sponsoring specific posts.', 'Suggesting content to post.', 'Generating content based on a description.']\n",
      "reached here\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Generated agent kernel code!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-01 02:34:30,252 - INFO - HTTP Request: GET https://langchain-ai.github.io/langgraph/concepts/low_level/ \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Generated JSON code!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-01 02:35:23,730 - INFO - HTTP Request: GET https://langchain-ai.github.io/langgraph/concepts/low_level/ \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Generated updated JSON code!\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Generated final python code!\n",
      "User (q/Q to quit): q\n",
      "AI: Byebye\n"
     ]
    }
   ],
   "source": [
    "import uuid\n",
    "\n",
    "cached_human_responses = [\"hi!\", \"rag prompt\", \"1 rag, 2 none, 3 no, 4 no\", \"red\", \"q\"]\n",
    "cached_response_index = 0\n",
    "config = {\"configurable\": {\"thread_id\": str(uuid.uuid4())}}\n",
    "while True:\n",
    "    try:\n",
    "        user = input(\"User (q/Q to quit): \")\n",
    "    except:\n",
    "        user = cached_human_responses[cached_response_index]\n",
    "        cached_response_index += 1\n",
    "    print(f\"User (q/Q to quit): {user}\")\n",
    "    if user in {\"q\", \"Q\"}:\n",
    "        print(\"AI: Byebye\")\n",
    "        break\n",
    "    output = None\n",
    "    for output in infograph.stream(\n",
    "        {\"messages\": [HumanMessage(content=user)]}, config=config, stream_mode=\"updates\"\n",
    "    ):\n",
    "        last_message = next(iter(output.values()))[\"messages\"][-1]\n",
    "        last_message.pretty_print()\n",
    "\n",
    "    if output and \"prompt\" in output:\n",
    "        print(\"Done!\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a10f7aa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:2: SyntaxWarning: invalid escape sequence '\\A'\n",
      "<>:3: SyntaxWarning: invalid escape sequence '\\A'\n",
      "<>:2: SyntaxWarning: invalid escape sequence '\\A'\n",
      "<>:3: SyntaxWarning: invalid escape sequence '\\A'\n",
      "C:\\Users\\anupa\\AppData\\Local\\Temp\\ipykernel_24008\\3791049899.py:2: SyntaxWarning: invalid escape sequence '\\A'\n",
      "  dict_tool_link = json.load( open( \"D:\\AgentAgent\\AgentAgent\\experiments\\\\tool_creation\\\\tools_link_json.json\") )\n",
      "C:\\Users\\anupa\\AppData\\Local\\Temp\\ipykernel_24008\\3791049899.py:3: SyntaxWarning: invalid escape sequence '\\A'\n",
      "  dict_tool_doc = json.load( open( \"D:\\AgentAgent\\AgentAgent\\experiments\\\\tool_creation\\\\tools_doc_json.json\") )\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "dict_tool_link = json.load( open( \"D:\\AgentAgent\\AgentAgent\\experiments\\\\tool_creation\\\\tools_link_json.json\") )\n",
    "dict_tool_doc = json.load( open( \"D:\\AgentAgent\\AgentAgent\\experiments\\\\tool_creation\\\\tools_doc_json.json\") )\n",
    "def lowercase_keys(input_dict):\n",
    "    \"\"\"\n",
    "    Returns a new dictionary with all keys converted to lowercase.\n",
    "    \"\"\"\n",
    "    return {k.lower(): v for k, v in input_dict.items()}\n",
    "\n",
    "dict_tool_link = lowercase_keys(dict_tool_link)\n",
    "dict_tool_doc = lowercase_keys(dict_tool_doc)\n",
    "\n",
    "Initial_prompt = \"\"\"You are an expert python developer. You will be given a description of a python function. \n",
    "\n",
    "You job is to estimate and extract the following information:\n",
    "\n",
    "- What exactly does this python do. What is the detailed objective of the function. Please write 1-5 lines\n",
    "- Suggest or extract the name of the the function\n",
    "- What would be the inputs/arguements required into this function to make it work. Please all mentioned the type of each input\n",
    "- WHat would be output produced by this input. Please mention the output type \n",
    "\n",
    "Here is the description of the function you need to create:\n",
    "<description>\n",
    "{desc}\n",
    "</description>\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "class FunctionInstructions(BaseModel):\n",
    "    \"\"\"Instructions for defining a python function\"\"\"\n",
    "    objective: str = Field(description= \"what does this pythion function do\")\n",
    "    name: str = Field(description=\"name of the python function\")\n",
    "    input : List[str] = Field(description= \"what would be the input arguements to this function along with the types\")\n",
    "    output: List[str] = Field(description=\"what would be the output/return attributes for the function along with the types\")\n",
    "    name_toolkit: str = Field(description=\"what would be the toolkit/ code SDK that will be used\")\n",
    "    code: str = Field(description=\"the final python code\")\n",
    "# Annotated[str, operator.add]\n",
    "\n",
    "class CodebuilderState(BaseModel):\n",
    "    \"\"\"Instructions for defining a python function\"\"\"\n",
    "    code: str = Field(description= \"tailored code for the python function\")\n",
    "\n",
    "\n",
    "def functional_analysis_node(state: FunctionInstructions):\n",
    "  print(\"functional_analysis_node\")\n",
    "  llm_with_structured_output = llm.with_structured_output(FunctionInstructions)\n",
    "  functionalReport: FunctionInstructions = llm_with_structured_output.invoke(\n",
    "      [HumanMessage(content=Initial_prompt.format(desc = state.objective))])\n",
    "  return {  \"messages\": [AIMessage(content=\"Generated JSON code!\")],\n",
    "           \"objective\": functionalReport.objective,\n",
    "           \"name\": functionalReport.name,\n",
    "           \"input\": functionalReport.input,\n",
    "           \"output\": functionalReport.output}\n",
    "\n",
    "write_code_prompt = \"\"\"\n",
    "You are a skilled code generation assistant. Your task is to create executable code using the following information:\n",
    "- SDK Documentation: The provided documentation outlines the functionalities and usage details of the SDK. Use this as the reference for constructing your code.\n",
    "- Objective: A clear description of what the code is intended to achieve.\n",
    "- Input: The expected input for the code (e.g., variables, parameters, data types).\n",
    "- Output: The desired result or outcome of the code (e.g., format, type, or structure).\n",
    "- SDK Name: The name of the SDK that must be used in the code.\n",
    "\n",
    "Your goal is to generate executable code that:\n",
    "- Adheres to the requirements outlined above.\n",
    "- Follows standard coding practices and is optimized for readability and efficiency.\n",
    "- Utilizes the specified SDK appropriately based on the documentation provided.\n",
    "- Only return a self contained function\n",
    "- Your output should only contain a code block containing the required function and nothing else. Please do no include any explainantions\n",
    "- Write your code in python\n",
    "- Please also provide which API keys will be required and define the API keys as part of the function\n",
    "- Please also write the doc string for the python function\n",
    "- Ensure that the function you produce is decorated with @tool. That means its defination should be preceeded by '@tool' in the line above\n",
    "\n",
    "Here are some details about the python function you will be creating:\n",
    "<objective>\n",
    "{objective}\n",
    "</objective>\n",
    "\n",
    "<input schema>\n",
    "{inputs}\n",
    "</input schema>\n",
    "\n",
    "<output schema>\n",
    "{output}\n",
    "</output schema>\n",
    "\n",
    "<name of function>\n",
    "{name}\n",
    "</name of function>\n",
    "\n",
    "Documentation for SDK that might be helpful:\n",
    "<documentation>\n",
    "{docs}\n",
    "</documentation>\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "Best_sdk_prompt = \"\"\"\n",
    "You are a highly specialized language model designed to assist in selecting the most suitable SDK for a given use case. You are provided with the following:\n",
    "- A dictionary containing pairs of SDK names and their respective descriptions.\n",
    "- Requirements for a piece of code, including the objective, input, and output.\n",
    "\n",
    "Your task is to:\n",
    "- Identify the SDK from the provided dictionary whose description best matches the given use case described in the code requirements.\n",
    "- Also give preferences to SDKs that are generally more well known or are used more frequently in the industry (Use google tools for anything search related)\n",
    "- Return only the name of the matching SDK without any additional text or formatting.\n",
    "- Please ensure that the string you return is a valid key of the dictionary you get as input. PLEASE VERIFY THAT THE STRING YOU RETURN EXISTS AS A KEY IN THE INPUT DICTIONARY\n",
    "\n",
    "Input Example:\n",
    "Dictionary:\n",
    "{{\n",
    "\"SDK_A\": \"[SDK_CC_ABC]Provides tools for web scraping and data extraction.\",\n",
    "\"SDK_B\": \"Enables natural language processing for unstructured text.\",\n",
    "\"SDK_C\": \"Facilitates the integration of payment gateways in applications.\"\n",
    "}}\n",
    "Code Requirements:\n",
    "Objective: Extract data from multiple web pages.\n",
    "Input: URLs of the web pages.\n",
    "Output: Structured data in JSON format.\n",
    "\n",
    "Expected Output:\n",
    "SDK_A\n",
    "\n",
    "\n",
    "Input :\n",
    "<dictionary>\n",
    "{dictionary}\n",
    "</dictionary>\n",
    "\n",
    "<objective>\n",
    "{objective}\n",
    "</objective>\n",
    "\n",
    "<input schema>\n",
    "{inputs}\n",
    "</input schema>\n",
    "\n",
    "<output schema>\n",
    "{output}\n",
    "</output schema>\n",
    "\n",
    "<name of function>\n",
    "{name}\n",
    "</name of function>\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def sdk_production_node(state: FunctionInstructions):\n",
    "    objective_agent: str = state.objective\n",
    "    name: str = state.name\n",
    "    input_args : List[str] = state.input\n",
    "    output_args: List[str] = state.output\n",
    "    response = llm.invoke([HumanMessage(content=Best_sdk_prompt.format(\n",
    "          objective=objective_agent,\n",
    "          inputs=input_args,\n",
    "          output=output_args,\n",
    "          name=name,\n",
    "          dictionary = dict_tool_doc\n",
    "    ))])\n",
    "    code_snips = response\n",
    "    return { \"messages\": [AIMessage(content=\"SDK identified\")],\n",
    "            \"name_toolkit\": response.content.lower()}\n",
    "\n",
    "def code_production_node(state: FunctionInstructions):\n",
    "    objective_agent: str = state.objective\n",
    "    name: str = state.name\n",
    "    input_args : List[str] = state.input\n",
    "    output_args: List[str] = state.output\n",
    "    toolkit: str = state.name_toolkit\n",
    "    docs = fetch_documents(dict_tool_link[toolkit])\n",
    "    response = llm.invoke([HumanMessage(content=write_code_prompt.format(\n",
    "          objective=objective_agent,\n",
    "          inputs=input_args,\n",
    "          output=output_args,\n",
    "          name=name,\n",
    "          docs = docs,\n",
    "    ))])\n",
    "    code_snips = response\n",
    "    return {\"messages\": [AIMessage(content=\"Generated code for tool\")],\n",
    "            \"code\": response.content}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c3a7f1fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "workflow = StateGraph(FunctionInstructions)\n",
    "workflow.add_node(\"func_analysis\", functional_analysis_node)\n",
    "workflow.add_node(\"sdk_write\", sdk_production_node)\n",
    "workflow.add_node(\"code_write\", code_production_node)\n",
    "checkpointer = InMemorySaver()\n",
    "workflow.add_edge(\"code_write\", END)\n",
    "workflow.add_edge(\"sdk_write\",\"code_write\")\n",
    "workflow.add_edge(\"func_analysis\",\"sdk_write\")\n",
    "workflow.add_edge(START, \"func_analysis\")\n",
    "tool_infograph = workflow.compile(checkpointer=checkpointer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "7f583b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "class toolcollector(MessagesState):\n",
    "    total_code: List[str]\n",
    "    compiled_code: str\n",
    "test_message = \"\"\"\n",
    "def get_weather(location: str):\n",
    "    \\\"\\\"\\\"Call to get the current weather.\\\"\\\"\\\"\n",
    "    if location.lower() in [\"sf\", \"san francisco\"]:\n",
    "        return \"It's 60 degrees and foggy.\"\n",
    "    else:\n",
    "        return \"It's 90 degrees and sunny.\"\n",
    "\n",
    "\n",
    "@tool\n",
    "def get_coolest_cities():\n",
    "    \\\"\\\"\\\"Get a list of coolest cities\\\"\\\"\\\"\n",
    "    return \"nyc, sf\"\n",
    "\n",
    "tools = [get_weather, get_coolest_cities]\n",
    "\n",
    "# Bind the model(llm) with tools\n",
    "model_with_tools = ChatAnthropic(\n",
    "    model=\"claude-3-haiku-20240307\", temperature=0\n",
    ").bind_tools(tools)\n",
    "\n",
    "# Generate a tool node.\n",
    "tool_node = ToolNode(tools)\n",
    "\n",
    "# conditional edge\n",
    "def should_continue(state: MessagesState):\n",
    "    messages = state[\"messages\"]\n",
    "    last_message = messages[-1]\n",
    "    if last_message.tool_calls:\n",
    "        return \"tools\"\n",
    "    return END\n",
    "\"\"\"\n",
    "tool_desc_prompt = \"\"\"\n",
    "You are an AI assistant designed to analyze Python code. Your task is to identify all function definitions in the provided Python snippet that are decorated with @tool. You must return a dictionary where:\n",
    "- The keys are the names of the identified functions.\n",
    "- You only need to pick up a function if it is decorated with '@tool' or '@tool' just preceeds the function. Otherwise leave the function alone\n",
    "- The values are descriptions of what each function is supposed to do. If a function contains a docstring, extract it as the description. If a docstring is missing, infer the function's purpose from its structure and comments.\n",
    "- The output should just be a json. it should not include \"```json\" and \"```\" at the start or end of it. it should start with a \"{{\" and end with a \"}}\"\n",
    "Example Input:\n",
    "@tool\n",
    "def calculate_area(length, width):\n",
    "    \"Calculates the area of a rectangle.\"\n",
    "    return length * width\n",
    "\n",
    "@tool\n",
    "def greet(name):\n",
    "    return f\"Hello, {{name}}!\"\n",
    "\n",
    "\n",
    "Expected Output:\n",
    "{{\n",
    "    \"calculate_area\": \"Calculates the area of a rectangle.\",\n",
    "    \"greet\": \"Greets a user by name.\"\n",
    "}}\n",
    "\n",
    "\n",
    "Instructions:\n",
    "- Identify functions that have the @tool decorator.\n",
    "- Extract function names and descriptions (either from docstrings or inferred).\n",
    "- Return the output as a structured JSON.\n",
    "- Please only return a json object that can be converted into a json directly. DO NOT RETURN ANYTHING OTHER THAN A JSON\n",
    "\n",
    "Python code:\n",
    "<code>\n",
    "{code}\n",
    "</code>\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "tool_compile_prompt = \"\"\"\n",
    "You are python code writing expert. You are given 2 snippets of code, your job is to combine them. \n",
    "The first snippet of code contains a compilable code with some functions compilable but empty. \n",
    "The second snippet of code contains the defination of those functions. \n",
    "Please fo through the second snippet of code, match the function in the first snippet and replace the functional definition written in the first snippet with one found in second snippet\n",
    "\n",
    "Please only return compilable python code\n",
    "Here are the code snippets:\n",
    "<code_snippet1>\n",
    "{complete_code}\n",
    "</code_snippet1>\n",
    "<code_snippet2>\n",
    "{functions}\n",
    "</code_snippet2>\n",
    "\"\"\"\n",
    "\n",
    "import uuid\n",
    "\n",
    "\n",
    "def graph_map_step(state: toolcollector):\n",
    "    # Extract nodes and edges from json_objects\n",
    "    current_code = state['compiled_code']\n",
    "    print(current_code)\n",
    "    response_1 = llm.invoke([HumanMessage(content=tool_desc_prompt.format(code = current_code))])\n",
    "    response_content =  response_1.content\n",
    "    if(response_content[0]=='j'):\n",
    "        json_objects = json.loads(response_content[8:-3])\n",
    "    else:\n",
    "        json_objects = json.loads(response_content)\n",
    "    print(json_objects)\n",
    "    uuid_str = uuid.uuid4()\n",
    "    config = {\"configurable\": {\"thread_id\": str(uuid_str)}}\n",
    "    send = []\n",
    "    for key in json_objects:\n",
    "        print(key + \" \" + json_objects[key])\n",
    "        for output in tool_infograph.stream({\"objective\":key + \" \" + json_objects[key], \"name\": key, \"input\":[], \"output\": [], \"name_toolkit\": \"\", \"code\":\"\"}, config, stream_mode=\"updates\"):\n",
    "            print(output)\n",
    "        send.append(tool_infograph.get_state(config).values[\"code\"])\n",
    "    return {\n",
    "        \"messages\": [AIMessage(content=\"Generated compiled code!\")],\n",
    "        \"total_code\": send,\n",
    "        \"compiled_code\" : current_code\n",
    "    }\n",
    "\n",
    "def compile_code(state: toolcollector):\n",
    "    tool_code_list = state['total_code']\n",
    "    normal_code = state['compiled_code']\n",
    "    full_tool_code = \" \".join(tool_code_list)\n",
    "    response = llm.invoke([HumanMessage(content=tool_compile_prompt.format(complete_code = normal_code, functions = full_tool_code))])\n",
    "    return {\n",
    "        \"messages\": response.content\n",
    "    }\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "7af79008",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow1 = StateGraph(toolcollector)\n",
    "# workflow1.add_node(\"tool_infograph\", tool_infograph)\n",
    "workflow1.add_node(\"graph_map_step\", graph_map_step)\n",
    "workflow1.add_node(\"compile_code\", compile_code)\n",
    "\n",
    "workflow1.add_edge(START, \"graph_map_step\")\n",
    "workflow1.add_edge(\"graph_map_step\",\"compile_code\")\n",
    "workflow1.add_edge(\"compile_code\", END)\n",
    "tool_compile = workflow1.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "514e2b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_subgraph(state: AgentBuilderState):\n",
    "    subgraph_output = tool_compile.invoke({\"total_code\": [], \"compiled_code\": state[\"python_code\"]})  \n",
    "    return {\"python_code\": subgraph_output[\"compiled_code\"]}  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "d86e4ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = StateGraph(AgentBuilderState)\n",
    "workflow.add_node(\"requirement_analysis\", requirement_analysis_node)\n",
    "workflow.add_node(\"agent_kernel_builder\", agent_kernel_builder)\n",
    "workflow.add_node(\"code_to_json\", code_to_json_node)\n",
    "workflow.add_node(\"json_update\", json_better_node)\n",
    "workflow.add_node(\"code_node\",code_node)\n",
    "# workflow.add_node(\"tool_compile\",tool_compile)\n",
    "workflow.add_node(\"node_1\", call_subgraph)\n",
    "\n",
    "@workflow.add_node\n",
    "def add_tool_message(state: AgentBuilderState):\n",
    "    \n",
    "   \n",
    "    return {\n",
    "        \"messages\": [\n",
    "            ToolMessage(\n",
    "                content=\"Requirements generated!\",\n",
    "                tool_call_id=state[\"messages\"][-1].tool_calls[0][\"id\"],\n",
    "            )\n",
    "        ]\n",
    "    }\n",
    "\n",
    "workflow.add_edge(\"node_1\", END)\n",
    "workflow.add_edge(\"code_node\", \"node_1\")\n",
    "workflow.add_edge(\"json_update\", \"code_node\")\n",
    "workflow.add_edge(\"code_to_json\", \"json_update\")\n",
    "workflow.add_edge(\"agent_kernel_builder\", \"code_to_json\")\n",
    "workflow.add_conditional_edges(\"requirement_analysis\", route_state, [\"agent_kernel_builder\", \"requirement_analysis\", END])\n",
    "workflow.add_edge(START, \"requirement_analysis\")\n",
    "infograph = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "dfaaac35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User (q/Q to quit): I need to create a worklow with the objective of managing my social media.  It should be able to tell me trends from social media for sports, get me all the relevant people I should contant for a spsonsoring a specific post,  suggest me content I should be posting, make content for me if I give it a description * Identifying social media trends in sports. *   Finding relevant people for sponsoring specific posts. *   Suggesting content to post. *   Generating content based on a description.  Examples: Given I ask it about trends in football, it goes through recent viral reels in football and tell me Q&A reels are trending If I tell it I want people who would be interested in a post about UCL football, it gives me current players like Dembele etc If I tell it I want to make a post about UCL football, it goes through what is trending and an agent that thinks about social media posts and tells me we should post a highlight reel\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  AgentInstructions (d4a03c63-c6b5-4f82-a36e-9f956bfc4047)\n",
      " Call ID: d4a03c63-c6b5-4f82-a36e-9f956bfc4047\n",
      "  Args:\n",
      "    examples: Given I ask it about trends in football, it goes through recent viral reels in football and tell me Q&A reels are trending If I tell it I want people who would be interested in a post about UCL football, it gives me current players like Dembele etc If I tell it I want to make a post about UCL football, it goes through what is trending and an agent that thinks about social media posts and tells me we should post a highlight reel\n",
      "    usecases: ['Identifying social media trends in sports.', 'Finding relevant people for sponsoring specific posts.', 'Suggesting content to post.', 'Generating content based on a description.']\n",
      "    objective: managing my social media\n",
      "reached here\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Generated agent kernel code!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-01 15:41:41,934 - INFO - HTTP Request: GET https://langchain-ai.github.io/langgraph/concepts/low_level/ \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Generated JSON code!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-01 15:42:33,403 - INFO - HTTP Request: GET https://langchain-ai.github.io/langgraph/concepts/low_level/ \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Generated updated JSON code!\n",
      "```python\n",
      "import operator\n",
      "from typing import TypedDict, List, Optional, Dict, Any\n",
      "\n",
      "from langgraph.graph import StateGraph, START, END\n",
      "from langgraph.checkpoint.memory import InMemoryCheckpointer\n",
      "\n",
      "# TODO: Choose your preferred LLM client.\n",
      "# For OpenAI:\n",
      "from langchain_openai import ChatOpenAI\n",
      "# For Google GenAI:\n",
      "# from langchain_google_genai import ChatGoogleGenerativeAI\n",
      "\n",
      "from langchain_core.pydantic_v1 import BaseModel, Field\n",
      "from langchain_core.tools import tool\n",
      "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage\n",
      "from langchain_core.output_parsers import StrOutputParser\n",
      "from langchain_core.prompts import ChatPromptTemplate\n",
      "\n",
      "# --- Phase 1: Graph Architecture Analysis & Strategy ---\n",
      "# This graph implements a Router Agent / Agent Supervisor architecture.\n",
      "# The 'parse_input' node acts as a central router, analyzing the user's query\n",
      "# to determine the primary task. Based on this task, it conditionally dispatches\n",
      "# the workflow to specialized \"worker\" nodes (identify_trends, find_sponsors,\n",
      "# suggest_content, generate_content). After the specialized task is completed,\n",
      "# all paths converge to a 'final_response' node, which summarizes the results.\n",
      "# This architecture is chosen because the input JSON clearly defines a\n",
      "# decision-making node ('parse_input') that routes to distinct, specialized\n",
      "# functional nodes, which is a hallmark of a supervisor pattern.\n",
      "\n",
      "# --- Phase 2: Python Code Generation ---\n",
      "\n",
      "# TODO: Initialize your LLM.\n",
      "# Ensure you have the necessary API key set as an environment variable (e.g., OPENAI_API_KEY, GOOGLE_API_KEY).\n",
      "# Example for OpenAI:\n",
      "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
      "# Example for Google GenAI:\n",
      "# llm = ChatGoogleGenerativeAI(model=\"gemini-pro\", temperature=0)\n",
      "\n",
      "# 1. State Definition\n",
      "class GraphState(TypedDict):\n",
      "    \"\"\"\n",
      "    Represents the state of our graph.\n",
      "\n",
      "    Attributes:\n",
      "        user_query: The initial query from the user.\n",
      "        task: The identified task from the user's query (e.g., \"identify_trends\", \"find_sponsors\").\n",
      "        sport: The specific sport mentioned in the query.\n",
      "        topic: A general topic or subject from the query.\n",
      "        trends: A list of identified social media trends.\n",
      "        relevant_people: A list of relevant people/influencers.\n",
      "        content_suggestion: A suggested content idea.\n",
      "        generated_content: The actual generated content.\n",
      "        chat_history: A list of chat messages for conversational context.\n",
      "        error: Any error message encountered during processing.\n",
      "    \"\"\"\n",
      "    user_query: str\n",
      "    task: Optional[str]\n",
      "    sport: Optional[str]\n",
      "    topic: Optional[str]\n",
      "    trends: Optional[List[str]]\n",
      "    relevant_people: Optional[List[str]]\n",
      "    content_suggestion: Optional[str]\n",
      "    generated_content: Optional[str]\n",
      "    chat_history: List[BaseMessage]\n",
      "    error: Optional[str]\n",
      "\n",
      "# 2. Tool Definitions\n",
      "# These are placeholder tools. In a real application, they would interact with external APIs or databases.\n",
      "# Each tool's internal logic should be self-contained and perform its advertised action.\n",
      "\n",
      "@tool\n",
      "def social_media_trend_search(sport: str) -> List[str]:\n",
      "    \"\"\"\n",
      "    Searches for current social media trends related to a specific sport.\n",
      "    Args:\n",
      "        sport (str): The name of the sport (e.g., \"football\", \"basketball\").\n",
      "    Returns:\n",
      "        List[str]: A list of trending topics or hashtags.\n",
      "    \"\"\"\n",
      "    print(f\"---TOOL: social_media_trend_search for {sport}---\")\n",
      "    # Simulate an API call or database lookup\n",
      "    if \"football\" in sport.lower():\n",
      "        return [\"#UCLFinal\", \"#Messi\", \"#Ronaldo\", \"#PremierLeagueHighlights\"]\n",
      "    elif \"basketball\" in sport.lower():\n",
      "        return [\"#NBAFinals\", \"#LeBronJames\", \"#StephCurry\"]\n",
      "    elif \"tennis\" in sport.lower():\n",
      "        return [\"#Wimbledon\", \"#GrandSlam\"]\n",
      "    else:\n",
      "        return [f\"#{sport.replace(' ', '')}Trends\", \"#SportsNews\"]\n",
      "\n",
      "@tool\n",
      "def influencer_search(topic: str) -> List[str]:\n",
      "    \"\"\"\n",
      "    Finds relevant social media influencers or public figures for a given topic.\n",
      "    Args:\n",
      "        topic (str): The topic for which to find influencers (e.g., \"UCL football\", \"NBA analysis\").\n",
      "    Returns:\n",
      "        List[str]: A list of names of relevant people.\n",
      "    \"\"\"\n",
      "    print(f\"---TOOL: influencer_search for {topic}---\")\n",
      "    # Simulate an API call or database lookup\n",
      "    if \"UCL football\" in topic.lower():\n",
      "        return [\"Gary Lineker\", \"Rio Ferdinand\", \"Jamie Carragher\", \"Micah Richards\"]\n",
      "    elif \"NBA\" in topic.lower():\n",
      "        return [\"Shaquille O'Neal\", \"Stephen A. Smith\", \"Skip Bayless\"]\n",
      "    else:\n",
      "        return [f\"InfluencerA for {topic}\", f\"InfluencerB for {topic}\"]\n",
      "\n",
      "@tool\n",
      "def content_generator(description: str) -> str:\n",
      "    \"\"\"\n",
      "    Generates a short piece of social media content based on a description.\n",
      "    Args:\n",
      "        description (str): A description or prompt for the content to be generated.\n",
      "    Returns:\n",
      "        str: The generated social media content.\n",
      "    \"\"\"\n",
      "    print(f\"---TOOL: content_generator for '{description}'---\")\n",
      "    # Simulate content generation. In a real scenario, this might involve a more complex LLM call\n",
      "    # or a specialized content generation service.\n",
      "    return f\"Draft content based on '{description}': 'Get ready for an epic moment!  #SportsHighlights #MustWatch' (This is a placeholder. A real LLM would generate more detailed content here.)\"\n",
      "\n",
      "# 3. Node Implementations\n",
      "\n",
      "class ParseInputOutput(BaseModel):\n",
      "    \"\"\"Structured output schema for the parse_input_node.\"\"\"\n",
      "    task: str = Field(description=\"The primary objective of the user's query. Possible values: 'identify_trends', 'find_sponsors', 'suggest_content', 'generate_content', 'unknown'.\")\n",
      "    sport: Optional[str] = Field(default=None, description=\"The specific sport mentioned in the query, if any.\")\n",
      "    topic: Optional[str] = Field(default=None, description=\"A general topic or specific subject mentioned in the query, if any.\")\n",
      "\n",
      "def parse_input_node(state: GraphState) -> Dict[str, Any]:\n",
      "    \"\"\"\n",
      "    Parses the user's initial query to identify the task, sport, and topic.\n",
      "    This node acts as the router/supervisor, using an LLM for robust parsing.\n",
      "    \"\"\"\n",
      "    print(\"---NODE: parse_input_node---\")\n",
      "    user_query = state[\"user_query\"]\n",
      "    chat_history = state[\"chat_history\"]\n",
      "\n",
      "    parser_prompt = ChatPromptTemplate.from_messages([\n",
      "        (\"system\", \"\"\"You are an expert social media assistant. Your task is to analyze the user's query and extract the primary objective (task), any specific sport mentioned, and any general topic.\n",
      "        Possible tasks: \"identify_trends\", \"find_sponsors\", \"suggest_content\", \"generate_content\", \"unknown\".\n",
      "        - If the user asks about trends, the task is \"identify_trends\".\n",
      "        - If the user asks about people for sponsoring, the task is \"find_sponsors\".\n",
      "        - If the user asks for content suggestions, the task is \"suggest_content\".\n",
      "        - If the user asks to generate content, the task is \"generate_content\".\n",
      "        - If the query is ambiguous or cannot be categorized, set task to \"unknown\".\n",
      "\n",
      "        Return a JSON object strictly conforming to the ParseInputOutput schema.\n",
      "        \"\"\"),\n",
      "        (\"human\", \"{user_query}\")\n",
      "    ])\n",
      "    # Use LLM with structured output for robust and type-safe parsing\n",
      "    parser_chain = parser_prompt | llm.with_structured_output(ParseInputOutput)\n",
      "\n",
      "    try:\n",
      "        parsed_data = parser_chain.invoke({\"user_query\": user_query})\n",
      "        return {\n",
      "            \"task\": parsed_data.task,\n",
      "            \"sport\": parsed_data.sport,\n",
      "            \"topic\": parsed_data.topic,\n",
      "            \"chat_history\": chat_history + [HumanMessage(content=user_query)]\n",
      "        }\n",
      "    except Exception as e:\n",
      "        print(f\"Error parsing input: {e}\")\n",
      "        # Fallback to unknown task on parsing error and record the error\n",
      "        return {\n",
      "            \"task\": \"unknown\",\n",
      "            \"error\": f\"Failed to parse input: {e}\",\n",
      "            \"chat_history\": chat_history + [HumanMessage(content=user_query), AIMessage(content=f\"I had trouble understanding your request: {e}.\")]\n",
      "        }\n",
      "\n",
      "def identify_trends_node(state: GraphState) -> Dict[str, Any]:\n",
      "    \"\"\"\n",
      "    Identifies social media trends for a given sport using the social_media_trend_search tool.\n",
      "    \"\"\"\n",
      "    print(\"---NODE: identify_trends_node---\")\n",
      "    sport = state.get(\"sport\")\n",
      "    chat_history = state[\"chat_history\"]\n",
      "\n",
      "    if not sport:\n",
      "        return {\"error\": \"Sport not specified for trend identification.\",\n",
      "                \"chat_history\": chat_history + [AIMessage(content=\"Please specify a sport to identify trends.\")]}\n",
      "\n",
      "    try:\n",
      "        trends = social_media_trend_search.invoke({\"sport\": sport})\n",
      "        return {\"trends\": trends,\n",
      "                \"chat_history\": chat_history + [AIMessage(content=f\"Identified trends for {sport}: {', '.join(trends)}\")]}\n",
      "    except Exception as e:\n",
      "        return {\"error\": f\"Failed to identify trends: {e}\",\n",
      "                \"chat_history\": chat_history + [AIMessage(content=f\"An error occurred while identifying trends: {e}\")]}\n",
      "\n",
      "def find_sponsors_node(state: GraphState) -> Dict[str, Any]:\n",
      "    \"\"\"\n",
      "    Finds relevant people/influencers for a given topic using the influencer_search tool.\n",
      "    \"\"\"\n",
      "    print(\"---NODE: find_sponsors_node---\")\n",
      "    topic = state.get(\"topic\")\n",
      "    chat_history = state[\"chat_history\"]\n",
      "\n",
      "    if not topic:\n",
      "        return {\"error\": \"Topic not specified for finding sponsors.\",\n",
      "                \"chat_history\": chat_history + [AIMessage(content=\"Please specify a topic to find sponsors.\")]}\n",
      "\n",
      "    try:\n",
      "        people = influencer_search.invoke({\"topic\": topic})\n",
      "        return {\"relevant_people\": people,\n",
      "                \"chat_history\": chat_history + [AIMessage(content=f\"Found relevant people for '{topic}': {', '.join(people)}\")]}\n",
      "    except Exception as e:\n",
      "        return {\"error\": f\"Failed to find sponsors: {e}\",\n",
      "                \"chat_history\": chat_history + [AIMessage(content=f\"An error occurred while finding sponsors: {e}\")]}\n",
      "\n",
      "def suggest_content_node(state: GraphState) -> Dict[str, Any]:\n",
      "    \"\"\"\n",
      "    Suggests content ideas based on a topic and identified trends using an LLM.\n",
      "    \"\"\"\n",
      "    print(\"---NODE: suggest_content_node---\")\n",
      "    topic = state.get(\"topic\")\n",
      "    trends = state.get(\"trends\")\n",
      "    chat_history = state[\"chat_history\"]\n",
      "\n",
      "    if not topic:\n",
      "        return {\"error\": \"Topic not specified for content suggestion.\",\n",
      "                \"chat_history\": chat_history + [AIMessage(content=\"Please specify a topic for content suggestion.\")]}\n",
      "\n",
      "    suggestion_prompt = ChatPromptTemplate.from_messages([\n",
      "        (\"system\", \"\"\"You are a creative social media content strategist.\n",
      "        Based on the given topic and current social media trends, suggest a compelling content idea.\n",
      "        Consider what type of content (e.g., highlight reel, Q&A, short clip) would perform best.\n",
      "        Be concise and provide a single, actionable content idea.\"\"\"),\n",
      "        (\"human\", \"Topic: {topic}\\nTrends: {trends}\\nSuggest a content idea.\")\n",
      "    ])\n",
      "\n",
      "    trends_str = \", \".join(trends) if trends else \"No specific trends identified.\"\n",
      "    content_suggestion_chain = suggestion_prompt | llm | StrOutputParser()\n",
      "    try:\n",
      "        suggestion = content_suggestion_chain.invoke({\"topic\": topic, \"trends\": trends_str})\n",
      "        return {\"content_suggestion\": suggestion,\n",
      "                \"chat_history\": chat_history + [AIMessage(content=f\"Suggested content for '{topic}': {suggestion}\")]}\n",
      "    except Exception as e:\n",
      "        return {\"error\": f\"Failed to suggest content: {e}\",\n",
      "                \"chat_history\": chat_history + [AIMessage(content=f\"An error occurred while suggesting content: {e}\")]}\n",
      "\n",
      "def generate_content_node(state: GraphState) -> Dict[str, Any]:\n",
      "    \"\"\"\n",
      "    Generates content based on a description or suggested content using the content_generator tool.\n",
      "    \"\"\"\n",
      "    print(\"---NODE: generate_content_node---\")\n",
      "    content_description = state.get(\"content_suggestion\") or state.get(\"topic\")\n",
      "    chat_history = state[\"chat_history\"]\n",
      "\n",
      "    if not content_description:\n",
      "        return {\"error\": \"Content description or topic not available for generation.\",\n",
      "                \"chat_history\": chat_history + [AIMessage(content=\"Please provide a description or topic to generate content.\")]}\n",
      "\n",
      "    try:\n",
      "        generated_content = content_generator.invoke({\"description\": content_description})\n",
      "        return {\"generated_content\": generated_content,\n",
      "                \"chat_history\": chat_history + [AIMessage(content=f\"Generated content based on '{content_description}': {generated_content}\")]}\n",
      "    except Exception as e:\n",
      "        return {\"error\": f\"Failed to generate content: {e}\",\n",
      "                \"chat_history\": chat_history + [AIMessage(content=f\"An error occurred while generating content: {e}\")]}\n",
      "\n",
      "def final_response_node(state: GraphState) -> Dict[str, Any]:\n",
      "    \"\"\"\n",
      "    Crafts a final, coherent response to the user based on the accumulated state.\n",
      "    \"\"\"\n",
      "    print(\"---NODE: final_response_node---\")\n",
      "    task = state.get(\"task\")\n",
      "    trends = state.get(\"trends\")\n",
      "    relevant_people = state.get(\"relevant_people\")\n",
      "    content_suggestion = state.get(\"content_suggestion\")\n",
      "    generated_content = state.get(\"generated_content\")\n",
      "    error = state.get(\"error\")\n",
      "    user_query = state.get(\"user_query\")\n",
      "    chat_history = state[\"chat_history\"]\n",
      "\n",
      "    final_prompt = ChatPromptTemplate.from_messages([\n",
      "        (\"system\", \"\"\"You are a helpful social media assistant. Based on the user's original query and the results of the processing, provide a concise and helpful summary.\n",
      "        If there was an error, report it clearly.\n",
      "        If trends were identified, state them.\n",
      "        If relevant people were found, list them.\n",
      "        If content was suggested, provide the suggestion.\n",
      "        If content was generated, provide it.\n",
      "        If the task was unknown, politely state that you couldn't understand the request and offer to help with known tasks.\n",
      "        Always end with a polite closing.\n",
      "        \"\"\"),\n",
      "        (\"human\", \"Original Query: {user_query}\\nTask: {task}\\nTrends: {trends}\\nRelevant People: {relevant_people}\\nContent Suggestion: {content_suggestion}\\nGenerated Content: {generated_content}\\nError: {error}\")\n",
      "    ])\n",
      "\n",
      "    trends_str = \", \".join(trends) if trends else \"None identified.\"\n",
      "    people_str = \", \".join(relevant_people) if relevant_people else \"None found.\"\n",
      "    content_suggestion_str = content_suggestion if content_suggestion else \"None suggested.\"\n",
      "    generated_content_str = generated_content if generated_content else \"None generated.\"\n",
      "    error_str = f\"An error occurred: {error}\" if error else \"No errors.\"\n",
      "\n",
      "    final_response_chain = final_prompt | llm | StrOutputParser()\n",
      "    try:\n",
      "        final_message_content = final_response_chain.invoke({\n",
      "            \"user_query\": user_query,\n",
      "            \"task\": task,\n",
      "            \"trends\": trends_str,\n",
      "            \"relevant_people\": people_str,\n",
      "            \"content_suggestion\": content_suggestion_str,\n",
      "            \"generated_content\": generated_content_str,\n",
      "            \"error\": error_str\n",
      "        })\n",
      "        return {\"chat_history\": chat_history + [AIMessage(content=final_message_content)]}\n",
      "    except Exception as e:\n",
      "        return {\"error\": f\"Failed to craft final response: {e}\",\n",
      "                \"chat_history\": chat_history + [AIMessage(content=f\"An error occurred while preparing the final response: {e}\")]}\n",
      "\n",
      "\n",
      "# 4. Graph Construction\n",
      "graph = StateGraph(GraphState)\n",
      "\n",
      "# Add nodes\n",
      "graph.add_node(\"parse_input\", parse_input_node)\n",
      "graph.add_node(\"identify_trends\", identify_trends_node)\n",
      "graph.add_node(\"find_sponsors\", find_sponsors_node)\n",
      "graph.add_node(\"suggest_content\", suggest_content_node)\n",
      "graph.add_node(\"generate_content\", generate_content_node)\n",
      "graph.add_node(\"final_response\", final_response_node)\n",
      "\n",
      "# 5. Edge Implementation\n",
      "\n",
      "# Set the entry point\n",
      "graph.add_edge(START, \"parse_input\")\n",
      "\n",
      "# Define the routing function for conditional edges from 'parse_input'\n",
      "def route_parse_input(state: GraphState) -> str:\n",
      "    \"\"\"\n",
      "    Routes the graph based on the 'task' identified in the parse_input node.\n",
      "    \"\"\"\n",
      "    task = state.get(\"task\")\n",
      "    print(f\"---ROUTER: parse_input -> Task identified: {task}---\")\n",
      "    if task == \"identify_trends\":\n",
      "        return \"identify_trends\"\n",
      "    elif task == \"find_sponsors\":\n",
      "        return \"find_sponsors\"\n",
      "    elif task == \"suggest_content\":\n",
      "        return \"suggest_content\"\n",
      "    elif task == \"generate_content\":\n",
      "        return \"generate_content\"\n",
      "    else: # Includes \"unknown\" task or any parsing error\n",
      "        return \"final_response\"\n",
      "\n",
      "graph.add_conditional_edges(\n",
      "    \"parse_input\",\n",
      "    route_parse_input,\n",
      "    {\n",
      "        \"identify_trends\": \"identify_trends\",\n",
      "        \"find_sponsors\": \"find_sponsors\",\n",
      "        \"suggest_content\": \"suggest_content\",\n",
      "        \"generate_content\": \"generate_content\",\n",
      "        \"final_response\": \"final_response\" # Route to final_response for unknown/error tasks\n",
      "    }\n",
      ")\n",
      "\n",
      "# Add unconditional edges from task-specific nodes to final_response\n",
      "graph.add_edge(\"identify_trends\", \"final_response\")\n",
      "graph.add_edge(\"find_sponsors\", \"final_response\")\n",
      "graph.add_edge(\"suggest_content\", \"final_response\")\n",
      "graph.add_edge(\"generate_content\", \"final_response\")\n",
      "\n",
      "# Set the end point\n",
      "graph.add_edge(\"final_response\", END)\n",
      "\n",
      "# 6. Compilation\n",
      "checkpointer = InMemoryCheckpointer()\n",
      "final_app = graph.compile(checkpointer=checkpointer)\n",
      "\n",
      "# Example Usage (Optional, for testing)\n",
      "if __name__ == \"__main__\":\n",
      "    print(\"---Starting LangGraph Application---\")\n",
      "\n",
      "    # Test queries\n",
      "    queries = [\n",
      "        \"What are the latest trends in football?\",\n",
      "        \"Find me some influencers for UCL football.\",\n",
      "        \"Suggest a content idea about NBA highlights.\",\n",
      "        \"Generate a short post about the importance of sportsmanship.\",\n",
      "        \"What's up?\", # Unknown task\n",
      "        \"Find trends for tennis.\",\n",
      "        \"Who are some good people for sponsoring related to basketball analysis?\",\n",
      "        \"Suggest content for tennis trends.\",\n",
      "        \"Generate content about the latest football match.\"\n",
      "    ]\n",
      "\n",
      "    for i, query in enumerate(queries):\n",
      "        print(f\"\\n--- Running query {i+1}: '{query}' ---\")\n",
      "        initial_state = {\n",
      "            \"user_query\": query,\n",
      "            \"chat_history\": []\n",
      "        }\n",
      "        # The graph will run until it reaches END\n",
      "        for s in final_app.stream(initial_state):\n",
      "            if \"__end__\" not in s:\n",
      "                # Print intermediate state changes (optional)\n",
      "                # print(s)\n",
      "                pass\n",
      "            else:\n",
      "                final_state = s[\"__end__\"]\n",
      "                # print(f\"Final State: {final_state}\") # Uncomment to see full final state\n",
      "                print(f\"Agent Response: {final_state['chat_history'][-1].content}\")\n",
      "                print(\"--- End of query ---\")\n",
      "\n",
      "```\n",
      "\n",
      "## Required Keys and Credentials\n",
      "\n",
      "To run the generated Python code, you will need to set up the following environment variables or provide them directly in the code:\n",
      "\n",
      "*   **`OPENAI_API_KEY`**: If you choose to use `ChatOpenAI` as your Large Language Model.\n",
      "    *   You can set it in your environment: `export OPENAI_API_KEY='your_api_key_here'`\n",
      "    *   Or pass it directly during `ChatOpenAI` initialization (not recommended for production): `ChatOpenAI(openai_api_key=\"your_api_key_here\")`\n",
      "\n",
      "*   **`GOOGLE_API_KEY`**: If you choose to use `ChatGoogleGenerativeAI` as your Large Language Model.\n",
      "    *   You can set it in your environment: `export GOOGLE_API_KEY='your_api_key_here'`\n",
      "    *   Or pass it directly during `ChatGoogleGenerativeAI` initialization (not recommended for production): `ChatGoogleGenerativeAI(google_api_key=\"your_api_key_here\")`\n",
      "\n",
      "The placeholder tools (`social_media_trend_search`, `influencer_search`, `content_generator`) do not require external API keys as implemented. However, if these were replaced with real-world integrations (e.g., a social media analytics API, an influencer database API, a more advanced content generation service), those specific services would likely require their own respective API keys or authentication credentials.\n",
      ".\n",
      "```python\n",
      "import operator\n",
      "from typing import TypedDict, List, Optional, Dict, Any\n",
      "\n",
      "from langgraph.graph import StateGraph, START, END\n",
      "from langgraph.checkpoint.memory import InMemoryCheckpointer\n",
      "\n",
      "# TODO: Choose your preferred LLM client.\n",
      "# For OpenAI:\n",
      "from langchain_openai import ChatOpenAI\n",
      "# For Google GenAI:\n",
      "# from langchain_google_genai import ChatGoogleGenerativeAI\n",
      "\n",
      "from langchain_core.pydantic_v1 import BaseModel, Field\n",
      "from langchain_core.tools import tool\n",
      "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage\n",
      "from langchain_core.output_parsers import StrOutputParser\n",
      "from langchain_core.prompts import ChatPromptTemplate\n",
      "\n",
      "# --- Phase 1: Graph Architecture Analysis & Strategy ---\n",
      "# This graph implements a Router Agent / Agent Supervisor architecture.\n",
      "# The 'parse_input' node acts as a central router, analyzing the user's query\n",
      "# to determine the primary task. Based on this task, it conditionally dispatches\n",
      "# the workflow to specialized \"worker\" nodes (identify_trends, find_sponsors,\n",
      "# suggest_content, generate_content). After the specialized task is completed,\n",
      "# all paths converge to a 'final_response' node, which summarizes the results.\n",
      "# This architecture is chosen because the input JSON clearly defines a\n",
      "# decision-making node ('parse_input') that routes to distinct, specialized\n",
      "# functional nodes, which is a hallmark of a supervisor pattern.\n",
      "\n",
      "# --- Phase 2: Python Code Generation ---\n",
      "\n",
      "# TODO: Initialize your LLM.\n",
      "# Ensure you have the necessary API key set as an environment variable (e.g., OPENAI_API_KEY, GOOGLE_API_KEY).\n",
      "# Example for OpenAI:\n",
      "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
      "# Example for Google GenAI:\n",
      "# llm = ChatGoogleGenerativeAI(model=\"gemini-pro\", temperature=0)\n",
      "\n",
      "# 1. State Definition\n",
      "class GraphState(TypedDict):\n",
      "    \"\"\"\n",
      "    Represents the state of our graph.\n",
      "\n",
      "    Attributes:\n",
      "        user_query: The initial query from the user.\n",
      "        task: The identified task from the user's query (e.g., \"identify_trends\", \"find_sponsors\").\n",
      "        sport: The specific sport mentioned in the query.\n",
      "        topic: A general topic or subject from the query.\n",
      "        trends: A list of identified social media trends.\n",
      "        relevant_people: A list of relevant people/influencers.\n",
      "        content_suggestion: A suggested content idea.\n",
      "        generated_content: The actual generated content.\n",
      "        chat_history: A list of chat messages for conversational context.\n",
      "        error: Any error message encountered during processing.\n",
      "    \"\"\"\n",
      "    user_query: str\n",
      "    task: Optional[str]\n",
      "    sport: Optional[str]\n",
      "    topic: Optional[str]\n",
      "    trends: Optional[List[str]]\n",
      "    relevant_people: Optional[List[str]]\n",
      "    content_suggestion: Optional[str]\n",
      "    generated_content: Optional[str]\n",
      "    chat_history: List[BaseMessage]\n",
      "    error: Optional[str]\n",
      "\n",
      "# 2. Tool Definitions\n",
      "# These are placeholder tools. In a real application, they would interact with external APIs or databases.\n",
      "# Each tool's internal logic should be self-contained and perform its advertised action.\n",
      "\n",
      "@tool\n",
      "def social_media_trend_search(sport: str) -> List[str]:\n",
      "    \"\"\"\n",
      "    Searches for current social media trends related to a specific sport.\n",
      "    Args:\n",
      "        sport (str): The name of the sport (e.g., \"football\", \"basketball\").\n",
      "    Returns:\n",
      "        List[str]: A list of trending topics or hashtags.\n",
      "    \"\"\"\n",
      "    print(f\"---TOOL: social_media_trend_search for {sport}---\")\n",
      "    # Simulate an API call or database lookup\n",
      "    if \"football\" in sport.lower():\n",
      "        return [\"#UCLFinal\", \"#Messi\", \"#Ronaldo\", \"#PremierLeagueHighlights\"]\n",
      "    elif \"basketball\" in sport.lower():\n",
      "        return [\"#NBAFinals\", \"#LeBronJames\", \"#StephCurry\"]\n",
      "    elif \"tennis\" in sport.lower():\n",
      "        return [\"#Wimbledon\", \"#GrandSlam\"]\n",
      "    else:\n",
      "        return [f\"#{sport.replace(' ', '')}Trends\", \"#SportsNews\"]\n",
      "\n",
      "@tool\n",
      "def influencer_search(topic: str) -> List[str]:\n",
      "    \"\"\"\n",
      "    Finds relevant social media influencers or public figures for a given topic.\n",
      "    Args:\n",
      "        topic (str): The topic for which to find influencers (e.g., \"UCL football\", \"NBA analysis\").\n",
      "    Returns:\n",
      "        List[str]: A list of names of relevant people.\n",
      "    \"\"\"\n",
      "    print(f\"---TOOL: influencer_search for {topic}---\")\n",
      "    # Simulate an API call or database lookup\n",
      "    if \"UCL football\" in topic.lower():\n",
      "        return [\"Gary Lineker\", \"Rio Ferdinand\", \"Jamie Carragher\", \"Micah Richards\"]\n",
      "    elif \"NBA\" in topic.lower():\n",
      "        return [\"Shaquille O'Neal\", \"Stephen A. Smith\", \"Skip Bayless\"]\n",
      "    else:\n",
      "        return [f\"InfluencerA for {topic}\", f\"InfluencerB for {topic}\"]\n",
      "\n",
      "@tool\n",
      "def content_generator(description: str) -> str:\n",
      "    \"\"\"\n",
      "    Generates a short piece of social media content based on a description.\n",
      "    Args:\n",
      "        description (str): A description or prompt for the content to be generated.\n",
      "    Returns:\n",
      "        str: The generated social media content.\n",
      "    \"\"\"\n",
      "    print(f\"---TOOL: content_generator for '{description}'---\")\n",
      "    # Simulate content generation. In a real scenario, this might involve a more complex LLM call\n",
      "    # or a specialized content generation service.\n",
      "    return f\"Draft content based on '{description}': 'Get ready for an epic moment!  #SportsHighlights #MustWatch' (This is a placeholder. A real LLM would generate more detailed content here.)\"\n",
      "\n",
      "# 3. Node Implementations\n",
      "\n",
      "class ParseInputOutput(BaseModel):\n",
      "    \"\"\"Structured output schema for the parse_input_node.\"\"\"\n",
      "    task: str = Field(description=\"The primary objective of the user's query. Possible values: 'identify_trends', 'find_sponsors', 'suggest_content', 'generate_content', 'unknown'.\")\n",
      "    sport: Optional[str] = Field(default=None, description=\"The specific sport mentioned in the query, if any.\")\n",
      "    topic: Optional[str] = Field(default=None, description=\"A general topic or specific subject mentioned in the query, if any.\")\n",
      "\n",
      "def parse_input_node(state: GraphState) -> Dict[str, Any]:\n",
      "    \"\"\"\n",
      "    Parses the user's initial query to identify the task, sport, and topic.\n",
      "    This node acts as the router/supervisor, using an LLM for robust parsing.\n",
      "    \"\"\"\n",
      "    print(\"---NODE: parse_input_node---\")\n",
      "    user_query = state[\"user_query\"]\n",
      "    chat_history = state[\"chat_history\"]\n",
      "\n",
      "    parser_prompt = ChatPromptTemplate.from_messages([\n",
      "        (\"system\", \"\"\"You are an expert social media assistant. Your task is to analyze the user's query and extract the primary objective (task), any specific sport mentioned, and any general topic.\n",
      "        Possible tasks: \"identify_trends\", \"find_sponsors\", \"suggest_content\", \"generate_content\", \"unknown\".\n",
      "        - If the user asks about trends, the task is \"identify_trends\".\n",
      "        - If the user asks about people for sponsoring, the task is \"find_sponsors\".\n",
      "        - If the user asks for content suggestions, the task is \"suggest_content\".\n",
      "        - If the user asks to generate content, the task is \"generate_content\".\n",
      "        - If the query is ambiguous or cannot be categorized, set task to \"unknown\".\n",
      "\n",
      "        Return a JSON object strictly conforming to the ParseInputOutput schema.\n",
      "        \"\"\"),\n",
      "        (\"human\", \"{user_query}\")\n",
      "    ])\n",
      "    # Use LLM with structured output for robust and type-safe parsing\n",
      "    parser_chain = parser_prompt | llm.with_structured_output(ParseInputOutput)\n",
      "\n",
      "    try:\n",
      "        parsed_data = parser_chain.invoke({\"user_query\": user_query})\n",
      "        return {\n",
      "            \"task\": parsed_data.task,\n",
      "            \"sport\": parsed_data.sport,\n",
      "            \"topic\": parsed_data.topic,\n",
      "            \"chat_history\": chat_history + [HumanMessage(content=user_query)]\n",
      "        }\n",
      "    except Exception as e:\n",
      "        print(f\"Error parsing input: {e}\")\n",
      "        # Fallback to unknown task on parsing error and record the error\n",
      "        return {\n",
      "            \"task\": \"unknown\",\n",
      "            \"error\": f\"Failed to parse input: {e}\",\n",
      "            \"chat_history\": chat_history + [HumanMessage(content=user_query), AIMessage(content=f\"I had trouble understanding your request: {e}.\")]\n",
      "        }\n",
      "\n",
      "def identify_trends_node(state: GraphState) -> Dict[str, Any]:\n",
      "    \"\"\"\n",
      "    Identifies social media trends for a given sport using the social_media_trend_search tool.\n",
      "    \"\"\"\n",
      "    print(\"---NODE: identify_trends_node---\")\n",
      "    sport = state.get(\"sport\")\n",
      "    chat_history = state[\"chat_history\"]\n",
      "\n",
      "    if not sport:\n",
      "        return {\"error\": \"Sport not specified for trend identification.\",\n",
      "                \"chat_history\": chat_history + [AIMessage(content=\"Please specify a sport to identify trends.\")]}\n",
      "\n",
      "    try:\n",
      "        trends = social_media_trend_search.invoke({\"sport\": sport})\n",
      "        return {\"trends\": trends,\n",
      "                \"chat_history\": chat_history + [AIMessage(content=f\"Identified trends for {sport}: {', '.join(trends)}\")]}\n",
      "    except Exception as e:\n",
      "        return {\"error\": f\"Failed to identify trends: {e}\",\n",
      "                \"chat_history\": chat_history + [AIMessage(content=f\"An error occurred while identifying trends: {e}\")]}\n",
      "\n",
      "def find_sponsors_node(state: GraphState) -> Dict[str, Any]:\n",
      "    \"\"\"\n",
      "    Finds relevant people/influencers for a given topic using the influencer_search tool.\n",
      "    \"\"\"\n",
      "    print(\"---NODE: find_sponsors_node---\")\n",
      "    topic = state.get(\"topic\")\n",
      "    chat_history = state[\"chat_history\"]\n",
      "\n",
      "    if not topic:\n",
      "        return {\"error\": \"Topic not specified for finding sponsors.\",\n",
      "                \"chat_history\": chat_history + [AIMessage(content=\"Please specify a topic to find sponsors.\")]}\n",
      "\n",
      "    try:\n",
      "        people = influencer_search.invoke({\"topic\": topic})\n",
      "        return {\"relevant_people\": people,\n",
      "                \"chat_history\": chat_history + [AIMessage(content=f\"Found relevant people for '{topic}': {', '.join(people)}\")]}\n",
      "    except Exception as e:\n",
      "        return {\"error\": f\"Failed to find sponsors: {e}\",\n",
      "                \"chat_history\": chat_history + [AIMessage(content=f\"An error occurred while finding sponsors: {e}\")]}\n",
      "\n",
      "def suggest_content_node(state: GraphState) -> Dict[str, Any]:\n",
      "    \"\"\"\n",
      "    Suggests content ideas based on a topic and identified trends using an LLM.\n",
      "    \"\"\"\n",
      "    print(\"---NODE: suggest_content_node---\")\n",
      "    topic = state.get(\"topic\")\n",
      "    trends = state.get(\"trends\")\n",
      "    chat_history = state[\"chat_history\"]\n",
      "\n",
      "    if not topic:\n",
      "        return {\"error\": \"Topic not specified for content suggestion.\",\n",
      "                \"chat_history\": chat_history + [AIMessage(content=\"Please specify a topic for content suggestion.\")]}\n",
      "\n",
      "    suggestion_prompt = ChatPromptTemplate.from_messages([\n",
      "        (\"system\", \"\"\"You are a creative social media content strategist.\n",
      "        Based on the given topic and current social media trends, suggest a compelling content idea.\n",
      "        Consider what type of content (e.g., highlight reel, Q&A, short clip) would perform best.\n",
      "        Be concise and provide a single, actionable content idea.\"\"\"),\n",
      "        (\"human\", \"Topic: {topic}\\nTrends: {trends}\\nSuggest a content idea.\")\n",
      "    ])\n",
      "\n",
      "    trends_str = \", \".join(trends) if trends else \"No specific trends identified.\"\n",
      "    content_suggestion_chain = suggestion_prompt | llm | StrOutputParser()\n",
      "    try:\n",
      "        suggestion = content_suggestion_chain.invoke({\"topic\": topic, \"trends\": trends_str})\n",
      "        return {\"content_suggestion\": suggestion,\n",
      "                \"chat_history\": chat_history + [AIMessage(content=f\"Suggested content for '{topic}': {suggestion}\")]}\n",
      "    except Exception as e:\n",
      "        return {\"error\": f\"Failed to suggest content: {e}\",\n",
      "                \"chat_history\": chat_history + [AIMessage(content=f\"An error occurred while suggesting content: {e}\")]}\n",
      "\n",
      "def generate_content_node(state: GraphState) -> Dict[str, Any]:\n",
      "    \"\"\"\n",
      "    Generates content based on a description or suggested content using the content_generator tool.\n",
      "    \"\"\"\n",
      "    print(\"---NODE: generate_content_node---\")\n",
      "    content_description = state.get(\"content_suggestion\") or state.get(\"topic\")\n",
      "    chat_history = state[\"chat_history\"]\n",
      "\n",
      "    if not content_description:\n",
      "        return {\"error\": \"Content description or topic not available for generation.\",\n",
      "                \"chat_history\": chat_history + [AIMessage(content=\"Please provide a description or topic to generate content.\")]}\n",
      "\n",
      "    try:\n",
      "        generated_content = content_generator.invoke({\"description\": content_description})\n",
      "        return {\"generated_content\": generated_content,\n",
      "                \"chat_history\": chat_history + [AIMessage(content=f\"Generated content based on '{content_description}': {generated_content}\")]}\n",
      "    except Exception as e:\n",
      "        return {\"error\": f\"Failed to generate content: {e}\",\n",
      "                \"chat_history\": chat_history + [AIMessage(content=f\"An error occurred while generating content: {e}\")]}\n",
      "\n",
      "def final_response_node(state: GraphState) -> Dict[str, Any]:\n",
      "    \"\"\"\n",
      "    Crafts a final, coherent response to the user based on the accumulated state.\n",
      "    \"\"\"\n",
      "    print(\"---NODE: final_response_node---\")\n",
      "    task = state.get(\"task\")\n",
      "    trends = state.get(\"trends\")\n",
      "    relevant_people = state.get(\"relevant_people\")\n",
      "    content_suggestion = state.get(\"content_suggestion\")\n",
      "    generated_content = state.get(\"generated_content\")\n",
      "    error = state.get(\"error\")\n",
      "    user_query = state.get(\"user_query\")\n",
      "    chat_history = state[\"chat_history\"]\n",
      "\n",
      "    final_prompt = ChatPromptTemplate.from_messages([\n",
      "        (\"system\", \"\"\"You are a helpful social media assistant. Based on the user's original query and the results of the processing, provide a concise and helpful summary.\n",
      "        If there was an error, report it clearly.\n",
      "        If trends were identified, state them.\n",
      "        If relevant people were found, list them.\n",
      "        If content was suggested, provide the suggestion.\n",
      "        If content was generated, provide it.\n",
      "        If the task was unknown, politely state that you couldn't understand the request and offer to help with known tasks.\n",
      "        Always end with a polite closing.\n",
      "        \"\"\"),\n",
      "        (\"human\", \"Original Query: {user_query}\\nTask: {task}\\nTrends: {trends}\\nRelevant People: {relevant_people}\\nContent Suggestion: {content_suggestion}\\nGenerated Content: {generated_content}\\nError: {error}\")\n",
      "    ])\n",
      "\n",
      "    trends_str = \", \".join(trends) if trends else \"None identified.\"\n",
      "    people_str = \", \".join(relevant_people) if relevant_people else \"None found.\"\n",
      "    content_suggestion_str = content_suggestion if content_suggestion else \"None suggested.\"\n",
      "    generated_content_str = generated_content if generated_content else \"None generated.\"\n",
      "    error_str = f\"An error occurred: {error}\" if error else \"No errors.\"\n",
      "\n",
      "    final_response_chain = final_prompt | llm | StrOutputParser()\n",
      "    try:\n",
      "        final_message_content = final_response_chain.invoke({\n",
      "            \"user_query\": user_query,\n",
      "            \"task\": task,\n",
      "            \"trends\": trends_str,\n",
      "            \"relevant_people\": people_str,\n",
      "            \"content_suggestion\": content_suggestion_str,\n",
      "            \"generated_content\": generated_content_str,\n",
      "            \"error\": error_str\n",
      "        })\n",
      "        return {\"chat_history\": chat_history + [AIMessage(content=final_message_content)]}\n",
      "    except Exception as e:\n",
      "        return {\"error\": f\"Failed to craft final response: {e}\",\n",
      "                \"chat_history\": chat_history + [AIMessage(content=f\"An error occurred while preparing the final response: {e}\")]}\n",
      "\n",
      "\n",
      "# 4. Graph Construction\n",
      "graph = StateGraph(GraphState)\n",
      "\n",
      "# Add nodes\n",
      "graph.add_node(\"parse_input\", parse_input_node)\n",
      "graph.add_node(\"identify_trends\", identify_trends_node)\n",
      "graph.add_node(\"find_sponsors\", find_sponsors_node)\n",
      "graph.add_node(\"suggest_content\", suggest_content_node)\n",
      "graph.add_node(\"generate_content\", generate_content_node)\n",
      "graph.add_node(\"final_response\", final_response_node)\n",
      "\n",
      "# 5. Edge Implementation\n",
      "\n",
      "# Set the entry point\n",
      "graph.add_edge(START, \"parse_input\")\n",
      "\n",
      "# Define the routing function for conditional edges from 'parse_input'\n",
      "def route_parse_input(state: GraphState) -> str:\n",
      "    \"\"\"\n",
      "    Routes the graph based on the 'task' identified in the parse_input node.\n",
      "    \"\"\"\n",
      "    task = state.get(\"task\")\n",
      "    print(f\"---ROUTER: parse_input -> Task identified: {task}---\")\n",
      "    if task == \"identify_trends\":\n",
      "        return \"identify_trends\"\n",
      "    elif task == \"find_sponsors\":\n",
      "        return \"find_sponsors\"\n",
      "    elif task == \"suggest_content\":\n",
      "        return \"suggest_content\"\n",
      "    elif task == \"generate_content\":\n",
      "        return \"generate_content\"\n",
      "    else: # Includes \"unknown\" task or any parsing error\n",
      "        return \"final_response\"\n",
      "\n",
      "graph.add_conditional_edges(\n",
      "    \"parse_input\",\n",
      "    route_parse_input,\n",
      "    {\n",
      "        \"identify_trends\": \"identify_trends\",\n",
      "        \"find_sponsors\": \"find_sponsors\",\n",
      "        \"suggest_content\": \"suggest_content\",\n",
      "        \"generate_content\": \"generate_content\",\n",
      "        \"final_response\": \"final_response\" # Route to final_response for unknown/error tasks\n",
      "    }\n",
      ")\n",
      "\n",
      "# Add unconditional edges from task-specific nodes to final_response\n",
      "graph.add_edge(\"identify_trends\", \"final_response\")\n",
      "graph.add_edge(\"find_sponsors\", \"final_response\")\n",
      "graph.add_edge(\"suggest_content\", \"final_response\")\n",
      "graph.add_edge(\"generate_content\", \"final_response\")\n",
      "\n",
      "# Set the end point\n",
      "graph.add_edge(\"final_response\", END)\n",
      "\n",
      "# 6. Compilation\n",
      "checkpointer = InMemoryCheckpointer()\n",
      "final_app = graph.compile(checkpointer=checkpointer)\n",
      "\n",
      "# Example Usage (Optional, for testing)\n",
      "if __name__ == \"__main__\":\n",
      "    print(\"---Starting LangGraph Application---\")\n",
      "\n",
      "    # Test queries\n",
      "    queries = [\n",
      "        \"What are the latest trends in football?\",\n",
      "        \"Find me some influencers for UCL football.\",\n",
      "        \"Suggest a content idea about NBA highlights.\",\n",
      "        \"Generate a short post about the importance of sportsmanship.\",\n",
      "        \"What's up?\", # Unknown task\n",
      "        \"Find trends for tennis.\",\n",
      "        \"Who are some good people for sponsoring related to basketball analysis?\",\n",
      "        \"Suggest content for tennis trends.\",\n",
      "        \"Generate content about the latest football match.\"\n",
      "    ]\n",
      "\n",
      "    for i, query in enumerate(queries):\n",
      "        print(f\"\\n--- Running query {i+1}: '{query}' ---\")\n",
      "        initial_state = {\n",
      "            \"user_query\": query,\n",
      "            \"chat_history\": []\n",
      "        }\n",
      "        # The graph will run until it reaches END\n",
      "        for s in final_app.stream(initial_state):\n",
      "            if \"__end__\" not in s:\n",
      "                # Print intermediate state changes (optional)\n",
      "                # print(s)\n",
      "                pass\n",
      "            else:\n",
      "                final_state = s[\"__end__\"]\n",
      "                # print(f\"Final State: {final_state}\") # Uncomment to see full final state\n",
      "                print(f\"Agent Response: {final_state['chat_history'][-1].content}\")\n",
      "                print(\"--- End of query ---\")\n",
      "\n",
      "```\n",
      "\n",
      "## Required Keys and Credentials\n",
      "\n",
      "To run the generated Python code, you will need to set up the following environment variables or provide them directly in the code:\n",
      "\n",
      "*   **`OPENAI_API_KEY`**: If you choose to use `ChatOpenAI` as your Large Language Model.\n",
      "    *   You can set it in your environment: `export OPENAI_API_KEY='your_api_key_here'`\n",
      "    *   Or pass it directly during `ChatOpenAI` initialization (not recommended for production): `ChatOpenAI(openai_api_key=\"your_api_key_here\")`\n",
      "\n",
      "*   **`GOOGLE_API_KEY`**: If you choose to use `ChatGoogleGenerativeAI` as your Large Language Model.\n",
      "    *   You can set it in your environment: `export GOOGLE_API_KEY='your_api_key_here'`\n",
      "    *   Or pass it directly during `ChatGoogleGenerativeAI` initialization (not recommended for production): `ChatGoogleGenerativeAI(google_api_key=\"your_api_key_here\")`\n",
      "\n",
      "The placeholder tools (`social_media_trend_search`, `influencer_search`, `content_generator`) do not require external API keys as implemented. However, if these were replaced with real-world integrations (e.g., a social media analytics API, an influencer database API, a more advanced content generation service), those specific services would likely require their own respective API keys or authentication credentials.\n",
      "{'social_media_trend_search': 'Searches for current social media trends related to a specific sport.', 'influencer_search': 'Finds relevant social media influencers or public figures for a given topic.', 'content_generator': 'Generates a short piece of social media content based on a description.'}\n",
      "social_media_trend_search Searches for current social media trends related to a specific sport.\n",
      "functional_analysis_node\n",
      "{'func_analysis': {'objective': 'Searches for current social media trends related to a specific sport.', 'name': 'social_media_trend_search', 'input': ['sport: str'], 'output': ['trends: list[str]']}}\n",
      "{'sdk_write': {'name_toolkit': 'google_trends'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-01 15:44:37,020 - INFO - HTTP Request: GET https://python.langchain.com/docs/integrations/tools/google_trends \"HTTP/1.1 308 Permanent Redirect\"\n",
      "2025-06-01 15:44:37,078 - INFO - HTTP Request: GET https://python.langchain.com/docs/integrations/tools/google_trends/ \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'code_write': {'code': '```python\\nfrom langchain_community.tools.google_trends import GoogleTrendsQueryRun\\nfrom langchain_community.utilities.google_trends import GoogleTrendsAPIWrapper\\nfrom typing import List\\nfrom tool import tool\\n\\n@tool\\ndef social_media_trend_search(sport: str, serpapi_api_key: str) -> List[str]:\\n    \"\"\"\\n    Searches for current social media trends related to a specific sport using Google Trends.\\n\\n    Args:\\n        sport (str): The name of the sport to search for trends.\\n        serpapi_api_key (str): Your SerpApi key for accessing Google Trends data.\\n\\n    Returns:\\n        list[str]: A list of social media trends related to the specified sport.\\n                   This includes both rising and top related queries.\\n    \"\"\"\\n    api_wrapper = GoogleTrendsAPIWrapper(serp_api_key=serpapi_api_key)\\n    google_trends_tool = GoogleTrendsQueryRun(api_wrapper=api_wrapper)\\n\\n    query_result = google_trends_tool.run(sport)\\n\\n    trends = []\\n    # Parse the output string to extract related queries\\n    # The output format is \"Rising Related Queries: ...,Top Related Queries: ...\"\\n    if \"Rising Related Queries:\" in query_result:\\n        rising_start = query_result.find(\"Rising Related Queries:\") + len(\"Rising Related Queries:\")\\n        top_start = query_result.find(\"Top Related Queries:\")\\n        \\n        rising_queries_str = \"\"\\n        if top_start != -1:\\n            rising_queries_str = query_result[rising_start:top_start].strip()\\n        else:\\n            rising_queries_str = query_result[rising_start:].strip()\\n\\n        if rising_queries_str:\\n            trends.extend([q.strip() for q in rising_queries_str.split(\\',\\') if q.strip()])\\n\\n    if \"Top Related Queries:\" in query_result:\\n        top_start = query_result.find(\"Top Related Queries:\") + len(\"Top Related Queries:\")\\n        top_queries_str = query_result[top_start:].strip()\\n        if top_queries_str:\\n            trends.extend([q.strip() for q in top_queries_str.split(\\',\\') if q.strip()])\\n            \\n    # Remove duplicates and return\\n    return list(set(trends))\\n```'}}\n",
      "influencer_search Finds relevant social media influencers or public figures for a given topic.\n",
      "functional_analysis_node\n",
      "{'func_analysis': {'objective': 'This function finds relevant social media influencers or public figures for a given topic.', 'name': 'influencer_search', 'input': ['topic: str'], 'output': ['influencers: list[str]']}}\n",
      "{'sdk_write': {'name_toolkit': 'google_search'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-01 15:45:22,499 - INFO - HTTP Request: GET https://python.langchain.com/docs/integrations/tools/google_search \"HTTP/1.1 308 Permanent Redirect\"\n",
      "2025-06-01 15:45:22,563 - INFO - HTTP Request: GET https://python.langchain.com/docs/integrations/tools/google_search/ \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'code_write': {'code': ''}}\n",
      "content_generator Generates a short piece of social media content based on a description.\n",
      "functional_analysis_node\n",
      "{'func_analysis': {'objective': 'This function generates a short piece of social media content. It takes a description as input and produces relevant social media content.', 'name': 'content_generator', 'input': ['description: str'], 'output': ['social_media_content: str']}}\n",
      "{'sdk_write': {'name_toolkit': 'writer'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-01 15:50:04,609 - INFO - HTTP Request: GET https://python.langchain.com/docs/integrations/tools/writer \"HTTP/1.1 308 Permanent Redirect\"\n",
      "2025-06-01 15:50:04,700 - INFO - HTTP Request: GET https://python.langchain.com/docs/integrations/tools/writer/ \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'code_write': {'code': '```python\\nimport os\\nfrom langchain_core.tools import tool\\nfrom langchain_writer.chat_models import ChatWriter\\nfrom langchain_core.messages import HumanMessage\\n\\n@tool\\ndef content_generator(description: str, writer_api_key: str) -> str:\\n    \"\"\"\\n    Generates a short piece of social media content based on a given description.\\n\\n    Args:\\n        description: A string describing the content for which social media content is to be generated.\\n        writer_api_key: The API key for accessing the Writer AI Studio. This key is required to authenticate with the Writer API.\\n\\n    Returns:\\n        A string containing the generated social media content.\\n    \"\"\"\\n    # Set the Writer API key as an environment variable.\\n    # The ChatWriter class expects the API key to be available in the environment.\\n    os.environ[\"WRITER_API_KEY\"] = writer_api_key\\n\\n    # Initialize the ChatWriter model.\\n    chat = ChatWriter()\\n\\n    # Construct the prompt for generating social media content.\\n    # The prompt instructs the AI to create engaging content suitable for social media.\\n    prompt = f\"Generate a short, engaging social media post (e.g., for Twitter or Instagram) based on the following description: \\'{description}\\'\"\\n\\n    # Create a HumanMessage object with the constructed prompt.\\n    # This message will be sent to the ChatWriter model for processing.\\n    messages = [\\n        HumanMessage(content=prompt)\\n    ]\\n\\n    # Invoke the ChatWriter model with the messages to get a response.\\n    response = chat.invoke(messages)\\n\\n    # Extract the generated social media content from the response.\\n    # The \\'content\\' attribute of the response object holds the AI\\'s generated text.\\n    social_media_content = response.content\\n\\n    return social_media_content\\n```'}}\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'messages'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[91]\u001b[39m\u001b[32m, line 20\u001b[39m\n\u001b[32m     16\u001b[39m output = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m infograph.stream(\n\u001b[32m     18\u001b[39m     {\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m: [HumanMessage(content=user)]}, config=config, stream_mode=\u001b[33m\"\u001b[39m\u001b[33mupdates\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     19\u001b[39m ):\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m     last_message = \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m[-\u001b[32m1\u001b[39m]\n\u001b[32m     21\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     22\u001b[39m         last_message.pretty_print()\n",
      "\u001b[31mKeyError\u001b[39m: 'messages'"
     ]
    }
   ],
   "source": [
    "import uuid\n",
    "\n",
    "cached_human_responses = [\"hi!\", \"rag prompt\", \"1 rag, 2 none, 3 no, 4 no\", \"red\", \"q\"]\n",
    "cached_response_index = 0\n",
    "config = {\"configurable\": {\"thread_id\": str(uuid.uuid4())}}\n",
    "while True:\n",
    "    try:\n",
    "        user = input(\"User (q/Q to quit): \")\n",
    "    except:\n",
    "        user = cached_human_responses[cached_response_index]\n",
    "        cached_response_index += 1\n",
    "    print(f\"User (q/Q to quit): {user}\")\n",
    "    if user in {\"q\", \"Q\"}:\n",
    "        print(\"AI: Byebye\")\n",
    "        break\n",
    "    output = None\n",
    "    for output in infograph.stream(\n",
    "        {\"messages\": [HumanMessage(content=user)]}, config=config, stream_mode=\"updates\"\n",
    "    ):\n",
    "        last_message = next(iter(output.values()))[\"messages\"][-1]\n",
    "        try:\n",
    "            last_message.pretty_print()\n",
    "        except:\n",
    "            print(last_message)\n",
    "\n",
    "    if output and \"prompt\" in output:\n",
    "        print(\"Done!\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3887803",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2812058957.py, line 1)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[35]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mIdentifying social media trends in sports.\u001b[39m\n                ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "I need to create a worklow with the objective of managing my social media.\n",
    " It should be able to tell me trends from social media for sports, get me all the relevant people I should contant for a spsonsoring a specific post, \n",
    "suggest me content I should be posting, make content for me if I give it a description\n",
    "* Identifying social media trends in sports.\n",
    "*   Finding relevant people for sponsoring specific posts.\n",
    "*   Suggesting content to post.\n",
    "*   Generating content based on a description.\n",
    "\n",
    "Examples:\n",
    "Given I ask it about trends in football, it goes through recent viral reels in football and tell me Q&A reels are trending\n",
    "If I tell it I want people who would be interested in a post about UCL football, it gives me current players like Dembele etc\n",
    "If I tell it I want to make a post about UCL football, it goes through what is trending and an agent that thinks about social media posts and tells me we should post a highlight reel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8898949",
   "metadata": {},
   "outputs": [],
   "source": [
    "```python\n",
    "from langchain_community.tools.google_trends import GoogleTrendsQueryRun\n",
    "from langchain_community.utilities.google_trends import GoogleTrendsAPIWrapper\n",
    "from typing import List\n",
    "from tool import tool\n",
    "import os\n",
    "from langchain_core.tools import tool\n",
    "from langchain_writer.chat_models import ChatWriter\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "@tool\n",
    "def social_media_trend_search(sport: str, serpapi_api_key: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Searches for current social media trends related to a specific sport using Google Trends.\n",
    "\n",
    "    Args:\n",
    "        sport (str): The name of the sport to search for trends.\n",
    "        serpapi_api_key (str): Your SerpApi key for accessing Google Trends data.\n",
    "\n",
    "    Returns:\n",
    "        list[str]: A list of social media trends related to the specified sport.\n",
    "                   This includes both rising and top related queries.\n",
    "    \"\"\"\n",
    "    api_wrapper = GoogleTrendsAPIWrapper(serp_api_key=serpapi_api_key)\n",
    "    google_trends_tool = GoogleTrendsQueryRun(api_wrapper=api_wrapper)\n",
    "\n",
    "    query_result = google_trends_tool.run(sport)\n",
    "\n",
    "    trends = []\n",
    "    # Parse the output string to extract related queries\n",
    "    # The output format is \"Rising Related Queries: ...,Top Related Queries: ...\"\n",
    "    if \"Rising Related Queries:\" in query_result:\n",
    "        rising_start = query_result.find(\"Rising Related Queries:\") + len(\"Rising Related Queries:\")\n",
    "        top_start = query_result.find(\"Top Related Queries:\")\n",
    "        \n",
    "        rising_queries_str = \"\"\n",
    "        if top_start != -1:\n",
    "            rising_queries_str = query_result[rising_start:top_start].strip()\n",
    "        else:\n",
    "            rising_queries_str = query_result[rising_start:].strip()\n",
    "\n",
    "        if rising_queries_str:\n",
    "            trends.extend([q.strip() for q in rising_queries_str.split(',') if q.strip()])\n",
    "\n",
    "    if \"Top Related Queries:\" in query_result:\n",
    "        top_start = query_result.find(\"Top Related Queries:\") + len(\"Top Related Queries:\")\n",
    "        top_queries_str = query_result[top_start:].strip()\n",
    "        if top_queries_str:\n",
    "            trends.extend([q.strip() for q in top_queries_str.split(',') if q.strip()])\n",
    "            \n",
    "    # Remove duplicates and return\n",
    "    return list(set(trends))\n",
    "\n",
    "@tool\n",
    "def content_generator(description: str, writer_api_key: str) -> str:\n",
    "    \"\"\"\n",
    "    Generates a short piece of social media content based on a given description.\n",
    "\n",
    "    Args:\n",
    "        description: A string describing the content for which social media content is to be generated.\n",
    "        writer_api_key: The API key for accessing the Writer AI Studio. This key is required to authenticate with the Writer API.\n",
    "\n",
    "    Returns:\n",
    "        A string containing the generated social media content.\n",
    "    \"\"\"\n",
    "    # Set the Writer API key as an environment variable.\n",
    "    # The ChatWriter class expects the API key to be available in the environment.\n",
    "    os.environ[\"WRITER_API_KEY\"] = writer_api_key\n",
    "\n",
    "    # Initialize the ChatWriter model.\n",
    "    chat = ChatWriter()\n",
    "\n",
    "    # Construct the prompt for generating social media content.\n",
    "    # The prompt instructs the AI to create engaging content suitable for social media.\n",
    "    prompt = f\"Generate a short, engaging social media post (e.g., for Twitter or Instagram) based on the following description: '{description}'\"\n",
    "\n",
    "    # Create a HumanMessage object with the constructed prompt.\n",
    "    # This message will be sent to the ChatWriter model for processing.\n",
    "    messages = [\n",
    "        HumanMessage(content=prompt)\n",
    "    ]\n",
    "\n",
    "    # Invoke the ChatWriter model with the messages to get a response.\n",
    "    response = chat.invoke(messages)\n",
    "\n",
    "    # Extract the generated social media content from the response.\n",
    "    # The 'content' attribute of the response object holds the AI's generated text.\n",
    "    social_media_content = response.content\n",
    "\n",
    "    return social_media_content\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
