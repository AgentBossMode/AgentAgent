{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "103bf42a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\agentagent\\AgentAgent\\myenv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain_pinecone import PineconeVectorStore\n",
    "from pinecone import Pinecone\n",
    "import os\n",
    "from langchain_openai import AzureOpenAIEmbeddings\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "embeddings = AzureOpenAIEmbeddings(\n",
    "openai_api_type=\"azure\",\n",
    "openai_api_version=os.environ[\"OPENAI_API_EMBEDDING_VERSION\"],\n",
    "openai_api_key=os.environ[\"OPENAI_API_EMBEDDING_KEY\"],\n",
    "azure_endpoint=os.environ[\"AZURE_OPENAI_EMBEDDING_ENDPOINT\"],\n",
    "deployment=os.environ[\"AZURE_OPENAI_EMBEDDING_DEPLOYMENT\"],\n",
    "model=os.environ[\"AZURE_OPENAI_EMBEDDING_MODEL\"],\n",
    "validate_base_url=True,\n",
    ")\n",
    "\n",
    "pc = Pinecone(api_key=os.environ.get(\"PINECONE_API_KEY\"))\n",
    "from langchain.tools.retriever import create_retriever_tool\n",
    "vector_store = PineconeVectorStore(index=pc.Index(\"langstuffindex\"), embedding=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "55166b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools.retriever import create_retriever_tool\n",
    "retriever = vector_store.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7294e802",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import html2text\n",
    "import httpx\n",
    "\n",
    "def fetch_documents(url: str) -> str:\n",
    "    \"\"\"Fetch a document from a URL and return the markdownified text.\n",
    "\n",
    "    Args:\n",
    "        url (str): The URL of the document to fetch.\n",
    "\n",
    "    Returns:\n",
    "        str: The markdownified text of the document.\n",
    "    \"\"\"\n",
    "    httpx_client = httpx.Client(follow_redirects=True, timeout=10)\n",
    "\n",
    "    try:\n",
    "        response = httpx_client.get(url, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        html_content = response\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    \n",
    "        img_tags = soup.find_all('img')\n",
    "        for img_tag in img_tags:\n",
    "            img_tag.decompose()\n",
    "\n",
    "        target_div = soup.find('div', class_= \"theme-doc-markdown markdown\") #langchain\n",
    "        \n",
    "        if not target_div:\n",
    "            target_div = soup.find('article') #langraph\n",
    "\n",
    "        if not target_div:\n",
    "            target_div = soup.find('html') #langraph\n",
    "\n",
    "        if not target_div:\n",
    "            return html2text.html2text(str(soup))\n",
    "        \n",
    "        return html2text.html2text(str(target_div))\n",
    "    except (httpx.HTTPStatusError, httpx.RequestError) as e:\n",
    "        return f\"Encountered an HTTP error: {str(e)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "18ad2bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "# Choose the LLM that will drive the agent\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e17c7264",
   "metadata": {},
   "source": [
    "# AGENT state creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f06d29f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from langgraph.graph import  MessagesState\n",
    "class NodeBuilderState(MessagesState):\n",
    "    \"\"\"State for the node builder.\"\"\"\n",
    "    next: str = Field(description=\"Next node to go to\")\n",
    "    schema_info: str = Field(description=\"Schema information about the node\")\n",
    "    input_schema: str = Field(description=\"Input schema of the node\")\n",
    "    output_schema: str = Field(description=\"Output schema of the node\")\n",
    "    description: str = Field(description=\"Description of the node\")\n",
    "    function_name: str = Field(description=\"Function name of the node\")\n",
    "    code: str = Field(description=\"Code for the node\")\n",
    "    toolset: list[str] = Field(description=\"List of tools to be used in the node by the llm\")\n",
    "    node_type : str = Field(description=\"Type of node, deterministic if the function is deterministic and requires simple python code generation, ai if the function is not deterministic and requires a llm usage for meeting the requirements\")\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e29399",
   "metadata": {},
   "source": [
    "# NODE type identification and deterministic code gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "5eddc5d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from typing import Literal\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "\n",
    "class NodeType(BaseModel):\n",
    "    node_type: Literal[\"deterministic\", \"ai\"] = Field(description=\n",
    "                                                      \"\"\"Type of node, \n",
    "                                                      : deterministic if the function is deterministic and requires simple python code generation,\n",
    "                                                      : ai if the function is not deterministic, like analysis of input, plan generation or any other thing which is fuzzy logic requires Artificial intelligencefor meeting the requirements\"\"\")\n",
    "\n",
    "\n",
    "def determine_node_type(state: NodeBuilderState):\n",
    "    \"\"\"Determine the type of node.\"\"\"\n",
    "    node_type: str = state[\"node_type\"]\n",
    "    if node_type == \"deterministic\":\n",
    "        return \"deterministic_code\"\n",
    "    elif node_type == \"ai\":\n",
    "        return \"ai_node_gen_supervisor\"\n",
    "\n",
    "node_info_prompt= \"\"\"\n",
    "You are provided with the following information about the node:\n",
    "<SchemaInfo>\n",
    "{schema_info}\n",
    "</SchemaInfo>\n",
    "<InputSchema>\n",
    "{input_schema}\n",
    "</InputSchema>\n",
    "<OutputSchema>\n",
    "{output_schema}\n",
    "</OutputSchema>\n",
    "<Description>\n",
    "{description}\n",
    "</Description>\n",
    "<FunctionName>\n",
    "{function_name}\n",
    "</FunctionName>\n",
    "\n",
    "Below is the skeleton of the function that you need to implement:\n",
    "def {function_name}(state:{input_schema}) -> {output_schema}:\n",
    "    \\\"\\\"\\\"{description}\\\"\\\"\\\"\n",
    "    # Implement the function to meet the description.\n",
    "    \n",
    "the state is of type {input_schema} and the function is of type {output_schema}\n",
    "The general idea is that the implementation would involve extracting the input from the state, and updating the state with the output. Description contains the logic for this blackbox\n",
    "\"\"\"\n",
    "    \n",
    "def identify_node(state: NodeBuilderState):\n",
    "    \"\"\"Identify the node and return the information.\"\"\"\n",
    "    # Extract the information from the state\n",
    "    llm_with_structured_output = llm.with_structured_output(NodeType)\n",
    "    prompt = ChatPromptTemplate.from_template(\n",
    "        \"\"\"You are supposed to identify the type of node based on the information provided.\n",
    "            <SchemaInfo>\n",
    "            {schema_info}\n",
    "            </SchemaInfo>\n",
    "            <InputSchema>\n",
    "            {input_schema}\n",
    "            </InputSchema>\n",
    "            <OutputSchema>\n",
    "            {output_schema}\n",
    "            </OutputSchema>\n",
    "            <Description>\n",
    "            {description}\n",
    "            </Description>\n",
    "            <FunctionName>\n",
    "            {function_name}\n",
    "            </FunctionName>\n",
    "\"\"\")\n",
    "    type_of_node = llm_with_structured_output.invoke(prompt.format(\n",
    "        schema_info=state[\"schema_info\"],\n",
    "        input_schema=state[\"input_schema\"],\n",
    "        output_schema=state[\"output_schema\"],\n",
    "        description=state[\"description\"],\n",
    "        function_name=state[\"function_name\"]))\n",
    "    return {\"node_type\": type_of_node.node_type, \"messages\": [HumanMessage(content=node_info_prompt.format(\n",
    "        schema_info=state[\"schema_info\"],\n",
    "        input_schema=state[\"input_schema\"],\n",
    "        output_schema=state[\"output_schema\"],\n",
    "        description=state[\"description\"],\n",
    "        function_name=state[\"function_name\"])) ]}\n",
    "\n",
    "def deterministic_code_gen(state: NodeBuilderState):\n",
    "    \"\"\"Generate the code for the node.\"\"\"\n",
    "    prompt = ChatPromptTemplate.from_template(\n",
    "        \"\"\"Generate the python code for the function {function_name}.\n",
    "You are provided with the following information about the node:\n",
    "The schema information is as follows: {schema_info}\n",
    "The input schema is: {input_schema}\n",
    "The output schema is: {output_schema}\n",
    "The description of the function is: {description}\n",
    "\n",
    "Implement the function to meet the requirements.\n",
    "\"\"\")\n",
    "    code = llm.invoke(prompt.format(\n",
    "        schema_info=state[\"schema_info\"],\n",
    "        input_schema=state[\"input_schema\"],\n",
    "        output_schema=state[\"output_schema\"],\n",
    "        description=state[\"description\"],\n",
    "        function_name=state[\"function_name\"]))\n",
    "    return {\"code\": code.content}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14074b44",
   "metadata": {},
   "source": [
    "# Supervisor creation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "e31de38d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.types import Command\n",
    "from typing import  Literal, TypedDict\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.language_models.chat_models import BaseChatModel\n",
    "from langchain_core.messages import SystemMessage, AIMessage\n",
    "\n",
    "\n",
    "def make_supervisor_node(llm: BaseChatModel, members: list[str]) -> str:\n",
    "    options = [\"FINISH\"] + members\n",
    "    system_prompt = (\n",
    "        f\"\"\"You are a supervisor tasked with managing a conversation between the\n",
    "        following workers: {members}. Given the following user request,\n",
    "        respond with the worker to act next. Each worker will perform a\n",
    "        task and respond with their results and status. When finished,\n",
    "        respond with FINISH.\n",
    "        \n",
    "        As a supervisor you need to identify which workers to call and the sequence of calling them. \n",
    "        Your goal is to build a langgraph node. \n",
    "        \n",
    "        <NodeBuildingStrategies>\n",
    "        1. prompting + structured_output: \n",
    "            a. Decision making nodes: the node's objective is to analyze the input and use structured output for decision making. Example: analyze the sentiment of the user_query, return llm output as a structured class.\n",
    "        2. Use prompting + Interrupt: analyze the input using LLM with an appropriate prompt, use interupt to get human approval or input. \n",
    "        3. Use prompting + tool-calling:\n",
    "            a. if the node's function is to answer a user_query which needs factual information to answer\n",
    "            b. if the user_query is to make calls to downstream APIs the inputs of which are determined by llm(model) \n",
    "            c. Whenever the node involves a LLM(model) needing sensors or actuators, for information retrieval, and allowing llm(model) to 'DO' stuff\n",
    "            d. One of the best use cases of this technique is when you know that there would be 'n' different tools which could be used in different scenarios to handle a query in a particular domain.\n",
    "        </NodeBuildingStrategies>\n",
    "        \n",
    "        Select the best nodebuilding strategy to meet the functional requirements of the node. \n",
    "\n",
    "        Once you have queried enough information, respond with 'compile_code' at last to orchestrate all information from different workers into a final prodict. \n",
    "        That is when your task will be finished then respond with FINISH\n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "    class Router(BaseModel):\n",
    "        \"\"\"Worker to route to next. If no workers needed, route to FINISH.\"\"\"\n",
    "        reason: str = Field(description=\"Analysis of supervisor, justification of selecting the next worker, why exactly is this particular worker selected and expectations from it.\")\n",
    "        next: Literal[*options] = Field(description=\"worker to act next\") # type: ignore\n",
    "\n",
    "    def supervisor_node(state: NodeBuilderState) -> Command[Literal[*members, \"__end__\"]]:\n",
    "        \"\"\"An LLM-based router.\"\"\"\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "        ] + state[\"messages\"]\n",
    "        response = llm.with_structured_output(Router).invoke(messages)\n",
    "        goto = response.next\n",
    "        if goto == \"FINISH\":\n",
    "            goto = END\n",
    "\n",
    "        return Command(goto=goto, update={\"next\": goto, \"messages\": [AIMessage(content= response.reason)]})\n",
    "\n",
    "    return supervisor_node\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "902e38e9",
   "metadata": {},
   "source": [
    "# TOOLSETGEN agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "62bd2ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "\n",
    "toolset_retrieval_prompt = \"\"\"\n",
    "    User will provide information about a node, Your task is to see if meeting the requirements of the node needs llm(aka model)'s tool calling capability.\n",
    "    \n",
    "    Information about tool calling capabilities of LLMs, how to create tools and how to bind them with a LLM is provided below:\n",
    "    <TOOL_CALLING_EXAMPLE>\n",
    "Example 1: \n",
    "``` python\n",
    "from typing import Literal\n",
    "from langgraph.graph import StateGraph, MessagesState, START, END\n",
    "from langchain_core.messages import AIMessage\n",
    "from langchain_core.tools import tool\n",
    "from langgraph.prebuilt import ToolNode\n",
    "\n",
    "@tool\n",
    "def get_weather(location: str):\n",
    "    \\\"\\\"\\\"Call to get the current weather.\\\"\\\"\\\"\n",
    "    if location.lower() in [\"sf\", \"san francisco\"]:\n",
    "        return \"It's 60 degrees and foggy.\"\n",
    "    else:\n",
    "        return \"It's 90 degrees and sunny.\"\n",
    "\n",
    "\n",
    "@tool\n",
    "def get_coolest_cities():\n",
    "    \\\"\\\"\\\"Get a list of coolest cities\\\"\\\"\\\"\n",
    "    return \"nyc, sf\"\n",
    "\n",
    "tools = [get_weather, get_coolest_cities]\n",
    "tool_node = ToolNode(tools)\n",
    "\n",
    "def should_continue(state: MessagesState):\n",
    "    messages = state[\"messages\"]\n",
    "    last_message = messages[-1]\n",
    "    if last_message.tool_calls:\n",
    "        return \"tools\"\n",
    "    return END\n",
    "\n",
    "\n",
    "def call_model(state: MessagesState):\n",
    "    \\\"\\\"\\\"This node answers questions related to weather using a varied weather related toolset\\\"\\\"\\\"\n",
    "    messages = state[\"messages\"]\n",
    "    response = model_with_tools.invoke([SystemMessage(content= \"Please analyze the following weather-related query and provide a detailed response with relevant information: You have tools like get_weather and get_coolest_cities to help you answer the queries\" )] + messages)\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "\n",
    "workflow = StateGraph(MessagesState)\n",
    "\n",
    "# Define the two nodes we will cycle between\n",
    "workflow.add_node(\"agent\", call_model)\n",
    "workflow.add_node(\"tools\", tool_node)\n",
    "\n",
    "workflow.add_edge(START, \"agent\")\n",
    "workflow.add_conditional_edges(\"agent\", should_continue, [\"tools\", END])\n",
    "workflow.add_edge(\"tools\", \"agent\")\n",
    "\n",
    "app = workflow.compile()\n",
    "```\n",
    "***Dry run***\n",
    "\n",
    "================================[1m Human Message [0m=================================\n",
    "\n",
    "what's the weather in sf?\n",
    "==================================[1m Ai Message [0m==================================\n",
    "\n",
    "[{'text': \"Okay, let's check the weather in San Francisco:\", 'type': 'text'}, {'id': 'toolu_01LdmBXYeccWKdPrhZSwFCDX', 'input': {'location': 'San Francisco'}, 'name': 'get_weather', 'type': 'tool_use'}]\n",
    "Tool Calls:\n",
    "  get_weather (toolu_01LdmBXYeccWKdPrhZSwFCDX)\n",
    " Call ID: toolu_01LdmBXYeccWKdPrhZSwFCDX\n",
    "  Args:\n",
    "    location: San Francisco\n",
    "=================================[1m Tool Message [0m=================================\n",
    "Name: get_weather\n",
    "\n",
    "It's 60 degrees and foggy.\n",
    "==================================[1m Ai Message [0m==================================\n",
    "\n",
    "The weather in San Francisco is currently 60 degrees with foggy conditions\n",
    "\n",
    "</TOOL_CALLING_EXAMPLE>\n",
    "\n",
    "<Output> \n",
    "Identify if tool_calling is relevant:\n",
    "if yes: identify what kind of tools may be needed to be used by a LLM, and then write code for the llm binding functionality\n",
    "if no: just respond no tool_calling needed.\n",
    "</Output>\n",
    "    \"\"\"\n",
    "\n",
    "def toolset_generation(state: NodeBuilderState) -> Command[Literal[\"ai_node_gen_supervisor\"]]:\n",
    "    \"\"\"Generate the code for the node.\"\"\"\n",
    "    result = llm.invoke([SystemMessage(content=toolset_retrieval_prompt)] + state[\"messages\"])\n",
    "    return Command(\n",
    "        update={\n",
    "            \"messages\": [\n",
    "                HumanMessage(content=result.content, name=\"toolset_suggester\")\n",
    "            ]\n",
    "        },\n",
    "        goto=\"ai_node_gen_supervisor\",\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e1a9651",
   "metadata": {},
   "source": [
    "# PROMPTGEN agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "8d1fcb24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "prompt_vector_store = PineconeVectorStore(index=pc.Index(\"promptguide\"), embedding=embeddings)\n",
    "prompt_retriever = prompt_vector_store.as_retriever()\n",
    "prompt_gen_retriever = create_retriever_tool(prompt_retriever, \"Retrieve_info_on_prompting\", \"Search information about what are different prompting techniques relevant to the user requirements.\")\n",
    "\n",
    "promptgen_prompt = ChatPromptTemplate.from_messages([\n",
    "         (\"system\", \"\"\"\n",
    "    You are a ReAct (Reasoning and Act) agent.\n",
    "    You are tasked with generating a prompt to meet the objectives of the langgraph node.\n",
    "    The langgraph node information is provided. \n",
    "    \n",
    "    For example: \n",
    "    User query: the node is supposed to generate a plan using llms\n",
    "    Thought: I need to generate a prompt that will make the LLM generate a plan for the given task.\n",
    "    Action: Use the  'Retrieve_info_on_prompting' tool to search for plan-and-execute prompting techniques.\n",
    "    Observation: I found a plan-and-execute prompting technique that can generate a plan for the given task.\n",
    "    Action: I will customize the observed prompt to meet the requirements of the node.\n",
    "    \n",
    "    IMPORTANT: Your final output will be only a prompt, no code\n",
    "    \"\"\"),\n",
    "    (\"placeholder\", \"{messages}\"),\n",
    "    ])\n",
    "prompt_gen_agent = create_react_agent(llm, tools=[prompt_gen_retriever], prompt = promptgen_prompt)\n",
    "\n",
    "def prompt_generation(state: NodeBuilderState) -> Command[Literal[\"ai_node_gen_supervisor\"]]:\n",
    "    \"\"\"Generate the code for the node.\"\"\"\n",
    "    response = prompt_gen_agent.invoke({\"messages\": state[\"messages\"]})\n",
    "    return Command(\n",
    "        update={\n",
    "            \"messages\": [\n",
    "                HumanMessage(content=response[\"messages\"][-1].content, name=\"prompt_generator\")\n",
    "            ]\n",
    "        },\n",
    "        goto=\"ai_node_gen_supervisor\",\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "226799c9",
   "metadata": {},
   "source": [
    "# STRUCTUTRED OUTPUT GEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "6fdc4480",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "struc_output_prompt = \"\"\"\n",
    "    User will provide you with the information about the node,  you are supposed to analyze the information and see if it requires LLM (aka model)'s 'with_structured_output()' function.\n",
    "\n",
    "Here is information about structured outputs, why and when to use structured outputs:\n",
    "<AboutStructuredOutput>\n",
    "Structured output is beneficial in situations where consistent and verifiable data formats are needed, especially when integrating with databases, APIs, or building complex workflows. It helps reduce hallucinations, simplifies prompting, and enables reliable type-safety, making applications more predictable and easier to evaluate. \n",
    "\n",
    "Here's a more detailed breakdown:\n",
    "1. Reduced Hallucinations: By enforcing adherence to a JSON Schema, structured outputs minimize the chance of models generating incorrect or irrelevant data. \n",
    "2. Simplified Prompting: You don't need overly complex or specific prompts to get consistently formatted output, as the schema provides the structure. \n",
    "3. Reliable Type-Safety: Structured outputs ensure that the model always generates data that fits the defined schema, eliminating the need for validation or retries due to format errors. \n",
    "4. Building Complex Workflows: Structured outputs are particularly useful for building multi-step workflows where the output of one step serves as input for the next. \n",
    "5. Integration with Databases and APIs: When integrating with systems that require structured data formats (like databases or APIs), structured outputs ensure consistency and avoid integration problems. \n",
    "6. Function Calling and Data Extraction: Structured outputs are recommended for function calling and extracting structured data from various sources. \n",
    "7. Consistent and Verifiable Output: The predictable format of structured outputs makes it easier to test, debug, and evaluate applications that rely on them. \n",
    "8. Explicit Refusals: You can now detect model refusals programmatically when using structured outputs, as the model will explicitly return a structured error message instead of a text-based one. \n",
    "\n",
    "In essence, structured output provides a robust and predictable way to manage the output of language models, making them more reliable and easier to integrate into various applications. \n",
    "</AboutStructuredOutput>\n",
    "\n",
    "Here is an example showing how to use 'with_structured_output' method is below:\n",
    "<StructuredOutputExample>\n",
    "Here is an example of how to use structured output. In this example, we want the LLM to generate and fill up the pydantic class Joke, based on user query. \n",
    "``` python\n",
    "from typing import Optional\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "# Pydantic class for structured output\n",
    "class Joke(BaseModel):\n",
    "    setup: str = Field(description=\"The setup of the joke\")\n",
    "    punchline: str = Field(description=\"The punchline to the joke\")\n",
    "    rating: Optional[int] = Field(\n",
    "        default=None, description=\"How funny the joke is, from 1 to 10\"\n",
    "    )\n",
    "\n",
    "class JokeBuilderState(MessagesState):\n",
    "    joke: Joke = Field(description= \"joke generated by the GenerateJoke node.\")\n",
    "\n",
    "def GenerateJoke(state: JokeBuilderState):\n",
    "    structured_llm = llm.with_structured_output(Joke)\n",
    "    joke: Joke = structured_llm.invoke(\"Tell me a joke about cats\")\n",
    "    return { \"joke\": joke }\n",
    "```\n",
    "\n",
    "Output:\n",
    "Joke(setup='Why was the cat sitting on the computer?', punchline='Because it wanted to keep an eye on the mouse!', rating=7)\n",
    "</StructuredOutputExample>\n",
    "    \n",
    "<Notes> \n",
    "1. The structured output pydantic class is not to be the same as the input_schema or output_schema, need to be specific\n",
    "2. Do not hallucinate or write your own code to implement structured output, refer to 'StructuredOutputExample' section\n",
    "</Notes>\n",
    "    \n",
    "<Output>\n",
    "Check if the node needs llm's structured output functionality based on 'AboutStructuredOutput' section.\n",
    "If yes, you are supposed to generate the code for the structured output functionality for the llm to use. \n",
    "If not needed, just mention there is no need for structured output functionality.\n",
    "</Output>\n",
    "    \"\"\"\n",
    "\n",
    "def structured_output_generation(state: NodeBuilderState) -> Command[Literal[\"ai_node_gen_supervisor\"]]:\n",
    "    \"\"\"Generate the code for the node.\"\"\"\n",
    "    # fetch_documents(\"https://python.langchain.com/docs/how_to/structured_output/\")\n",
    "    response = llm.invoke( [SystemMessage(content=struc_output_prompt)]  + state[\"messages\"])\n",
    "\n",
    "    return Command(\n",
    "        update={\n",
    "            \"messages\": [\n",
    "                HumanMessage(content=response.content, name=\"struct_output_generator\")\n",
    "            ]\n",
    "        },\n",
    "        goto=\"ai_node_gen_supervisor\",\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "209b2f8f",
   "metadata": {},
   "source": [
    "# Interrupt gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "bc504e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "interrupt_gen_prompt: str = \"\"\"\n",
    "    User will provide you with the information about the node, you are supposed to analyze the information and see if it requires interrupt functionality.\n",
    "    \n",
    "    Following is the information about the interrupt functionality, what is the purpose of interrupt, design patterns, how to implement them:\n",
    "    <InterruptInfo>\n",
    "    The interrupt function in LangGraph enables human-in-the-loop workflows by pausing the graph at a specific node, presenting information to a human, and resuming the graph with their input. This function is useful for tasks like approvals, edits, or collecting additional input. The interrupt function is used in conjunction with the Command object to resume the graph with a value provided by the human.\n",
    "\n",
    "\n",
    "``` python\n",
    "from langgraph.types import interrupt\n",
    "\n",
    "def human_node(state: State):\n",
    "    value = interrupt(\n",
    "        # Any JSON serializable value to surface to the human.\n",
    "        # For example, a question or a piece of text or a set of keys in the state\n",
    "       {\n",
    "          \"text_to_revise\": state[\"some_text\"]\n",
    "       }\n",
    "    )\n",
    "    # Update the state with the human's input or route the graph based on the input.\n",
    "    return {\n",
    "        \"some_text\": value\n",
    "    }\n",
    "\n",
    "graph = graph_builder.compile(\n",
    "    checkpointer=checkpointer # Required for `interrupt` to work\n",
    ")\n",
    "\n",
    "# Run the graph until the interrupt\n",
    "thread_config = {\"configurable\": {\"thread_id\": \"some_id\"}}\n",
    "graph.invoke(some_input, config=thread_config)\n",
    "\n",
    "# Resume the graph with the human's input\n",
    "graph.invoke(Command(resume=value_from_human), config=thread_config)\n",
    "```\n",
    "    </InterruptInfo>\n",
    "    \n",
    "    <Output>\n",
    "    Unless explicitly mentioned in node requirements, human-in-loop aka interrupt is not needed in the scenario.\n",
    "    If needed: implement the code with interrupt functionality tailored to the use case\n",
    "    If not needed: just respond that interrupt functionality is not needed\n",
    "    </Output>\n",
    "    \"\"\"\n",
    "\n",
    "def interrupt_generation(state: NodeBuilderState) -> Command[Literal[\"ai_node_gen_supervisor\"]]:\n",
    "    \"\"\"Generate the code for the node.\"\"\"\n",
    "    # fetch_docs=fetch_documents(\"https://langchain-ai.github.io/langgraph/concepts/human_in_the_loop/\")\n",
    "    response = llm.invoke( [SystemMessage(content=interrupt_gen_prompt)]  + state[\"messages\"])\n",
    "    return Command(\n",
    "        update={\n",
    "            \"messages\": [\n",
    "                HumanMessage(content=response.content, name=\"interrupt_generator\")\n",
    "            ]\n",
    "        },\n",
    "        goto=\"ai_node_gen_supervisor\",\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0deeedc8",
   "metadata": {},
   "source": [
    "# Code compiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "dbec43be",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "code_compiler_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "    You are tasked with furnishing a final output code for the user provided node information. \n",
    "    \n",
    "    Analyze the message history, you will find multiple code pieces. which are related to prompt_gen, interrupt_gen, toolset_gen, structured_output_gen\n",
    "    You need to carefully merge all the generate code together to furnish a final output.\n",
    "    \"\"\")\n",
    "\n",
    "\n",
    "def code_compiler(state: NodeBuilderState) -> Command[Literal[\"ai_node_gen_supervisor\"]]:\n",
    "    \"\"\"Generate the code for the node.\"\"\"\n",
    "    response = llm.invoke(state[\"messages\"])\n",
    "    return Command(\n",
    "        update={\n",
    "            \"messages\": [\n",
    "                HumanMessage(content=response.content, name=\"code_compiler\")\n",
    "            ]\n",
    "        },\n",
    "        goto=\"ai_node_gen_supervisor\",\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "862c59c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ai_node_gen_supervisor = make_supervisor_node(llm, [\"prompt_generation\", \"toolset_generation\", \"structured_output_generation\", \"interrupt_generation\", \"code_compiler\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da4e52b1",
   "metadata": {},
   "source": [
    "# Graph creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "d05ec844",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph.state import CompiledStateGraph\n",
    "workflow = StateGraph(NodeBuilderState)\n",
    "workflow.add_node(\"identify_node\", identify_node)\n",
    "workflow.add_node(\"deterministic_code\", deterministic_code_gen)\n",
    "\n",
    "workflow.add_node(\"ai_node_gen_supervisor\", ai_node_gen_supervisor)\n",
    "workflow.add_node(\"prompt_generation\", prompt_generation)\n",
    "workflow.add_node(\"toolset_generation\", toolset_generation)\n",
    "workflow.add_node(\"structured_output_generation\", structured_output_generation)\n",
    "workflow.add_node(\"interrupt_generation\", interrupt_generation)\n",
    "workflow.add_node(\"code_compiler\", code_compiler)\n",
    "\n",
    "workflow.add_edge(START, \"identify_node\")\n",
    "workflow.add_conditional_edges(\"identify_node\", determine_node_type, [\"deterministic_code\", \"ai_node_gen_supervisor\"])\n",
    "workflow.add_edge(\"deterministic_code\", END)\n",
    "\n",
    "app : CompiledStateGraph = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c0a5ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "display(Image(app.get_graph(xray=True).draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "804f0c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_for_node = {\n",
    "    \"planner\": {\n",
    "        \"schema_info\": \"\"\"OverallState:\n",
    "      type: TypedDict\n",
    "      fields:\n",
    "      - name: input\n",
    "        type: str\n",
    "      - name: plan\n",
    "        type: List[str]\n",
    "      - name: past_steps\n",
    "        type: List[Tuple]\n",
    "      - name: response\n",
    "        type: str\n",
    "      - name: messages\n",
    "        type: Annotated[list[AnyMessage], add_messages]\"\"\",\n",
    "    \"input_schema\": \"OverallState\",\n",
    "    \"output_schema\":\"OverallState\",\n",
    "    \"description\":\"Plan step generates a plan based on the input using llm structured output functionality, stores it to the plan field\",\n",
    "    \"function_name\": \"plan_step\"\n",
    "    },\n",
    "    \"math_node\":{\n",
    "        \"schema_info\": \"\"\"MathState:\n",
    "      type: TypedDict\n",
    "      fields:\n",
    "      - name: a\n",
    "        type: int\n",
    "      - name: b\n",
    "        type: int\n",
    "      - name: response\n",
    "        type: int\"\"\",\n",
    "    \"input_schema\": \"MathState\",\n",
    "    \"output_schema\":\"MathState\",\n",
    "    \"description\":\"Gets the two numbers a and b from the state, and returns the sum of a and b., stores it to the response field\",\n",
    "    \"function_name\": \"math_step\"\n",
    "    },\n",
    "    \"interrupt_node\":{\n",
    "        \"schema_info\": \"\"\"RequirementAnalysisState:\n",
    "      type: TypedDict\n",
    "      fields:\n",
    "      - name: user_input\n",
    "        type: str\n",
    "      - name: messages\n",
    "        type: Annotated[list[AnyMessage], add_messages]\"\"\",\n",
    "    \"input_schema\": \"RequirementAnalysisState\",\n",
    "    \"output_schema\":\"RequirementAnalysisState\",\n",
    "    \"description\":\"Analyze the user input, see if they require any human intervention, if yes, return the human intervention required, else return no human intervention required\",\n",
    "    \"function_name\": \"interrupt_step\"\n",
    "    },\n",
    "    \"decision_node\":{\n",
    "        \"schema_info\": \"\"\"RequirementAnalysisState:\n",
    "      type: TypedDict\n",
    "      fields:\n",
    "      - name: user_input\n",
    "        type: str\n",
    "      - name: messages\n",
    "        type: Annotated[list[AnyMessage], add_messages]\"\"\",\n",
    "    \"input_schema\": \"RequirementAnalysisState\",\n",
    "    \"output_schema\":\"RequirementAnalysisState\",\n",
    "    \"description\":\"Analyze the user input which contain the requirements for building a product, acts as a decision node if more info is needed or not\",\n",
    "    \"function_name\": \"decision_step\"\n",
    "    },\n",
    "    \"code_node\":{\n",
    "        \"schema_info\": \"\"\"CodeWriterState:\n",
    "      type: TypedDict\n",
    "      fields:\n",
    "      - name: user_query\n",
    "        type: str\n",
    "      - name: execution_result\n",
    "        type: str\"\"\",\n",
    "    \"input_schema\": \"CodeWriterState\",\n",
    "    \"output_schema\":\"RequiremenCodeWriterStatetAnalysisState\",\n",
    "    \"description\":\"This node analyzes the user_query, if the query is to write a code, it will make a tool call to run the proposed code\",\n",
    "    \"function_name\": \"code_step\"\n",
    "    },\n",
    "    \"weather_node\":{\n",
    "        \"schema_info\": \"\"\"WeatherNodeState:\n",
    "      type: TypedDict\n",
    "      fields:\n",
    "      - name: user_query\n",
    "        type: str\n",
    "      - name: execution_result\n",
    "        type: str\"\"\",\n",
    "    \"input_schema\": \"WeatherNodeState\",\n",
    "    \"output_schema\":\"WeatherNodeState\",\n",
    "    \"description\":\"This node analyzes the user_query, makes tool calls to retrieve relevant information and answer the query\",\n",
    "    \"function_name\": \"weather_step\",\n",
    "    },\n",
    "    \"stock_node\":{\n",
    "        \"schema_info\": \"\"\"StockState:\n",
    "      type: TypedDict\n",
    "      fields:\n",
    "      - name: user_query\n",
    "        type: str\n",
    "      - name: execution_result\n",
    "        type: str\"\"\",\n",
    "    \"input_schema\": \"StockState\",\n",
    "    \"output_schema\":\"StockState\",\n",
    "    \"description\":\"This node analyzes the user_query related to finance/stock-markets, makes tool calls to retrieve relevant information and answer the query\",\n",
    "    \"function_name\": \"stock_step\",\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "6ceabf10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'identify_node': {'node_type': 'ai', 'messages': [HumanMessage(content='\\nYou are provided with the following information about the node:\\n<SchemaInfo>\\nStockState:\\n      type: TypedDict\\n      fields:\\n      - name: user_query\\n        type: str\\n      - name: execution_result\\n        type: str\\n</SchemaInfo>\\n<InputSchema>\\nStockState\\n</InputSchema>\\n<OutputSchema>\\nStockState\\n</OutputSchema>\\n<Description>\\nThis node analyzes the user_query related to finance/stock-markets, makes tool calls to retrieve relevant information and answer the query\\n</Description>\\n<FunctionName>\\nstock_step\\n</FunctionName>\\n\\nBelow is the skeleton of the function that you need to implement:\\ndef stock_step(state:StockState) -> StockState:\\n    \"\"\"This node analyzes the user_query related to finance/stock-markets, makes tool calls to retrieve relevant information and answer the query\"\"\"\\n    # Implement the function to meet the description.\\n\\nthe state is of type StockState and the function is of type StockState\\nThe general idea is that the implementation would involve extracting the input from the state, and updating the state with the output. Description contains the logic for this blackbox\\n', additional_kwargs={}, response_metadata={}, id='e2916f67-2378-4b3f-be95-583ca67fc95a')]}}\n",
      "{'ai_node_gen_supervisor': {'next': 'prompt_generation', 'messages': [AIMessage(content=\"To implement the function that meets the description of analyzing user_query and making tool calls to retrieve relevant financial information, the first step is to utilize the prompt_generation worker. This worker will create a prompt to process the user_query effectively, taking into account that we're dealing with finance/stock-markets. After obtaining the response from prompt generation, further processing can be carried out by another worker (e.g., toolset_generation) to retrieve relevant information. Hence, the next appropriate worker to act is 'prompt_generation'.\", additional_kwargs={}, response_metadata={}, id='06d09161-c781-4a0a-931f-b3d0b8ffced9')]}}\n",
      "{'prompt_generation': {'messages': [HumanMessage(content='Based on the information retrieved about prompting techniques and their applications in analyzing user queries related to finance and stock markets, here is a prompt tailored for this purpose:\\n\\n\"Analyze the following user query related to finance and stock markets: \\'{user_query}\\'. Identify the key components of the query, determine the relevant financial data or trends needed to respond accurately, and suggest appropriate tools or methods for retrieving this information.\"', additional_kwargs={}, response_metadata={}, name='prompt_generator', id='cb473d42-d4d9-4ef8-97d1-7adf50fa4c58')]}}\n",
      "{'ai_node_gen_supervisor': {'next': 'toolset_generation', 'messages': [AIMessage(content='The prompt generation worker has successfully created a tailored prompt to analyze a user query related to finance and stock markets. This prompt is intended to extract key components and identify relevant data. The next logical step is to utilize the toolset_generation worker to execute the prompt and retrieve the necessary financial information based on the user query.', additional_kwargs={}, response_metadata={}, id='2a02b735-58e6-439e-b8dc-e8f9005568ee')]}}\n",
      "{'toolset_generation': {'messages': [HumanMessage(content='Here\\'s the implementation for the `stock_step` function in accordance with the provided schema and description. This function will extract the `user_query` from the state, analyze it using a tool, and then update the `execution_result` in the state with the relevant financial information.\\n\\n```python\\nfrom typing import TypedDict\\nfrom langgraph.graph import StateGraph, MessagesState, START, END\\nfrom langchain_core.messages import AIMessage\\nfrom langchain_core.tools import tool\\nfrom langgraph.prebuilt import ToolNode\\n\\n# TypedDict for the StockState\\nclass StockState(TypedDict):\\n    user_query: str\\n    execution_result: str\\n\\n@tool\\ndef get_financial_data(query: str):\\n    \"\"\"Retrieve financial data based on the user query.\"\"\"\\n    # Implement tool logic to fetch financial data here\\n    # For simplicity, let\\'s assume it returns hardcoded data.\\n    if \"stock price\" in query.lower():\\n        return \"The current stock price of XYZ is $100.\"\\n    elif \"market trend\" in query.lower():\\n        return \"The market is experiencing an uptrend.\"\\n    else:\\n        return \"No relevant financial data found.\"\\n\\ntools = [get_financial_data]\\ntool_node = ToolNode(tools)\\n\\ndef stock_step(state: StockState) -> StockState:\\n    \"\"\"This node analyzes the user_query related to finance/stock-markets, makes tool calls to retrieve relevant information and answer the query.\"\"\"\\n    \\n    user_query = state[\"user_query\"]\\n    response = get_financial_data(user_query)\\n    \\n    # Update the state with the execution result\\n    state[\"execution_result\"] = response\\n    return state\\n\\n# Create a workflow for processing\\nworkflow = StateGraph(MessagesState)\\n\\n# Add nodes to the workflow\\nworkflow.add_node(\"stock_analysis\", stock_step)\\nworkflow.add_node(\"tools\", tool_node)\\n\\nworkflow.add_edge(START, \"stock_analysis\")\\nworkflow.add_conditional_edges(\"stock_analysis\", lambda state: END if state[\"execution_result\"] else \"tools\", [\"tools\"])\\nworkflow.add_edge(\"tools\", \"stock_analysis\")\\n\\napp = workflow.compile()\\n```\\n\\n### Explanation:\\n- **Tool Definition**: Created a simple tool `get_financial_data` that simulates the retrieval of financial information based on defined queries.\\n- **Function Implementation**: `stock_step` extracts the user\\'s query, calls the tool to get relevant data, and updates the state with the result.\\n- **Workflow Management**: Added nodes to the `StateGraph` to handle the analysis and tool calls, ensuring a flow between them until the execution is completed. \\n\\nThis setup allows the LLM to call tools as necessary based on the user query regarding finance and stock markets.', additional_kwargs={}, response_metadata={}, name='toolset_suggester', id='1b9e0893-6742-400a-8dc9-3590f00c5d09')]}}\n",
      "{'ai_node_gen_supervisor': {'next': 'code_compiler', 'messages': [AIMessage(content='The toolset_generation worker has effectively implemented the `stock_step` function that processes the user query and retrieves financial data. The function is properly designed to analyze the query, invoke a tool for fetching information, and update the execution result in the state accordingly. Now that we have the complete implementation for the node, the next step is to compile all the information from the workers into the final product. Thus, the next worker to call is `code_compiler`.', additional_kwargs={}, response_metadata={}, id='5bbb8ed5-1770-4008-a8ce-3a712cffc61c')]}}\n",
      "{'code_compiler': {'messages': [HumanMessage(content='Here\\'s the full implementation of the `stock_step` function along with the tool setup, as outlined:\\n\\n```python\\nfrom typing import TypedDict\\nfrom langgraph.graph import StateGraph, MessagesState, START, END\\nfrom langchain_core.messages import AIMessage\\nfrom langchain_core.tools import tool\\nfrom langgraph.prebuilt import ToolNode\\n\\n# TypedDict for the StockState\\nclass StockState(TypedDict):\\n    user_query: str\\n    execution_result: str\\n\\n@tool\\ndef get_financial_data(query: str) -> str:\\n    \"\"\"Retrieve financial data based on the user query.\"\"\"\\n    # Implement tool logic to fetch financial data here\\n    # For simplicity, let\\'s assume it returns hardcoded data.\\n    if \"stock price\" in query.lower():\\n        return \"The current stock price of XYZ is $100.\"\\n    elif \"market trend\" in query.lower():\\n        return \"The market is experiencing an uptrend.\"\\n    else:\\n        return \"No relevant financial data found.\"\\n\\ntools = [get_financial_data]\\ntool_node = ToolNode(tools)\\n\\ndef stock_step(state: StockState) -> StockState:\\n    \"\"\"This node analyzes the user_query related to finance/stock-markets, makes tool calls to retrieve relevant information and answer the query.\"\"\"\\n    \\n    user_query = state[\"user_query\"]\\n    response = get_financial_data(user_query)\\n    \\n    # Update the state with the execution result\\n    state[\"execution_result\"] = response\\n    return state\\n\\n# Create a workflow for processing\\nworkflow = StateGraph(MessagesState)\\n\\n# Add nodes to the workflow\\nworkflow.add_node(\"stock_analysis\", stock_step)\\nworkflow.add_node(\"tools\", tool_node)\\n\\n# Define the flow logic\\nworkflow.add_edge(START, \"stock_analysis\")\\nworkflow.add_conditional_edges(\"stock_analysis\", lambda state: END if state[\"execution_result\"] else \"tools\", [\"tools\"])\\nworkflow.add_edge(\"tools\", \"stock_analysis\")\\n\\n# Compile the workflow into an app\\napp = workflow.compile()\\n```\\n\\n### Explanation of Components:\\n\\n1. **TypedDict**: The `StockState` TypedDict defines the structure for the state, including the user\\'s query and the execution result.\\n\\n2. **Tool Definition**: The `get_financial_data` function is a mock implementation that returns hardcoded responses based on the query.\\n\\n3. **Function Implementation**: The `stock_step` function extracts the `user_query`, retrieves the relevant financial data using the tool, and updates the `execution_result`.\\n\\n4. **StateGraph Workflow**: The `StateGraph` manages the workflow, allowing for transitions based on whether relevant data is found. It uses conditional edges to tie the analysis and tool invocation together.\\n\\n5. **Compilation**: Finally, the workflow is compiled into an app for execution.\\n\\nThis structured approach effectively answers user queries related to finance and the stock market by utilizing the defined tools and processing logic. If there are any additional components or changes needed, please let me know!', additional_kwargs={}, response_metadata={}, name='code_compiler', id='7e633f40-f6b5-4f34-9cfb-8609121bea24')]}}\n",
      "{'ai_node_gen_supervisor': {'next': '__end__', 'messages': [AIMessage(content='The code_compiler worker has successfully compiled the complete implementation of the `stock_step` function along with tool setup within the workflow. The implementation correctly integrates functionality for analyzing finance-related queries and retrieving pertinent financial information. Since all tasks have been constructed and finalized, the task is complete.', additional_kwargs={}, response_metadata={}, id='ebfcea53-f5e3-40dc-99f3-390d6ec0e769')]}}\n"
     ]
    }
   ],
   "source": [
    "for output in app.stream(dict_for_node[\"stock_node\"], stream_mode=\"updates\"\n",
    "    ):\n",
    "        print(output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
