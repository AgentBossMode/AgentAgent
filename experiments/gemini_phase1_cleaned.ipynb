{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "299c2764",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-03 02:34:12,627 - INFO - Tool information graph (tool_infograph) compiled.\n",
      "2025-06-03 02:34:12,636 - INFO - Tool compilation graph (tool_compile_graph) compiled.\n",
      "2025-06-03 02:34:12,642 - INFO - Main agent generator graph compiled.\n"
     ]
    }
   ],
   "source": [
    "# --- Imports and Environment Setup ---\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv() # Load environment variables from .env file\n",
    "\n",
    "import logging\n",
    "import json # Used for loading tool configurations\n",
    "import uuid # Used for generating unique thread IDs for graph execution\n",
    "\n",
    "from pydantic import Field, BaseModel # For data validation and settings management\n",
    "from typing import List, Literal # For type hinting\n",
    "\n",
    "from langgraph.graph import MessagesState, StateGraph, START, END # Core LangGraph components for building stateful graphs\n",
    "from langgraph.checkpoint.memory import MemorySaver, InMemorySaver # For saving and resuming graph state\n",
    "from langgraph.types import Command, interrupt # For controlling graph flow, e.g., human-in-the-loop\n",
    "\n",
    "from langchain_core.prompts import PromptTemplate # For creating flexible prompts for LLMs\n",
    "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage # For structuring messages in conversations\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI # Google Generative AI model wrapper\n",
    "\n",
    "from final_code.utils.fetch_docs import fetch_documents # Utility function to fetch documentation (presumably for SDKs)\n",
    "\n",
    "\n",
    "# --- Logging Configuration ---\n",
    "# Set up the logger for application-wide logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,  # Set to DEBUG for detailed logs, INFO for general information\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\", # Log message format\n",
    "    handlers=[\n",
    "        # logging.FileHandler(\"scraper.log\"),  # Option to log to a file (currently commented out)\n",
    "        logging.StreamHandler()  # Log to console\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__) # Get a logger instance for the current module\n",
    "\n",
    "# --- LLM Initialization ---\n",
    "# Initialize the Language Model (LLM) to be used throughout the application\n",
    "# Using Google's Gemini Flash model with a temperature of 0 for deterministic outputs\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash-preview-05-20\", temperature=0)\n",
    "\n",
    "\n",
    "# --- Agent Requirement Analysis ---\n",
    "# This section defines the process for understanding user requirements for building an AI agent.\n",
    "\n",
    "# Prompt template for the Requirement Analysis LLM\n",
    "template = \"\"\"Your job is to get information from a user about what kind of agent they wish to build.\n",
    "\n",
    "You should get the following information from them:\n",
    "\n",
    "- What the objective of the agent is\n",
    "- Various usecases of the agent\n",
    "- Some examples of what the agent will be doing (Input and expected output pairs)\n",
    "\n",
    "If you are not able to discern this info, ask them to clarify, you can suggest and see if user confirms the suggestions.\n",
    "\n",
    "After you are able to discern all the information, call the tool AgentInstruction\"\"\"\n",
    "\n",
    "class AgentInstructions(BaseModel):\n",
    "    \"\"\"\n",
    "    Pydantic model to structure the instructions for building the Agent.\n",
    "    This model is used as a tool for the LLM to output structured information.\n",
    "    \"\"\"\n",
    "    objective: str = Field(description=\"What is the primary objective of the agent\")\n",
    "    usecases: List[str] = Field(description=\"What are the various responsibilities of the agent which it needs to fulfill\")\n",
    "    examples: str = Field(description=\"What are some examples of the usage of the agent (input query and expected output from the agent) ?\")\n",
    "\n",
    "class AgentBuilderState(MessagesState):\n",
    "    \"\"\"\n",
    "    Represents the state of the main agent building graph.\n",
    "    Inherits from MessagesState to include a list of messages.\n",
    "    \"\"\"\n",
    "    agent_instructions: AgentInstructions = Field(description=\"The requirement analysis generated by the model.\")\n",
    "    # json_code: str = Field(\"The json code generated\") # Not actively used in the provided graph logic\n",
    "    python_code: str = Field(description=\"The Python code generated for the agent\")\n",
    "\n",
    "# class ArchitectureEvaluationState(MessagesState): # This state is not used\n",
    "#     agent_instructions: AgentInstructions = Field(\"the requirement analysis generated by the model.\")\n",
    "#     url: str = Field(\"url of the agent architecture to evaluate against\")\n",
    "\n",
    "\n",
    "def requirement_analysis_node(state: AgentBuilderState) -> Command[Literal[\"requirement_analysis_node\", \"code_node\"]]:\n",
    "    \"\"\"\n",
    "    LangGraph node for performing requirement analysis.\n",
    "    It interacts with the LLM to gather agent specifications from the user.\n",
    "    If information is insufficient, it interrupts the graph for user input.\n",
    "    Otherwise, it proceeds to the code generation node.\n",
    "    \"\"\"\n",
    "    logger.info(\"Executing requirement_analysis_node\")\n",
    "    llm_with_tool = llm.bind_tools([AgentInstructions]) # Bind the AgentInstructions Pydantic model as a tool\n",
    "    \n",
    "    # Invoke the LLM with the system prompt and current message history\n",
    "    response = llm_with_tool.invoke([SystemMessage(content=template)] + state[\"messages\"])\n",
    "    \n",
    "    if not response.tool_calls: # If no tool call, means LLM needs more info or is just conversing\n",
    "        logger.info(\"LLM requires more information or is in conversation.\")\n",
    "        # Interrupt the graph to get user input. The content of 'value' will be sent to the user.\n",
    "        value = interrupt(response.content)\n",
    "        # Return a command to stay in the current node and update messages with LLM's response and the interruption\n",
    "        return Command(goto=\"requirement_analysis_node\", update={\"messages\": [response, HumanMessage(content=value)]})\n",
    "        \n",
    "    # If a tool call is present, it means the LLM has gathered the required information\n",
    "    logger.info(\"LLM successfully called AgentInstructions tool.\")\n",
    "    agent_instructions_args = response.tool_calls[0][\"args\"]\n",
    "    agent_instructions = AgentInstructions(**agent_instructions_args)\n",
    "    \n",
    "    # Return a command to proceed to 'code_node' and update state\n",
    "    return Command(\n",
    "        goto=\"code_node\",\n",
    "        update={\"messages\": [response], \"agent_instructions\": agent_instructions}\n",
    "    )\n",
    "\n",
    "# --- Agent Code Generation ---\n",
    "# This section focuses on generating the main Python code for the agent based on the gathered requirements.\n",
    "\n",
    "CODE_GEN_PROMPT = PromptTemplate.from_template(\"\"\"\n",
    "You are an expert Python programmer specializing in AI agent development via the Langgraph and Langchain SDK . Your primary task is to generate compilable, logical, and complete Python code for a LangGraph state graph based on user input below. You must prioritize LLM-based implementations for relevant tasks and consider advanced graph architectures.\n",
    "\n",
    "**Input:**\n",
    "<INPUT>\n",
    "<OBJECTIVE>\n",
    "{objective}\n",
    "</OBJECTIVE>\n",
    "<USECASES>\n",
    "{usecases}\n",
    "</USECASES>\n",
    "<EXAMPLES>\n",
    "{examples}\n",
    "</EXAMPLES>\n",
    "</INPUT>\n",
    "---\n",
    "**Phase 1: Evaluating best architecture for the given INPUT**\n",
    "\n",
    "1.  **Identify Potential Architectures:** Consider if the described INPUT aligns with or would benefit from known advanced LangGraph architectures such as:\n",
    "    * **Plan and Execute**: Does the INPUT imply an agent which might need a planning step (e.g., breaking down a complex task) followed by the execution of those plans by one or more action nodes?\n",
    "    * **Agent Supervisor / Hierarchical Agent Teams**: Is the INPUT best served by a supervisor agent dispatching tasks to specialized worker agents, or a hierarchy of agents making decisions and delegating?\n",
    "    * **Multi-Agent Collaboration (e.g., Swarm Architecture)**: Does the problem benefit from multiple agents working in parallel or collaboratively, perhaps sharing insights or contributing to a common goal?\n",
    "    * **Reflection / Self-Correction (e.g., Self-Discover frameworks)**: Are there indications of iterative refinement, where results are evaluated and the process is adjusted?\n",
    "    * **Human in the Loop (HITL)**: Does the `description` of any node, or the overall process, imply a need for human review, approval, correction, or explicit input at specific stages (e.g., before executing a critical action, when confidence is low, or for subjective assessments)?\n",
    "\n",
    "2.  **Architectural Decision:**\n",
    "    * If you determine that one or more of these architectures are strongly applicable to the INPUT, choose to implement it.\n",
    "    * If no specific advanced architecture seems directly applicable for the given INPUT, proceed with a standard stateful graph construction based on the explicit langgraph nodes and edges.\n",
    "\n",
    "3.  **Initial Comment:** At the very beginning of your generated Python script, include a comment block stating:\n",
    "    * Which LangGraph architecture(s) (if any) you've identified and chosen to implement, with a brief justification based on your interpretation of the INPUT, provide dry runs of the usecases/examples.\n",
    "    * If you are proceeding with a standard graph, mention that.\n",
    "\n",
    "4. Generate a JSON representation of the architecture you have chosen, including:\n",
    "a.  `nodes`: A dictionary where each key is a unique node ID. The value for each node ID is an object containing:\n",
    "    * `id`: The node's identifier.\n",
    "    * `schema_info`: A string describing the structure of the `GraphState` (e.g., \"GraphState:\\\\n type: TypedDict\\\\n fields:\\\\n - name: input\\\\n type: str...\"). You will need to parse this to define the `GraphState` TypedDict.\n",
    "    * `input_schema`: The expected input schema for the node (typically \"GraphState\").\n",
    "    * `output_schema`: The schema of the output produced by the node (typically \"GraphState\", indicating a partial update).\n",
    "    * `description`: A natural language description of what the node does. This is crucial for determining implementation strategy and overall architecture.\n",
    "    * `function_name`: The suggested Python function name for this node.\n",
    "    * `code` (optional): A string containing Python code for the node's function. **Treat this `code` primarily as an illustration or a very basic version. Prioritize LLM-based solutions if the `description` suggests a more robust approach is needed.**\n",
    "\n",
    "b.  `edges`: A list of objects, each describing a directed edge in the graph. Each edge object contains:\n",
    "    * `source`: The ID of the source node (or \"__START__\" for the graph's entry point).\n",
    "    * `target`: The ID of the target node (or \"__END__\" for a graph termination point).\n",
    "    * `routing_conditions`: A natural language description of the condition under which this edge is taken, especially for conditional edges.\n",
    "    * `conditional`: A boolean flag, `true` if the edge is part of a conditional branch, `false` otherwise.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "** Phase 2: Graph Creation\n",
    "**Phase 2: Python Code Generation**\n",
    "\n",
    "Generate a single, self-contained, and compilable Python script that implements your chosen strategy.\n",
    "\n",
    "1.  **Imports:** Include all necessary Python libraries (e.g., `typing`, `langgraph.graph`, `langgraph.checkpoint.memory`, LLM client libraries like `langchain_openai`, `langchain_google_genai`, `langchain_core.pydantic_v1`, `langchain_core.tools`, `re`).\n",
    "\n",
    "2.  **State Definition (`GraphState`):**\n",
    "    * Define a `GraphState` class using `MessagesState` (langgraph prebuilt class).\n",
    "\n",
    "3.  **Node Implementation (Python Functions):**\n",
    "    For each conceptual node in your chosen architecture (these may map directly to JSON you define):\n",
    "    * Create a Python function. This function must accept the `GraphState` and return a dictionary representing the partial update to the state.\n",
    "    * **Decision Logic for Implementation (Prioritize LLM, No Mock Data):**\n",
    "        * **Default to LLM-Based Solutions:** Your default stance should be to implement an **LLM-based solution** if the node's `description` (from JSON or your architectural design) suggests tasks like:\n",
    "            * Natural Language Understanding (NLU)\n",
    "            * Complex classification or routing\n",
    "            * Content generation or summarization\n",
    "            * Tool selection and usage\n",
    "            * Planning or complex decision-making.\n",
    "            * Any task where an LLM would provide more robust, flexible, or intelligent behavior than simple hardcoded logic.\n",
    "        * **Handling Provided `code`:** If `code` is present in the JSON for a node, treat it as a **low-priority hint or a simplistic example**. Do **not** simply copy it if an LLM approach is more appropriate for the described task.\n",
    "        * **Algorithmic Logic (Use Sparingly):** Only use purely algorithmic Python code (like from the `code` attribute or written new) if the node's task is genuinely simple, deterministic (e.g., basic data formatting, fixed calculation), *and* an LLM would offer no significant benefit for that specific, narrow function.\n",
    "        * **Functional LLM Calls:** When an LLM is used, instantiate a generic model (e.g., `llm = ChatOpenAI(model=\"gpt-3.5-turbo\")` or `llm = ChatGoogleGenerativeAI(model=\"gemini-pro\")`) and include a **functional, descriptive prompt** relevant to the node's task. Ensure the code for the LLM call is complete and not just a comment. Add a `TODO` comment for the user to specify API keys and potentially refine the model/prompt.\n",
    "        * **No Mock Data:** Generated functions must be logical and aim for completeness. **Avoid using mock data or overly simplistic placeholder logic** where an LLM or a proper algorithmic implementation is expected.\n",
    "        * **Structured Output & Tools:** If the task implies structured output from an LLM or the use of tools, define necessary Pydantic models and/or LangChain tools, and integrate them with the LLM call.\n",
    "            * Define a Pydantic model (e.g., `from langchain_core.pydantic_v1 import BaseModel, Field`) representing the desired structured output.\n",
    "            * If implementing an LLM call, configure it to use the Pydantic model for its output (e.g., with OpenAI's function calling/tool usage features, or by instructing the LLM to generate JSON conforming to the model).\n",
    "        * **Tool Definition and Usage:** If a node's `description` (or your architectural design) implies the LLM within that node needs to interact with external systems, perform specific actions, or fetch data (e.g., \"search customer database,\" \"get weather update\"):\n",
    "                * Define these capabilities as discrete LangChain tools using the `@tool` decorator (e.g., `from langchain_core.tools import tool`).\n",
    "                * **Crucially, each tool's internal Python function should be self-contained and directly perform its advertised action** (e.g., make a specific API call to an external service, run a local script, perform a calculation, retrieve data algorithmically). **Avoid embedding a *new, separate general-purpose LLM call within the tool's own implementation logic* unless the tool's explicit and documented purpose is to be a specialized, self-contained sub-agent (which is an advanced case).** The primary LLM within the graph node is responsible for *deciding to call* the tool and for interpreting its output.\n",
    "                * Bind these well-defined tools to the LLM instance operating within that graph node. The node's LLM will then intelligently decide when to call a tool and with what inputs.\n",
    "        * **Human in the Loop Nodes:** If you've designed a HITL step as a dedicated node, its function might primarily format data for human review and then process the subsequent human input (which would be added to the state, potentially by an external mechanism or a subsequent node). The graph might pause using an interruption mechanism tied to this node.\n",
    "        * **State Coherence:** Ensure variable assignments and updates within node functions are coherent with the `GraphState` definition and how state is managed in LangGraph.\n",
    "\n",
    "4.  **Graph Construction (`StatefulGraph`):**\n",
    "    * Instantiate `StatefulGraph(GraphState)`.\n",
    "    * Add each implemented node function to the graph using `graph.add_node(\"node_id\", node_function)`.\n",
    "    * Set the graph's entry point using `graph.add_edge(START, \"entry_node_id\")` where `\"entry_node_id\"` is the target of the edge originating from `\"__START__\"`.\n",
    "\n",
    "5.  **Edge Implementation:**\n",
    "    * Iterate through the `edges` list in the JSON.\n",
    "    * **Regular Edges:** If `conditional` is `false`:\n",
    "        * If `target` is `__END__`, use `graph.add_edge(source_node_id, END)`.\n",
    "        * Otherwise, use `graph.add_edge(source_node_id, target_node_id)`.\n",
    "    * **Conditional Edges:** If `conditional` is `true`:\n",
    "        * The `source` node of these conditional edges is expected to produce some output in the `GraphState` (e.g., an `intent` field) that determines the next path.\n",
    "        * Create a separate routing function (e.g., `def route_after_source_node(state: GraphState) -> str:`).\n",
    "        * This routing function must inspect the relevant fields in the `state` and return the string ID of the next node to execute, based on the logic described in the `routing_conditions` for each conditional edge originating from that source.\n",
    "        * Use `graph.add_conditional_edges(source_node_id, routing_function, {{ \"target_id_1\": \"target_id_1\", \"target_id_2\": \"target_id_2\", ... \"__END__\": END }})`. The keys in the dictionary are the possible return values from your routing function, and the values are the actual node IDs or `END`.\n",
    "\n",
    "6.  **Compilation:**\n",
    "    * Instantiate an `InMemoryCheckpointer`: `checkpointer = InMemoryCheckpointer()`.\n",
    "    * Compile the graph: `final_app = graph.compile(checkpointer=checkpointer)`. The compiled graph must be assigned to a variable named `final_app`.\n",
    "\n",
    "---\n",
    "**Phase 3: Required Keys/Credentials Identification**\n",
    "\n",
    "After generating the complete Python script, add a separate section at the end of your response, clearly titled:\n",
    "`## Required Keys and Credentials`\n",
    "\n",
    "In this section, list all environment variables or API keys a user would need to set for the generated code to execute successfully (e.g., `OPENAI_API_KEY`, `GOOGLE_API_KEY`, tool-specific keys). If no external keys are needed, state that.\n",
    "\n",
    "---\n",
    "**Important Considerations (General):**\n",
    "* The primary goal is **compilable, logical, and functionally plausible Python code** that intelligently interprets the JSON input.\n",
    "* Focus on creating a system that leverages LLMs effectively for tasks suited to them.\n",
    "* Ensure node functions correctly update and return relevant parts of the `GraphState`.\n",
    "* If the provided `code` in the JSON uses specific libraries (e.g., `re`), make sure the corresponding import is included at the top of the script.\n",
    "* Handle `__START__` and `__END__` correctly in edge definitions. `langgraph.graph.START` and `langgraph.graph.END` should be used.\n",
    "\n",
    "Please generate the Python code and the list of required keys now:\n",
    "\"\"\")\n",
    "\n",
    "def code_node(state: AgentBuilderState):\n",
    "    \"\"\"\n",
    "    LangGraph node to generate the final Python code for the agent.\n",
    "    It uses the gathered agent_instructions and the CODE_GEN_PROMPT.\n",
    "    \"\"\"\n",
    "    logger.info(\"Executing code_node\")\n",
    "    instructions: AgentInstructions = state[\"agent_instructions\"]\n",
    "    \n",
    "    # Invoke LLM to generate code based on the detailed prompt and instructions\n",
    "    code_output = llm.invoke([HumanMessage(content=CODE_GEN_PROMPT.format(\n",
    "        objective=instructions.objective,\n",
    "        usecases=instructions.usecases,\n",
    "        examples=instructions.examples\n",
    "    ))])\n",
    "    \n",
    "    logger.info(\"Python code generated by LLM.\")\n",
    "    # Return the generated Python code and an AI message\n",
    "    return {\n",
    "        \"messages\": [AIMessage(content=\"Generated final python code!\")],\n",
    "        \"python_code\": code_output.content,\n",
    "    }\n",
    "\n",
    "# --- Tool/Function Definition Sub-Graph Components ---\n",
    "# This section defines components for a sub-graph that creates individual Python functions (tools).\n",
    "\n",
    "# Load tool/SDK information from JSON files.\n",
    "# Note: Hardcoded paths might need adjustment depending on the deployment environment.\n",
    "try:\n",
    "    with open(\"D:\\\\AgentAgent\\\\AgentAgent\\\\experiments\\\\tool_creation\\\\tools_link_json.json\") as f:\n",
    "        dict_tool_link = json.load(f)\n",
    "    with open(\"D:\\\\AgentAgent\\\\AgentAgent\\\\experiments\\\\tool_creation\\\\tools_doc_json.json\") as f:\n",
    "        dict_tool_doc = json.load(f)\n",
    "except FileNotFoundError:\n",
    "    logger.error(\"Tool link/doc JSON files not found. Please check the paths.\")\n",
    "    dict_tool_link = {} # Default to empty dict if file not found\n",
    "    dict_tool_doc = {}  # Default to empty dict if file not found\n",
    "\n",
    "\n",
    "def lowercase_keys(input_dict: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Utility function to convert all keys in a dictionary to lowercase.\n",
    "    \"\"\"\n",
    "    return {k.lower(): v for k, v in input_dict.items()}\n",
    "\n",
    "# Normalize keys in the loaded tool dictionaries\n",
    "dict_tool_link = lowercase_keys(dict_tool_link)\n",
    "dict_tool_doc = lowercase_keys(dict_tool_doc)\n",
    "\n",
    "# Prompt for initial analysis of a function's description\n",
    "Initial_prompt = \"\"\"You are an expert python developer. You will be given a description of a python function. \n",
    "\n",
    "Your job is to estimate and extract the following information:\n",
    "\n",
    "- What exactly does this python do. What is the detailed objective of the function. Please write 1-5 lines\n",
    "- Suggest or extract the name of the the function\n",
    "- What would be the inputs/arguements required into this function to make it work. Please all mentioned the type of each input\n",
    "- WHat would be output produced by this input. Please mention the output type \n",
    "\n",
    "Here is the description of the function you need to create:\n",
    "<description>\n",
    "{desc}\n",
    "</description>\n",
    "\"\"\"\n",
    "\n",
    "class FunctionInstructions(BaseModel):\n",
    "    \"\"\"\n",
    "    Pydantic model to structure the definition of a Python function (tool).\n",
    "    Used for structured output from the LLM during function analysis and code generation.\n",
    "    \"\"\"\n",
    "    objective: str = Field(description=\"what does this python function do\")\n",
    "    name: str = Field(description=\"name of the python function\")\n",
    "    input: List[str] = Field(description=\"what would be the input arguements to this function along with the types\")\n",
    "    output: List[str] = Field(description=\"what would be the output/return attributes for the function along with the types\")\n",
    "    name_toolkit: str = Field(description=\"what would be the toolkit/ code SDK that will be used\") # Name of the SDK\n",
    "    code: str = Field(description=\"the final python code for the function\")\n",
    "\n",
    "# class CodebuilderState(BaseModel): # This state is not used\n",
    "#     \"\"\"Instructions for defining a python function\"\"\"\n",
    "#     code: str = Field(description= \"tailored code for the python function\")\n",
    "\n",
    "\n",
    "def functional_analysis_node(state: FunctionInstructions):\n",
    "    \"\"\"\n",
    "    LangGraph node for analyzing a function description.\n",
    "    It uses the LLM with structured output (FunctionInstructions) to parse the description.\n",
    "    \"\"\"\n",
    "    logger.info(f\"Executing functional_analysis_node for: {state.objective[:50]}...\") # Log snippet of objective\n",
    "    llm_with_structured_output = llm.with_structured_output(FunctionInstructions)\n",
    "    \n",
    "    # Invoke LLM to get structured information about the function\n",
    "    functionalReport: FunctionInstructions = llm_with_structured_output.invoke(\n",
    "        [HumanMessage(content=Initial_prompt.format(desc=state.objective))]\n",
    "    )\n",
    "    logger.info(f\"Functional analysis complete for {functionalReport.name}.\")\n",
    "    return {\n",
    "        \"messages\": [AIMessage(content=\"Generated JSON code for function analysis!\")], # Consider a more descriptive message\n",
    "        \"objective\": functionalReport.objective,\n",
    "        \"name\": functionalReport.name,\n",
    "        \"input\": functionalReport.input,\n",
    "        \"output\": functionalReport.output,\n",
    "        # name_toolkit and code are not set here, they are determined in subsequent nodes\n",
    "    }\n",
    "\n",
    "# Prompt for writing the code of a function using a specific SDK\n",
    "write_code_prompt = \"\"\"\n",
    "You are a skilled code generation assistant. Your task is to create executable code using the following information:\n",
    "- SDK Documentation: The provided documentation outlines the functionalities and usage details of the SDK. Use this as the reference for constructing your code.\n",
    "- Objective: A clear description of what the code is intended to achieve.\n",
    "- Input: The expected input for the code (e.g., variables, parameters, data types).\n",
    "- Output: The desired result or outcome of the code (e.g., format, type, or structure).\n",
    "- SDK Name: The name of the SDK that must be used in the code.\n",
    "\n",
    "Your goal is to generate executable code that:\n",
    "- Adheres to the requirements outlined above.\n",
    "- Follows standard coding practices and is optimized for readability and efficiency.\n",
    "- Utilizes the specified SDK appropriately based on the documentation provided.\n",
    "- Only return a self contained function\n",
    "- Your output should only contain a code block containing the required function and nothing else. Please do no include any explainantions\n",
    "- Write your code in python\n",
    "- Please also provide which API keys will be required and define the API keys as part of the function\n",
    "- Please also write the doc string for the python function\n",
    "- Ensure that the function you produce is decorated with @tool. That means its defination should be preceeded by '@tool' in the line above\n",
    "\n",
    "Here are some details about the python function you will be creating:\n",
    "<objective>\n",
    "{objective}\n",
    "</objective>\n",
    "\n",
    "<input schema>\n",
    "{inputs}\n",
    "</input schema>\n",
    "\n",
    "<output schema>\n",
    "{output}\n",
    "</output schema>\n",
    "\n",
    "<name of function>\n",
    "{name}\n",
    "</name of function>\n",
    "\n",
    "Documentation for SDK that might be helpful:\n",
    "<documentation>\n",
    "{docs}\n",
    "</documentation>\n",
    "\"\"\"\n",
    "\n",
    "# Prompt for selecting the best SDK for a given function\n",
    "Best_sdk_prompt = \"\"\"\n",
    "You are a highly specialized language model designed to assist in selecting the most suitable SDK for a given use case. You are provided with the following:\n",
    "- A dictionary containing pairs of SDK names and their respective descriptions.\n",
    "- Requirements for a piece of code, including the objective, input, and output.\n",
    "\n",
    "Your task is to:\n",
    "- Identify the SDK from the provided dictionary whose description best matches the given use case described in the code requirements.\n",
    "- Also give preferences to SDKs that are generally more well known or are used more frequently in the industry (Use google tools for anything search related)\n",
    "- Return only the name of the matching SDK without any additional text or formatting.\n",
    "- Please ensure that the string you return is a valid key of the dictionary you get as input. PLEASE VERIFY THAT THE STRING YOU RETURN EXISTS AS A KEY IN THE INPUT DICTIONARY\n",
    "\n",
    "Input Example:\n",
    "Dictionary:\n",
    "{{\n",
    "\"SDK_A\": \"[SDK_CC_ABC]Provides tools for web scraping and data extraction.\",\n",
    "\"SDK_B\": \"Enables natural language processing for unstructured text.\",\n",
    "\"SDK_C\": \"Facilitates the integration of payment gateways in applications.\"\n",
    "}}\n",
    "Code Requirements:\n",
    "Objective: Extract data from multiple web pages.\n",
    "Input: URLs of the web pages.\n",
    "Output: Structured data in JSON format.\n",
    "\n",
    "Expected Output:\n",
    "SDK_A\n",
    "\n",
    "\n",
    "Input :\n",
    "<dictionary>\n",
    "{dictionary}\n",
    "</dictionary>\n",
    "\n",
    "<objective>\n",
    "{objective}\n",
    "</objective>\n",
    "\n",
    "<input schema>\n",
    "{inputs}\n",
    "</input schema>\n",
    "\n",
    "<output schema>\n",
    "{output}\n",
    "</output schema>\n",
    "\n",
    "<name of function>\n",
    "{name}\n",
    "</name of function>\n",
    "\"\"\"\n",
    "\n",
    "def sdk_production_node(state: FunctionInstructions):\n",
    "    \"\"\"\n",
    "    LangGraph node to identify the most suitable SDK for the function.\n",
    "    It uses the LLM with the Best_sdk_prompt and the loaded SDK documentation.\n",
    "    \"\"\"\n",
    "    logger.info(f\"Executing sdk_production_node for function: {state.name}\")\n",
    "    objective_agent: str = state.objective\n",
    "    name: str = state.name\n",
    "    input_args: List[str] = state.input\n",
    "    output_args: List[str] = state.output\n",
    "    \n",
    "    response = llm.invoke([HumanMessage(content=Best_sdk_prompt.format(\n",
    "        objective=objective_agent,\n",
    "        inputs=input_args,\n",
    "        output=output_args,\n",
    "        name=name,\n",
    "        dictionary=dict_tool_doc # Using the dictionary of SDK descriptions\n",
    "    ))])\n",
    "    \n",
    "    sdk_name = response.content.lower().strip()\n",
    "    logger.info(f\"SDK identified: {sdk_name} for function {name}\")\n",
    "    \n",
    "    # Ensure the identified SDK is valid\n",
    "    if sdk_name not in dict_tool_doc:\n",
    "        logger.warning(f\"LLM returned an SDK name ('{sdk_name}') not found in dict_tool_doc. Defaulting or error handling might be needed.\")\n",
    "        # Potentially raise an error or pick a default if this happens\n",
    "    \n",
    "    return {\n",
    "        \"messages\": [AIMessage(content=f\"SDK '{sdk_name}' identified for function {name}.\")],\n",
    "        \"name_toolkit\": sdk_name\n",
    "    }\n",
    "\n",
    "def code_production_node(state: FunctionInstructions):\n",
    "    \"\"\"\n",
    "    LangGraph node to generate the Python code for the function using the identified SDK.\n",
    "    It fetches SDK documentation and uses the write_code_prompt.\n",
    "    \"\"\"\n",
    "    logger.info(f\"Executing code_production_node for function: {state.name} using SDK: {state.name_toolkit}\")\n",
    "    objective_agent: str = state.objective\n",
    "    name: str = state.name\n",
    "    input_args: List[str] = state.input\n",
    "    output_args: List[str] = state.output\n",
    "    toolkit: str = state.name_toolkit\n",
    "    \n",
    "    docs = \"\"\n",
    "    if toolkit in dict_tool_link:\n",
    "        docs = fetch_documents(dict_tool_link[toolkit]) # Fetch documentation for the chosen SDK\n",
    "        logger.info(f\"Fetched documentation for SDK: {toolkit}\")\n",
    "    else:\n",
    "        logger.warning(f\"No documentation link found for SDK: {toolkit}. Proceeding without SDK-specific docs.\")\n",
    "\n",
    "    response = llm.invoke([HumanMessage(content=write_code_prompt.format(\n",
    "        objective=objective_agent,\n",
    "        inputs=input_args,\n",
    "        output=output_args,\n",
    "        name=name,\n",
    "        docs=docs,\n",
    "    ))])\n",
    "    \n",
    "    logger.info(f\"Code generated for function: {name}\")\n",
    "    return {\n",
    "        \"messages\": [AIMessage(content=f\"Generated code for tool/function: {name}\")],\n",
    "        \"code\": response.content\n",
    "    }\n",
    "\n",
    "# --- Tool Information Graph Definition (`tool_infograph`) ---\n",
    "# This graph orchestrates the creation of a single tool/function.\n",
    "# It takes a function description and produces its Python code.\n",
    "\n",
    "# Using InMemorySaver for this sub-graph as its state might not need to persist long-term\n",
    "tool_info_checkpointer = InMemorySaver()\n",
    "tool_info_workflow = StateGraph(FunctionInstructions) # Define state type for this graph\n",
    "\n",
    "# Add nodes to the tool information graph\n",
    "tool_info_workflow.add_node(\"func_analysis\", functional_analysis_node)\n",
    "tool_info_workflow.add_node(\"sdk_write\", sdk_production_node)\n",
    "tool_info_workflow.add_node(\"code_write\", code_production_node)\n",
    "\n",
    "# Define edges for the tool information graph\n",
    "tool_info_workflow.add_edge(START, \"func_analysis\") # Start with functional analysis\n",
    "tool_info_workflow.add_edge(\"func_analysis\", \"sdk_write\") # Then identify SDK\n",
    "tool_info_workflow.add_edge(\"sdk_write\", \"code_write\")   # Then write code\n",
    "tool_info_workflow.add_edge(\"code_write\", END)          # End after code writing\n",
    "\n",
    "# Compile the tool information graph\n",
    "tool_infograph = tool_info_workflow.compile(checkpointer=tool_info_checkpointer)\n",
    "logger.info(\"Tool information graph (tool_infograph) compiled.\")\n",
    "\n",
    "\n",
    "# --- Tool Compilation Sub-Graph Components ---\n",
    "# This section defines components for a sub-graph that identifies tools in the main agent code,\n",
    "# generates their implementations (using tool_infograph), and compiles them back.\n",
    "\n",
    "class ToolCollectorState(MessagesState): # Renamed from 'toolcollector' for convention\n",
    "    \"\"\"\n",
    "    State for the graph that collects and compiles multiple tool codes.\n",
    "    \"\"\"\n",
    "    total_code: List[str] = Field(default_factory=list, description=\"List of generated Python code snippets for tools.\")\n",
    "    compiled_code: str = Field(description=\"The main agent Python code, potentially with placeholders for tools.\")\n",
    "\n",
    "# test_message = \"\"\" ... \"\"\" # This variable was not used, so it's removed.\n",
    "\n",
    "# Prompt to extract descriptions of functions decorated with @tool\n",
    "tool_desc_prompt = \"\"\"\n",
    "You are an AI assistant designed to analyze Python code. Your task is to identify all function definitions in the provided Python snippet that are decorated with @tool. You must return a dictionary where:\n",
    "- The keys are the names of the identified functions.\n",
    "- You only need to pick up a function if it is decorated with '@tool' or '@tool' just preceeds the function. Otherwise leave the function alone\n",
    "- The values are descriptions of what each function is supposed to do. If a function contains a docstring, extract it as the description. If a docstring is missing, infer the function's purpose from its structure and comments.\n",
    "- The output should just be a json. it should not include \"```json\" and \"```\" at the start or end of it. it should start with a \"{{\" and end with a \"}}\"\n",
    "Example Input:\n",
    "@tool\n",
    "def calculate_area(length, width):\n",
    "    \"Calculates the area of a rectangle.\"\n",
    "    return length * width\n",
    "\n",
    "@tool\n",
    "def greet(name):\n",
    "    return f\"Hello, {{name}}!\"\n",
    "\n",
    "\n",
    "Expected Output:\n",
    "{{\n",
    "    \"calculate_area\": \"Calculates the area of a rectangle.\",\n",
    "    \"greet\": \"Greets a user by name.\"\n",
    "}}\n",
    "\n",
    "\n",
    "Instructions:\n",
    "- Identify functions that have the @tool decorator.\n",
    "- Extract function names and descriptions (either from docstrings or inferred).\n",
    "- Return the output as a structured JSON.\n",
    "- Please only return a json object that can be converted into a json directly. DO NOT RETURN ANYTHING OTHER THAN A JSON. it should start with a \"{{\" and end with a \"}}\" and there should not be any markers in the response to show that it is a json\n",
    "\n",
    "Python code:\n",
    "<code>\n",
    "{code}\n",
    "</code>\n",
    "\"\"\"\n",
    "\n",
    "# Prompt to compile main code with generated tool function definitions\n",
    "tool_compile_prompt = \"\"\"\n",
    "You are python code writing expert. You are given 2 snippets of code, your job is to combine them. \n",
    "The first snippet of code contains a compilable code with some functions compilable but empty. \n",
    "The second snippet of code contains the defination of those functions. \n",
    "Please fo through the second snippet of code, match the function in the first snippet and replace the functional definition written in the first snippet with one found in second snippet\n",
    "\n",
    "Please only return compilable python code\n",
    "Here are the code snippets:\n",
    "<code_snippet1>\n",
    "{complete_code}\n",
    "</code_snippet1>\n",
    "<code_snippet2>\n",
    "{functions}\n",
    "</code_snippet2>\n",
    "\"\"\"\n",
    "\n",
    "def graph_map_step(state: ToolCollectorState):\n",
    "    \"\"\"\n",
    "    LangGraph node to identify tools in the compiled agent code and generate their implementations.\n",
    "    It iterates over functions marked with @tool, invokes `tool_infograph` for each,\n",
    "    and collects the generated code.\n",
    "    \"\"\"\n",
    "    logger.info(\"Executing graph_map_step to identify and generate tool implementations.\")\n",
    "    current_code = state['compiled_code']\n",
    "    \n",
    "    # Use LLM to find @tool decorated functions and their descriptions\n",
    "    response_1 = llm.invoke([HumanMessage(content=tool_desc_prompt.format(code=current_code))])\n",
    "    response_content = response_1.content.strip()\n",
    "    \n",
    "    json_objects = {}\n",
    "    try:\n",
    "        # Handle potential \"```json\\n\" prefix and \"\\n```\" suffix from LLM\n",
    "        if response_content.startswith(\"```json\"):\n",
    "            response_content = response_content[7:]\n",
    "        if response_content.endswith(\"```\"):\n",
    "            response_content = response_content[:-3]\n",
    "        json_objects = json.loads(response_content.strip())\n",
    "        logger.info(f\"Identified tools for implementation: {list(json_objects.keys())}\")\n",
    "    except json.JSONDecodeError as e:\n",
    "        logger.error(f\"Failed to parse JSON from tool_desc_prompt output: {e}\")\n",
    "        logger.error(f\"LLM response content was: {response_content}\")\n",
    "        # If parsing fails, no tools will be generated in this step.\n",
    "        # Consider how to handle this error robustly (e.g., retry, skip, or error state).\n",
    "\n",
    "    generated_tool_codes = []\n",
    "    for tool_name, tool_description in json_objects.items():\n",
    "        logger.info(f\"Generating implementation for tool: {tool_name} - Description: {tool_description[:50]}...\")\n",
    "        # Each tool generation runs in its own \"thread\" or configuration for the sub-graph\n",
    "        thread_id = str(uuid.uuid4())\n",
    "        config = {\"configurable\": {\"thread_id\": thread_id}}\n",
    "        \n",
    "        # Initial state for the tool_infograph\n",
    "        initial_tool_state = {\n",
    "            \"objective\": tool_description, # The description from tool_desc_prompt becomes the objective\n",
    "            \"name\": tool_name,\n",
    "            \"input\": [], # Inputs/outputs could be further refined or extracted by tool_desc_prompt\n",
    "            \"output\": [],\n",
    "            \"name_toolkit\": \"\", # To be determined by sdk_production_node in tool_infograph\n",
    "            \"code\": \"\"          # To be generated by code_production_node in tool_infograph\n",
    "        }\n",
    "\n",
    "        # Stream updates from the tool_infograph execution (optional, good for logging/debugging)\n",
    "        for output_update in tool_infograph.stream(initial_tool_state, config, stream_mode=\"updates\"):\n",
    "            logger.debug(f\"Update from tool_infograph for {tool_name}: {output_update}\")\n",
    "        \n",
    "        # Get the final state of the tool_infograph to retrieve the generated code\n",
    "        final_tool_state = tool_infograph.get_state(config)\n",
    "        if \"code\" in final_tool_state.values and final_tool_state.values[\"code\"]:\n",
    "            generated_tool_codes.append(final_tool_state.values[\"code\"])\n",
    "            logger.info(f\"Successfully generated code for tool: {tool_name}\")\n",
    "        else:\n",
    "            logger.warning(f\"No code generated for tool: {tool_name}. State: {final_tool_state.values}\")\n",
    "            \n",
    "    return {\n",
    "        \"messages\": [AIMessage(content=\"Tool identification and individual code generation complete.\")],\n",
    "        \"total_code\": generated_tool_codes, # List of code strings for each tool\n",
    "        \"compiled_code\": current_code # Pass along the original compiled code\n",
    "    }\n",
    "\n",
    "def compile_tool_code_node(state: ToolCollectorState): # Renamed for clarity\n",
    "    \"\"\"\n",
    "    LangGraph node to combine the main agent code with the generated tool function codes.\n",
    "    Uses an LLM with `tool_compile_prompt` to perform the merge.\n",
    "    \"\"\"\n",
    "    logger.info(\"Executing compile_tool_code_node to merge tool codes with main agent code.\")\n",
    "    tool_code_list = state['total_code']\n",
    "    main_agent_code = state['compiled_code'] # Renamed for clarity\n",
    "    \n",
    "    if not tool_code_list:\n",
    "        logger.info(\"No tool codes to compile. Returning original agent code.\")\n",
    "        return {\n",
    "            \"messages\": [AIMessage(content=\"No new tool functions to compile.\")],\n",
    "            \"compiled_code\": main_agent_code # Return original if no tools were generated\n",
    "        }\n",
    "\n",
    "    full_tool_code = \"\\n\\n\".join(tool_code_list) # Join tool codes with newlines\n",
    "    \n",
    "    # Use LLM to merge the main agent code with the generated tool definitions\n",
    "    response = llm.invoke([HumanMessage(content=tool_compile_prompt.format(\n",
    "        complete_code=main_agent_code,\n",
    "        functions=full_tool_code\n",
    "    ))])\n",
    "    \n",
    "    logger.info(\"Main agent code compiled with tool function definitions.\")\n",
    "    # The response from this LLM call is expected to be the final, complete Python code\n",
    "    return {\n",
    "        \"messages\": [AIMessage(content=response.content)], # Storing the LLM's final code as a message for now\n",
    "        \"compiled_code\": response.content # Update compiled_code with the final merged code\n",
    "    }\n",
    "\n",
    "# --- Tool Compilation Graph Definition (`tool_compile_graph`) ---\n",
    "# This graph manages the process of finding @tool placeholders in the main generated code,\n",
    "# generating their implementations, and then merging them back.\n",
    "tool_compile_workflow = StateGraph(ToolCollectorState)\n",
    "\n",
    "# Add nodes to the tool compilation graph\n",
    "tool_compile_workflow.add_node(\"graph_map_step\", graph_map_step)\n",
    "tool_compile_workflow.add_node(\"compile_tools\", compile_tool_code_node) # Renamed node\n",
    "\n",
    "# Define edges for the tool compilation graph\n",
    "tool_compile_workflow.add_edge(START, \"graph_map_step\")\n",
    "tool_compile_workflow.add_edge(\"graph_map_step\", \"compile_tools\")\n",
    "tool_compile_workflow.add_edge(\"compile_tools\", END)\n",
    "\n",
    "# Compile the tool compilation graph\n",
    "# This graph doesn't seem to require a checkpointer in this setup,\n",
    "# but adding one if state needs to be inspected or persisted.\n",
    "tool_compile_graph = tool_compile_workflow.compile()\n",
    "logger.info(\"Tool compilation graph (tool_compile_graph) compiled.\")\n",
    "\n",
    "\n",
    "# --- Main Agent Generation Graph ---\n",
    "# This is the primary graph that orchestrates the entire agent building process.\n",
    "\n",
    "def call_tool_subgraph_node(state: AgentBuilderState): # Renamed for clarity\n",
    "    \"\"\"\n",
    "    LangGraph node that invokes the `tool_compile_graph` sub-graph.\n",
    "    It passes the currently generated agent code (which might have tool placeholders)\n",
    "    to the sub-graph for tool implementation and compilation.\n",
    "    \"\"\"\n",
    "    logger.info(\"Executing call_tool_subgraph_node.\")\n",
    "    # Initial state for the tool_compile_graph\n",
    "    subgraph_input = {\n",
    "        \"compiled_code\": state[\"python_code\"],\n",
    "        # total_code will be populated by graph_map_step within the subgraph\n",
    "        \"messages\": [] # Start with fresh messages for the subgraph\n",
    "    }\n",
    "    \n",
    "    # Invoke the tool compilation sub-graph\n",
    "    subgraph_output = tool_compile_graph.invoke(subgraph_input)\n",
    "    \n",
    "    logger.info(\"Tool compilation sub-graph finished.\")\n",
    "    # Update the main graph's python_code with the output from the sub-graph\n",
    "    return {\"python_code\": subgraph_output[\"compiled_code\"], \"messages\": subgraph_output[\"messages\"]}\n",
    "\n",
    "\n",
    "# Checkpointer for the main agent generation graph\n",
    "main_checkpointer = MemorySaver()\n",
    "main_workflow = StateGraph(AgentBuilderState) # Define state type\n",
    "\n",
    "# Add nodes to the main workflow\n",
    "main_workflow.add_node(\"requirement_analysis_node\", requirement_analysis_node)\n",
    "main_workflow.add_node(\"code_node\", code_node)\n",
    "main_workflow.add_node(\"tool_subgraph_processing\", call_tool_subgraph_node) # Renamed node\n",
    "\n",
    "# Define edges for the main workflow\n",
    "main_workflow.add_edge(START, \"requirement_analysis_node\")             # Start with requirement analysis\n",
    "main_workflow.add_edge(\"requirement_analysis_node\", \"code_node\")        # Then generate initial code\n",
    "main_workflow.add_edge(\"code_node\", \"tool_subgraph_processing\")    # Then process/implement tools via sub-graph\n",
    "main_workflow.add_edge(\"tool_subgraph_processing\", END)            # End after tool processing\n",
    "\n",
    "# Compile the main agent generation graph\n",
    "agent_generator_graph = main_workflow.compile(checkpointer=main_checkpointer)\n",
    "logger.info(\"Main agent generator graph compiled.\")\n",
    "\n",
    "\n",
    "# --- Example Invocation (Optional) ---\n",
    "# This section can be used to demonstrate how to run the main graph.\n",
    "# For example:\n",
    "# if __name__ == \"__main__\":\n",
    "#     logger.info(\"Starting agent generation process...\")\n",
    "#     initial_input = {\"messages\": [HumanMessage(content=\"I want to build an agent that can tell me the weather.\")]}\n",
    "#     config = {\"configurable\": {\"thread_id\": \"user-thread-1\"}}\n",
    "    \n",
    "#     for event in agent_generator_graph.stream(initial_input, config=config):\n",
    "#         for key, value in event.items():\n",
    "#             logger.info(f\"Event from graph: {key} - {value}\")\n",
    "#             if key == \"requirement_analysis\" and isinstance(value, dict) and 'messages' in value:\n",
    "#                 last_message = value['messages'][-1]\n",
    "#                 if isinstance(last_message, HumanMessage) and last_message.content.startswith(\"__interrupt__\"):\n",
    "#                     # This is a simplified way to handle interruption for demo.\n",
    "#                     # In a real app, you'd present this to the user and get their input.\n",
    "#                     user_response = input(f\"Agent asks: {last_message.content[len('__interrupt__'):].strip()} Your response: \")\n",
    "#                     # How to reinvoke or continue with new input needs careful handling with stream/invoke\n",
    "#                     # For simplicity, this example doesn't fully implement the interactive loop here.\n",
    "#                     logger.info(f\"User input received: {user_response} (manual continuation needed for stream)\")\n",
    "\n",
    "\n",
    "#     final_state = agent_generator_graph.get_state(config)\n",
    "#     logger.info(\"\\n--- Final Generated Python Code ---\")\n",
    "#     print(final_state.values.get(\"python_code\"))\n",
    "#     logger.info(\"Agent generation process complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "be41e904",
   "metadata": {},
   "outputs": [],
   "source": [
    "query= \" need to create a worklow with the objective of managing my social media.  It should be able to tell me trends from social media for sports, get me all the relevant people I should contant for a spsonsoring a specific post,  suggest me content I should be posting, make content for me if I give it a description * Identifying social media trends in sports. *   Finding relevant people for sponsoring specific posts. *   Suggesting content to post. *   Generating content based on a description.  Examples: Given I ask it about trends in football, it goes through recent viral reels in football and tell me Q&A reels are trending If I tell it I want people who would be interested in a post about UCL football, it gives me current players like Dembele etc If I tell it I want to make a post about UCL football, it goes through what is trending and an agent that thinks about social media posts and tells me we should post a highlight reel\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2fd66855",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-03 02:34:15,765 - INFO - Executing requirement_analysis_node\n",
      "2025-06-03 02:34:18,900 - INFO - LLM successfully called AgentInstructions tool.\n",
      "2025-06-03 02:34:18,903 - INFO - Executing code_node\n",
      "2025-06-03 02:34:56,649 - INFO - Python code generated by LLM.\n",
      "2025-06-03 02:34:56,652 - INFO - Executing call_tool_subgraph_node.\n",
      "2025-06-03 02:34:56,657 - INFO - Executing graph_map_step to identify and generate tool implementations.\n",
      "2025-06-03 02:35:00,351 - INFO - Identified tools for implementation: ['search_viral_content', 'influencer_search']\n",
      "2025-06-03 02:35:00,351 - INFO - Generating implementation for tool: search_viral_content - Description: Searches for recent viral social media content or ...\n",
      "2025-06-03 02:35:00,355 - INFO - Executing functional_analysis_node for: Searches for recent viral social media content or ...\n",
      "2025-06-03 02:35:04,252 - INFO - Functional analysis complete for search_trending_content.\n",
      "2025-06-03 02:35:04,254 - INFO - Executing sdk_production_node for function: search_trending_content\n",
      "2025-06-03 02:35:34,714 - INFO - SDK identified: google_search for function search_trending_content\n",
      "2025-06-03 02:35:34,716 - INFO - Executing code_production_node for function: search_trending_content using SDK: google_search\n",
      "2025-06-03 02:35:35,425 - INFO - HTTP Request: GET https://python.langchain.com/docs/integrations/tools/google_search \"HTTP/1.1 308 Permanent Redirect\"\n",
      "2025-06-03 02:35:35,468 - INFO - HTTP Request: GET https://python.langchain.com/docs/integrations/tools/google_search/ \"HTTP/1.1 200 OK\"\n",
      "2025-06-03 02:35:35,593 - INFO - Fetched documentation for SDK: google_search\n",
      "2025-06-03 02:35:40,438 - INFO - Code generated for function: search_trending_content\n",
      "2025-06-03 02:35:40,441 - INFO - Successfully generated code for tool: search_viral_content\n",
      "2025-06-03 02:35:40,441 - INFO - Generating implementation for tool: influencer_search - Description: Searches for relevant influencers or public figure...\n",
      "2025-06-03 02:35:40,445 - INFO - Executing functional_analysis_node for: Searches for relevant influencers or public figure...\n",
      "2025-06-03 02:35:44,074 - INFO - Functional analysis complete for search_influencers.\n",
      "2025-06-03 02:35:44,077 - INFO - Executing sdk_production_node for function: search_influencers\n",
      "2025-06-03 02:36:10,081 - INFO - SDK identified: golden_query for function search_influencers\n",
      "2025-06-03 02:36:10,083 - INFO - Executing code_production_node for function: search_influencers using SDK: golden_query\n",
      "2025-06-03 02:36:10,763 - INFO - HTTP Request: GET https://python.langchain.com/docs/integrations/tools/golden_query \"HTTP/1.1 308 Permanent Redirect\"\n",
      "2025-06-03 02:36:10,792 - INFO - HTTP Request: GET https://python.langchain.com/docs/integrations/tools/golden_query/ \"HTTP/1.1 200 OK\"\n",
      "2025-06-03 02:36:10,913 - INFO - Fetched documentation for SDK: golden_query\n",
      "2025-06-03 02:36:18,964 - INFO - Code generated for function: search_influencers\n",
      "2025-06-03 02:36:18,968 - INFO - Successfully generated code for tool: influencer_search\n",
      "2025-06-03 02:36:18,971 - INFO - Executing compile_tool_code_node to merge tool codes with main agent code.\n",
      "2025-06-03 02:37:08,063 - INFO - Main agent code compiled with tool function definitions.\n",
      "2025-06-03 02:37:08,066 - INFO - Tool compilation sub-graph finished.\n"
     ]
    }
   ],
   "source": [
    "# Run the graph until the interrupt is hit.\n",
    "config = {\"configurable\": {\"thread_id\": \"some_id\"}}\n",
    "result = agent_generator_graph.invoke({\"messages\": HumanMessage(content=query)}, config=config) \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef6274e",
   "metadata": {},
   "source": [
    "### Langsmith link: https://smith.langchain.com/public/b8c5cb7a-7c20-4f72-bbc0-300becdcd4ab/r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4536c452",
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "import os\n",
    "import json\n",
    "from typing import Annotated, List, Literal, TypedDict\n",
    "\n",
    "from langchain_core.messages import BaseMessage, HumanMessage, SystemMessage\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_core.tools import tool\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.checkpoint.memory import InMemoryCheckpointer\n",
    "from langgraph.graph import END, START, StateGraph\n",
    "from langgraph.prebuilt import MessagesState\n",
    "\n",
    "# New imports for the actual tool implementations\n",
    "from langchain_google_community import GoogleSearchAPIWrapper\n",
    "from langchain_community.utilities.golden_query import GoldenQueryAPIWrapper\n",
    "\n",
    "# --- Phase 1: Architectural Decision and Justification ---\n",
    "\"\"\"\n",
    "LangGraph Architecture: Agent Supervisor / Hierarchical Agent Teams\n",
    "\n",
    "Justification:\n",
    "The problem of \"managing social media\" naturally breaks down into distinct, specialized sub-tasks:\n",
    "1.  Identifying social media trends.\n",
    "2.  Finding relevant people for sponsoring posts.\n",
    "3.  Suggesting content ideas.\n",
    "4.  Generating content.\n",
    "\n",
    "A Supervisor/Hierarchical Agent Team architecture is ideal because:\n",
    "-   It allows for a central \"SocialMediaManager\" (supervisor) agent to interpret the user's intent and intelligently route the request to the most appropriate specialized worker agent.\n",
    "-   Each worker agent (TrendAnalyst, SponsorFinder, ContentStrategist, ContentGenerator) can be highly focused and optimized for its specific task, potentially using different tools or prompt strategies.\n",
    "-   This modularity enhances maintainability, scalability, and clarity of responsibilities within the graph.\n",
    "\n",
    "Dry Runs of Use Cases:\n",
    "\n",
    "1.  **'Identifying social media trends in sports.' (e.g., \"trends in football\")**\n",
    "    *   **User Input:** \"Tell me about trends in football.\"\n",
    "    *   **SocialMediaManager:** Receives input, identifies intent as \"identify trends,\" sets `next_agent` to \"TrendAnalyst\".\n",
    "    *   **TrendAnalyst:** Receives the query, uses its LLM and potentially a `search_viral_content` tool (simulated) to find trends. Returns \"Q&A reels are trending in football.\"\n",
    "    *   **EndNode:** Formats the final response.\n",
    "    *   **Output:** \"Based on your query about trends in football, the Trend Analyst found that Q&A reels are currently trending.\"\n",
    "\n",
    "2.  **'Finding relevant people for sponsoring specific posts.' (e.g., \"people who would be interested in a post about UCL football\")**\n",
    "    *   **User Input:** \"I need people interested in a post about UCL football for sponsorship.\"\n",
    "    *   **SocialMediaManager:** Receives input, identifies intent as \"find sponsors,\" sets `next_agent` to \"SponsorFinder\".\n",
    "    *   **SponsorFinder:** Receives the query, uses its LLM and potentially an `influencer_search` tool (simulated) to identify relevant individuals. Returns \"Current players like Dembele, Mbappe, and influencers focusing on European football.\"\n",
    "    *   **EndNode:** Formats the final response.\n",
    "    *   **Output:** \"For your post about UCL football, the Sponsor Finder suggests current players like Dembele, Mbappe, and influencers focusing on European football.\"\n",
    "\n",
    "3.  **'Suggesting content to post.' (e.g., \"make a post about UCL football\")**\n",
    "    *   **User Input:** \"I want to make a post about UCL football, what should I post?\"\n",
    "    *   **SocialMediaManager:** Receives input, identifies intent as \"suggest content,\" sets `next_agent` to \"ContentStrategist\".\n",
    "    *   **ContentStrategist:** Receives the query, uses its LLM to brainstorm content ideas, potentially considering trends (either from its own knowledge or by implicitly 'consulting' a trend-like function). Returns \"A highlight reel of recent UCL matches would be highly engaging.\"\n",
    "    *   **EndNode:** Formats the final response.\n",
    "    *   **Output:** \"Regarding your UCL football post, the Content Strategist suggests that a highlight reel of recent UCL matches would be highly engaging.\"\n",
    "\n",
    "4.  **'Generating content based on a description.' (e.g., \"Generate a script for a UCL highlight reel.\")**\n",
    "    *   **User Input:** \"Generate a script for a UCL highlight reel focusing on top goals.\"\n",
    "    *   **SocialMediaManager:** Receives input, identifies intent as \"generate content,\" sets `next_agent` to \"ContentGenerator\".\n",
    "    *   **ContentGenerator:** Receives the query, uses its LLM to generate a detailed script. Returns a multi-line script.\n",
    "    *   **EndNode:** Formats the final response.\n",
    "    *   **Output:** \"Here is the generated content for your UCL highlight reel script: [Generated Script Content]\"\n",
    "\"\"\"\n",
    "\n",
    "# --- Phase 2: Python Code Generation ---\n",
    "\n",
    "# 1. State Definition\n",
    "class GraphState(MessagesState):\n",
    "    \"\"\"\n",
    "    Represents the state of our graph.\n",
    "\n",
    "    Attributes:\n",
    "        user_query: The initial query from the user.\n",
    "        agent_output: The output generated by the last specialized agent.\n",
    "        next_agent: The name of the next agent to route to, determined by the supervisor.\n",
    "        final_response: The consolidated response to be presented to the user.\n",
    "    \"\"\"\n",
    "    user_query: str = Field(default=\"\")\n",
    "    agent_output: str = Field(default=\"\")\n",
    "    next_agent: Literal[\"TrendAnalyst\", \"SponsorFinder\", \"ContentStrategist\", \"ContentGenerator\", \"END\"] = Field(default=\"END\")\n",
    "    final_response: str = Field(default=\"\")\n",
    "\n",
    "\n",
    "# 2. Node Implementations (Python Functions)\n",
    "\n",
    "# Initialize LLM\n",
    "# TODO: Replace with your preferred LLM. Ensure API key is set as an environment variable.\n",
    "# For OpenAI: OPENAI_API_KEY\n",
    "# For Google: GOOGLE_API_KEY\n",
    "llm = ChatOpenAI(model=\"gpt-4o\", temperature=0) # Using gpt-4o for better reasoning and tool use\n",
    "# llm = ChatGoogleGenerativeAI(model=\"gemini-pro\", temperature=0)\n",
    "\n",
    "# Define tools (now using real implementations from snippet2)\n",
    "@tool\n",
    "def search_viral_content(query: str) -> str:\n",
    "    \"\"\"\n",
    "    Searches for recent viral social media content or trending topics related to the query.\n",
    "    Useful for identifying current trends in sports, entertainment, etc.\n",
    "    Requires GOOGLE_API_KEY and GOOGLE_CSE_ID environment variables to be set.\n",
    "    \"\"\"\n",
    "    # The GoogleSearchAPIWrapper will automatically pick up GOOGLE_API_KEY and GOOGLE_CSE_ID from os.environ\n",
    "    search = GoogleSearchAPIWrapper()\n",
    "\n",
    "    # Perform a search for the query, focusing on recent and trending aspects\n",
    "    # Limiting to 5 results as a reasonable number for trending topics\n",
    "    search_results = search.results(f\"recent viral OR trending {query} social media\", num_results=5)\n",
    "\n",
    "    trending_content = []\n",
    "    for result in search_results:\n",
    "        # Combine title and snippet to form a descriptive string for each trending item\n",
    "        content_string = f\"Title: {result.get('title', 'N/A')}\\nSnippet: {result.get('snippet', 'N/A')}\"\n",
    "        trending_content.append(content_string)\n",
    "    \n",
    "    # Join results into a single string, as the original tool returned a string\n",
    "    return \"\\n\\n\".join(trending_content) if trending_content else \"No trending content found.\"\n",
    "\n",
    "@tool\n",
    "def influencer_search(topic: str) -> str:\n",
    "    \"\"\"\n",
    "    Searches for relevant influencers or public figures interested in a specific topic\n",
    "    who might be suitable for sponsorships or collaborations.\n",
    "    Requires GOLDEN_API_KEY environment variable to be set.\n",
    "    \"\"\"\n",
    "    # The GoldenQueryAPIWrapper will automatically pick up GOLDEN_API_KEY from os.environ\n",
    "    golden_query = GoldenQueryAPIWrapper()\n",
    "\n",
    "    # Construct a query to find public figures or influencers related to the topic\n",
    "    query = f\"public figures interested in {topic}\"\n",
    "\n",
    "    try:\n",
    "        response_str = golden_query.run(query)\n",
    "        response_data = json.loads(response_str)\n",
    "\n",
    "        influencers_info = []\n",
    "        if 'results' in response_data:\n",
    "            for item in response_data['results']:\n",
    "                name = \"Unknown\"\n",
    "                # Extract the name from the properties\n",
    "                for prop in item.get('properties', []):\n",
    "                    if prop.get('predicateId') == 'name' and prop.get('instances'):\n",
    "                        name = prop['instances'][0]['value']\n",
    "                        break\n",
    "                \n",
    "                # The Golden Query API, as per the provided documentation, does not\n",
    "                # directly return 'platform' or 'relevance_score'.\n",
    "                # These are placeholders to match the output schema if needed, but here we just format.\n",
    "                influencers_info.append(f\"Name: {name}, Platform: Unknown, Relevance: 0.0\")\n",
    "        \n",
    "        # Join results into a single string, as the original tool returned a string\n",
    "        return \"Potential Influencers:\\n\" + \"\\n\".join(influencers_info) if influencers_info else \"No influencers found for this topic.\"\n",
    "    except Exception as e:\n",
    "        # Handle potential errors during API call or JSON parsing\n",
    "        print(f\"An error occurred during influencer search: {e}\")\n",
    "        return f\"An error occurred during influencer search: {e}\"\n",
    "\n",
    "\n",
    "# Pydantic model for SocialMediaManager's output\n",
    "class AgentDecision(BaseModel):\n",
    "    \"\"\"\n",
    "    Represents the decision of the SocialMediaManager regarding the next agent to route to.\n",
    "    \"\"\"\n",
    "    next_agent: Literal[\"TrendAnalyst\", \"SponsorFinder\", \"ContentStrategist\", \"ContentGenerator\", \"END\"] = Field(\n",
    "        description=\"The name of the specialized agent that should handle the user's request next, or 'END' if the task is complete or cannot be routed.\"\n",
    "    )\n",
    "    reasoning: str = Field(\n",
    "        description=\"Brief explanation for the routing decision.\"\n",
    "    )\n",
    "\n",
    "# Bind tools to LLM for agents that might use them\n",
    "llm_with_tools = llm.bind_tools([search_viral_content, influencer_search])\n",
    "\n",
    "async def social_media_manager(state: GraphState) -> dict:\n",
    "    \"\"\"\n",
    "    The central supervisor agent. It analyzes the user's query and determines which\n",
    "    specialized agent should handle the request next.\n",
    "    \"\"\"\n",
    "    print(\"---SOCIAL MEDIA MANAGER---\")\n",
    "    user_query = state.user_query\n",
    "    messages = [\n",
    "        SystemMessage(\n",
    "            \"You are a Social Media Manager. Your task is to analyze the user's request \"\n",
    "            \"and determine which specialized agent should handle it. \"\n",
    "            \"Choose from 'TrendAnalyst', 'SponsorFinder', 'ContentStrategist', 'ContentGenerator', or 'END' if the task is complete or cannot be routed.\"\n",
    "            \"The user's query is: \" + user_query\n",
    "        ),\n",
    "        HumanMessage(content=user_query)\n",
    "    ]\n",
    "\n",
    "    # Use LLM with Pydantic output parser for structured decision making\n",
    "    structured_llm = llm.with_structured_output(AgentDecision)\n",
    "    decision: AgentDecision = await structured_llm.ainvoke(messages)\n",
    "\n",
    "    print(f\"Manager Decision: {decision.next_agent} (Reasoning: {decision.reasoning})\")\n",
    "    return {\"next_agent\": decision.next_agent, \"messages\": [HumanMessage(content=f\"Manager decided to route to {decision.next_agent}.\")]}\n",
    "\n",
    "async def trend_analyst(state: GraphState) -> dict:\n",
    "    \"\"\"\n",
    "    Specialized agent for identifying social media trends.\n",
    "    \"\"\"\n",
    "    print(\"---TREND ANALYST---\")\n",
    "    user_query = state.user_query\n",
    "    messages = [\n",
    "        SystemMessage(\n",
    "            \"You are a Trend Analyst. Your goal is to identify social media trends \"\n",
    "            \"based on the user's query. Use the 'search_viral_content' tool if relevant. \"\n",
    "            \"Provide concise and actionable trend insights.\"\n",
    "        ),\n",
    "        HumanMessage(content=user_query)\n",
    "    ]\n",
    "    response = await llm_with_tools.ainvoke(messages)\n",
    "    agent_output = response.content\n",
    "    print(f\"Trend Analyst Output: {agent_output}\")\n",
    "    return {\"agent_output\": agent_output, \"messages\": [HumanMessage(content=f\"Trend Analyst completed task.\")]}\n",
    "\n",
    "async def sponsor_finder(state: GraphState) -> dict:\n",
    "    \"\"\"\n",
    "    Specialized agent for finding relevant people or influencers for sponsoring specific posts.\n",
    "    \"\"\"\n",
    "    print(\"---SPONSOR FINDER---\")\n",
    "    user_query = state.user_query\n",
    "    messages = [\n",
    "        SystemMessage(\n",
    "            \"You are a Sponsor Finder. Your task is to identify relevant people or influencers \"\n",
    "            \"for sponsoring posts based on the user's description. Use the 'influencer_search' tool if relevant. \"\n",
    "            \"List potential sponsors clearly.\"\n",
    "        ),\n",
    "        HumanMessage(content=user_query)\n",
    "    ]\n",
    "    response = await llm_with_tools.ainvoke(messages)\n",
    "    agent_output = response.content\n",
    "    print(f\"Sponsor Finder Output: {agent_output}\")\n",
    "    return {\"agent_output\": agent_output, \"messages\": [HumanMessage(content=f\"Sponsor Finder completed task.\")]}\n",
    "\n",
    "async def content_strategist(state: GraphState) -> dict:\n",
    "    \"\"\"\n",
    "    Specialized agent for suggesting content ideas.\n",
    "    \"\"\"\n",
    "    print(\"---CONTENT STRATEGIST---\")\n",
    "    user_query = state.user_query\n",
    "    messages = [\n",
    "        SystemMessage(\n",
    "            \"You are a Content Strategist. Your role is to suggest creative and engaging \"\n",
    "            \"content ideas for social media based on the user's topic. Consider current trends \"\n",
    "            \"and social media best practices. Be specific with your suggestions (e.g., 'a short video series', 'an interactive poll').\"\n",
    "        ),\n",
    "        HumanMessage(content=user_query)\n",
    "    ]\n",
    "    response = await llm.ainvoke(messages) # No specific tools needed for this agent, just LLM reasoning\n",
    "    agent_output = response.content\n",
    "    print(f\"Content Strategist Output: {agent_output}\")\n",
    "    return {\"agent_output\": agent_output, \"messages\": [HumanMessage(content=f\"Content Strategist completed task.\")]}\n",
    "\n",
    "async def content_generator(state: GraphState) -> dict:\n",
    "    \"\"\"\n",
    "    Specialized agent for generating actual content based on a description.\n",
    "    \"\"\"\n",
    "    print(\"---CONTENT GENERATOR---\")\n",
    "    user_query = state.user_query\n",
    "    messages = [\n",
    "        SystemMessage(\n",
    "            \"You are a Content Generator. Your task is to generate specific content \"\n",
    "            \"based on the user's detailed description. This could be text, a script, \"\n",
    "            \"or a detailed outline. Be creative and fulfill the request precisely.\"\n",
    "        ),\n",
    "        HumanMessage(content=user_query)\n",
    "    ]\n",
    "    response = await llm.ainvoke(messages)\n",
    "    agent_output = response.content\n",
    "    print(f\"Content Generator Output: {agent_output}\")\n",
    "    return {\"agent_output\": agent_output, \"messages\": [HumanMessage(content=f\"Content Generator completed task.\")]}\n",
    "\n",
    "async def end_node(state: GraphState) -> dict:\n",
    "    \"\"\"\n",
    "    A final node to consolidate the output from the specialized agents and prepare the final response.\n",
    "    \"\"\"\n",
    "    print(\"---END NODE---\")\n",
    "    final_response = f\"Based on your query: '{state.user_query}', here is the result from our social media assistant:\\n\\n{state.agent_output}\"\n",
    "    print(f\"Final Response: {final_response}\")\n",
    "    return {\"final_response\": final_response, \"messages\": [HumanMessage(content=f\"Final response generated.\")]}\n",
    "\n",
    "# 3. Graph Construction\n",
    "graph_builder = StateGraph(GraphState)\n",
    "\n",
    "# Add nodes\n",
    "graph_builder.add_node(\"SocialMediaManager\", social_media_manager)\n",
    "graph_builder.add_node(\"TrendAnalyst\", trend_analyst)\n",
    "graph_builder.add_node(\"SponsorFinder\", sponsor_finder)\n",
    "graph_builder.add_node(\"ContentStrategist\", content_strategist)\n",
    "graph_builder.add_node(\"ContentGenerator\", content_generator)\n",
    "graph_builder.add_node(\"EndNode\", end_node)\n",
    "\n",
    "# Set entry point\n",
    "graph_builder.add_edge(START, \"SocialMediaManager\")\n",
    "\n",
    "# Define the routing function for the SocialMediaManager\n",
    "def route_social_media_manager(state: GraphState) -> str:\n",
    "    \"\"\"\n",
    "    Routes the graph based on the 'next_agent' field set by the SocialMediaManager.\n",
    "    \"\"\"\n",
    "    print(f\"Routing from SocialMediaManager to: {state.next_agent}\")\n",
    "    if state.next_agent == \"TrendAnalyst\":\n",
    "        return \"TrendAnalyst\"\n",
    "    elif state.next_agent == \"SponsorFinder\":\n",
    "        return \"SponsorFinder\"\n",
    "    elif state.next_agent == \"ContentStrategist\":\n",
    "        return \"ContentStrategist\"\n",
    "    elif state.next_agent == \"ContentGenerator\":\n",
    "        return \"ContentGenerator\"\n",
    "    else: # Default to END if no specific agent is identified or task is complete\n",
    "        return END\n",
    "\n",
    "# Add conditional edges from SocialMediaManager\n",
    "graph_builder.add_conditional_edges(\n",
    "    \"SocialMediaManager\",\n",
    "    route_social_media_manager,\n",
    "    {\n",
    "        \"TrendAnalyst\": \"TrendAnalyst\",\n",
    "        \"SponsorFinder\": \"SponsorFinder\",\n",
    "        \"ContentStrategist\": \"ContentStrategist\",\n",
    "        \"ContentGenerator\": \"ContentGenerator\",\n",
    "        END: END # If the manager decides the task is complete or unroutable\n",
    "    }\n",
    ")\n",
    "\n",
    "# Add regular edges from specialized agents to the EndNode\n",
    "graph_builder.add_edge(\"TrendAnalyst\", \"EndNode\")\n",
    "graph_builder.add_edge(\"SponsorFinder\", \"EndNode\")\n",
    "graph_builder.add_edge(\"ContentStrategist\", \"EndNode\")\n",
    "graph_builder.add_edge(\"ContentGenerator\", \"EndNode\")\n",
    "\n",
    "# Add edge from EndNode to END\n",
    "graph_builder.add_edge(\"EndNode\", END)\n",
    "\n",
    "# 4. Compilation\n",
    "checkpointer = InMemoryCheckpointer()\n",
    "final_app = graph_builder.compile(checkpointer=checkpointer)\n",
    "\n",
    "# Example Usage (for testing)\n",
    "async def run_graph(query: str):\n",
    "    print(f\"\\n--- Running graph for query: '{query}' ---\")\n",
    "    inputs = {\"user_query\": query, \"messages\": [HumanMessage(content=query)]}\n",
    "    async for s in final_app.astream(inputs):\n",
    "        if \"__end__\" not in s:\n",
    "            print(s)\n",
    "            print(\"---\")\n",
    "    # After the stream, get the final state\n",
    "    final_state = await final_app.aget_state(None)\n",
    "    print(f\"\\nFinal State: {final_state.values}\")\n",
    "    print(f\"\\nFinal Response: {final_state.values['final_response']}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import asyncio\n",
    "\n",
    "    # Test cases based on examples\n",
    "    queries = [\n",
    "        \"Tell me about trends in football.\",\n",
    "        \"I need people who would be interested in a post about UCL football for sponsorship.\",\n",
    "        \"I want to make a post about UCL football, what should I post?\",\n",
    "        \"Generate a script for a UCL highlight reel focusing on top goals.\",\n",
    "        \"What is the weather like today?\" # Example of a query that might lead to END\n",
    "    ]\n",
    "\n",
    "    for q in queries:\n",
    "        asyncio.run(run_graph(q))\n",
    "        print(\"\\n\" + \"=\"*80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f435305c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```python\n",
      "import operator\n",
      "import os\n",
      "import json\n",
      "from typing import Annotated, List, Literal, TypedDict\n",
      "\n",
      "from langchain_core.messages import BaseMessage, HumanMessage, SystemMessage\n",
      "from langchain_core.pydantic_v1 import BaseModel, Field\n",
      "from langchain_core.tools import tool\n",
      "from langchain_openai import ChatOpenAI\n",
      "from langgraph.checkpoint.memory import InMemoryCheckpointer\n",
      "from langgraph.graph import END, START, StateGraph\n",
      "from langgraph.prebuilt import MessagesState\n",
      "\n",
      "# New imports for the actual tool implementations\n",
      "from langchain_google_community import GoogleSearchAPIWrapper\n",
      "from langchain_community.utilities.golden_query import GoldenQueryAPIWrapper\n",
      "\n",
      "# --- Phase 1: Architectural Decision and Justification ---\n",
      "\"\"\"\n",
      "LangGraph Architecture: Agent Supervisor / Hierarchical Agent Teams\n",
      "\n",
      "Justification:\n",
      "The problem of \"managing social media\" naturally breaks down into distinct, specialized sub-tasks:\n",
      "1.  Identifying social media trends.\n",
      "2.  Finding relevant people for sponsoring posts.\n",
      "3.  Suggesting content ideas.\n",
      "4.  Generating content.\n",
      "\n",
      "A Supervisor/Hierarchical Agent Team architecture is ideal because:\n",
      "-   It allows for a central \"SocialMediaManager\" (supervisor) agent to interpret the user's intent and intelligently route the request to the most appropriate specialized worker agent.\n",
      "-   Each worker agent (TrendAnalyst, SponsorFinder, ContentStrategist, ContentGenerator) can be highly focused and optimized for its specific task, potentially using different tools or prompt strategies.\n",
      "-   This modularity enhances maintainability, scalability, and clarity of responsibilities within the graph.\n",
      "\n",
      "Dry Runs of Use Cases:\n",
      "\n",
      "1.  **'Identifying social media trends in sports.' (e.g., \"trends in football\")**\n",
      "    *   **User Input:** \"Tell me about trends in football.\"\n",
      "    *   **SocialMediaManager:** Receives input, identifies intent as \"identify trends,\" sets `next_agent` to \"TrendAnalyst\".\n",
      "    *   **TrendAnalyst:** Receives the query, uses its LLM and potentially a `search_viral_content` tool (simulated) to find trends. Returns \"Q&A reels are trending in football.\"\n",
      "    *   **EndNode:** Formats the final response.\n",
      "    *   **Output:** \"Based on your query about trends in football, the Trend Analyst found that Q&A reels are currently trending.\"\n",
      "\n",
      "2.  **'Finding relevant people for sponsoring specific posts.' (e.g., \"people who would be interested in a post about UCL football\")**\n",
      "    *   **User Input:** \"I need people interested in a post about UCL football for sponsorship.\"\n",
      "    *   **SocialMediaManager:** Receives input, identifies intent as \"find sponsors,\" sets `next_agent` to \"SponsorFinder\".\n",
      "    *   **SponsorFinder:** Receives the query, uses its LLM and potentially an `influencer_search` tool (simulated) to identify relevant individuals. Returns \"Current players like Dembele, Mbappe, and influencers focusing on European football.\"\n",
      "    *   **EndNode:** Formats the final response.\n",
      "    *   **Output:** \"For your post about UCL football, the Sponsor Finder suggests current players like Dembele, Mbappe, and influencers focusing on European football.\"\n",
      "\n",
      "3.  **'Suggesting content to post.' (e.g., \"make a post about UCL football\")**\n",
      "    *   **User Input:** \"I want to make a post about UCL football, what should I post?\"\n",
      "    *   **SocialMediaManager:** Receives input, identifies intent as \"suggest content,\" sets `next_agent` to \"ContentStrategist\".\n",
      "    *   **ContentStrategist:** Receives the query, uses its LLM to brainstorm content ideas, potentially considering trends (either from its own knowledge or by implicitly 'consulting' a trend-like function). Returns \"A highlight reel of recent UCL matches would be highly engaging.\"\n",
      "    *   **EndNode:** Formats the final response.\n",
      "    *   **Output:** \"Regarding your UCL football post, the Content Strategist suggests that a highlight reel of recent UCL matches would be highly engaging.\"\n",
      "\n",
      "4.  **'Generating content based on a description.' (e.g., \"Generate a script for a UCL highlight reel.\")**\n",
      "    *   **User Input:** \"Generate a script for a UCL highlight reel focusing on top goals.\"\n",
      "    *   **SocialMediaManager:** Receives input, identifies intent as \"generate content,\" sets `next_agent` to \"ContentGenerator\".\n",
      "    *   **ContentGenerator:** Receives the query, uses its LLM to generate a detailed script. Returns a multi-line script.\n",
      "    *   **EndNode:** Formats the final response.\n",
      "    *   **Output:** \"Here is the generated content for your UCL highlight reel script: [Generated Script Content]\"\n",
      "\"\"\"\n",
      "\n",
      "# --- Phase 2: Python Code Generation ---\n",
      "\n",
      "# 1. State Definition\n",
      "class GraphState(MessagesState):\n",
      "    \"\"\"\n",
      "    Represents the state of our graph.\n",
      "\n",
      "    Attributes:\n",
      "        user_query: The initial query from the user.\n",
      "        agent_output: The output generated by the last specialized agent.\n",
      "        next_agent: The name of the next agent to route to, determined by the supervisor.\n",
      "        final_response: The consolidated response to be presented to the user.\n",
      "    \"\"\"\n",
      "    user_query: str = Field(default=\"\")\n",
      "    agent_output: str = Field(default=\"\")\n",
      "    next_agent: Literal[\"TrendAnalyst\", \"SponsorFinder\", \"ContentStrategist\", \"ContentGenerator\", \"END\"] = Field(default=\"END\")\n",
      "    final_response: str = Field(default=\"\")\n",
      "\n",
      "\n",
      "# 2. Node Implementations (Python Functions)\n",
      "\n",
      "# Initialize LLM\n",
      "# TODO: Replace with your preferred LLM. Ensure API key is set as an environment variable.\n",
      "# For OpenAI: OPENAI_API_KEY\n",
      "# For Google: GOOGLE_API_KEY\n",
      "llm = ChatOpenAI(model=\"gpt-4o\", temperature=0) # Using gpt-4o for better reasoning and tool use\n",
      "# llm = ChatGoogleGenerativeAI(model=\"gemini-pro\", temperature=0)\n",
      "\n",
      "# Define tools (now using real implementations from snippet2)\n",
      "@tool\n",
      "def search_viral_content(query: str) -> str:\n",
      "    \"\"\"\n",
      "    Searches for recent viral social media content or trending topics related to the query.\n",
      "    Useful for identifying current trends in sports, entertainment, etc.\n",
      "    Requires GOOGLE_API_KEY and GOOGLE_CSE_ID environment variables to be set.\n",
      "    \"\"\"\n",
      "    # The GoogleSearchAPIWrapper will automatically pick up GOOGLE_API_KEY and GOOGLE_CSE_ID from os.environ\n",
      "    search = GoogleSearchAPIWrapper()\n",
      "\n",
      "    # Perform a search for the query, focusing on recent and trending aspects\n",
      "    # Limiting to 5 results as a reasonable number for trending topics\n",
      "    search_results = search.results(f\"recent viral OR trending {query} social media\", num_results=5)\n",
      "\n",
      "    trending_content = []\n",
      "    for result in search_results:\n",
      "        # Combine title and snippet to form a descriptive string for each trending item\n",
      "        content_string = f\"Title: {result.get('title', 'N/A')}\\nSnippet: {result.get('snippet', 'N/A')}\"\n",
      "        trending_content.append(content_string)\n",
      "    \n",
      "    # Join results into a single string, as the original tool returned a string\n",
      "    return \"\\n\\n\".join(trending_content) if trending_content else \"No trending content found.\"\n",
      "\n",
      "@tool\n",
      "def influencer_search(topic: str) -> str:\n",
      "    \"\"\"\n",
      "    Searches for relevant influencers or public figures interested in a specific topic\n",
      "    who might be suitable for sponsorships or collaborations.\n",
      "    Requires GOLDEN_API_KEY environment variable to be set.\n",
      "    \"\"\"\n",
      "    # The GoldenQueryAPIWrapper will automatically pick up GOLDEN_API_KEY from os.environ\n",
      "    golden_query = GoldenQueryAPIWrapper()\n",
      "\n",
      "    # Construct a query to find public figures or influencers related to the topic\n",
      "    query = f\"public figures interested in {topic}\"\n",
      "\n",
      "    try:\n",
      "        response_str = golden_query.run(query)\n",
      "        response_data = json.loads(response_str)\n",
      "\n",
      "        influencers_info = []\n",
      "        if 'results' in response_data:\n",
      "            for item in response_data['results']:\n",
      "                name = \"Unknown\"\n",
      "                # Extract the name from the properties\n",
      "                for prop in item.get('properties', []):\n",
      "                    if prop.get('predicateId') == 'name' and prop.get('instances'):\n",
      "                        name = prop['instances'][0]['value']\n",
      "                        break\n",
      "                \n",
      "                # The Golden Query API, as per the provided documentation, does not\n",
      "                # directly return 'platform' or 'relevance_score'.\n",
      "                # These are placeholders to match the output schema if needed, but here we just format.\n",
      "                influencers_info.append(f\"Name: {name}, Platform: Unknown, Relevance: 0.0\")\n",
      "        \n",
      "        # Join results into a single string, as the original tool returned a string\n",
      "        return \"Potential Influencers:\\n\" + \"\\n\".join(influencers_info) if influencers_info else \"No influencers found for this topic.\"\n",
      "    except Exception as e:\n",
      "        # Handle potential errors during API call or JSON parsing\n",
      "        print(f\"An error occurred during influencer search: {e}\")\n",
      "        return f\"An error occurred during influencer search: {e}\"\n",
      "\n",
      "\n",
      "# Pydantic model for SocialMediaManager's output\n",
      "class AgentDecision(BaseModel):\n",
      "    \"\"\"\n",
      "    Represents the decision of the SocialMediaManager regarding the next agent to route to.\n",
      "    \"\"\"\n",
      "    next_agent: Literal[\"TrendAnalyst\", \"SponsorFinder\", \"ContentStrategist\", \"ContentGenerator\", \"END\"] = Field(\n",
      "        description=\"The name of the specialized agent that should handle the user's request next, or 'END' if the task is complete or cannot be routed.\"\n",
      "    )\n",
      "    reasoning: str = Field(\n",
      "        description=\"Brief explanation for the routing decision.\"\n",
      "    )\n",
      "\n",
      "# Bind tools to LLM for agents that might use them\n",
      "llm_with_tools = llm.bind_tools([search_viral_content, influencer_search])\n",
      "\n",
      "async def social_media_manager(state: GraphState) -> dict:\n",
      "    \"\"\"\n",
      "    The central supervisor agent. It analyzes the user's query and determines which\n",
      "    specialized agent should handle the request next.\n",
      "    \"\"\"\n",
      "    print(\"---SOCIAL MEDIA MANAGER---\")\n",
      "    user_query = state.user_query\n",
      "    messages = [\n",
      "        SystemMessage(\n",
      "            \"You are a Social Media Manager. Your task is to analyze the user's request \"\n",
      "            \"and determine which specialized agent should handle it. \"\n",
      "            \"Choose from 'TrendAnalyst', 'SponsorFinder', 'ContentStrategist', 'ContentGenerator', or 'END' if the task is complete or cannot be routed.\"\n",
      "            \"The user's query is: \" + user_query\n",
      "        ),\n",
      "        HumanMessage(content=user_query)\n",
      "    ]\n",
      "\n",
      "    # Use LLM with Pydantic output parser for structured decision making\n",
      "    structured_llm = llm.with_structured_output(AgentDecision)\n",
      "    decision: AgentDecision = await structured_llm.ainvoke(messages)\n",
      "\n",
      "    print(f\"Manager Decision: {decision.next_agent} (Reasoning: {decision.reasoning})\")\n",
      "    return {\"next_agent\": decision.next_agent, \"messages\": [HumanMessage(content=f\"Manager decided to route to {decision.next_agent}.\")]}\n",
      "\n",
      "async def trend_analyst(state: GraphState) -> dict:\n",
      "    \"\"\"\n",
      "    Specialized agent for identifying social media trends.\n",
      "    \"\"\"\n",
      "    print(\"---TREND ANALYST---\")\n",
      "    user_query = state.user_query\n",
      "    messages = [\n",
      "        SystemMessage(\n",
      "            \"You are a Trend Analyst. Your goal is to identify social media trends \"\n",
      "            \"based on the user's query. Use the 'search_viral_content' tool if relevant. \"\n",
      "            \"Provide concise and actionable trend insights.\"\n",
      "        ),\n",
      "        HumanMessage(content=user_query)\n",
      "    ]\n",
      "    response = await llm_with_tools.ainvoke(messages)\n",
      "    agent_output = response.content\n",
      "    print(f\"Trend Analyst Output: {agent_output}\")\n",
      "    return {\"agent_output\": agent_output, \"messages\": [HumanMessage(content=f\"Trend Analyst completed task.\")]}\n",
      "\n",
      "async def sponsor_finder(state: GraphState) -> dict:\n",
      "    \"\"\"\n",
      "    Specialized agent for finding relevant people or influencers for sponsoring specific posts.\n",
      "    \"\"\"\n",
      "    print(\"---SPONSOR FINDER---\")\n",
      "    user_query = state.user_query\n",
      "    messages = [\n",
      "        SystemMessage(\n",
      "            \"You are a Sponsor Finder. Your task is to identify relevant people or influencers \"\n",
      "            \"for sponsoring posts based on the user's description. Use the 'influencer_search' tool if relevant. \"\n",
      "            \"List potential sponsors clearly.\"\n",
      "        ),\n",
      "        HumanMessage(content=user_query)\n",
      "    ]\n",
      "    response = await llm_with_tools.ainvoke(messages)\n",
      "    agent_output = response.content\n",
      "    print(f\"Sponsor Finder Output: {agent_output}\")\n",
      "    return {\"agent_output\": agent_output, \"messages\": [HumanMessage(content=f\"Sponsor Finder completed task.\")]}\n",
      "\n",
      "async def content_strategist(state: GraphState) -> dict:\n",
      "    \"\"\"\n",
      "    Specialized agent for suggesting content ideas.\n",
      "    \"\"\"\n",
      "    print(\"---CONTENT STRATEGIST---\")\n",
      "    user_query = state.user_query\n",
      "    messages = [\n",
      "        SystemMessage(\n",
      "            \"You are a Content Strategist. Your role is to suggest creative and engaging \"\n",
      "            \"content ideas for social media based on the user's topic. Consider current trends \"\n",
      "            \"and social media best practices. Be specific with your suggestions (e.g., 'a short video series', 'an interactive poll').\"\n",
      "        ),\n",
      "        HumanMessage(content=user_query)\n",
      "    ]\n",
      "    response = await llm.ainvoke(messages) # No specific tools needed for this agent, just LLM reasoning\n",
      "    agent_output = response.content\n",
      "    print(f\"Content Strategist Output: {agent_output}\")\n",
      "    return {\"agent_output\": agent_output, \"messages\": [HumanMessage(content=f\"Content Strategist completed task.\")]}\n",
      "\n",
      "async def content_generator(state: GraphState) -> dict:\n",
      "    \"\"\"\n",
      "    Specialized agent for generating actual content based on a description.\n",
      "    \"\"\"\n",
      "    print(\"---CONTENT GENERATOR---\")\n",
      "    user_query = state.user_query\n",
      "    messages = [\n",
      "        SystemMessage(\n",
      "            \"You are a Content Generator. Your task is to generate specific content \"\n",
      "            \"based on the user's detailed description. This could be text, a script, \"\n",
      "            \"or a detailed outline. Be creative and fulfill the request precisely.\"\n",
      "        ),\n",
      "        HumanMessage(content=user_query)\n",
      "    ]\n",
      "    response = await llm.ainvoke(messages)\n",
      "    agent_output = response.content\n",
      "    print(f\"Content Generator Output: {agent_output}\")\n",
      "    return {\"agent_output\": agent_output, \"messages\": [HumanMessage(content=f\"Content Generator completed task.\")]}\n",
      "\n",
      "async def end_node(state: GraphState) -> dict:\n",
      "    \"\"\"\n",
      "    A final node to consolidate the output from the specialized agents and prepare the final response.\n",
      "    \"\"\"\n",
      "    print(\"---END NODE---\")\n",
      "    final_response = f\"Based on your query: '{state.user_query}', here is the result from our social media assistant:\\n\\n{state.agent_output}\"\n",
      "    print(f\"Final Response: {final_response}\")\n",
      "    return {\"final_response\": final_response, \"messages\": [HumanMessage(content=f\"Final response generated.\")]}\n",
      "\n",
      "# 3. Graph Construction\n",
      "graph_builder = StateGraph(GraphState)\n",
      "\n",
      "# Add nodes\n",
      "graph_builder.add_node(\"SocialMediaManager\", social_media_manager)\n",
      "graph_builder.add_node(\"TrendAnalyst\", trend_analyst)\n",
      "graph_builder.add_node(\"SponsorFinder\", sponsor_finder)\n",
      "graph_builder.add_node(\"ContentStrategist\", content_strategist)\n",
      "graph_builder.add_node(\"ContentGenerator\", content_generator)\n",
      "graph_builder.add_node(\"EndNode\", end_node)\n",
      "\n",
      "# Set entry point\n",
      "graph_builder.add_edge(START, \"SocialMediaManager\")\n",
      "\n",
      "# Define the routing function for the SocialMediaManager\n",
      "def route_social_media_manager(state: GraphState) -> str:\n",
      "    \"\"\"\n",
      "    Routes the graph based on the 'next_agent' field set by the SocialMediaManager.\n",
      "    \"\"\"\n",
      "    print(f\"Routing from SocialMediaManager to: {state.next_agent}\")\n",
      "    if state.next_agent == \"TrendAnalyst\":\n",
      "        return \"TrendAnalyst\"\n",
      "    elif state.next_agent == \"SponsorFinder\":\n",
      "        return \"SponsorFinder\"\n",
      "    elif state.next_agent == \"ContentStrategist\":\n",
      "        return \"ContentStrategist\"\n",
      "    elif state.next_agent == \"ContentGenerator\":\n",
      "        return \"ContentGenerator\"\n",
      "    else: # Default to END if no specific agent is identified or task is complete\n",
      "        return END\n",
      "\n",
      "# Add conditional edges from SocialMediaManager\n",
      "graph_builder.add_conditional_edges(\n",
      "    \"SocialMediaManager\",\n",
      "    route_social_media_manager,\n",
      "    {\n",
      "        \"TrendAnalyst\": \"TrendAnalyst\",\n",
      "        \"SponsorFinder\": \"SponsorFinder\",\n",
      "        \"ContentStrategist\": \"ContentStrategist\",\n",
      "        \"ContentGenerator\": \"ContentGenerator\",\n",
      "        END: END # If the manager decides the task is complete or unroutable\n",
      "    }\n",
      ")\n",
      "\n",
      "# Add regular edges from specialized agents to the EndNode\n",
      "graph_builder.add_edge(\"TrendAnalyst\", \"EndNode\")\n",
      "graph_builder.add_edge(\"SponsorFinder\", \"EndNode\")\n",
      "graph_builder.add_edge(\"ContentStrategist\", \"EndNode\")\n",
      "graph_builder.add_edge(\"ContentGenerator\", \"EndNode\")\n",
      "\n",
      "# Add edge from EndNode to END\n",
      "graph_builder.add_edge(\"EndNode\", END)\n",
      "\n",
      "# 4. Compilation\n",
      "checkpointer = InMemoryCheckpointer()\n",
      "final_app = graph_builder.compile(checkpointer=checkpointer)\n",
      "\n",
      "# Example Usage (for testing)\n",
      "async def run_graph(query: str):\n",
      "    print(f\"\\n--- Running graph for query: '{query}' ---\")\n",
      "    inputs = {\"user_query\": query, \"messages\": [HumanMessage(content=query)]}\n",
      "    async for s in final_app.astream(inputs):\n",
      "        if \"__end__\" not in s:\n",
      "            print(s)\n",
      "            print(\"---\")\n",
      "    # After the stream, get the final state\n",
      "    final_state = await final_app.aget_state(None)\n",
      "    print(f\"\\nFinal State: {final_state.values}\")\n",
      "    print(f\"\\nFinal Response: {final_state.values['final_response']}\")\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    import asyncio\n",
      "\n",
      "    # Test cases based on examples\n",
      "    queries = [\n",
      "        \"Tell me about trends in football.\",\n",
      "        \"I need people who would be interested in a post about UCL football for sponsorship.\",\n",
      "        \"I want to make a post about UCL football, what should I post?\",\n",
      "        \"Generate a script for a UCL highlight reel focusing on top goals.\",\n",
      "        \"What is the weather like today?\" # Example of a query that might lead to END\n",
      "    ]\n",
      "\n",
      "    for q in queries:\n",
      "        asyncio.run(run_graph(q))\n",
      "        print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
      "\n",
      "```\n",
      "\n",
      "## Required Keys and Credentials\n",
      "\n",
      "To run the generated Python code, you will need to set up the API keys for the Large Language Model (LLM) and the external tools you choose to use.\n",
      "\n",
      "*   **For OpenAI models (e.g., `gpt-4o`, `gpt-3.5-turbo`):**\n",
      "    Set the `OPENAI_API_KEY` environment variable.\n",
      "    ```bash\n",
      "    export OPENAI_API_KEY=\"your_openai_api_key_here\"\n",
      "    ```\n",
      "\n",
      "*   **For Google Generative AI models (e.g., `gemini-pro`):**\n",
      "    Set the `GOOGLE_API_KEY` environment variable.\n",
      "    ```bash\n",
      "    export GOOGLE_API_KEY=\"your_google_api_key_here\"\n",
      "    ```\n",
      "\n",
      "*   **For Google Search API (used by `search_viral_content`):**\n",
      "    Set the `GOOGLE_API_KEY` and `GOOGLE_CSE_ID` environment variables.\n",
      "    ```bash\n",
      "    export GOOGLE_API_KEY=\"your_google_cloud_api_key_here\"\n",
      "    export GOOGLE_CSE_ID=\"your_google_custom_search_engine_id_here\"\n",
      "    ```\n",
      "    (Note: `GOOGLE_API_KEY` might be the same as for Google Generative AI, but it's good to specify its use for the search API.)\n",
      "\n",
      "*   **For Golden Query API (used by `influencer_search`):**\n",
      "    Set the `GOLDEN_API_KEY` environment variable.\n",
      "    ```bash\n",
      "    export GOLDEN_API_KEY=\"your_golden_api_key_here\"\n",
      "    ```\n",
      "\n",
      "Choose the relevant environment variables based on which LLM and tools you intend to use.\n"
     ]
    }
   ],
   "source": [
    "print(result[\"python_code\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f660e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
