{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "62b92789",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_objects = {\n",
    "  \"nodes\": {\n",
    "    \"__START__\": {\n",
    "      \"schema_info\": \"\",\n",
    "      \"input_schema\": \"\",\n",
    "      \"output_schema\": \"\",\n",
    "      \"description\": \"Entry point of the graph.\",\n",
    "      \"function_name\": \"\"\n",
    "    },\n",
    "    \"planner\": {\n",
    "      \"schema_info\": \"PlanExecute: TypedDict with fields input (str), plan (List[str]), past_steps (Annotated[List[Tuple], operator.add]), response (str)\",\n",
    "      \"input_schema\": \"PlanExecute\",\n",
    "      \"output_schema\": \"PlanExecute\",\n",
    "      \"description\": \"Plan step generates a plan based on the input using llm structured output functionality, stores it to the plan field\",\n",
    "      \"function_name\": \"plan_step\"\n",
    "    },\n",
    "    \"agent\": {\n",
    "      \"schema_info\": \"PlanExecute: TypedDict with fields input (str), plan (List[str]), past_steps (Annotated[List[Tuple], operator.add]), response (str)\",\n",
    "      \"input_schema\": \"PlanExecute\",\n",
    "      \"output_schema\": \"PlanExecute\",\n",
    "      \"description\": \"Uses llm with tool binding for the stock related queries\",\n",
    "      \"function_name\": \"execute_step\"\n",
    "    },\n",
    "    \"replan\": {\n",
    "      \"schema_info\": \"PlanExecute: TypedDict with fields input (str), plan (List[str]), past_steps (Annotated[List[Tuple], operator.add]), response (str)\",\n",
    "      \"input_schema\": \"PlanExecute\",\n",
    "      \"output_schema\": \"Union[Response, Plan]\",\n",
    "      \"description\": \"Evaluates progress and uses an LLM to either revise the plan or generate a final response.\",\n",
    "      \"function_name\": \"replan_step\"\n",
    "    },\n",
    "    \"__END__\": {\n",
    "      \"schema_info\": \"\",\n",
    "      \"input_schema\": \"\",\n",
    "      \"output_schema\": \"\",\n",
    "      \"description\": \"End point of the graph.\",\n",
    "      \"function_name\": \"\"\n",
    "    }\n",
    "  },\n",
    "  \"edges\": {\n",
    "    \"edge_1\": {\n",
    "      \"source\": \"__START__\",\n",
    "      \"target\": \"planner\",\n",
    "      \"routing_conditions\": \"Start the planning process.\",\n",
    "      \"conditional\": False\n",
    "    },\n",
    "    \"edge_2\": {\n",
    "      \"source\": \"planner\",\n",
    "      \"target\": \"agent\",\n",
    "      \"routing_conditions\": \"After planning, execute the first step.\",\n",
    "      \"conditional\": False\n",
    "    },\n",
    "    \"edge_3\": {\n",
    "      \"source\": \"agent\",\n",
    "      \"target\": \"replan\",\n",
    "      \"routing_conditions\": \"After executing a step, check if replanning is needed.\",\n",
    "      \"conditional\": False\n",
    "    },\n",
    "    \"edge_4\": {\n",
    "      \"source\": \"replan\",\n",
    "      \"target\": \"agent\",\n",
    "      \"routing_conditions\": \"If no response is generated, continue to agent for further execution.\",\n",
    "      \"conditional\": True\n",
    "    },\n",
    "    \"edge_5\": {\n",
    "      \"source\": \"replan\",\n",
    "      \"target\": \"__END__\",\n",
    "      \"routing_conditions\": \"If a response is generated, end the process.\",\n",
    "      \"conditional\": True\n",
    "    }\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb05768b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kanishkgupta/Documents/GitHub/AgentAgent/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import import_ipynb\n",
    "from phase1_edge_handling import edge_builder_agent\n",
    "from phase1_node_to_code import node_to_code_app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd16f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "skipList = [\"__START__\", \"__END__\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cff28bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "from typing import Annotated, List\n",
    "from langgraph.graph import StateGraph, START, END, MessagesState\n",
    "from langgraph.types import Send\n",
    "from pydantic import BaseModel\n",
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from model_factory import get_model, ModelName\n",
    "import uuid\n",
    "\n",
    "\n",
    "class NodeEvaluationReport(BaseModel):\n",
    "    node_name: str\n",
    "    node_code_stub: str\n",
    "    edge_code: str\n",
    "\n",
    "class GraphCompilerState(MessagesState):\n",
    "    json_objects: dict\n",
    "    node_reports: Annotated[List[NodeEvaluationReport], operator.add]\n",
    "\n",
    "\n",
    "class NodeProcessState():\n",
    "    node_name: str\n",
    "    node_info: dict\n",
    "    edge_info: dict\n",
    "\n",
    "def graph_map_step(state: GraphCompilerState):\n",
    "      # Get edges originating from the current node\n",
    "    json_objects = state[\"json_objects\"]\n",
    "    edges = json_objects[\"edges\"]\n",
    "    nodes = json_objects[\"nodes\"]\n",
    "    sends = []\n",
    "    for node_key, node_val in nodes.items():\n",
    "        outgoing_edges = list(filter(lambda edge: edge[\"source\"] == node_key, edges.values()))\n",
    "        sends.append(Send(\"node_process\", {\"node_name\": node_key, \"node_info\": node_val, \"edge_info\": outgoing_edges}))\n",
    "    return sends\n",
    "\n",
    "edge_info_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "<GraphNodeImplementation>\n",
    "{node_code}\n",
    "</GraphNodeImplementation>\n",
    "<EdgeInformation>\n",
    "{edge_json}\n",
    "</Edgeinformation>\"\"\")\n",
    "\n",
    "def node_process(state: NodeProcessState):\n",
    "    uuid_str = uuid.uuid4()\n",
    "    config = {\"configurable\": {\"thread_id\": str(uuid_str)}}\n",
    "    if state[\"node_name\"] not in skipList:\n",
    "        for output in node_to_code_app.stream(state[\"node_info\"], config, stream_mode=\"updates\"):\n",
    "            print(output)\n",
    "        code= node_to_code_app.get_state(config).values[\"final_code\"]\n",
    "    else:\n",
    "        code = \"no implementation needed\"\n",
    "    edge_code = edge_builder_agent.invoke({\"messages\": [HumanMessage(content = edge_info_prompt.format(node_code=code,edge_json=state[\"edge_info\"]))]},config)\n",
    "    return {\n",
    "        \"node_reports\" : [NodeEvaluationReport(node_name=state[\"node_name\"], node_code_stub=code, edge_code=edge_code[\"messages\"][-1].content)] \n",
    "    }\n",
    "\n",
    "code_compiler_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "<Graph>\n",
    "{graphEdgeDict}\n",
    "</Graph>\n",
    "\n",
    "<GraphNodeImplementation>\n",
    "{graphImplementations}\n",
    "</GraphNodeImplementation>\n",
    "\n",
    "<EdgeImplementation>\n",
    "{edgeImplementations}\n",
    "</EdgeImplementation>\n",
    "\"\"\")\n",
    "\n",
    "def graph_compile(state: GraphCompilerState):\n",
    "    node_evals : List[NodeEvaluationReport]= state[\"node_reports\"]\n",
    "\n",
    "    code_stubs = [node_eval.node_code_stub for node_eval in node_evals]\n",
    "    edge_stubs = [node_eval.edge_code for node_eval in node_evals]\n",
    "    json_objects = state[\"json_objects\"]\n",
    "    edges = json_objects[\"edges\"]\n",
    "    llm = get_model()\n",
    "    response = llm.invoke([SystemMessage(content=\n",
    "\"\"\"You are a langgraph coding expert, you are given a workflow with edges as well as code implementation of each node.\n",
    "You are supposed to merge the code, make sure there is no simulation of llm operations, use langchain for llm invocations\n",
    "Make sure that the graph is compiled with an InMemoryCheckpointer and finally assign to a variable called final_app\"\"\")]\n",
    "                          +[HumanMessage(content= code_compiler_prompt.format(\n",
    "        graphEdgeDict=edges,\n",
    "        graphImplementations=code_stubs,\n",
    "        edgeImplementations=edge_stubs))])\n",
    "    return {\n",
    "        \"messages\": [response]\n",
    "    }\n",
    "\n",
    "\n",
    "workflow = StateGraph(GraphCompilerState)\n",
    "workflow.add_node(graph_map_step, \"graph_map\")\n",
    "workflow.add_node(node_process, \"node_process\")\n",
    "workflow.add_node(graph_compile,\"graph_compile\")\n",
    "\n",
    "workflow.add_conditional_edges(START, graph_map_step, [\"node_process\"])\n",
    "workflow.add_edge(\"node_process\", \"graph_compile\")\n",
    "workflow.add_edge(\"graph_compile\", END)\n",
    "\n",
    "compiler_graph = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e7d68840",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'identify_node': {'node_type': 'planner', 'messages': [HumanMessage(content='\\nYou are provided with the following information about the node:\\n<SchemaInfo>\\nPlanExecute: TypedDict with fields input (str), plan (List[str]), past_steps (Annotated[List[Tuple], operator.add]), response (str)\\n</SchemaInfo>\\n<InputSchema>\\nPlanExecute\\n</InputSchema>\\n<OutputSchema>\\nPlanExecute\\n</OutputSchema>\\n<Description>\\nUses llm with tool binding for the stock related queries\\n</Description>\\n<FunctionName>\\nexecute_step\\n</FunctionName>\\n\\nBelow is the skeleton of the function that you need to implement:\\ndef execute_step(state:PlanExecute) -> PlanExecute:\\n    \"\"\"Uses llm with tool binding for the stock related queries\"\"\"\\n    # Implement the function to meet the description.\\n\\nthe state is of type PlanExecute and the function is of type PlanExecute\\nThe general idea is that the implementation would involve extracting the input from the state, and updating the state with the output. Description contains the logic for this blackbox\\n', additional_kwargs={}, response_metadata={}, id='26023887-a2b7-4f2d-9797-b6edd7550665')], 'node_info': '\\nYou are provided with the following information about the node:\\n<SchemaInfo>\\nPlanExecute: TypedDict with fields input (str), plan (List[str]), past_steps (Annotated[List[Tuple], operator.add]), response (str)\\n</SchemaInfo>\\n<InputSchema>\\nPlanExecute\\n</InputSchema>\\n<OutputSchema>\\nPlanExecute\\n</OutputSchema>\\n<Description>\\nUses llm with tool binding for the stock related queries\\n</Description>\\n<FunctionName>\\nexecute_step\\n</FunctionName>\\n\\nBelow is the skeleton of the function that you need to implement:\\ndef execute_step(state:PlanExecute) -> PlanExecute:\\n    \"\"\"Uses llm with tool binding for the stock related queries\"\"\"\\n    # Implement the function to meet the description.\\n\\nthe state is of type PlanExecute and the function is of type PlanExecute\\nThe general idea is that the implementation would involve extracting the input from the state, and updating the state with the output. Description contains the logic for this blackbox\\n'}}\n",
      "{'identify_node': {'node_type': 'planner', 'messages': [HumanMessage(content='\\nYou are provided with the following information about the node:\\n<SchemaInfo>\\nPlanExecute: TypedDict with fields input (str), plan (List[str]), past_steps (Annotated[List[Tuple], operator.add]), response (str)\\n</SchemaInfo>\\n<InputSchema>\\nPlanExecute\\n</InputSchema>\\n<OutputSchema>\\nPlanExecute\\n</OutputSchema>\\n<Description>\\nPlan step generates a plan based on the input using llm structured output functionality, stores it to the plan field\\n</Description>\\n<FunctionName>\\nplan_step\\n</FunctionName>\\n\\nBelow is the skeleton of the function that you need to implement:\\ndef plan_step(state:PlanExecute) -> PlanExecute:\\n    \"\"\"Plan step generates a plan based on the input using llm structured output functionality, stores it to the plan field\"\"\"\\n    # Implement the function to meet the description.\\n\\nthe state is of type PlanExecute and the function is of type PlanExecute\\nThe general idea is that the implementation would involve extracting the input from the state, and updating the state with the output. Description contains the logic for this blackbox\\n', additional_kwargs={}, response_metadata={}, id='3c50b5a9-abaf-412c-8006-4db69772631a')], 'node_info': '\\nYou are provided with the following information about the node:\\n<SchemaInfo>\\nPlanExecute: TypedDict with fields input (str), plan (List[str]), past_steps (Annotated[List[Tuple], operator.add]), response (str)\\n</SchemaInfo>\\n<InputSchema>\\nPlanExecute\\n</InputSchema>\\n<OutputSchema>\\nPlanExecute\\n</OutputSchema>\\n<Description>\\nPlan step generates a plan based on the input using llm structured output functionality, stores it to the plan field\\n</Description>\\n<FunctionName>\\nplan_step\\n</FunctionName>\\n\\nBelow is the skeleton of the function that you need to implement:\\ndef plan_step(state:PlanExecute) -> PlanExecute:\\n    \"\"\"Plan step generates a plan based on the input using llm structured output functionality, stores it to the plan field\"\"\"\\n    # Implement the function to meet the description.\\n\\nthe state is of type PlanExecute and the function is of type PlanExecute\\nThe general idea is that the implementation would involve extracting the input from the state, and updating the state with the output. Description contains the logic for this blackbox\\n'}}\n",
      "{'identify_node': {'node_type': 'planner', 'messages': [HumanMessage(content='\\nYou are provided with the following information about the node:\\n<SchemaInfo>\\nPlanExecute: TypedDict with fields input (str), plan (List[str]), past_steps (Annotated[List[Tuple], operator.add]), response (str)\\n</SchemaInfo>\\n<InputSchema>\\nPlanExecute\\n</InputSchema>\\n<OutputSchema>\\nUnion[Response, Plan]\\n</OutputSchema>\\n<Description>\\nEvaluates progress and uses an LLM to either revise the plan or generate a final response.\\n</Description>\\n<FunctionName>\\nreplan_step\\n</FunctionName>\\n\\nBelow is the skeleton of the function that you need to implement:\\ndef replan_step(state:PlanExecute) -> Union[Response, Plan]:\\n    \"\"\"Evaluates progress and uses an LLM to either revise the plan or generate a final response.\"\"\"\\n    # Implement the function to meet the description.\\n\\nthe state is of type PlanExecute and the function is of type Union[Response, Plan]\\nThe general idea is that the implementation would involve extracting the input from the state, and updating the state with the output. Description contains the logic for this blackbox\\n', additional_kwargs={}, response_metadata={}, id='0f2ff031-b91e-4b79-aa42-389f69545c28')], 'node_info': '\\nYou are provided with the following information about the node:\\n<SchemaInfo>\\nPlanExecute: TypedDict with fields input (str), plan (List[str]), past_steps (Annotated[List[Tuple], operator.add]), response (str)\\n</SchemaInfo>\\n<InputSchema>\\nPlanExecute\\n</InputSchema>\\n<OutputSchema>\\nUnion[Response, Plan]\\n</OutputSchema>\\n<Description>\\nEvaluates progress and uses an LLM to either revise the plan or generate a final response.\\n</Description>\\n<FunctionName>\\nreplan_step\\n</FunctionName>\\n\\nBelow is the skeleton of the function that you need to implement:\\ndef replan_step(state:PlanExecute) -> Union[Response, Plan]:\\n    \"\"\"Evaluates progress and uses an LLM to either revise the plan or generate a final response.\"\"\"\\n    # Implement the function to meet the description.\\n\\nthe state is of type PlanExecute and the function is of type Union[Response, Plan]\\nThe general idea is that the implementation would involve extracting the input from the state, and updating the state with the output. Description contains the logic for this blackbox\\n'}}\n",
      "{'node_process': {'node_reports': [NodeEvaluationReport(node_name='__END__', node_code_stub='no implementation needed', edge_code='```python\\n# No edges to implement as the edge information list is empty.\\n# No code is needed for edges or conditional edges.\\n```')]}}\n",
      "{'planner': {'plan': ['Generate a prompt that is designed to handle stock-related queries effectively.', 'Bind the generated prompt with appropriate stock-related tools using the toolset_generation capability to ensure accurate and factual responses.']}}\n",
      "{'planner': {'plan': ['Generate a prompt to ask the LLM to create a plan.', \"Use the LLM's structured output functionality to generate a plan in the form of a list of steps based on the prompt.\"]}}\n",
      "{'node_process': {'node_reports': [NodeEvaluationReport(node_name='__START__', node_code_stub='no implementation needed', edge_code='```python\\n# Since the edge is non-conditional from __START__ to planner,\\n# we use the add_edge method directly as per the NonConditionalEdges section.\\n\\ngraph.add_edge(\"__START__\", \"planner\")\\n```')]}}\n",
      "{'ai_node_gen_supervisor': {'next': 'prompt_generation', 'task': 'Generate a prompt to ask the LLM to create a plan.', 'messages': [AIMessage(content=\"The current step is to generate a prompt to ask the LLM to create a plan. The 'prompt_generation' worker is responsible for generating prompts, so it is the appropriate worker to act next.\", additional_kwargs={}, response_metadata={}, id='38261efe-8041-42cb-9f87-8e7d033b21f6')]}}\n",
      "{'planner': {'plan': [\"Analyze the 'past_steps' to evaluate the progress made so far and identify if the current plan is sufficient or needs revision.\", 'Use a prompt to query the LLM with the extracted input and past steps to decide whether to revise the plan or generate a final response.', \"If the LLM suggests revising the plan, update the 'plan' field with the new steps provided by the LLM and return the updated plan.\", \"If the LLM suggests that the task is complete, generate the final 'response' and return it as the final answer.\"]}}\n",
      "{'ai_node_gen_supervisor': {'next': 'prompt_generation', 'task': \"Analyze the 'past_steps' to evaluate the progress made so far and identify if the current plan is sufficient or needs revision.\", 'messages': [AIMessage(content=\"Step 1 requires analyzing 'past_steps' to evaluate progress and identify if the current plan is sufficient or needs revision. The 'prompt_generation' worker is best suited to generate the appropriate prompt to query the LLM with the extracted input and past steps, enabling the decision on whether to revise the plan or generate a final response.\", additional_kwargs={}, response_metadata={}, id='bcd24794-5d9c-47fe-89c8-8dc487b2cfa3')]}}\n",
      "{'ai_node_gen_supervisor': {'next': 'prompt_generation', 'task': 'Generate a prompt that is designed to handle stock-related queries effectively.', 'messages': [AIMessage(content=\"The first step in the plan is to generate a prompt designed to handle stock-related queries effectively. Therefore, the 'prompt_generation' worker should act next to execute this step.\", additional_kwargs={}, response_metadata={}, id='c1094b14-724f-410a-9761-5c4d5a4ac62b')]}}\n",
      "{'prompt_generation': {'past_steps': [('Generate a prompt to ask the LLM to create a plan.', 'Given the input task, generate a detailed step-by-step plan to accomplish it. The plan should be a list of clear, actionable steps that logically progress from start to finish. Format the output as a JSON object with a single field \"plan\" containing the list of steps. For example:\\n\\n{\\n  \"plan\": [\\n    \"Step 1 description\",\\n    \"Step 2 description\",\\n    \"Step 3 description\",\\n    ...\\n  ]\\n}\\n\\nEnsure the plan is comprehensive and covers all necessary actions to complete the input task.')], 'messages': [HumanMessage(content='Given the input task, generate a detailed step-by-step plan to accomplish it. The plan should be a list of clear, actionable steps that logically progress from start to finish. Format the output as a JSON object with a single field \"plan\" containing the list of steps. For example:\\n\\n{\\n  \"plan\": [\\n    \"Step 1 description\",\\n    \"Step 2 description\",\\n    \"Step 3 description\",\\n    ...\\n  ]\\n}\\n\\nEnsure the plan is comprehensive and covers all necessary actions to complete the input task.', additional_kwargs={}, response_metadata={}, name='prompt_generator', id='84f533b9-ad76-4c3c-bc38-ac11685c2fd3')]}}\n",
      "{'prompt_generation': {'past_steps': [(\"Analyze the 'past_steps' to evaluate the progress made so far and identify if the current plan is sufficient or needs revision.\", 'You are given a task input, a current plan (a list of steps), and a history of past steps with their outcomes. Your job is to evaluate the progress made so far and decide whether to revise the plan or produce a final response. \\n\\n- Carefully analyze the past steps and their results to determine if the current plan is still valid or needs adjustment.\\n- If the plan needs revision, generate a new plan as a list of clear, actionable steps.\\n- If the task is complete or no further planning is needed, generate a final response summarizing the outcome or answer.\\n- Provide your output in one of two formats:\\n  - If revising the plan, output: {\"plan\": [list of steps]}\\n  - If producing a final response, output: {\"response\": \"final answer or summary\"}\\n\\nUse the input task description and the history of past steps to guide your decision.')], 'messages': [HumanMessage(content='You are given a task input, a current plan (a list of steps), and a history of past steps with their outcomes. Your job is to evaluate the progress made so far and decide whether to revise the plan or produce a final response. \\n\\n- Carefully analyze the past steps and their results to determine if the current plan is still valid or needs adjustment.\\n- If the plan needs revision, generate a new plan as a list of clear, actionable steps.\\n- If the task is complete or no further planning is needed, generate a final response summarizing the outcome or answer.\\n- Provide your output in one of two formats:\\n  - If revising the plan, output: {\"plan\": [list of steps]}\\n  - If producing a final response, output: {\"response\": \"final answer or summary\"}\\n\\nUse the input task description and the history of past steps to guide your decision.', additional_kwargs={}, response_metadata={}, name='prompt_generator', id='6715fc89-96d9-4f60-b1b4-13b16391cd5d')]}}\n",
      "{'replan': {'plan': [\"Use the LLM's structured output functionality to generate a plan in the form of a list of steps based on the prompt.\"]}}\n",
      "{'prompt_generation': {'past_steps': [('Generate a prompt that is designed to handle stock-related queries effectively.', 'You are an expert assistant specialized in handling stock-related queries using large language models (LLMs) with tool integration. Given the current state containing an input query about stocks, a plan consisting of ordered steps, and a history of past steps and responses, your task is to execute the next step in the plan by generating a precise and context-aware prompt for the LLM. This prompt should enable the LLM to utilize relevant stock data tools effectively to answer the query. Incorporate the input query, the plan steps, and the past interactions to produce a coherent and actionable prompt that guides the LLM to provide an accurate and informative response related to stock information. Return the updated state including the original input, the unchanged plan, the extended past steps with the current step and its output, and the new response generated by the LLM.')], 'messages': [HumanMessage(content='You are an expert assistant specialized in handling stock-related queries using large language models (LLMs) with tool integration. Given the current state containing an input query about stocks, a plan consisting of ordered steps, and a history of past steps and responses, your task is to execute the next step in the plan by generating a precise and context-aware prompt for the LLM. This prompt should enable the LLM to utilize relevant stock data tools effectively to answer the query. Incorporate the input query, the plan steps, and the past interactions to produce a coherent and actionable prompt that guides the LLM to provide an accurate and informative response related to stock information. Return the updated state including the original input, the unchanged plan, the extended past steps with the current step and its output, and the new response generated by the LLM.', additional_kwargs={}, response_metadata={}, name='prompt_generator', id='81925da5-56fe-4f87-950a-ea72e3bfeaa4')]}}\n",
      "{'replan': {'plan': ['Use a prompt to query the LLM with the extracted input and past steps to decide whether to revise the plan or generate a final response.', \"If the LLM suggests revising the plan, update the 'plan' field with the new steps provided by the LLM and return the updated plan.\", \"If the LLM suggests that the task is complete, generate the final 'response' and return it as the final answer.\"]}}\n",
      "{'ai_node_gen_supervisor': {'next': 'structured_output_generation', 'task': \"Use the LLM's structured output functionality to generate a plan in the form of a list of steps based on the prompt.\", 'messages': [AIMessage(content=\"The step requires generating a plan in the form of a list of steps using the LLM's structured output functionality. The 'structured_output_generation' worker is specialized for this task and should be called to execute step 1.\", additional_kwargs={}, response_metadata={}, id='1d086c59-d3e5-4e70-9626-330095c3c2c2')]}}\n",
      "{'ai_node_gen_supervisor': {'next': 'prompt_generation', 'task': 'Use a prompt to query the LLM with the extracted input and past steps to decide whether to revise the plan or generate a final response.', 'messages': [AIMessage(content=\"The step requires querying the LLM with the extracted input and past steps to decide whether to revise the plan or generate a final response. The 'prompt_generation' worker is responsible for creating the prompt to query the LLM, making it the appropriate next worker to act.\", additional_kwargs={}, response_metadata={}, id='36a16770-5723-45d9-8897-1d95c4d07584')]}}\n",
      "{'replan': {'plan': ['Bind the generated prompt with appropriate stock-related tools using the toolset_generation capability to ensure accurate and factual responses.']}}\n",
      "{'ai_node_gen_supervisor': {'next': 'toolset_generation', 'task': 'Bind the generated prompt with appropriate stock-related tools using the toolset_generation capability to ensure accurate and factual responses.', 'messages': [AIMessage(content=\"The step requires binding the generated prompt with appropriate stock-related tools, which is exactly the responsibility of the 'toolset_generation' worker to ensure accurate and factual responses.\", additional_kwargs={}, response_metadata={}, id='7ef10976-e70d-4214-8395-6e6e9fef6425')]}}\n",
      "{'structured_output_generation': {'past_steps': [(\"Use the LLM's structured output functionality to generate a plan in the form of a list of steps based on the prompt.\", 'Yes, this node requires the LLM\\'s with_structured_output() function because:\\n\\n- The node generates a plan based on the input.\\n- The plan is a list of strings that must be stored in the \"plan\" field.\\n- The output needs to be a structured JSON object with a \"plan\" field containing a list of steps.\\n- Using structured output ensures consistent, verifiable, and type-safe output, reducing hallucinations and simplifying integration.\\n\\nHere is the code snippet implementing the structured output functionality for the LLM to use:\\n\\n```python\\nfrom typing import List\\nfrom pydantic import BaseModel, Field\\n\\n# Define a Pydantic class for the structured output\\nclass PlanOutput(BaseModel):\\n    plan: List[str] = Field(description=\"A list of actionable steps to accomplish the input task\")\\n\\ndef plan_step(state: PlanExecute) -> PlanExecute:\\n    \"\"\"Plan step generates a plan based on the input using llm structured output functionality, stores it to the plan field\"\"\"\\n    structured_llm = llm.with_structured_output(PlanOutput)\\n    # Invoke the LLM with the input task to generate the plan\\n    plan_output: PlanOutput = structured_llm.invoke(state[\"input\"])\\n    # Update the state with the generated plan\\n    state[\"plan\"] = plan_output.plan\\n    return state\\n```')], 'messages': [HumanMessage(content='Yes, this node requires the LLM\\'s with_structured_output() function because:\\n\\n- The node generates a plan based on the input.\\n- The plan is a list of strings that must be stored in the \"plan\" field.\\n- The output needs to be a structured JSON object with a \"plan\" field containing a list of steps.\\n- Using structured output ensures consistent, verifiable, and type-safe output, reducing hallucinations and simplifying integration.\\n\\nHere is the code snippet implementing the structured output functionality for the LLM to use:\\n\\n```python\\nfrom typing import List\\nfrom pydantic import BaseModel, Field\\n\\n# Define a Pydantic class for the structured output\\nclass PlanOutput(BaseModel):\\n    plan: List[str] = Field(description=\"A list of actionable steps to accomplish the input task\")\\n\\ndef plan_step(state: PlanExecute) -> PlanExecute:\\n    \"\"\"Plan step generates a plan based on the input using llm structured output functionality, stores it to the plan field\"\"\"\\n    structured_llm = llm.with_structured_output(PlanOutput)\\n    # Invoke the LLM with the input task to generate the plan\\n    plan_output: PlanOutput = structured_llm.invoke(state[\"input\"])\\n    # Update the state with the generated plan\\n    state[\"plan\"] = plan_output.plan\\n    return state\\n```', additional_kwargs={}, response_metadata={}, name='struct_output_generator', id='bbeafd8f-f8bc-4223-a6a8-1fe801bec54b')]}}\n",
      "{'prompt_generation': {'past_steps': [('Use a prompt to query the LLM with the extracted input and past steps to decide whether to revise the plan or generate a final response.', 'You are given a task input, a current plan (a list of steps), and a history of past steps with their outcomes. Your job is to evaluate the progress made so far and decide whether to revise the plan or produce a final response. \\n\\n- Carefully analyze the past steps and their results to determine if the current plan is still valid or needs adjustment.\\n- If the plan needs revision, generate a new plan as a list of clear, actionable steps.\\n- If the task is complete or no further planning is needed, generate a final response summarizing the outcome or answer.\\n- Provide your output in one of two formats:\\n  - If revising the plan, output: {\"plan\": [list of steps]}\\n  - If producing a final response, output: {\"response\": \"final answer or summary\"}\\n\\nUse the input task description and the history of past steps to guide your decision.')], 'messages': [HumanMessage(content='You are given a task input, a current plan (a list of steps), and a history of past steps with their outcomes. Your job is to evaluate the progress made so far and decide whether to revise the plan or produce a final response. \\n\\n- Carefully analyze the past steps and their results to determine if the current plan is still valid or needs adjustment.\\n- If the plan needs revision, generate a new plan as a list of clear, actionable steps.\\n- If the task is complete or no further planning is needed, generate a final response summarizing the outcome or answer.\\n- Provide your output in one of two formats:\\n  - If revising the plan, output: {\"plan\": [list of steps]}\\n  - If producing a final response, output: {\"response\": \"final answer or summary\"}\\n\\nUse the input task description and the history of past steps to guide your decision.', additional_kwargs={}, response_metadata={}, name='prompt_generator', id='57a230a9-33ab-45bb-a45e-167bb5d5171c')]}}\n",
      "{'replan': {'response': \"The plan to use the LLM's structured output functionality to generate a plan in the form of a list of steps based on the prompt has been fully executed as per the provided steps. No further action is required.\"}}\n",
      "{'replan': {'response': 'All steps in the OriginalPlan have been executed as per the PlanStepsExecuted. Therefore, the task is ready for final response generation or conclusion as per the plan.'}}\n",
      "{'code_compiler': {'final_code': '```python\\nfrom typing import List\\nfrom pydantic import BaseModel, Field\\n\\n# Define a Pydantic class for the structured output\\nclass PlanOutput(BaseModel):\\n    plan: List[str] = Field(description=\"A list of actionable steps to accomplish the input task\")\\n\\ndef plan_step(state: PlanExecute) -> PlanExecute:\\n    \"\"\"Plan step generates a plan based on the input using llm structured output functionality, stores it to the plan field\"\"\"\\n    structured_llm = llm.with_structured_output(PlanOutput)\\n    # Invoke the LLM with the input task to generate the plan\\n    plan_output: PlanOutput = structured_llm.invoke(state[\"input\"])\\n    # Update the state with the generated plan\\n    state[\"plan\"] = plan_output.plan\\n    return state\\n```'}}\n",
      "{'node_process': {'node_reports': [NodeEvaluationReport(node_name='planner', node_code_stub='```python\\nfrom typing import List\\nfrom pydantic import BaseModel, Field\\n\\n# Define a Pydantic class for the structured output\\nclass PlanOutput(BaseModel):\\n    plan: List[str] = Field(description=\"A list of actionable steps to accomplish the input task\")\\n\\ndef plan_step(state: PlanExecute) -> PlanExecute:\\n    \"\"\"Plan step generates a plan based on the input using llm structured output functionality, stores it to the plan field\"\"\"\\n    structured_llm = llm.with_structured_output(PlanOutput)\\n    # Invoke the LLM with the input task to generate the plan\\n    plan_output: PlanOutput = structured_llm.invoke(state[\"input\"])\\n    # Update the state with the generated plan\\n    state[\"plan\"] = plan_output.plan\\n    return state\\n```', edge_code='```python\\n# Since the edge is non-conditional (always go from planner to agent),\\n# we use the add_edge method directly.\\n\\ngraph.add_edge(\"planner\", \"agent\")\\n```')]}}\n",
      "{'toolset_generation': {'past_steps': [('Bind the generated prompt with appropriate stock-related tools using the toolset_generation capability to ensure accurate and factual responses.', '```python\\nfrom typing import List, Tuple, Annotated, TypedDict\\nfrom langchain_core.tools import tool\\nfrom langchain_core.messages import SystemMessage\\nfrom langgraph.prebuilt import ToolNode\\nfrom langchain.chat_models import ChatAnthropic\\n\\n# Define the tools needed by the LLM for stock-related queries\\n\\n@tool\\ndef get_stock_price(ticker: str):\\n    \"\"\"Get the current stock price for a given ticker symbol.\"\"\"\\n    # Placeholder implementation\\n    stock_prices = {\\n        \"AAPL\": \"Apple Inc. stock price is $150.\",\\n        \"GOOG\": \"Alphabet Inc. stock price is $2800.\",\\n        \"TSLA\": \"Tesla Inc. stock price is $700.\"\\n    }\\n    return stock_prices.get(ticker.upper(), \"Stock ticker not found.\")\\n\\n@tool\\ndef get_stock_news(ticker: str):\\n    \"\"\"Get the latest news for a given stock ticker.\"\"\"\\n    # Placeholder implementation\\n    news = {\\n        \"AAPL\": \"Apple releases new iPhone model.\",\\n        \"GOOG\": \"Google announces AI breakthrough.\",\\n        \"TSLA\": \"Tesla opens new factory in Texas.\"\\n    }\\n    return news.get(ticker.upper(), \"No recent news found for this ticker.\")\\n\\ntools = [get_stock_price, get_stock_news]\\n\\n# Bind the model (llm) with tools\\nmodel_with_tools = ChatAnthropic(\\n    model=\"claude-3-haiku-20240307\", temperature=0\\n).bind_tools(tools)\\n\\n# Generate a tool node.\\ntool_node = ToolNode(tools)\\n\\ndef execute_step(state: TypedDict) -> TypedDict:\\n    \"\"\"Uses llm with tool binding for the stock related queries\"\"\"\\n    input_query = state[\"input\"]\\n    plan: List[str] = state[\"plan\"]\\n    past_steps: Annotated[List[Tuple], object] = state[\"past_steps\"]\\n\\n    # Compose system message to instruct the LLM about the stock tools available\\n    system_message = SystemMessage(\\n        content=(\\n            \"You are a stock query assistant. You have access to tools like get_stock_price and get_stock_news \"\\n            \"to help answer stock-related questions. Use these tools as needed to provide accurate and factual responses.\"\\n        )\\n    )\\n\\n    # Compose user messages including input query and plan context\\n    user_messages = [\\n        SystemMessage(content=f\"Input query: {input_query}\"),\\n        SystemMessage(content=f\"Plan steps: {plan}\"),\\n        SystemMessage(content=f\"Past steps and responses: {past_steps}\")\\n    ]\\n\\n    # Invoke the model with tools\\n    response = model_with_tools.invoke([system_message] + user_messages)\\n\\n    # Update past_steps with current step and response text\\n    updated_past_steps = past_steps + [(\"execute_step\", response.content)]\\n\\n    # Return updated state\\n    return {\\n        \"input\": input_query,\\n        \"plan\": plan,\\n        \"past_steps\": updated_past_steps,\\n        \"response\": response.content\\n    }\\n```')], 'messages': [HumanMessage(content='```python\\nfrom typing import List, Tuple, Annotated, TypedDict\\nfrom langchain_core.tools import tool\\nfrom langchain_core.messages import SystemMessage\\nfrom langgraph.prebuilt import ToolNode\\nfrom langchain.chat_models import ChatAnthropic\\n\\n# Define the tools needed by the LLM for stock-related queries\\n\\n@tool\\ndef get_stock_price(ticker: str):\\n    \"\"\"Get the current stock price for a given ticker symbol.\"\"\"\\n    # Placeholder implementation\\n    stock_prices = {\\n        \"AAPL\": \"Apple Inc. stock price is $150.\",\\n        \"GOOG\": \"Alphabet Inc. stock price is $2800.\",\\n        \"TSLA\": \"Tesla Inc. stock price is $700.\"\\n    }\\n    return stock_prices.get(ticker.upper(), \"Stock ticker not found.\")\\n\\n@tool\\ndef get_stock_news(ticker: str):\\n    \"\"\"Get the latest news for a given stock ticker.\"\"\"\\n    # Placeholder implementation\\n    news = {\\n        \"AAPL\": \"Apple releases new iPhone model.\",\\n        \"GOOG\": \"Google announces AI breakthrough.\",\\n        \"TSLA\": \"Tesla opens new factory in Texas.\"\\n    }\\n    return news.get(ticker.upper(), \"No recent news found for this ticker.\")\\n\\ntools = [get_stock_price, get_stock_news]\\n\\n# Bind the model (llm) with tools\\nmodel_with_tools = ChatAnthropic(\\n    model=\"claude-3-haiku-20240307\", temperature=0\\n).bind_tools(tools)\\n\\n# Generate a tool node.\\ntool_node = ToolNode(tools)\\n\\ndef execute_step(state: TypedDict) -> TypedDict:\\n    \"\"\"Uses llm with tool binding for the stock related queries\"\"\"\\n    input_query = state[\"input\"]\\n    plan: List[str] = state[\"plan\"]\\n    past_steps: Annotated[List[Tuple], object] = state[\"past_steps\"]\\n\\n    # Compose system message to instruct the LLM about the stock tools available\\n    system_message = SystemMessage(\\n        content=(\\n            \"You are a stock query assistant. You have access to tools like get_stock_price and get_stock_news \"\\n            \"to help answer stock-related questions. Use these tools as needed to provide accurate and factual responses.\"\\n        )\\n    )\\n\\n    # Compose user messages including input query and plan context\\n    user_messages = [\\n        SystemMessage(content=f\"Input query: {input_query}\"),\\n        SystemMessage(content=f\"Plan steps: {plan}\"),\\n        SystemMessage(content=f\"Past steps and responses: {past_steps}\")\\n    ]\\n\\n    # Invoke the model with tools\\n    response = model_with_tools.invoke([system_message] + user_messages)\\n\\n    # Update past_steps with current step and response text\\n    updated_past_steps = past_steps + [(\"execute_step\", response.content)]\\n\\n    # Return updated state\\n    return {\\n        \"input\": input_query,\\n        \"plan\": plan,\\n        \"past_steps\": updated_past_steps,\\n        \"response\": response.content\\n    }\\n```', additional_kwargs={}, response_metadata={}, name='toolset_generator', id='55049b00-d923-4697-938e-ab7befaa5501')]}}\n",
      "{'replan': {'response': 'The plan has been fully executed as per the OriginalPlan. The generated prompt has been successfully bound with the appropriate stock-related tools using the toolset_generation capability, ensuring accurate and factual responses. No further steps are required.'}}\n",
      "{'code_compiler': {'final_code': '```python\\nfrom typing import Union, List, Tuple, Annotated, TypedDict, Dict, Any\\nimport operator\\nimport json\\n\\n# Define the types as per the schema info\\nPlanExecute = TypedDict(\\'PlanExecute\\', {\\n    \\'input\\': str,\\n    \\'plan\\': List[str],\\n    \\'past_steps\\': Annotated[List[Tuple[Any, Any]], operator.add],\\n    \\'response\\': str\\n})\\n\\nResponse = TypedDict(\\'Response\\', {\\n    \\'response\\': str\\n})\\n\\nPlan = TypedDict(\\'Plan\\', {\\n    \\'plan\\': List[str]\\n})\\n\\ndef replan_step(state: PlanExecute) -> Union[Response, Plan]:\\n    \"\"\"Evaluates progress and uses an LLM to either revise the plan or generate a final response.\"\"\"\\n    # Extract relevant information from the state\\n    task_input = state[\\'input\\']\\n    current_plan = state[\\'plan\\']\\n    past_steps = state[\\'past_steps\\']\\n\\n    # Construct the prompt for the LLM based on the prompt_generator instructions\\n    # Format past_steps as a readable string\\n    past_steps_str = \"\"\\n    for i, (step, outcome) in enumerate(past_steps, 1):\\n        past_steps_str += f\"Step {i}: {step}\\\\nOutcome: {outcome}\\\\n\\\\n\"\\n\\n    prompt = (\\n        \"You are given a task input, a current plan (a list of steps), and a history of past steps with their outcomes. \"\\n        \"Your job is to evaluate the progress made so far and decide whether to revise the plan or produce a final response.\\\\n\\\\n\"\\n        f\"Task input: {task_input}\\\\n\\\\n\"\\n        f\"Current plan: {current_plan}\\\\n\\\\n\"\\n        f\"Past steps and outcomes:\\\\n{past_steps_str}\\\\n\"\\n        \"- Carefully analyze the past steps and their results to determine if the current plan is still valid or needs adjustment.\\\\n\"\\n        \"- If the plan needs revision, generate a new plan as a list of clear, actionable steps.\\\\n\"\\n        \"- If the task is complete or no further planning is needed, generate a final response summarizing the outcome or answer.\\\\n\"\\n        \"- Provide your output in one of two formats:\\\\n\"\\n        \"  - If revising the plan, output: {\\\\\"plan\\\\\": [list of steps]}\\\\n\"\\n        \"  - If producing a final response, output: {\\\\\"response\\\\\": \\\\\"final answer or summary\\\\\"}\\\\n\\\\n\"\\n        \"Use the input task description and the history of past steps to guide your decision.\"\\n    )\\n\\n    # Here we simulate the LLM call by calling a placeholder function `call_llm`\\n    # In a real implementation, this would be an API call or similar\\n    llm_output_str = call_llm(prompt)\\n\\n    # Parse the LLM output as JSON\\n    try:\\n        llm_output = json.loads(llm_output_str)\\n    except json.JSONDecodeError:\\n        # If parsing fails, return a response indicating failure\\n        return {\"response\": \"Failed to parse LLM output.\"}\\n\\n    # Determine if the output is a plan or a response and return accordingly\\n    if \\'plan\\' in llm_output and isinstance(llm_output[\\'plan\\'], list):\\n        # Return the new plan\\n        return {\"plan\": llm_output[\\'plan\\']}\\n    elif \\'response\\' in llm_output and isinstance(llm_output[\\'response\\'], str):\\n        # Return the final response\\n        return {\"response\": llm_output[\\'response\\']}\\n    else:\\n        # Unexpected output format\\n        return {\"response\": \"LLM output format not recognized.\"}\\n\\n\\ndef call_llm(prompt: str) -> str:\\n    \"\"\"\\n    Placeholder function to simulate an LLM call.\\n    In practice, this function would send the prompt to an LLM and return its output as a string.\\n    \"\"\"\\n    # For demonstration, return a dummy response indicating no revision needed\\n    # This should be replaced with actual LLM integration\\n    dummy_response = json.dumps({\\n        \"response\": \"Task completed successfully with the current plan.\"\\n    })\\n    return dummy_response\\n```'}}\n",
      "{'node_process': {'node_reports': [NodeEvaluationReport(node_name='replan', node_code_stub='```python\\nfrom typing import Union, List, Tuple, Annotated, TypedDict, Dict, Any\\nimport operator\\nimport json\\n\\n# Define the types as per the schema info\\nPlanExecute = TypedDict(\\'PlanExecute\\', {\\n    \\'input\\': str,\\n    \\'plan\\': List[str],\\n    \\'past_steps\\': Annotated[List[Tuple[Any, Any]], operator.add],\\n    \\'response\\': str\\n})\\n\\nResponse = TypedDict(\\'Response\\', {\\n    \\'response\\': str\\n})\\n\\nPlan = TypedDict(\\'Plan\\', {\\n    \\'plan\\': List[str]\\n})\\n\\ndef replan_step(state: PlanExecute) -> Union[Response, Plan]:\\n    \"\"\"Evaluates progress and uses an LLM to either revise the plan or generate a final response.\"\"\"\\n    # Extract relevant information from the state\\n    task_input = state[\\'input\\']\\n    current_plan = state[\\'plan\\']\\n    past_steps = state[\\'past_steps\\']\\n\\n    # Construct the prompt for the LLM based on the prompt_generator instructions\\n    # Format past_steps as a readable string\\n    past_steps_str = \"\"\\n    for i, (step, outcome) in enumerate(past_steps, 1):\\n        past_steps_str += f\"Step {i}: {step}\\\\nOutcome: {outcome}\\\\n\\\\n\"\\n\\n    prompt = (\\n        \"You are given a task input, a current plan (a list of steps), and a history of past steps with their outcomes. \"\\n        \"Your job is to evaluate the progress made so far and decide whether to revise the plan or produce a final response.\\\\n\\\\n\"\\n        f\"Task input: {task_input}\\\\n\\\\n\"\\n        f\"Current plan: {current_plan}\\\\n\\\\n\"\\n        f\"Past steps and outcomes:\\\\n{past_steps_str}\\\\n\"\\n        \"- Carefully analyze the past steps and their results to determine if the current plan is still valid or needs adjustment.\\\\n\"\\n        \"- If the plan needs revision, generate a new plan as a list of clear, actionable steps.\\\\n\"\\n        \"- If the task is complete or no further planning is needed, generate a final response summarizing the outcome or answer.\\\\n\"\\n        \"- Provide your output in one of two formats:\\\\n\"\\n        \"  - If revising the plan, output: {\\\\\"plan\\\\\": [list of steps]}\\\\n\"\\n        \"  - If producing a final response, output: {\\\\\"response\\\\\": \\\\\"final answer or summary\\\\\"}\\\\n\\\\n\"\\n        \"Use the input task description and the history of past steps to guide your decision.\"\\n    )\\n\\n    # Here we simulate the LLM call by calling a placeholder function `call_llm`\\n    # In a real implementation, this would be an API call or similar\\n    llm_output_str = call_llm(prompt)\\n\\n    # Parse the LLM output as JSON\\n    try:\\n        llm_output = json.loads(llm_output_str)\\n    except json.JSONDecodeError:\\n        # If parsing fails, return a response indicating failure\\n        return {\"response\": \"Failed to parse LLM output.\"}\\n\\n    # Determine if the output is a plan or a response and return accordingly\\n    if \\'plan\\' in llm_output and isinstance(llm_output[\\'plan\\'], list):\\n        # Return the new plan\\n        return {\"plan\": llm_output[\\'plan\\']}\\n    elif \\'response\\' in llm_output and isinstance(llm_output[\\'response\\'], str):\\n        # Return the final response\\n        return {\"response\": llm_output[\\'response\\']}\\n    else:\\n        # Unexpected output format\\n        return {\"response\": \"LLM output format not recognized.\"}\\n\\n\\ndef call_llm(prompt: str) -> str:\\n    \"\"\"\\n    Placeholder function to simulate an LLM call.\\n    In practice, this function would send the prompt to an LLM and return its output as a string.\\n    \"\"\"\\n    # For demonstration, return a dummy response indicating no revision needed\\n    # This should be replaced with actual LLM integration\\n    dummy_response = json.dumps({\\n        \"response\": \"Task completed successfully with the current plan.\"\\n    })\\n    return dummy_response\\n```', edge_code='```python\\nfrom langgraph.graph import END\\n\\n# Define the routing function for conditional edges from \\'replan\\'\\ndef replan_routing_function(state):\\n    # If \\'response\\' key exists in state and is non-empty, end the process\\n    if \"response\" in state and state[\"response\"]:\\n        return END  # Terminate the graph execution\\n    else:\\n        # Otherwise, continue to \\'agent\\' node for further execution\\n        return \"agent\"\\n\\n# Add conditional edges from \\'replan\\' node using the routing function\\ngraph.add_conditional_edges(\"replan\", replan_routing_function)\\n```\\n\\n# Explanation:\\n# - We define a routing function `replan_routing_function` that checks the state after the \\'replan\\' node executes.\\n# - If a \\'response\\' is present in the state (indicating the task is complete), it returns END to stop the graph.\\n# - Otherwise, it returns \\'agent\\' to continue execution to the \\'agent\\' node.\\n# - We then add this conditional edge from \\'replan\\' using `add_conditional_edges`.\\n```')]}}\n",
      "{'code_compiler': {'final_code': '```python\\nfrom typing import List, Tuple, Annotated, TypedDict\\nfrom langchain_core.tools import tool\\nfrom langchain_core.messages import SystemMessage\\nfrom langgraph.prebuilt import ToolNode\\nfrom langchain.chat_models import ChatAnthropic\\n\\nclass PlanExecute(TypedDict):\\n    input: str\\n    plan: List[str]\\n    past_steps: Annotated[List[Tuple], object]\\n    response: str\\n\\n# Define the tools needed by the LLM for stock-related queries\\n\\n@tool\\ndef get_stock_price(ticker: str):\\n    \"\"\"Get the current stock price for a given ticker symbol.\"\"\"\\n    # Placeholder implementation\\n    stock_prices = {\\n        \"AAPL\": \"Apple Inc. stock price is $150.\",\\n        \"GOOG\": \"Alphabet Inc. stock price is $2800.\",\\n        \"TSLA\": \"Tesla Inc. stock price is $700.\"\\n    }\\n    return stock_prices.get(ticker.upper(), \"Stock ticker not found.\")\\n\\n@tool\\ndef get_stock_news(ticker: str):\\n    \"\"\"Get the latest news for a given stock ticker.\"\"\"\\n    # Placeholder implementation\\n    news = {\\n        \"AAPL\": \"Apple releases new iPhone model.\",\\n        \"GOOG\": \"Google announces AI breakthrough.\",\\n        \"TSLA\": \"Tesla opens new factory in Texas.\"\\n    }\\n    return news.get(ticker.upper(), \"No recent news found for this ticker.\")\\n\\ntools = [get_stock_price, get_stock_news]\\n\\n# Bind the model (llm) with tools\\nmodel_with_tools = ChatAnthropic(\\n    model=\"claude-3-haiku-20240307\", temperature=0\\n).bind_tools(tools)\\n\\n# Generate a tool node.\\ntool_node = ToolNode(tools)\\n\\ndef execute_step(state: PlanExecute) -> PlanExecute:\\n    \"\"\"Uses llm with tool binding for the stock related queries\"\"\"\\n    input_query = state[\"input\"]\\n    plan: List[str] = state[\"plan\"]\\n    past_steps: Annotated[List[Tuple], object] = state[\"past_steps\"]\\n\\n    # Compose system message to instruct the LLM about the stock tools available\\n    system_message = SystemMessage(\\n        content=(\\n            \"You are a stock query assistant. You have access to tools like get_stock_price and get_stock_news \"\\n            \"to help answer stock-related questions. Use these tools as needed to provide accurate and factual responses.\"\\n        )\\n    )\\n\\n    # Compose user messages including input query and plan context\\n    user_messages = [\\n        SystemMessage(content=f\"Input query: {input_query}\"),\\n        SystemMessage(content=f\"Plan steps: {plan}\"),\\n        SystemMessage(content=f\"Past steps and responses: {past_steps}\")\\n    ]\\n\\n    # Invoke the model with tools\\n    response = model_with_tools.invoke([system_message] + user_messages)\\n\\n    # Update past_steps with current step and response text\\n    updated_past_steps = past_steps + [(\"execute_step\", response.content)]\\n\\n    # Return updated state\\n    return {\\n        \"input\": input_query,\\n        \"plan\": plan,\\n        \"past_steps\": updated_past_steps,\\n        \"response\": response.content\\n    }\\n```'}}\n",
      "{'node_process': {'node_reports': [NodeEvaluationReport(node_name='agent', node_code_stub='```python\\nfrom typing import List, Tuple, Annotated, TypedDict\\nfrom langchain_core.tools import tool\\nfrom langchain_core.messages import SystemMessage\\nfrom langgraph.prebuilt import ToolNode\\nfrom langchain.chat_models import ChatAnthropic\\n\\nclass PlanExecute(TypedDict):\\n    input: str\\n    plan: List[str]\\n    past_steps: Annotated[List[Tuple], object]\\n    response: str\\n\\n# Define the tools needed by the LLM for stock-related queries\\n\\n@tool\\ndef get_stock_price(ticker: str):\\n    \"\"\"Get the current stock price for a given ticker symbol.\"\"\"\\n    # Placeholder implementation\\n    stock_prices = {\\n        \"AAPL\": \"Apple Inc. stock price is $150.\",\\n        \"GOOG\": \"Alphabet Inc. stock price is $2800.\",\\n        \"TSLA\": \"Tesla Inc. stock price is $700.\"\\n    }\\n    return stock_prices.get(ticker.upper(), \"Stock ticker not found.\")\\n\\n@tool\\ndef get_stock_news(ticker: str):\\n    \"\"\"Get the latest news for a given stock ticker.\"\"\"\\n    # Placeholder implementation\\n    news = {\\n        \"AAPL\": \"Apple releases new iPhone model.\",\\n        \"GOOG\": \"Google announces AI breakthrough.\",\\n        \"TSLA\": \"Tesla opens new factory in Texas.\"\\n    }\\n    return news.get(ticker.upper(), \"No recent news found for this ticker.\")\\n\\ntools = [get_stock_price, get_stock_news]\\n\\n# Bind the model (llm) with tools\\nmodel_with_tools = ChatAnthropic(\\n    model=\"claude-3-haiku-20240307\", temperature=0\\n).bind_tools(tools)\\n\\n# Generate a tool node.\\ntool_node = ToolNode(tools)\\n\\ndef execute_step(state: PlanExecute) -> PlanExecute:\\n    \"\"\"Uses llm with tool binding for the stock related queries\"\"\"\\n    input_query = state[\"input\"]\\n    plan: List[str] = state[\"plan\"]\\n    past_steps: Annotated[List[Tuple], object] = state[\"past_steps\"]\\n\\n    # Compose system message to instruct the LLM about the stock tools available\\n    system_message = SystemMessage(\\n        content=(\\n            \"You are a stock query assistant. You have access to tools like get_stock_price and get_stock_news \"\\n            \"to help answer stock-related questions. Use these tools as needed to provide accurate and factual responses.\"\\n        )\\n    )\\n\\n    # Compose user messages including input query and plan context\\n    user_messages = [\\n        SystemMessage(content=f\"Input query: {input_query}\"),\\n        SystemMessage(content=f\"Plan steps: {plan}\"),\\n        SystemMessage(content=f\"Past steps and responses: {past_steps}\")\\n    ]\\n\\n    # Invoke the model with tools\\n    response = model_with_tools.invoke([system_message] + user_messages)\\n\\n    # Update past_steps with current step and response text\\n    updated_past_steps = past_steps + [(\"execute_step\", response.content)]\\n\\n    # Return updated state\\n    return {\\n        \"input\": input_query,\\n        \"plan\": plan,\\n        \"past_steps\": updated_past_steps,\\n        \"response\": response.content\\n    }\\n```', edge_code='```python\\n# Since the edge from \\'agent\\' to \\'replan\\' is non-conditional (always goes from agent to replan),\\n# we use the add_edge method directly as per the NonConditionalEdges section.\\n\\ngraph.add_edge(\"agent\", \"replan\")\\n```')]}}\n",
      "{'graph_compile': {'messages': [AIMessage(content='```python\\nfrom typing import List, Tuple, Annotated, TypedDict, Union, Any\\nfrom pydantic import BaseModel, Field\\nfrom langchain.chat_models import ChatAnthropic\\nfrom langchain_core.tools import tool\\nfrom langchain_core.messages import SystemMessage\\nfrom langgraph.prebuilt import ToolNode\\nfrom langgraph.graph import Graph, END\\nfrom langgraph.checkpointer import InMemoryCheckpointer\\n\\n# Define the state type used across nodes\\nclass PlanOutput(BaseModel):\\n    plan: List[str] = Field(description=\"A list of actionable steps to accomplish the input task\")\\n\\nPlanExecute = TypedDict(\\'PlanExecute\\', {\\n    \\'input\\': str,\\n    \\'plan\\': List[str],\\n    \\'past_steps\\': Annotated[List[Tuple[Any, Any]], object],\\n    \\'response\\': str\\n})\\n\\n# Initialize the graph\\ngraph = Graph()\\n\\n# === Node implementations ===\\n\\n# 1. planner node: generate a plan using LLM structured output\\ndef planner(state: PlanExecute) -> PlanExecute:\\n    structured_llm = llm.with_structured_output(PlanOutput)\\n    plan_output: PlanOutput = structured_llm.invoke(state[\"input\"])\\n    state[\"plan\"] = plan_output.plan\\n    return state\\n\\n# 2. agent node: execute a step using LLM with tool binding for stock queries\\n\\n# Define tools\\n@tool\\ndef get_stock_price(ticker: str):\\n    stock_prices = {\\n        \"AAPL\": \"Apple Inc. stock price is $150.\",\\n        \"GOOG\": \"Alphabet Inc. stock price is $2800.\",\\n        \"TSLA\": \"Tesla Inc. stock price is $700.\"\\n    }\\n    return stock_prices.get(ticker.upper(), \"Stock ticker not found.\")\\n\\n@tool\\ndef get_stock_news(ticker: str):\\n    news = {\\n        \"AAPL\": \"Apple releases new iPhone model.\",\\n        \"GOOG\": \"Google announces AI breakthrough.\",\\n        \"TSLA\": \"Tesla opens new factory in Texas.\"\\n    }\\n    return news.get(ticker.upper(), \"No recent news found for this ticker.\")\\n\\ntools = [get_stock_price, get_stock_news]\\n\\n# Bind the model with tools\\nllm = ChatAnthropic(model=\"claude-3-haiku-20240307\", temperature=0)\\nmodel_with_tools = llm.bind_tools(tools)\\n\\ntool_node = ToolNode(tools)\\n\\ndef agent(state: PlanExecute) -> PlanExecute:\\n    input_query = state[\"input\"]\\n    plan = state[\"plan\"]\\n    past_steps = state[\"past_steps\"]\\n\\n    system_message = SystemMessage(\\n        content=(\\n            \"You are a stock query assistant. You have access to tools like get_stock_price and get_stock_news \"\\n            \"to help answer stock-related questions. Use these tools as needed to provide accurate and factual responses.\"\\n        )\\n    )\\n\\n    user_messages = [\\n        SystemMessage(content=f\"Input query: {input_query}\"),\\n        SystemMessage(content=f\"Plan steps: {plan}\"),\\n        SystemMessage(content=f\"Past steps and responses: {past_steps}\")\\n    ]\\n\\n    response = model_with_tools.invoke([system_message] + user_messages)\\n\\n    updated_past_steps = past_steps + [(\"execute_step\", response.content)]\\n\\n    return {\\n        \"input\": input_query,\\n        \"plan\": plan,\\n        \"past_steps\": updated_past_steps,\\n        \"response\": response.content\\n    }\\n\\n# 3. replan node: evaluate progress and decide to replan or finish\\n\\ndef replan(state: PlanExecute) -> Union[dict, dict]:\\n    task_input = state[\\'input\\']\\n    current_plan = state[\\'plan\\']\\n    past_steps = state[\\'past_steps\\']\\n\\n    past_steps_str = \"\"\\n    for i, (step, outcome) in enumerate(past_steps, 1):\\n        past_steps_str += f\"Step {i}: {step}\\\\nOutcome: {outcome}\\\\n\\\\n\"\\n\\n    prompt = (\\n        \"You are given a task input, a current plan (a list of steps), and a history of past steps with their outcomes. \"\\n        \"Your job is to evaluate the progress made so far and decide whether to revise the plan or produce a final response.\\\\n\\\\n\"\\n        f\"Task input: {task_input}\\\\n\\\\n\"\\n        f\"Current plan: {current_plan}\\\\n\\\\n\"\\n        f\"Past steps and outcomes:\\\\n{past_steps_str}\\\\n\"\\n        \"- Carefully analyze the past steps and their results to determine if the current plan is still valid or needs adjustment.\\\\n\"\\n        \"- If the plan needs revision, generate a new plan as a list of clear, actionable steps.\\\\n\"\\n        \"- If the task is complete or no further planning is needed, generate a final response summarizing the outcome or answer.\\\\n\"\\n        \"- Provide your output in one of two formats:\\\\n\"\\n        \"  - If revising the plan, output: {\\\\\"plan\\\\\": [list of steps]}\\\\n\"\\n        \"  - If producing a final response, output: {\\\\\"response\\\\\": \\\\\"final answer or summary\\\\\"}\\\\n\\\\n\"\\n        \"Use the input task description and the history of past steps to guide your decision.\"\\n    )\\n\\n    # Use the llm to get the output string\\n    llm_response = llm.invoke(prompt)\\n\\n    # Parse the output as JSON\\n    import json\\n    try:\\n        llm_output = json.loads(llm_response)\\n    except json.JSONDecodeError:\\n        return {\"response\": \"Failed to parse LLM output.\"}\\n\\n    if \\'plan\\' in llm_output and isinstance(llm_output[\\'plan\\'], list):\\n        return {\"plan\": llm_output[\\'plan\\']}\\n    elif \\'response\\' in llm_output and isinstance(llm_output[\\'response\\'], str):\\n        return {\"response\": llm_output[\\'response\\']}\\n    else:\\n        return {\"response\": \"LLM output format not recognized.\"}\\n\\n# === Add nodes to graph ===\\ngraph.add_node(\"planner\", planner)\\ngraph.add_node(\"agent\", agent)\\ngraph.add_node(\"replan\", replan)\\n\\n# === Add edges ===\\n\\n# edge_1: __START__ -> planner (non-conditional)\\ngraph.add_edge(\"__START__\", \"planner\")\\n\\n# edge_2: planner -> agent (non-conditional)\\ngraph.add_edge(\"planner\", \"agent\")\\n\\n# edge_3: agent -> replan (non-conditional)\\ngraph.add_edge(\"agent\", \"replan\")\\n\\n# edge_4 and edge_5: replan -> agent or __END__ (conditional)\\ndef replan_routing_function(state):\\n    if \"response\" in state and state[\"response\"]:\\n        return END\\n    else:\\n        return \"agent\"\\n\\ngraph.add_conditional_edges(\"replan\", replan_routing_function)\\n\\n# === Setup checkpointer and compile graph ===\\ncheckpointer = InMemoryCheckpointer()\\nfinal_app = graph.compile(checkpointer=checkpointer)\\n```', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 1421, 'prompt_tokens': 2622, 'total_tokens': 4043, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4.1-mini-2025-04-14', 'system_fingerprint': 'fp_38647f5e19', 'id': 'chatcmpl-BRzDaCwuUHj5kgRDWtlNdsYNEzKhU', 'finish_reason': 'stop', 'logprobs': None}, id='run-9d281af2-cde5-4d26-984d-acfb85961cc5-0', usage_metadata={'input_tokens': 2622, 'output_tokens': 1421, 'total_tokens': 4043, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}}\n"
     ]
    }
   ],
   "source": [
    "import uuid\n",
    "uuid_str1= uuid.uuid4()\n",
    "config = {\"configurable\": {\"thread_id\": str(uuid_str1)}}\n",
    "\n",
    "for output in compiler_graph.stream({\"json_objects\": json_objects}, config, stream_mode=\"updates\"\n",
    "    ):\n",
    "        print(output)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1656998b",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = get_model()\n",
    "\n",
    "result = llm.invoke(\"write langgraph code for plan-and-execute, use structured output functionality\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bcaf441d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Certainly! Below is an example of how you can write LangGraph code for a **plan-and-execute** pattern using the **structured output** functionality. This example demonstrates a simple agent that first **plans** a sequence of steps to achieve a goal, then **executes** each step in order.\n",
      "\n",
      "```langgraph\n",
      "agent PlanAndExecuteAgent {\n",
      "  input: string goal\n",
      "\n",
      "  // Step 1: Plan - generate a structured plan as a list of steps\n",
      "  step plan {\n",
      "    prompt: \"\"\"\n",
      "    You are an AI assistant. Given the goal below, create a step-by-step plan to achieve it.\n",
      "    Output the plan as a JSON array of steps, each with a \"step_number\" and \"instruction\".\n",
      "\n",
      "    Goal: {goal}\n",
      "\n",
      "    Example output:\n",
      "    [\n",
      "      {\"step_number\": 1, \"instruction\": \"Do this first\"},\n",
      "      {\"step_number\": 2, \"instruction\": \"Then do this\"}\n",
      "    ]\n",
      "\n",
      "    Your output:\n",
      "    \"\"\"\n",
      "    output: json array of {\n",
      "      step_number: int,\n",
      "      instruction: string\n",
      "    }\n",
      "  }\n",
      "\n",
      "  // Step 2: Execute - iterate over the plan steps and execute each instruction\n",
      "  step execute {\n",
      "    input: plan_steps = plan.output\n",
      "    prompt: \"\"\"\n",
      "    You are an AI assistant executing instructions one by one.\n",
      "\n",
      "    Execute the following instruction:\n",
      "    {current_instruction}\n",
      "\n",
      "    Provide the result or output of this step.\n",
      "    \"\"\"\n",
      "    // This step will be called multiple times, once per instruction\n",
      "    // We'll define a loop in the graph to iterate over plan steps\n",
      "    output: string\n",
      "  }\n",
      "\n",
      "  // Control flow: iterate over plan steps and execute them\n",
      "  flow {\n",
      "    plan_output = plan(goal)\n",
      "    results = []\n",
      "    for step in plan_output {\n",
      "      current_instruction = step.instruction\n",
      "      result = execute(current_instruction)\n",
      "      results.append({\n",
      "        step_number: step.step_number,\n",
      "        instruction: current_instruction,\n",
      "        result: result\n",
      "      })\n",
      "    }\n",
      "    return results\n",
      "  }\n",
      "}\n",
      "```\n",
      "\n",
      "### Explanation:\n",
      "\n",
      "- **plan step**: Takes the goal as input and outputs a structured JSON array of steps.\n",
      "- **execute step**: Takes a single instruction and outputs the result of executing that instruction.\n",
      "- **flow**: Runs the plan step, then loops over each planned step, executing it and collecting results.\n",
      "\n",
      "You can adapt the prompts and execution logic to your specific use case. Let me know if you want a runnable example or integration with a specific LLM!\n"
     ]
    }
   ],
   "source": [
    "result.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b5ae7e32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "# LangGraph Code for Plan-and-Execute with Structured Output\n",
      "\n",
      "Below is a complete implementation of a plan-and-execute pattern using LangGraph with structured output functionality. This implementation creates a graph that:\n",
      "\n",
      "1. Takes a user query\n",
      "2. Plans steps to address the query\n",
      "3. Executes each step in sequence\n",
      "4. Provides a final answer\n",
      "\n",
      "```python\n",
      "import os\n",
      "from typing import List, Tuple, TypedDict, Annotated, Literal\n",
      "from datetime import datetime\n",
      "\n",
      "from langchain_core.prompts import ChatPromptTemplate, PromptTemplate\n",
      "from langchain_openai import ChatOpenAI\n",
      "from langchain_core.output_parsers import JsonOutputParser\n",
      "from langchain_core.pydantic_v1 import BaseModel, Field\n",
      "from langgraph.graph import StateGraph, END\n",
      "\n",
      "# Define structured output schemas\n",
      "class PlanStep(BaseModel):\n",
      "    step_number: int = Field(description=\"The number of this step in the plan\")\n",
      "    step_description: str = Field(description=\"Description of what this step will do\")\n",
      "\n",
      "class Plan(BaseModel):\n",
      "    steps: List[PlanStep] = Field(description=\"The sequential steps to solve the problem\")\n",
      "    reasoning: str = Field(description=\"The reasoning behind the plan\")\n",
      "\n",
      "class ExecutionResult(BaseModel):\n",
      "    output: str = Field(description=\"The result of executing the step\")\n",
      "    success: bool = Field(description=\"Whether the step execution was successful\")\n",
      "    notes: str = Field(description=\"Any notes about the execution process or outcome\")\n",
      "\n",
      "class FinalAnswer(BaseModel):\n",
      "    answer: str = Field(description=\"The final answer to the user's query\")\n",
      "    summary: str = Field(description=\"A summary of the steps taken to arrive at the answer\")\n",
      "\n",
      "# Define the state of our graph\n",
      "class AgentState(TypedDict):\n",
      "    query: str\n",
      "    plan: Plan\n",
      "    current_step_index: int\n",
      "    execution_results: List[ExecutionResult]\n",
      "    final_answer: FinalAnswer\n",
      "\n",
      "# Initialize LLM\n",
      "llm = ChatOpenAI(model=\"gpt-4\", temperature=0)\n",
      "\n",
      "# Create prompts with structured output\n",
      "planner_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
      "You are a planning assistant that breaks down complex tasks into specific, concrete steps.\n",
      "\n",
      "Based on the user query, create a detailed plan with sequential steps.\n",
      "\n",
      "USER QUERY:\n",
      "{query}\n",
      "\n",
      "Create a plan with clear, executable steps. Be specific and thorough.\n",
      "\"\"\")\n",
      "\n",
      "executor_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
      "You are an execution assistant that carries out specific steps in a plan.\n",
      "\n",
      "PLAN: {plan}\n",
      "\n",
      "You need to execute the following step:\n",
      "Step {step_number}: {step_description}\n",
      "\n",
      "Previous execution results: {previous_results}\n",
      "\n",
      "Execute just this step and provide the result. Be thorough and detailed in your execution.\n",
      "\"\"\")\n",
      "\n",
      "final_answer_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
      "You are a synthesis assistant that provides final answers based on a plan and its execution.\n",
      "\n",
      "ORIGINAL QUERY: {query}\n",
      "\n",
      "PLAN: {plan}\n",
      "\n",
      "EXECUTION RESULTS:\n",
      "{execution_results}\n",
      "\n",
      "Synthesize the information above to provide a comprehensive final answer to the original query.\n",
      "\"\"\")\n",
      "\n",
      "# Create chains with structured output parsing\n",
      "planner_chain = (\n",
      "    planner_prompt \n",
      "    | llm.with_structured_output(Plan)\n",
      ")\n",
      "\n",
      "executor_chain = (\n",
      "    executor_prompt \n",
      "    | llm.with_structured_output(ExecutionResult)\n",
      ")\n",
      "\n",
      "final_answer_chain = (\n",
      "    final_answer_prompt \n",
      "    | llm.with_structured_output(FinalAnswer)\n",
      ")\n",
      "\n",
      "# Define the graph nodes\n",
      "def create_plan(state: AgentState) -> AgentState:\n",
      "    \"\"\"Create a plan based on the user query.\"\"\"\n",
      "    plan = planner_chain.invoke({\"query\": state[\"query\"]})\n",
      "    return {\"query\": state[\"query\"], \"plan\": plan, \"current_step_index\": 0, \"execution_results\": []}\n",
      "\n",
      "def execute_step(state: AgentState) -> AgentState:\n",
      "    \"\"\"Execute the current step in the plan.\"\"\"\n",
      "    current_index = state[\"current_step_index\"]\n",
      "    current_step = state[\"plan\"].steps[current_index]\n",
      "    \n",
      "    # Format previous results for context\n",
      "    previous_results = \"\\n\".join([\n",
      "        f\"Step {i+1}: {res.output}\"\n"
     ]
    }
   ],
   "source": [
    "from langchain_anthropic import ChatAnthropic\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "model = ChatAnthropic(model='claude-3-7-sonnet-20250219')\n",
    "result = model.invoke(\"write langgraph code for plan-and-execute, use structured output functionality\")\n",
    "result.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7e367551",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Certainly! Below is an example of how you might write LangGraph code to implement a **Plan-and-Execute** pattern. This pattern typically involves two main steps:\n",
      "\n",
      "1. **Plan**: Generate a plan or a sequence of steps to solve a problem.\n",
      "2. **Execute**: Carry out the plan step-by-step, possibly with feedback or verification after each step.\n",
      "\n",
      "---\n",
      "\n",
      "### LangGraph Code for Plan-and-Execute\n",
      "\n",
      "```langgraph\n",
      "graph PlanAndExecute {\n",
      "  \n",
      "  node Input {\n",
      "    type: \"text\"\n",
      "    description: \"User input or problem statement\"\n",
      "  }\n",
      "\n",
      "  node Planner {\n",
      "    type: \"llm\"\n",
      "    description: \"Generate a plan based on the input\"\n",
      "    model: \"gpt-4\"\n",
      "    prompt: \"\"\"\n",
      "    You are a planner. Given the following problem, generate a step-by-step plan to solve it.\n",
      "\n",
      "    Problem:\n",
      "    {Input}\n",
      "\n",
      "    Plan:\n",
      "    1.\n",
      "    \"\"\"\n",
      "  }\n",
      "\n",
      "  node Executor {\n",
      "    type: \"llm\"\n",
      "    description: \"Execute each step of the plan\"\n",
      "    model: \"gpt-4\"\n",
      "    prompt: \"\"\"\n",
      "    You are an executor. Given the following step from a plan, perform the step and provide the result.\n",
      "\n",
      "    Step:\n",
      "    {Planner.step}\n",
      "\n",
      "    Result:\n",
      "    \"\"\"\n",
      "  }\n",
      "\n",
      "  node StepSplitter {\n",
      "    type: \"function\"\n",
      "    description: \"Split the plan into individual steps\"\n",
      "    function: (planText) => {\n",
      "      // Simple split by lines starting with numbers\n",
      "      return planText.split(/\\n\\d+\\.\\s/).filter(s => s.trim().length > 0);\n",
      "    }\n",
      "  }\n",
      "\n",
      "  node Loop {\n",
      "    type: \"loop\"\n",
      "    description: \"Iterate over each step in the plan\"\n",
      "    input: StepSplitter.output\n",
      "    body: Executor\n",
      "  }\n",
      "\n",
      "  edge Input -> Planner\n",
      "  edge Planner.output -> StepSplitter.input\n",
      "  edge StepSplitter.output -> Loop.input\n",
      "  edge Loop.output -> Output\n",
      "\n",
      "  node Output {\n",
      "    type: \"collector\"\n",
      "    description: \"Collect results from executing all steps\"\n",
      "  }\n",
      "}\n",
      "```\n",
      "\n",
      "---\n",
      "\n",
      "### Explanation:\n",
      "\n",
      "- **Input**: The initial problem or user query.\n",
      "- **Planner**: Uses an LLM to generate a step-by-step plan.\n",
      "- **StepSplitter**: A function node that splits the plan text into individual steps.\n",
      "- **Loop**: Iterates over each step, feeding it into the **Executor** node.\n",
      "- **Executor**: Executes each step and returns the result.\n",
      "- **Output**: Collects all execution results.\n",
      "\n",
      "---\n",
      "\n",
      "If you want me to tailor this to a specific use case or add more details (like error handling, verification, or multi-turn interaction), just let me know!\n"
     ]
    }
   ],
   "source": [
    "result.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01384714",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
