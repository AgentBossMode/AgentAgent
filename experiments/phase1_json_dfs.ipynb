{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "62b92789",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_objects = {\n",
    "  \"nodes\": {\n",
    "    \"__START__\": {\n",
    "      \"schema_info\": \"\",\n",
    "      \"input_schema\": \"\",\n",
    "      \"output_schema\": \"\",\n",
    "      \"description\": \"Entry point of the graph.\",\n",
    "      \"function_name\": \"\"\n",
    "    },\n",
    "    \"planner\": {\n",
    "      \"schema_info\": \"PlanExecute: TypedDict with fields input (str), plan (List[str]), past_steps (Annotated[List[Tuple], operator.add]), response (str)\",\n",
    "      \"input_schema\": \"PlanExecute\",\n",
    "      \"output_schema\": \"PlanExecute\",\n",
    "      \"description\": \"Plan step generates a plan based on the input using llm structured output functionality, stores it to the plan field\",\n",
    "      \"function_name\": \"plan_step\"\n",
    "    },\n",
    "    \"agent\": {\n",
    "      \"schema_info\": \"PlanExecute: TypedDict with fields input (str), plan (List[str]), past_steps (Annotated[List[Tuple], operator.add]), response (str)\",\n",
    "      \"input_schema\": \"PlanExecute\",\n",
    "      \"output_schema\": \"PlanExecute\",\n",
    "      \"description\": \"Uses llm with tool binding for the stock related queries\",\n",
    "      \"function_name\": \"execute_step\"\n",
    "    },\n",
    "    \"replan\": {\n",
    "      \"schema_info\": \"PlanExecute: TypedDict with fields input (str), plan (List[str]), past_steps (Annotated[List[Tuple], operator.add]), response (str)\",\n",
    "      \"input_schema\": \"PlanExecute\",\n",
    "      \"output_schema\": \"Union[Response, Plan]\",\n",
    "      \"description\": \"Evaluates progress and uses an LLM to either revise the plan or generate a final response.\",\n",
    "      \"function_name\": \"replan_step\"\n",
    "    },\n",
    "    \"__END__\": {\n",
    "      \"schema_info\": \"\",\n",
    "      \"input_schema\": \"\",\n",
    "      \"output_schema\": \"\",\n",
    "      \"description\": \"End point of the graph.\",\n",
    "      \"function_name\": \"\"\n",
    "    }\n",
    "  },\n",
    "  \"edges\": {\n",
    "    \"edge_1\": {\n",
    "      \"source\": \"__START__\",\n",
    "      \"target\": \"planner\",\n",
    "      \"routing_conditions\": \"Start the planning process.\",\n",
    "      \"conditional\": False\n",
    "    },\n",
    "    \"edge_2\": {\n",
    "      \"source\": \"planner\",\n",
    "      \"target\": \"agent\",\n",
    "      \"routing_conditions\": \"After planning, execute the first step.\",\n",
    "      \"conditional\": False\n",
    "    },\n",
    "    \"edge_3\": {\n",
    "      \"source\": \"agent\",\n",
    "      \"target\": \"replan\",\n",
    "      \"routing_conditions\": \"After executing a step, check if replanning is needed.\",\n",
    "      \"conditional\": False\n",
    "    },\n",
    "    \"edge_4\": {\n",
    "      \"source\": \"replan\",\n",
    "      \"target\": \"agent\",\n",
    "      \"routing_conditions\": \"If no response is generated, continue to agent for further execution.\",\n",
    "      \"conditional\": True\n",
    "    },\n",
    "    \"edge_5\": {\n",
    "      \"source\": \"replan\",\n",
    "      \"target\": \"__END__\",\n",
    "      \"routing_conditions\": \"If a response is generated, end the process.\",\n",
    "      \"conditional\": True\n",
    "    }\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb05768b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kanishkgupta/Documents/GitHub/AgentAgent/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import import_ipynb\n",
    "from phase1_edge_handling import edge_builder_agent\n",
    "from phase1_node_to_code import node_to_code_app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd16f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "skipList = [\"__START__\", \"__END__\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cff28bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "from typing import Annotated, List\n",
    "from langgraph.graph import StateGraph, START, END, MessagesState\n",
    "from langgraph.types import Send\n",
    "from pydantic import BaseModel\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "import uuid\n",
    "\n",
    "\n",
    "class NodeEvaluationReport(BaseModel):\n",
    "    node_name: str\n",
    "    node_code_stub: str\n",
    "    edge_code: str\n",
    "\n",
    "class GraphCompilerState(MessagesState):\n",
    "    json_objects: dict\n",
    "    node_reports: Annotated[List[NodeEvaluationReport], operator.add]\n",
    "\n",
    "\n",
    "class NodeProcessState():\n",
    "    node_name: str\n",
    "    node_info: dict\n",
    "    edge_info: dict\n",
    "\n",
    "def graph_map_step(state: GraphCompilerState):\n",
    "      # Get edges originating from the current node\n",
    "    json_objects = state[\"json_objects\"]\n",
    "    edges = json_objects[\"edges\"]\n",
    "    nodes = json_objects[\"nodes\"]\n",
    "    sends = []\n",
    "    for node_key, node_val in nodes.items():\n",
    "        outgoing_edges = list(filter(lambda edge: edge[\"source\"] == node_key, edges.values()))\n",
    "        sends.append(Send(\"node_process\", {\"node_name\": node_key, \"node_info\": node_val, \"edge_info\": outgoing_edges}))\n",
    "    return sends\n",
    "\n",
    "edge_info_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "<GraphNodeImplementation>\n",
    "{node_code}\n",
    "</GraphNodeImplementation>\n",
    "<EdgeInformation>\n",
    "{edge_json}\n",
    "</Edgeinformation>\"\"\")\n",
    "\n",
    "def node_process(state: NodeProcessState):\n",
    "    uuid_str = uuid.uuid4()\n",
    "    config = {\"configurable\": {\"thread_id\": str(uuid_str)}}\n",
    "    if state[\"node_name\"] not in skipList:\n",
    "        for output in node_to_code_app.stream(state[\"node_info\"], config, stream_mode=\"updates\"):\n",
    "            print(output)\n",
    "        code= node_to_code_app.get_state(config).values[\"final_code\"]\n",
    "    else:\n",
    "        code = \"no implementation needed\"\n",
    "    edge_code = edge_builder_agent.invoke({\"messages\": [HumanMessage(content = edge_info_prompt.format(node_code=code,edge_json=state[\"edge_info\"]))]},config)\n",
    "    return {\n",
    "        \"node_reports\" : [NodeEvaluationReport(node_name=state[\"node_name\"], node_code_stub=code, edge_code=edge_code[\"messages\"][-1].content)] \n",
    "    }\n",
    "\n",
    "code_compiler_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "You are responsible for merging and compiling the code  for the given graph\n",
    "<Graph>\n",
    "{graphEdgeDict}\n",
    "</Graph>\n",
    "\n",
    "<GraphNodeImplementation>\n",
    "{graphImplementations}\n",
    "</GraphNodeImplementation>\n",
    "\n",
    "<EdgeImplementation>\n",
    "{edgeImplementations}\n",
    "</EdgeImplementation>\n",
    "\"\"\")\n",
    "\n",
    "def graph_compile(state: GraphCompilerState):\n",
    "    node_evals : List[NodeEvaluationReport]= state[\"node_reports\"]\n",
    "\n",
    "    code_stubs = [node_eval.node_code_stub for node_eval in node_evals]\n",
    "    edge_stubs = [node_eval.edge_code for node_eval in node_evals]\n",
    "    json_objects = state[\"json_objects\"]\n",
    "    edges = json_objects[\"edges\"]\n",
    "    llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "    response = llm.invoke([SystemMessage(content= code_compiler_prompt.format(\n",
    "        graphEdgeDict=edges,\n",
    "        graphImplementations=code_stubs,\n",
    "        edgeImplementations=edge_stubs))])\n",
    "    return {\n",
    "        \"messages\": [response]\n",
    "    }\n",
    "\n",
    "\n",
    "workflow = StateGraph(GraphCompilerState)\n",
    "workflow.add_node(graph_map_step, \"graph_map\")\n",
    "workflow.add_node(node_process, \"node_process\")\n",
    "workflow.add_node(graph_compile,\"graph_compile\")\n",
    "\n",
    "workflow.add_conditional_edges(START, graph_map_step, [\"node_process\"])\n",
    "workflow.add_edge(\"node_process\", \"graph_compile\")\n",
    "workflow.add_edge(\"graph_compile\", END)\n",
    "\n",
    "compiler_graph = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e7d68840",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'identify_node': {'node_type': 'planner', 'messages': [HumanMessage(content='\\nYou are provided with the following information about the node:\\n<SchemaInfo>\\nPlanExecute: TypedDict with fields input (str), plan (List[str]), past_steps (Annotated[List[Tuple], operator.add]), response (str)\\n</SchemaInfo>\\n<InputSchema>\\nPlanExecute\\n</InputSchema>\\n<OutputSchema>\\nUnion[Response, Plan]\\n</OutputSchema>\\n<Description>\\nEvaluates progress and uses an LLM to either revise the plan or generate a final response.\\n</Description>\\n<FunctionName>\\nreplan_step\\n</FunctionName>\\n\\nBelow is the skeleton of the function that you need to implement:\\ndef replan_step(state:PlanExecute) -> Union[Response, Plan]:\\n    \"\"\"Evaluates progress and uses an LLM to either revise the plan or generate a final response.\"\"\"\\n    # Implement the function to meet the description.\\n\\nthe state is of type PlanExecute and the function is of type Union[Response, Plan]\\nThe general idea is that the implementation would involve extracting the input from the state, and updating the state with the output. Description contains the logic for this blackbox\\n', additional_kwargs={}, response_metadata={}, id='b9003184-e60f-4f9e-8c02-2897d9e37441')], 'node_info': '\\nYou are provided with the following information about the node:\\n<SchemaInfo>\\nPlanExecute: TypedDict with fields input (str), plan (List[str]), past_steps (Annotated[List[Tuple], operator.add]), response (str)\\n</SchemaInfo>\\n<InputSchema>\\nPlanExecute\\n</InputSchema>\\n<OutputSchema>\\nUnion[Response, Plan]\\n</OutputSchema>\\n<Description>\\nEvaluates progress and uses an LLM to either revise the plan or generate a final response.\\n</Description>\\n<FunctionName>\\nreplan_step\\n</FunctionName>\\n\\nBelow is the skeleton of the function that you need to implement:\\ndef replan_step(state:PlanExecute) -> Union[Response, Plan]:\\n    \"\"\"Evaluates progress and uses an LLM to either revise the plan or generate a final response.\"\"\"\\n    # Implement the function to meet the description.\\n\\nthe state is of type PlanExecute and the function is of type Union[Response, Plan]\\nThe general idea is that the implementation would involve extracting the input from the state, and updating the state with the output. Description contains the logic for this blackbox\\n'}}\n",
      "{'identify_node': {'node_type': 'planner', 'messages': [HumanMessage(content='\\nYou are provided with the following information about the node:\\n<SchemaInfo>\\nPlanExecute: TypedDict with fields input (str), plan (List[str]), past_steps (Annotated[List[Tuple], operator.add]), response (str)\\n</SchemaInfo>\\n<InputSchema>\\nPlanExecute\\n</InputSchema>\\n<OutputSchema>\\nPlanExecute\\n</OutputSchema>\\n<Description>\\nUses llm with tool binding for the stock related queries\\n</Description>\\n<FunctionName>\\nexecute_step\\n</FunctionName>\\n\\nBelow is the skeleton of the function that you need to implement:\\ndef execute_step(state:PlanExecute) -> PlanExecute:\\n    \"\"\"Uses llm with tool binding for the stock related queries\"\"\"\\n    # Implement the function to meet the description.\\n\\nthe state is of type PlanExecute and the function is of type PlanExecute\\nThe general idea is that the implementation would involve extracting the input from the state, and updating the state with the output. Description contains the logic for this blackbox\\n', additional_kwargs={}, response_metadata={}, id='8b2f2055-92eb-46b8-9c5e-c2df08b325d6')], 'node_info': '\\nYou are provided with the following information about the node:\\n<SchemaInfo>\\nPlanExecute: TypedDict with fields input (str), plan (List[str]), past_steps (Annotated[List[Tuple], operator.add]), response (str)\\n</SchemaInfo>\\n<InputSchema>\\nPlanExecute\\n</InputSchema>\\n<OutputSchema>\\nPlanExecute\\n</OutputSchema>\\n<Description>\\nUses llm with tool binding for the stock related queries\\n</Description>\\n<FunctionName>\\nexecute_step\\n</FunctionName>\\n\\nBelow is the skeleton of the function that you need to implement:\\ndef execute_step(state:PlanExecute) -> PlanExecute:\\n    \"\"\"Uses llm with tool binding for the stock related queries\"\"\"\\n    # Implement the function to meet the description.\\n\\nthe state is of type PlanExecute and the function is of type PlanExecute\\nThe general idea is that the implementation would involve extracting the input from the state, and updating the state with the output. Description contains the logic for this blackbox\\n'}}\n",
      "{'node_process': {'node_reports': [NodeEvaluationReport(node_name='__END__', node_code_stub='no implementation needed', edge_code=\"Based on the provided information, there are no edges to implement between the graph nodes. Therefore, there is no Python code necessary for this task. \\n\\nIf there are any further specifications or additional details about edges or nodes you'd like to include, please let me know!\")]}}\n",
      "{'identify_node': {'node_type': 'planner', 'messages': [HumanMessage(content='\\nYou are provided with the following information about the node:\\n<SchemaInfo>\\nPlanExecute: TypedDict with fields input (str), plan (List[str]), past_steps (Annotated[List[Tuple], operator.add]), response (str)\\n</SchemaInfo>\\n<InputSchema>\\nPlanExecute\\n</InputSchema>\\n<OutputSchema>\\nPlanExecute\\n</OutputSchema>\\n<Description>\\nPlan step generates a plan based on the input using llm structured output functionality, stores it to the plan field\\n</Description>\\n<FunctionName>\\nplan_step\\n</FunctionName>\\n\\nBelow is the skeleton of the function that you need to implement:\\ndef plan_step(state:PlanExecute) -> PlanExecute:\\n    \"\"\"Plan step generates a plan based on the input using llm structured output functionality, stores it to the plan field\"\"\"\\n    # Implement the function to meet the description.\\n\\nthe state is of type PlanExecute and the function is of type PlanExecute\\nThe general idea is that the implementation would involve extracting the input from the state, and updating the state with the output. Description contains the logic for this blackbox\\n', additional_kwargs={}, response_metadata={}, id='1767c4e2-d54a-48ae-9ad0-f1f4792dfa73')], 'node_info': '\\nYou are provided with the following information about the node:\\n<SchemaInfo>\\nPlanExecute: TypedDict with fields input (str), plan (List[str]), past_steps (Annotated[List[Tuple], operator.add]), response (str)\\n</SchemaInfo>\\n<InputSchema>\\nPlanExecute\\n</InputSchema>\\n<OutputSchema>\\nPlanExecute\\n</OutputSchema>\\n<Description>\\nPlan step generates a plan based on the input using llm structured output functionality, stores it to the plan field\\n</Description>\\n<FunctionName>\\nplan_step\\n</FunctionName>\\n\\nBelow is the skeleton of the function that you need to implement:\\ndef plan_step(state:PlanExecute) -> PlanExecute:\\n    \"\"\"Plan step generates a plan based on the input using llm structured output functionality, stores it to the plan field\"\"\"\\n    # Implement the function to meet the description.\\n\\nthe state is of type PlanExecute and the function is of type PlanExecute\\nThe general idea is that the implementation would involve extracting the input from the state, and updating the state with the output. Description contains the logic for this blackbox\\n'}}\n",
      "{'planner': {'plan': ['Generate a prompt based on the extracted input to handle stock-related queries.', 'Identify and bind the appropriate tools that can be used to answer the stock-related query based on the generated prompt.']}}\n",
      "{'planner': {'plan': [\"Generate a prompt using the extracted 'input' to evaluate the current progress and determine if the plan needs revision or if a final response can be generated.\", 'Use the LLM to process the generated prompt and obtain either a revised plan or a final response.']}}\n",
      "{'planner': {'plan': ['Generate a prompt based on the extracted input.', 'Use structured output functionality to create a structured plan based on the generated prompt.']}}\n",
      "{'ai_node_gen_supervisor': {'next': 'prompt_generation', 'task': 'Generate a prompt based on the extracted input to handle stock-related queries.', 'messages': [AIMessage(content='The next step is to generate a prompt based on the extracted input to handle stock-related queries, which aligns with the task outlined in step 1.', additional_kwargs={}, response_metadata={}, id='0fb693bd-9998-4d46-9749-9db8977fff98')]}}\n",
      "{'ai_node_gen_supervisor': {'next': 'prompt_generation', 'task': \"Generate a prompt using the extracted 'input' to evaluate the current progress and determine if the plan needs revision or if a final response can be generated.\", 'messages': [AIMessage(content='To proceed with step 1, we need to generate a prompt based on the extracted input from the provided state. This prompt will assess the current progress of the plan and help determine whether a revision is needed.', additional_kwargs={}, response_metadata={}, id='956ba125-a129-40ed-8296-30504ef9be84')]}}\n",
      "{'ai_node_gen_supervisor': {'next': 'prompt_generation', 'task': 'Generate a prompt based on the extracted input.', 'messages': [AIMessage(content=\"The next worker needed is 'prompt_generation' to generate a prompt based on the extracted input, which is the first step of the plan. This will lead us to the next stage of creating a structured plan.\", additional_kwargs={}, response_metadata={}, id='b3f0be96-0ed3-4b31-af4e-e2fc58341ce4')]}}\n",
      "{'prompt_generation': {'past_steps': [('Generate a prompt based on the extracted input to handle stock-related queries.', 'To effectively generate a response using LLM for stock-related queries, the prompt should clearly instruct the model to process the input, extract relevant information related to stock queries, and provide a structured output. Here is a customized prompt:\\n\\n\"Using the stock-related query provided, analyze the input and perform the following steps: \\n1. Identify the primary aspects of the stock-related question asked.\\n2. Retrieve current stock data or any related information that pertains to the user\\'s query.\\n3. Summarize the findings with relevant details—like current stock prices, trends, and news relevant to the query.\\n4. Provide additional insights or recommendations based on the analysis.\\n\\nMake sure to structure the response clearly, highlighting key information for easy understanding.\"')], 'messages': [HumanMessage(content='To effectively generate a response using LLM for stock-related queries, the prompt should clearly instruct the model to process the input, extract relevant information related to stock queries, and provide a structured output. Here is a customized prompt:\\n\\n\"Using the stock-related query provided, analyze the input and perform the following steps: \\n1. Identify the primary aspects of the stock-related question asked.\\n2. Retrieve current stock data or any related information that pertains to the user\\'s query.\\n3. Summarize the findings with relevant details—like current stock prices, trends, and news relevant to the query.\\n4. Provide additional insights or recommendations based on the analysis.\\n\\nMake sure to structure the response clearly, highlighting key information for easy understanding.\"', additional_kwargs={}, response_metadata={}, name='prompt_generator', id='a109cbe5-6e0d-46d3-a2f9-9b2ccebf1add')]}}\n",
      "{'node_process': {'node_reports': [NodeEvaluationReport(node_name='__START__', node_code_stub='no implementation needed', edge_code='Here\\'s the Python code that implements the requested non-conditional edge in a LangGraph:\\n\\n```python\\nfrom langgraph.graph import StateGraph, START\\n\\n# Create an instance of StateGraph\\ngraph = StateGraph(dict)\\n\\n# Add nodes (the planner node is mentioned as target in the edge)\\ndef planner(state):\\n    # Implementation of planning logic\\n    return state  # This function would typically modify the state based on planning.\\n\\n# Adding the \\'planner\\' node to the graph\\ngraph.add_node(\"planner\", planner)\\n\\n# Add a non-conditional edge from the starting point to the \\'planner\\' node\\n# This edge directs the flow from the start to the planner.\\ngraph.add_edge(START, \"planner\") \\n\\n# Compile the graph to make it callable\\ncompiled_graph = graph.compile()\\n\\n# You can invoke the graph with initial state if necessary\\n# result = compiled_graph.invoke(initial_state)\\n# print(result)  # Print the result of the invocation if needed\\n```\\n\\n### Explanation:\\n- **StateGraph**: We initialize a `StateGraph`.\\n- **Node Definition**: A dummy `planner` function is defined where the actual planning logic would go. It currently returns the state unchanged.\\n- **Add Node**: We add that function to the graph as a node named \"planner\".\\n- **Add Edge**: The non-conditional edge is created to direct the flow from `START` to the `planner`.\\n- **Compile**: The graph is compiled to make it executable. This allows the graph to be invoked with a state.\\n\\nThis implementation fulfills the requirement of creating a non-conditional edge that leads to the `planner` node.')]}}\n",
      "{'prompt_generation': {'past_steps': [(\"Generate a prompt using the extracted 'input' to evaluate the current progress and determine if the plan needs revision or if a final response can be generated.\", 'Based on the information gathered on LLMs and insights related to revising plans or generating final responses, here’s a prompt that can effectively engage the LLM to evaluate progress and provide necessary revisions or a final response:\\n\\n---\\n\\n\"Evaluate the current progress of the following plan: {input}. Based on the steps completed so far, identify any areas where the plan may need revision or adjustment. If the plan appears to be progressing well, summarize the findings and generate a final response. Please take into account the steps taken and any obstacles encountered along the way to provide a comprehensive evaluation.\" \\n\\n--- \\n\\nThis prompt instructs the LLM to assess the plan using the provided input while considering performance and obstacles, which aligns with the task objectives.')], 'messages': [HumanMessage(content='Based on the information gathered on LLMs and insights related to revising plans or generating final responses, here’s a prompt that can effectively engage the LLM to evaluate progress and provide necessary revisions or a final response:\\n\\n---\\n\\n\"Evaluate the current progress of the following plan: {input}. Based on the steps completed so far, identify any areas where the plan may need revision or adjustment. If the plan appears to be progressing well, summarize the findings and generate a final response. Please take into account the steps taken and any obstacles encountered along the way to provide a comprehensive evaluation.\" \\n\\n--- \\n\\nThis prompt instructs the LLM to assess the plan using the provided input while considering performance and obstacles, which aligns with the task objectives.', additional_kwargs={}, response_metadata={}, name='prompt_generator', id='badec3fb-5146-4367-ad46-d62e407c276d')]}}\n",
      "{'prompt_generation': {'past_steps': [('Generate a prompt based on the extracted input.', 'Based on the information gathered, here is a prompt designed to instruct a large language model (LLM) to generate a structured plan based on a given input statement:\\n\\n---\\n\\n\"Given the input statement below, generate a detailed and structured plan to address the task described. The plan should include clear steps, necessary resources, and potential challenges for each step.\\n\\nInput Statement: {input_statement}\\n\\nPlease format the response in a list format, where each entry contains:\\n1. Step Description\\n2. Resources Needed\\n3. Potential Challenges\" \\n\\n--- \\n\\nThis prompt is structured to elicit a comprehensive response that meets the requirements of the plan execution objective.')], 'messages': [HumanMessage(content='Based on the information gathered, here is a prompt designed to instruct a large language model (LLM) to generate a structured plan based on a given input statement:\\n\\n---\\n\\n\"Given the input statement below, generate a detailed and structured plan to address the task described. The plan should include clear steps, necessary resources, and potential challenges for each step.\\n\\nInput Statement: {input_statement}\\n\\nPlease format the response in a list format, where each entry contains:\\n1. Step Description\\n2. Resources Needed\\n3. Potential Challenges\" \\n\\n--- \\n\\nThis prompt is structured to elicit a comprehensive response that meets the requirements of the plan execution objective.', additional_kwargs={}, response_metadata={}, name='prompt_generator', id='5a52654c-8685-4c49-a702-8c0f63c9b994')]}}\n",
      "{'replan': {'plan': ['Identify and bind the appropriate tools that can be used to answer the stock-related query based on the generated prompt.']}}\n",
      "{'replan': {'plan': ['Use structured output functionality to create a structured plan based on the generated prompt.']}}\n",
      "{'ai_node_gen_supervisor': {'next': 'toolset_generation', 'task': 'Identify and bind the appropriate tools that can be used to answer the stock-related query based on the generated prompt.', 'messages': [AIMessage(content=\"To identify and bind the appropriate tools that are required to answer the stock-related query based on the generated prompt, the 'toolset_generation' worker is the most suitable choice.\", additional_kwargs={}, response_metadata={}, id='1efc6483-ba82-4a67-92d0-21d4b903e506')]}}\n",
      "{'replan': {'plan': ['Use the LLM to process the generated prompt and obtain either a revised plan or a final response.']}}\n",
      "{'ai_node_gen_supervisor': {'next': 'structured_output_generation', 'task': 'Use structured output functionality to create a structured plan based on the generated prompt.', 'messages': [AIMessage(content='The next step involves creating a structured plan based on the generated prompt. The structured_output_generation worker is required to carry out this task effectively.', additional_kwargs={}, response_metadata={}, id='ec60534e-79f9-45d5-b5db-3eba6c9b9f9f')]}}\n",
      "{'ai_node_gen_supervisor': {'next': 'prompt_generation', 'task': 'Use the LLM to process the generated prompt and obtain either a revised plan or a final response.', 'messages': [AIMessage(content=\"The next logical step is to utilize the 'prompt_generation' worker to produce a prompt for the LLM based on the current state. This will allow us to obtain either a revised plan or a final response as described in the provided function.\", additional_kwargs={}, response_metadata={}, id='618e9e77-0cd8-4e5d-9969-f5b2aefbc112')]}}\n",
      "{'prompt_generation': {'past_steps': [('Use the LLM to process the generated prompt and obtain either a revised plan or a final response.', '\"Evaluate the current progress of the following plan: {input}. Based on the steps completed so far, identify any areas where the plan may need revision or adjustment. If the plan appears to be progressing well, summarize the findings and generate a final response. Please take into account the steps taken and any obstacles encountered along the way to provide a comprehensive evaluation.\"')], 'messages': [HumanMessage(content='\"Evaluate the current progress of the following plan: {input}. Based on the steps completed so far, identify any areas where the plan may need revision or adjustment. If the plan appears to be progressing well, summarize the findings and generate a final response. Please take into account the steps taken and any obstacles encountered along the way to provide a comprehensive evaluation.\"', additional_kwargs={}, response_metadata={}, name='prompt_generator', id='3f1baa03-ac59-4f99-a92a-5b261a973efe')]}}\n",
      "{'replan': {'response': 'All steps in the original plan have been successfully executed. The prompt was generated and processed by the LLM, leading to the evaluation of the current progress.'}}\n",
      "{'structured_output_generation': {'past_steps': [('Use structured output functionality to create a structured plan based on the generated prompt.', 'Given the provided information about the node, it is appropriate to implement LLM\\'s structured output functionality. The generated plan needs to be formatted according to a specific structured schema to ensure consistency and reliability.\\n\\nHere is the implementation of the `plan_step` function using the structured output:\\n\\n```python\\nfrom typing import List, Tuple\\nfrom pydantic import BaseModel, Field\\nfrom typing_extensions import TypedDict\\n\\n# Pydantic class for structured output\\nclass PlanStepOutput(BaseModel):\\n    step: str = Field(description=\"Description of the step in the plan\")\\n    resources: List[str] = Field(description=\"List of resources needed for the step\")\\n    challenges: List[str] = Field(description=\"List of potential challenges for the step\")\\n\\nclass PlanExecute(TypedDict):\\n    input: str\\n    plan: List[PlanStepOutput]\\n    past_steps: List[Tuple]\\n    response: str\\n\\ndef plan_step(state: PlanExecute) -> PlanExecute:\\n    \"\"\"Plan step generates a plan based on the input using llm structured output functionality, stores it to the plan field\"\"\"\\n    # Extracting input from the state\\n    input_statement = state[\\'input\\']\\n    \\n    # Generating the structured output using the LLM\\n    structured_llm = llm.with_structured_output(PlanStepOutput)\\n    plan_steps: List[PlanStepOutput] = structured_llm.invoke(f\"Given the input statement below, generate a detailed and structured plan to address the task described.\\\\n\\\\nInput Statement: {input_statement}\\\\n\\\\nPlease format the response in a list format, where each entry contains:\\\\n1. Step Description\\\\n2. Resources Needed\\\\n3. Potential Challenges\")\\n    \\n    # Updating the plan field in the state\\n    state[\\'plan\\'] = plan_steps\\n    \\n    return state\\n```\\n\\nIn this code:\\n- The `PlanStepOutput` class is defined to specify the structure required for each step in the generated plan.\\n- The `plan_step` function extracts the input from the `state`, invokes the LLM using structured output, and formats the response as per the requirements.\\n- Finally, it updates the `plan` field with the generated structured output.')], 'messages': [HumanMessage(content='Given the provided information about the node, it is appropriate to implement LLM\\'s structured output functionality. The generated plan needs to be formatted according to a specific structured schema to ensure consistency and reliability.\\n\\nHere is the implementation of the `plan_step` function using the structured output:\\n\\n```python\\nfrom typing import List, Tuple\\nfrom pydantic import BaseModel, Field\\nfrom typing_extensions import TypedDict\\n\\n# Pydantic class for structured output\\nclass PlanStepOutput(BaseModel):\\n    step: str = Field(description=\"Description of the step in the plan\")\\n    resources: List[str] = Field(description=\"List of resources needed for the step\")\\n    challenges: List[str] = Field(description=\"List of potential challenges for the step\")\\n\\nclass PlanExecute(TypedDict):\\n    input: str\\n    plan: List[PlanStepOutput]\\n    past_steps: List[Tuple]\\n    response: str\\n\\ndef plan_step(state: PlanExecute) -> PlanExecute:\\n    \"\"\"Plan step generates a plan based on the input using llm structured output functionality, stores it to the plan field\"\"\"\\n    # Extracting input from the state\\n    input_statement = state[\\'input\\']\\n    \\n    # Generating the structured output using the LLM\\n    structured_llm = llm.with_structured_output(PlanStepOutput)\\n    plan_steps: List[PlanStepOutput] = structured_llm.invoke(f\"Given the input statement below, generate a detailed and structured plan to address the task described.\\\\n\\\\nInput Statement: {input_statement}\\\\n\\\\nPlease format the response in a list format, where each entry contains:\\\\n1. Step Description\\\\n2. Resources Needed\\\\n3. Potential Challenges\")\\n    \\n    # Updating the plan field in the state\\n    state[\\'plan\\'] = plan_steps\\n    \\n    return state\\n```\\n\\nIn this code:\\n- The `PlanStepOutput` class is defined to specify the structure required for each step in the generated plan.\\n- The `plan_step` function extracts the input from the `state`, invokes the LLM using structured output, and formats the response as per the requirements.\\n- Finally, it updates the `plan` field with the generated structured output.', additional_kwargs={}, response_metadata={}, name='struct_output_generator', id='ffcd9d63-d0d9-4941-93d0-40793ddfb0c6')]}}\n",
      "{'toolset_generation': {'past_steps': [('Identify and bind the appropriate tools that can be used to answer the stock-related query based on the generated prompt.', '```python\\nfrom typing import List, Tuple\\nfrom langgraph.graph import StateGraph, MessagesState, START, END\\nfrom langchain_core.messages import SystemMessage\\nfrom langchain_core.tools import tool\\nfrom langgraph.prebuilt import ToolNode\\n\\n# Define the tools needed by the LLM for stock-related queries\\n\\n@tool\\ndef get_stock_data(symbol: str):\\n    \"\"\"Retrieve current stock data for a given symbol.\"\"\"\\n    # Placeholder logic to simulate stock data retrieval\\n    return f\"The current price of {symbol} is $150.\"\\n\\n@tool\\ndef get_stock_trends(symbol: str):\\n    \"\"\"Retrieve stock trends for a given symbol.\"\"\"\\n    # Placeholder logic to simulate stock trends retrieval\\n    return f\"The stock of {symbol} has increased by 5% over the last month.\"\\n\\ntools = [get_stock_data, get_stock_trends]\\n\\n# Bind the model (llm) with tools\\nmodel_with_tools = ChatAnthropic(\\n    model=\"claude-3-haiku-20240307\", temperature=0\\n).bind_tools(tools)\\n\\n# Generate a tool node.\\ntool_node = ToolNode(tools)\\n\\n# Define the function to execute the stock step\\ndef execute_step(state: PlanExecute) -> PlanExecute:\\n    \"\"\"Uses llm with tool binding for the stock related queries\"\"\"\\n    input_query = state[\\'input\\']\\n    \\n    # Prepare the prompt for the model\\n    prompt = f\"Using the stock-related query provided, analyze the input and perform the following steps:\\\\\\n    1. Identify the primary aspects of the stock-related question asked.\\\\\\n    2. Retrieve current stock data or any related information that pertains to the user\\'s query.\\\\\\n    3. Summarize the findings with relevant details—like current stock prices, trends, and news relevant to the query.\\\\\\n    4. Provide additional insights or recommendations based on the analysis.\\\\\\n    Input: {input_query}\"\\n\\n    # Pass the prompt to the model and get the response\\n    response = model_with_tools.invoke([SystemMessage(content=prompt)])\\n    \\n    # Update the state with the response\\n    state[\\'response\\'] = response.content\\n    return state\\n\\n# Stategraph compilation\\nworkflow = StateGraph(MessagesState)\\n\\n# Define the two nodes we will cycle between\\nworkflow.add_node(\"agent\", execute_step)\\nworkflow.add_node(\"tools\", tool_node)\\n\\n# Conditional edge\\ndef should_continue(state: MessagesState):\\n    messages = state[\"messages\"]\\n    last_message = messages[-1]\\n    if last_message.tool_calls:\\n        return \"tools\"\\n    return END\\n\\nworkflow.add_edge(START, \"agent\")\\nworkflow.add_conditional_edges(\"agent\", should_continue, [\"tools\", END])\\nworkflow.add_edge(\"tools\", \"agent\")\\napp = workflow.compile()\\n```')], 'messages': [HumanMessage(content='```python\\nfrom typing import List, Tuple\\nfrom langgraph.graph import StateGraph, MessagesState, START, END\\nfrom langchain_core.messages import SystemMessage\\nfrom langchain_core.tools import tool\\nfrom langgraph.prebuilt import ToolNode\\n\\n# Define the tools needed by the LLM for stock-related queries\\n\\n@tool\\ndef get_stock_data(symbol: str):\\n    \"\"\"Retrieve current stock data for a given symbol.\"\"\"\\n    # Placeholder logic to simulate stock data retrieval\\n    return f\"The current price of {symbol} is $150.\"\\n\\n@tool\\ndef get_stock_trends(symbol: str):\\n    \"\"\"Retrieve stock trends for a given symbol.\"\"\"\\n    # Placeholder logic to simulate stock trends retrieval\\n    return f\"The stock of {symbol} has increased by 5% over the last month.\"\\n\\ntools = [get_stock_data, get_stock_trends]\\n\\n# Bind the model (llm) with tools\\nmodel_with_tools = ChatAnthropic(\\n    model=\"claude-3-haiku-20240307\", temperature=0\\n).bind_tools(tools)\\n\\n# Generate a tool node.\\ntool_node = ToolNode(tools)\\n\\n# Define the function to execute the stock step\\ndef execute_step(state: PlanExecute) -> PlanExecute:\\n    \"\"\"Uses llm with tool binding for the stock related queries\"\"\"\\n    input_query = state[\\'input\\']\\n    \\n    # Prepare the prompt for the model\\n    prompt = f\"Using the stock-related query provided, analyze the input and perform the following steps:\\\\\\n    1. Identify the primary aspects of the stock-related question asked.\\\\\\n    2. Retrieve current stock data or any related information that pertains to the user\\'s query.\\\\\\n    3. Summarize the findings with relevant details—like current stock prices, trends, and news relevant to the query.\\\\\\n    4. Provide additional insights or recommendations based on the analysis.\\\\\\n    Input: {input_query}\"\\n\\n    # Pass the prompt to the model and get the response\\n    response = model_with_tools.invoke([SystemMessage(content=prompt)])\\n    \\n    # Update the state with the response\\n    state[\\'response\\'] = response.content\\n    return state\\n\\n# Stategraph compilation\\nworkflow = StateGraph(MessagesState)\\n\\n# Define the two nodes we will cycle between\\nworkflow.add_node(\"agent\", execute_step)\\nworkflow.add_node(\"tools\", tool_node)\\n\\n# Conditional edge\\ndef should_continue(state: MessagesState):\\n    messages = state[\"messages\"]\\n    last_message = messages[-1]\\n    if last_message.tool_calls:\\n        return \"tools\"\\n    return END\\n\\nworkflow.add_edge(START, \"agent\")\\nworkflow.add_conditional_edges(\"agent\", should_continue, [\"tools\", END])\\nworkflow.add_edge(\"tools\", \"agent\")\\napp = workflow.compile()\\n```', additional_kwargs={}, response_metadata={}, name='toolset_generator', id='1b6352cd-7b86-4fdd-993c-452913d8d2aa')]}}\n",
      "{'replan': {'response': 'All steps in the OriginalPlan have been successfully executed.'}}\n",
      "{'replan': {'response': 'All steps in the OriginalPlan have been successfully executed.'}}\n",
      "{'code_compiler': {'final_code': '```python\\nfrom typing import List, Tuple\\nfrom pydantic import BaseModel, Field\\nfrom typing_extensions import TypedDict\\n\\n# Pydantic class for structured output\\nclass PlanStepOutput(BaseModel):\\n    step: str = Field(description=\"Description of the step in the plan\")\\n    resources: List[str] = Field(description=\"List of resources needed for the step\")\\n    challenges: List[str] = Field(description=\"List of potential challenges for the step\")\\n\\nclass PlanExecute(TypedDict):\\n    input: str\\n    plan: List[PlanStepOutput]\\n    past_steps: List[Tuple]\\n    response: str\\n\\ndef plan_step(state: PlanExecute) -> PlanExecute:\\n    \"\"\"Plan step generates a plan based on the input using llm structured output functionality, stores it to the plan field\"\"\"\\n    # Extracting input from the state\\n    input_statement = state[\\'input\\']\\n    \\n    # Generating the structured output using the LLM\\n    structured_llm = llm.with_structured_output(PlanStepOutput)\\n    plan_steps: List[PlanStepOutput] = structured_llm.invoke(\\n        f\"Given the input statement below, generate a detailed and structured plan to address the task described.\\\\n\\\\nInput Statement: {input_statement}\\\\n\\\\nPlease format the response in a list format, where each entry contains:\\\\n1. Step Description\\\\n2. Resources Needed\\\\n3. Potential Challenges\"\\n    )\\n    \\n    # Updating the plan field in the state\\n    state[\\'plan\\'] = plan_steps\\n    \\n    return state\\n```'}}\n",
      "{'code_compiler': {'final_code': '```python\\nfrom typing import TypedDict, List, Tuple, Union\\nimport operator\\n\\n# Define the TypedDict schema for PlanExecute\\nclass PlanExecute(TypedDict):\\n    input: str\\n    plan: List[str]\\n    past_steps: List[Tuple]\\n    response: str\\n\\n# Define alias for Response and Plan\\nResponse = str  # Placeholder for actual Response type\\nPlan = List[str]  # Assuming Plan is a list of strings\\n\\ndef replan_step(state: PlanExecute) -> Union[Response, Plan]:\\n    \"\"\"Evaluates progress and uses an LLM to either revise the plan or generate a final response.\"\"\"\\n    \\n    # Extract input from state\\n    input_text = state[\\'input\\']\\n    \\n    # Generate prompt for the LLM\\n    prompt = (\\n        f\"Evaluate the current progress of the following plan: {input_text}. \"\\n        \"Based on the steps completed so far, identify any areas where the plan may need revision or adjustment. \"\\n        \"If the plan appears to be progressing well, summarize the findings and generate a final response. \"\\n        \"Please take into account the steps taken and any obstacles encountered along the way to provide a comprehensive evaluation.\"\\n    )\\n    \\n    # Here we would call the LLM with the prompt and get the output\\n    # For demonstration purposes, let\\'s assume we have a function `call_llm(prompt: str) -> Union[Response, Plan]`\\n    # which interacts with the LLM and returns either a revised plan or final response.\\n    \\n    output = call_llm(prompt)  # This function needs to be defined or imported from an LLM library\\n\\n    # Update the state with the output if necessary\\n    state[\\'response\\'] = output\\n\\n    return output\\n```'}}\n",
      "{'node_process': {'node_reports': [NodeEvaluationReport(node_name='planner', node_code_stub='```python\\nfrom typing import List, Tuple\\nfrom pydantic import BaseModel, Field\\nfrom typing_extensions import TypedDict\\n\\n# Pydantic class for structured output\\nclass PlanStepOutput(BaseModel):\\n    step: str = Field(description=\"Description of the step in the plan\")\\n    resources: List[str] = Field(description=\"List of resources needed for the step\")\\n    challenges: List[str] = Field(description=\"List of potential challenges for the step\")\\n\\nclass PlanExecute(TypedDict):\\n    input: str\\n    plan: List[PlanStepOutput]\\n    past_steps: List[Tuple]\\n    response: str\\n\\ndef plan_step(state: PlanExecute) -> PlanExecute:\\n    \"\"\"Plan step generates a plan based on the input using llm structured output functionality, stores it to the plan field\"\"\"\\n    # Extracting input from the state\\n    input_statement = state[\\'input\\']\\n    \\n    # Generating the structured output using the LLM\\n    structured_llm = llm.with_structured_output(PlanStepOutput)\\n    plan_steps: List[PlanStepOutput] = structured_llm.invoke(\\n        f\"Given the input statement below, generate a detailed and structured plan to address the task described.\\\\n\\\\nInput Statement: {input_statement}\\\\n\\\\nPlease format the response in a list format, where each entry contains:\\\\n1. Step Description\\\\n2. Resources Needed\\\\n3. Potential Challenges\"\\n    )\\n    \\n    # Updating the plan field in the state\\n    state[\\'plan\\'] = plan_steps\\n    \\n    return state\\n```', edge_code='Based on the provided information, the edge type is a non-conditional edge which connects the `planner` node to the `agent` node. Here\\'s the Python code snippet that implements this edge using the `add_edge` method as described in the NonConditionalEdges section:\\n\\n```python\\n# Assuming \\'graph\\' is an instance of your graph object\\n\\n# Add a non-conditional edge directly from \\'planner\\' to \\'agent\\'\\ngraph.add_edge(\"planner\", \"agent\")  # This edge indicates that after the planner node processes its logic, it will route directly to the agent node.\\n```\\n\\nThis code snippet creates a straightforward routing from the `planner` node to the `agent` node. After the `planner` node executes its function, control will transfer immediately to the `agent` node following the completion of the `plan_step` function.')]}}\n",
      "{'code_compiler': {'final_code': '```python\\nfrom typing import List, Tuple, TypedDict\\nfrom langchain_core.messages import SystemMessage\\nfrom langchain_core.tools import tool\\nfrom langgraph.graph import StateGraph, MessagesState, START, END\\nfrom langgraph.prebuilt import ToolNode\\n\\n# Define the PlanExecute structure\\nclass PlanExecute(TypedDict):\\n    input: str\\n    plan: List[str]\\n    past_steps: List[Tuple]\\n    response: str\\n\\n# Define the tools needed by the LLM for stock-related queries\\n\\n@tool\\ndef get_stock_data(symbol: str) -> str:\\n    \"\"\"Retrieve current stock data for a given symbol.\"\"\"\\n    # Placeholder logic to simulate stock data retrieval\\n    return f\"The current price of {symbol} is $150.\"\\n\\n@tool\\ndef get_stock_trends(symbol: str) -> str:\\n    \"\"\"Retrieve stock trends for a given symbol.\"\"\"\\n    # Placeholder logic to simulate stock trends retrieval\\n    return f\"The stock of {symbol} has increased by 5% over the last month.\"\\n\\n# Binding tools to the model\\ntools = [get_stock_data, get_stock_trends]\\nmodel_with_tools = ChatAnthropic(model=\"claude-3-haiku-20240307\", temperature=0).bind_tools(tools)\\n\\n# Generate a tool node\\ntool_node = ToolNode(tools)\\n\\ndef execute_step(state: PlanExecute) -> PlanExecute:\\n    \"\"\"Uses llm with tool binding for the stock related queries.\"\"\"\\n    input_query = state[\\'input\\']\\n    \\n    # Prepare the prompt for the model\\n    prompt = f\"Using the stock-related query provided, analyze the input and perform the following steps:\\\\\\n    1. Identify the primary aspects of the stock-related question asked.\\\\\\n    2. Retrieve current stock data or any related information that pertains to the user\\'s query.\\\\\\n    3. Summarize the findings with relevant details—like current stock prices, trends, and news relevant to the query.\\\\\\n    4. Provide additional insights or recommendations based on the analysis.\\\\\\n    Input: {input_query}\"\\n\\n    # Pass the prompt to the model and get the response\\n    response = model_with_tools.invoke([SystemMessage(content=prompt)])\\n    \\n    # Update the state with the response\\n    state[\\'response\\'] = response.content\\n    return state\\n\\n# Stategraph compilation\\nworkflow = StateGraph(MessagesState)\\n\\n# Define the two nodes we will cycle between\\nworkflow.add_node(\"agent\", execute_step)\\nworkflow.add_node(\"tools\", tool_node)\\n\\n# Conditional edge based on whether tool calls were made\\ndef should_continue(state: MessagesState):\\n    messages = state[\"messages\"]\\n    last_message = messages[-1]\\n    if last_message.tool_calls:\\n        return \"tools\"\\n    return END\\n\\nworkflow.add_edge(START, \"agent\")\\nworkflow.add_conditional_edges(\"agent\", should_continue, [\"tools\", END])\\nworkflow.add_edge(\"tools\", \"agent\")\\napp = workflow.compile()\\n```'}}\n",
      "{'node_process': {'node_reports': [NodeEvaluationReport(node_name='replan', node_code_stub='```python\\nfrom typing import TypedDict, List, Tuple, Union\\nimport operator\\n\\n# Define the TypedDict schema for PlanExecute\\nclass PlanExecute(TypedDict):\\n    input: str\\n    plan: List[str]\\n    past_steps: List[Tuple]\\n    response: str\\n\\n# Define alias for Response and Plan\\nResponse = str  # Placeholder for actual Response type\\nPlan = List[str]  # Assuming Plan is a list of strings\\n\\ndef replan_step(state: PlanExecute) -> Union[Response, Plan]:\\n    \"\"\"Evaluates progress and uses an LLM to either revise the plan or generate a final response.\"\"\"\\n    \\n    # Extract input from state\\n    input_text = state[\\'input\\']\\n    \\n    # Generate prompt for the LLM\\n    prompt = (\\n        f\"Evaluate the current progress of the following plan: {input_text}. \"\\n        \"Based on the steps completed so far, identify any areas where the plan may need revision or adjustment. \"\\n        \"If the plan appears to be progressing well, summarize the findings and generate a final response. \"\\n        \"Please take into account the steps taken and any obstacles encountered along the way to provide a comprehensive evaluation.\"\\n    )\\n    \\n    # Here we would call the LLM with the prompt and get the output\\n    # For demonstration purposes, let\\'s assume we have a function `call_llm(prompt: str) -> Union[Response, Plan]`\\n    # which interacts with the LLM and returns either a revised plan or final response.\\n    \\n    output = call_llm(prompt)  # This function needs to be defined or imported from an LLM library\\n\\n    # Update the state with the output if necessary\\n    state[\\'response\\'] = output\\n\\n    return output\\n```', edge_code='Based on the given `GraphNodeImplementation` and the `EdgeInformation`, I will implement the conditional edges for the routing logic from the `replan` node to the `agent` and `__END__` nodes. Since both edges are conditional based on whether a response is generated or not, we will create a routing function to check the output of the `replan_step` function.\\n\\nHere\\'s how you can implement this in Python code:\\n\\n```python\\nfrom langgraph.graph import StateGraph\\nfrom typing import Literal\\nfrom langgraph.types import Command\\n\\n# Assume graph is already defined\\ngraph = StateGraph()\\n\\ndef routing_function(state) -> str:\\n    # Check if a response is generated in the state\\n    if state.get(\\'response\\'):\\n        return \"__END__\"  # Go to END if a response is generated\\n    else:\\n        return \"agent\"  # Continue to agent if no response is generated\\n\\n# Adding conditional edges based on the logic defined in the routing function\\ngraph.add_conditional_edges(\"replan\", routing_function)\\n```\\n\\n### Explanation:\\n1. **Routing Function**: \\n   - This function checks the state to see if there is a response.\\n   - If a response exists, it routes to `__END__`; otherwise, it routes to `agent`.\\n\\n2. **Adding Conditional Edges**:\\n   - The `add_conditional_edges` method is used to define the edges from the `replan` node to both the `agent` and `__END__` nodes based on the output of the `routing_function`.\\n\\nThis ensures that when the `replan` node is executed, it evaluates the response generated and decides the next node to execute. If a response is generated, it will proceed to terminate the process; if not, it will pass control to the agent for further execution.')]}}\n",
      "{'node_process': {'node_reports': [NodeEvaluationReport(node_name='agent', node_code_stub='```python\\nfrom typing import List, Tuple, TypedDict\\nfrom langchain_core.messages import SystemMessage\\nfrom langchain_core.tools import tool\\nfrom langgraph.graph import StateGraph, MessagesState, START, END\\nfrom langgraph.prebuilt import ToolNode\\n\\n# Define the PlanExecute structure\\nclass PlanExecute(TypedDict):\\n    input: str\\n    plan: List[str]\\n    past_steps: List[Tuple]\\n    response: str\\n\\n# Define the tools needed by the LLM for stock-related queries\\n\\n@tool\\ndef get_stock_data(symbol: str) -> str:\\n    \"\"\"Retrieve current stock data for a given symbol.\"\"\"\\n    # Placeholder logic to simulate stock data retrieval\\n    return f\"The current price of {symbol} is $150.\"\\n\\n@tool\\ndef get_stock_trends(symbol: str) -> str:\\n    \"\"\"Retrieve stock trends for a given symbol.\"\"\"\\n    # Placeholder logic to simulate stock trends retrieval\\n    return f\"The stock of {symbol} has increased by 5% over the last month.\"\\n\\n# Binding tools to the model\\ntools = [get_stock_data, get_stock_trends]\\nmodel_with_tools = ChatAnthropic(model=\"claude-3-haiku-20240307\", temperature=0).bind_tools(tools)\\n\\n# Generate a tool node\\ntool_node = ToolNode(tools)\\n\\ndef execute_step(state: PlanExecute) -> PlanExecute:\\n    \"\"\"Uses llm with tool binding for the stock related queries.\"\"\"\\n    input_query = state[\\'input\\']\\n    \\n    # Prepare the prompt for the model\\n    prompt = f\"Using the stock-related query provided, analyze the input and perform the following steps:\\\\\\n    1. Identify the primary aspects of the stock-related question asked.\\\\\\n    2. Retrieve current stock data or any related information that pertains to the user\\'s query.\\\\\\n    3. Summarize the findings with relevant details—like current stock prices, trends, and news relevant to the query.\\\\\\n    4. Provide additional insights or recommendations based on the analysis.\\\\\\n    Input: {input_query}\"\\n\\n    # Pass the prompt to the model and get the response\\n    response = model_with_tools.invoke([SystemMessage(content=prompt)])\\n    \\n    # Update the state with the response\\n    state[\\'response\\'] = response.content\\n    return state\\n\\n# Stategraph compilation\\nworkflow = StateGraph(MessagesState)\\n\\n# Define the two nodes we will cycle between\\nworkflow.add_node(\"agent\", execute_step)\\nworkflow.add_node(\"tools\", tool_node)\\n\\n# Conditional edge based on whether tool calls were made\\ndef should_continue(state: MessagesState):\\n    messages = state[\"messages\"]\\n    last_message = messages[-1]\\n    if last_message.tool_calls:\\n        return \"tools\"\\n    return END\\n\\nworkflow.add_edge(START, \"agent\")\\nworkflow.add_conditional_edges(\"agent\", should_continue, [\"tools\", END])\\nworkflow.add_edge(\"tools\", \"agent\")\\napp = workflow.compile()\\n```', edge_code='Based on the provided edge information and the existing graph node implementation, we can define a non-conditional edge that connects the \"agent\" node to the \"replan\" node. Here’s how you can implement this in Python:\\n\\n```python\\nfrom langgraph.graph import StateGraph, MessagesState, START, END\\n\\n# Assuming the existing workflow from your GraphNodeImplementation\\n# Define your stategraph as before\\nworkflow = StateGraph(MessagesState)\\n\\n# Define the agent node\\nworkflow.add_node(\"agent\", execute_step)\\nworkflow.add_node(\"tools\", tool_node)\\n\\n# Add the new non-conditional edge from \"agent\" to \"replan\"\\nworkflow.add_edge(\"agent\", \"replan\")  # This means after executing \"agent\", it will always route to \"replan\"\\n\\n# Define the conditional edges based on whether tool calls were made\\ndef should_continue(state: MessagesState):\\n    messages = state[\"messages\"]\\n    last_message = messages[-1]\\n    if last_message.tool_calls:\\n        return \"tools\"\\n    return END\\n\\n# Existing edges and conditional edges\\nworkflow.add_edge(START, \"agent\")\\nworkflow.add_conditional_edges(\"agent\", should_continue, [\"tools\", END])\\nworkflow.add_edge(\"tools\", \"agent\")\\n\\n# Compile the workflow\\napp = workflow.compile()\\n```\\n\\n### Explanation:\\n- The `workflow.add_edge(\"agent\", \"replan\")` creates a non-conditional edge, meaning after executing the \"agent\" node, the flow will always proceed to the \"replan\" node.\\n- The other parts of your implementation remain unchanged; thus, the workflow continues to facilitate conditional edges effectively based on the defined conditions.')]}}\n",
      "{'graph_compile': {'messages': [AIMessage(content='Based on the provided graph structure and the implementations for the nodes and edges, here is the complete code that merges and compiles the entire workflow:\\n\\n```python\\nfrom typing import List, Tuple, TypedDict, Union\\nfrom pydantic import BaseModel, Field\\nfrom langchain_core.messages import SystemMessage\\nfrom langchain_core.tools import tool\\nfrom langgraph.graph import StateGraph, MessagesState, START, END\\nfrom langgraph.prebuilt import ToolNode\\n\\n# Pydantic class for structured output\\nclass PlanStepOutput(BaseModel):\\n    step: str = Field(description=\"Description of the step in the plan\")\\n    resources: List[str] = Field(description=\"List of resources needed for the step\")\\n    challenges: List[str] = Field(description=\"List of potential challenges for the step\")\\n\\n# Define the PlanExecute structure\\nclass PlanExecute(TypedDict):\\n    input: str\\n    plan: List[PlanStepOutput]\\n    past_steps: List[Tuple]\\n    response: str\\n\\n@tool\\ndef get_stock_data(symbol: str) -> str:\\n    \"\"\"Retrieve current stock data for a given symbol.\"\"\"\\n    return f\"The current price of {symbol} is $150.\"\\n\\n@tool\\ndef get_stock_trends(symbol: str) -> str:\\n    \"\"\"Retrieve stock trends for a given symbol.\"\"\"\\n    return f\"The stock of {symbol} has increased by 5% over the last month.\"\\n\\n# Binding tools to the model\\ntools = [get_stock_data, get_stock_trends]\\nmodel_with_tools = ChatAnthropic(model=\"claude-3-haiku-20240307\", temperature=0).bind_tools(tools)\\ntool_node = ToolNode(tools)\\n\\ndef plan_step(state: PlanExecute) -> PlanExecute:\\n    \"\"\"Plan step generates a plan based on the input using llm structured output functionality, stores it to the plan field\"\"\"\\n    input_statement = state[\\'input\\']\\n    structured_llm = llm.with_structured_output(PlanStepOutput)\\n    plan_steps: List[PlanStepOutput] = structured_llm.invoke(\\n        f\"Given the input statement below, generate a detailed and structured plan to address the task described.\\\\n\\\\nInput Statement: {input_statement}\\\\n\\\\nPlease format the response in a list format, where each entry contains:\\\\n1. Step Description\\\\n2. Resources Needed\\\\n3. Potential Challenges\"\\n    )\\n    state[\\'plan\\'] = plan_steps\\n    return state\\n\\ndef execute_step(state: PlanExecute) -> PlanExecute:\\n    \"\"\"Uses llm with tool binding for the stock related queries.\"\"\"\\n    input_query = state[\\'input\\']\\n    prompt = f\"Using the stock-related query provided, analyze the input and perform the following steps:\\\\n    1. Identify the primary aspects of the stock-related question asked.\\\\n    2. Retrieve current stock data or any related information that pertains to the user\\'s query.\\\\n    3. Summarize the findings with relevant details—like current stock prices, trends, and news relevant to the query.\\\\n    4. Provide additional insights or recommendations based on the analysis.\\\\n    Input: {input_query}\"\\n    response = model_with_tools.invoke([SystemMessage(content=prompt)])\\n    state[\\'response\\'] = response.content\\n    return state\\n\\ndef replan_step(state: PlanExecute) -> Union[str, List[str]]:\\n    \"\"\"Evaluates progress and uses an LLM to either revise the plan or generate a final response.\"\"\"\\n    input_text = state[\\'input\\']\\n    prompt = (\\n        f\"Evaluate the current progress of the following plan: {input_text}. \"\\n        \"Based on the steps completed so far, identify any areas where the plan may need revision or adjustment. \"\\n        \"If the plan appears to be progressing well, summarize the findings and generate a final response. \"\\n        \"Please take into account the steps taken and any obstacles encountered along the way to provide a comprehensive evaluation.\"\\n    )\\n    output = call_llm(prompt)  # This function needs to be defined or imported from an LLM library\\n    state[\\'response\\'] = output\\n    return output\\n\\n# Stategraph compilation\\nworkflow = StateGraph(MessagesState)\\n\\n# Define the nodes\\nworkflow.add_node(\"planner\", plan_step)\\nworkflow.add_node(\"agent\", execute_step)\\nworkflow.add_node(\"replan\", replan_step)\\nworkflow.add_node(\"tools\", tool_node)\\n\\n# Add edges\\nworkflow.add_edge(START, \"planner\")  # Non-conditional edge from START to planner\\nworkflow.add_edge(\"planner\", \"agent\")  # Non-conditional edge from planner to agent\\nworkflow.add_edge(\"agent\", \"replan\")  # Non-conditional edge from agent to replan\\n\\n# Conditional edge based on whether tool calls were made\\ndef should_continue(state: MessagesState):\\n    messages = state[\"messages\"]\\n    last_message = messages[-1]\\n    if last_message.tool_calls:\\n        return \"tools\"\\n    return END\\n\\n# Adding conditional edges\\nworkflow.add_conditional_edges(\"agent\", should_continue, [\"tools\", END])\\nworkflow.add_edge(\"tools\", \"agent\")  # Non-conditional edge from tools back to agent\\n\\n# Compile the workflow\\napp = workflow.compile()\\n```\\n\\n### Explanation:\\n1. **Node Implementations**: Each node (`planner`, `agent`, `replan`) has its own function that processes the state and returns it.\\n2. **Tool Integration**: The `get_stock_data` and `get_stock_trends` functions are defined as tools that can be invoked by the model.\\n3. **Graph Structure**: The `StateGraph` is created, and nodes and edges are added according to the specified graph structure.\\n4. **Conditional Logic**: The `should_continue` function determines the next node based on whether tool calls were made.\\n5. **Compilation**: Finally, the graph is compiled into an executable form.\\n\\nThis code provides a complete implementation of the graph as described in your input.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 1216, 'prompt_tokens': 3065, 'total_tokens': 4281, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_0392822090', 'id': 'chatcmpl-BRj9Dcvwbesh5ifr4z8rPq8WYfWXa', 'finish_reason': 'stop', 'logprobs': None}, id='run-4e908191-77eb-48b6-9590-54d2a8dd90d4-0', usage_metadata={'input_tokens': 3065, 'output_tokens': 1216, 'total_tokens': 4281, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}}\n"
     ]
    }
   ],
   "source": [
    "import uuid\n",
    "uuid_str1= uuid.uuid4()\n",
    "config = {\"configurable\": {\"thread_id\": str(uuid_str1)}}\n",
    "\n",
    "for output in compiler_graph.stream({\"json_objects\": json_objects}, config, stream_mode=\"updates\"\n",
    "    ):\n",
    "        print(output)   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
