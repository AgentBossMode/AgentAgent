{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62b92789",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_objects = {\n",
    "  \"nodes\": {\n",
    "    \"__START__\": {\n",
    "      \"schema_info\": \"\",\n",
    "      \"input_schema\": \"\",\n",
    "      \"output_schema\": \"\",\n",
    "      \"description\": \"Entry point of the graph.\",\n",
    "      \"function_name\": \"\"\n",
    "    },\n",
    "    \"planner\": {\n",
    "      \"schema_info\": \"PlanExecute: TypedDict with fields input (str), plan (List[str]), past_steps (Annotated[List[Tuple], operator.add]), response (str)\",\n",
    "      \"input_schema\": \"PlanExecute\",\n",
    "      \"output_schema\": \"PlanExecute\",\n",
    "      \"description\": \"Plan step generates a plan based on the input using llm structured output functionality, stores it to the plan field\",\n",
    "      \"function_name\": \"plan_step\"\n",
    "    },\n",
    "    \"agent\": {\n",
    "      \"schema_info\": \"PlanExecute: TypedDict with fields input (str), plan (List[str]), past_steps (Annotated[List[Tuple], operator.add]), response (str)\",\n",
    "      \"input_schema\": \"PlanExecute\",\n",
    "      \"output_schema\": \"PlanExecute\",\n",
    "      \"description\": \"Uses llm with tool binding for the stock related queries\",\n",
    "      \"function_name\": \"execute_step\"\n",
    "    },\n",
    "    \"replan\": {\n",
    "      \"schema_info\": \"PlanExecute: TypedDict with fields input (str), plan (List[str]), past_steps (Annotated[List[Tuple], operator.add]), response (str)\",\n",
    "      \"input_schema\": \"PlanExecute\",\n",
    "      \"output_schema\": \"Union[Response, Plan]\",\n",
    "      \"description\": \"Evaluates progress and uses an LLM to either revise the plan or generate a final response.\",\n",
    "      \"function_name\": \"replan_step\"\n",
    "    },\n",
    "    \"__END__\": {\n",
    "      \"schema_info\": \"\",\n",
    "      \"input_schema\": \"\",\n",
    "      \"output_schema\": \"\",\n",
    "      \"description\": \"End point of the graph.\",\n",
    "      \"function_name\": \"\"\n",
    "    }\n",
    "  },\n",
    "  \"edges\": {\n",
    "    \"edge_1\": {\n",
    "      \"source\": \"__START__\",\n",
    "      \"target\": \"planner\",\n",
    "      \"routing_conditions\": \"Start the planning process.\",\n",
    "      \"conditional\": False\n",
    "    },\n",
    "    \"edge_2\": {\n",
    "      \"source\": \"planner\",\n",
    "      \"target\": \"agent\",\n",
    "      \"routing_conditions\": \"After planning, execute the first step.\",\n",
    "      \"conditional\": False\n",
    "    },\n",
    "    \"edge_3\": {\n",
    "      \"source\": \"agent\",\n",
    "      \"target\": \"replan\",\n",
    "      \"routing_conditions\": \"After executing a step, check if replanning is needed.\",\n",
    "      \"conditional\": False\n",
    "    },\n",
    "    \"edge_4\": {\n",
    "      \"source\": \"replan\",\n",
    "      \"target\": \"agent\",\n",
    "      \"routing_conditions\": \"If no response is generated, continue to agent for further execution.\",\n",
    "      \"conditional\": True\n",
    "    },\n",
    "    \"edge_5\": {\n",
    "      \"source\": \"replan\",\n",
    "      \"target\": \"__END__\",\n",
    "      \"routing_conditions\": \"If a response is generated, end the process.\",\n",
    "      \"conditional\": True\n",
    "    }\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb05768b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kanishkgupta/Documents/GitHub/AgentAgent/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import import_ipynb\n",
    "from phase1_edge_handling import edge_builder_agent\n",
    "from phase1_node_to_code import node_to_code_app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cdd16f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "skipList = [\"__START__\", \"__END__\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cff28bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "from typing import Annotated, List\n",
    "from langgraph.graph import StateGraph, START, END, MessagesState\n",
    "from langgraph.types import Send\n",
    "from pydantic import BaseModel\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "import uuid\n",
    "\n",
    "\n",
    "class NodeEvaluationReport(BaseModel):\n",
    "    node_name: str\n",
    "    node_code_stub: str\n",
    "    edge_code: str\n",
    "\n",
    "class GraphCompilerState(MessagesState):\n",
    "    json_objects: dict\n",
    "    node_reports: Annotated[List[NodeEvaluationReport], operator.add]\n",
    "\n",
    "\n",
    "class NodeProcessState():\n",
    "    node_name: str\n",
    "    node_info: dict\n",
    "    edge_info: dict\n",
    "\n",
    "def graph_map_step(state: GraphCompilerState):\n",
    "      # Get edges originating from the current node\n",
    "    json_objects = state[\"json_objects\"]\n",
    "    edges = json_objects[\"edges\"]\n",
    "    nodes = json_objects[\"nodes\"]\n",
    "    sends = []\n",
    "    for node_key, node_val in nodes.items():\n",
    "        outgoing_edges = list(filter(lambda edge: edge[\"source\"] == node_key, edges.values()))\n",
    "        sends.append(Send(\"node_process\", {\"node_name\": node_key, \"node_info\": node_val, \"edge_info\": outgoing_edges}))\n",
    "    return sends\n",
    "\n",
    "edge_info_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "<GraphNodeImplementation>\n",
    "{node_code}\n",
    "</GraphNodeImplementation>\n",
    "<EdgeInformation>\n",
    "{edge_json}\n",
    "</Edgeinformation>\"\"\")\n",
    "\n",
    "def node_process(state: NodeProcessState):\n",
    "    uuid_str = uuid.uuid4()\n",
    "    config = {\"configurable\": {\"thread_id\": str(uuid_str)}}\n",
    "    if state[\"node_name\"] not in skipList:\n",
    "        for output in node_to_code_app.stream(state[\"node_info\"], config, stream_mode=\"updates\"):\n",
    "            print(output)\n",
    "        code= node_to_code_app.get_state(config).values[\"final_code\"]\n",
    "    else:\n",
    "        code = \"no implementation needed\"\n",
    "    edge_code = edge_builder_agent.invoke({\"messages\": [HumanMessage(content = edge_info_prompt.format(node_code=code,edge_json=state[\"edge_info\"]))]},config)\n",
    "    return {\n",
    "        \"node_reports\" : [NodeEvaluationReport(node_name=state[\"node_name\"], node_code_stub=code, edge_code=edge_code[\"messages\"][-1].content)] \n",
    "    }\n",
    "\n",
    "code_compiler_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "You are responsible for merging and compiling the code  for the given graph\n",
    "<Graph>\n",
    "{graphEdgeDict}\n",
    "</Graph>\n",
    "\n",
    "<GraphNodeImplementation>\n",
    "{graphImplementations}\n",
    "</GraphNodeImplementation>\n",
    "\n",
    "<EdgeImplementation>\n",
    "{edgeImplementations}\n",
    "</EdgeImplementation>\n",
    "\"\"\")\n",
    "\n",
    "def graph_compile(state: GraphCompilerState):\n",
    "    node_evals : List[NodeEvaluationReport]= state[\"node_reports\"]\n",
    "\n",
    "    code_stubs = [node_eval.node_code_stub for node_eval in node_evals]\n",
    "    edge_stubs = [node_eval.edge_code for node_eval in node_evals]\n",
    "    json_objects = state[\"json_objects\"]\n",
    "    edges = json_objects[\"edges\"]\n",
    "    llm = ChatOpenAI(model=\"gpt-4.1-mini\", temperature=0)\n",
    "    response = llm.invoke([SystemMessage(content= code_compiler_prompt.format(\n",
    "        graphEdgeDict=edges,\n",
    "        graphImplementations=code_stubs,\n",
    "        edgeImplementations=edge_stubs))])\n",
    "    return {\n",
    "        \"messages\": [response]\n",
    "    }\n",
    "\n",
    "\n",
    "workflow = StateGraph(GraphCompilerState)\n",
    "workflow.add_node(graph_map_step, \"graph_map\")\n",
    "workflow.add_node(node_process, \"node_process\")\n",
    "workflow.add_node(graph_compile,\"graph_compile\")\n",
    "\n",
    "workflow.add_conditional_edges(START, graph_map_step, [\"node_process\"])\n",
    "workflow.add_edge(\"node_process\", \"graph_compile\")\n",
    "workflow.add_edge(\"graph_compile\", END)\n",
    "\n",
    "compiler_graph = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e7d68840",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'identify_node': {'node_type': 'planner', 'messages': [HumanMessage(content='\\nYou are provided with the following information about the node:\\n<SchemaInfo>\\nPlanExecute: TypedDict with fields input (str), plan (List[str]), past_steps (Annotated[List[Tuple], operator.add]), response (str)\\n</SchemaInfo>\\n<InputSchema>\\nPlanExecute\\n</InputSchema>\\n<OutputSchema>\\nPlanExecute\\n</OutputSchema>\\n<Description>\\nUses llm with tool binding for the stock related queries\\n</Description>\\n<FunctionName>\\nexecute_step\\n</FunctionName>\\n\\nBelow is the skeleton of the function that you need to implement:\\ndef execute_step(state:PlanExecute) -> PlanExecute:\\n    \"\"\"Uses llm with tool binding for the stock related queries\"\"\"\\n    # Implement the function to meet the description.\\n\\nthe state is of type PlanExecute and the function is of type PlanExecute\\nThe general idea is that the implementation would involve extracting the input from the state, and updating the state with the output. Description contains the logic for this blackbox\\n', additional_kwargs={}, response_metadata={}, id='3f1355d5-1a9c-4eaa-91b2-e2a5fbf51bca')], 'node_info': '\\nYou are provided with the following information about the node:\\n<SchemaInfo>\\nPlanExecute: TypedDict with fields input (str), plan (List[str]), past_steps (Annotated[List[Tuple], operator.add]), response (str)\\n</SchemaInfo>\\n<InputSchema>\\nPlanExecute\\n</InputSchema>\\n<OutputSchema>\\nPlanExecute\\n</OutputSchema>\\n<Description>\\nUses llm with tool binding for the stock related queries\\n</Description>\\n<FunctionName>\\nexecute_step\\n</FunctionName>\\n\\nBelow is the skeleton of the function that you need to implement:\\ndef execute_step(state:PlanExecute) -> PlanExecute:\\n    \"\"\"Uses llm with tool binding for the stock related queries\"\"\"\\n    # Implement the function to meet the description.\\n\\nthe state is of type PlanExecute and the function is of type PlanExecute\\nThe general idea is that the implementation would involve extracting the input from the state, and updating the state with the output. Description contains the logic for this blackbox\\n'}}\n",
      "{'identify_node': {'node_type': 'planner', 'messages': [HumanMessage(content='\\nYou are provided with the following information about the node:\\n<SchemaInfo>\\nPlanExecute: TypedDict with fields input (str), plan (List[str]), past_steps (Annotated[List[Tuple], operator.add]), response (str)\\n</SchemaInfo>\\n<InputSchema>\\nPlanExecute\\n</InputSchema>\\n<OutputSchema>\\nUnion[Response, Plan]\\n</OutputSchema>\\n<Description>\\nEvaluates progress and uses an LLM to either revise the plan or generate a final response.\\n</Description>\\n<FunctionName>\\nreplan_step\\n</FunctionName>\\n\\nBelow is the skeleton of the function that you need to implement:\\ndef replan_step(state:PlanExecute) -> Union[Response, Plan]:\\n    \"\"\"Evaluates progress and uses an LLM to either revise the plan or generate a final response.\"\"\"\\n    # Implement the function to meet the description.\\n\\nthe state is of type PlanExecute and the function is of type Union[Response, Plan]\\nThe general idea is that the implementation would involve extracting the input from the state, and updating the state with the output. Description contains the logic for this blackbox\\n', additional_kwargs={}, response_metadata={}, id='227348fa-ef6f-4463-88e3-53305318f27a')], 'node_info': '\\nYou are provided with the following information about the node:\\n<SchemaInfo>\\nPlanExecute: TypedDict with fields input (str), plan (List[str]), past_steps (Annotated[List[Tuple], operator.add]), response (str)\\n</SchemaInfo>\\n<InputSchema>\\nPlanExecute\\n</InputSchema>\\n<OutputSchema>\\nUnion[Response, Plan]\\n</OutputSchema>\\n<Description>\\nEvaluates progress and uses an LLM to either revise the plan or generate a final response.\\n</Description>\\n<FunctionName>\\nreplan_step\\n</FunctionName>\\n\\nBelow is the skeleton of the function that you need to implement:\\ndef replan_step(state:PlanExecute) -> Union[Response, Plan]:\\n    \"\"\"Evaluates progress and uses an LLM to either revise the plan or generate a final response.\"\"\"\\n    # Implement the function to meet the description.\\n\\nthe state is of type PlanExecute and the function is of type Union[Response, Plan]\\nThe general idea is that the implementation would involve extracting the input from the state, and updating the state with the output. Description contains the logic for this blackbox\\n'}}\n",
      "{'identify_node': {'node_type': 'planner', 'messages': [HumanMessage(content='\\nYou are provided with the following information about the node:\\n<SchemaInfo>\\nPlanExecute: TypedDict with fields input (str), plan (List[str]), past_steps (Annotated[List[Tuple], operator.add]), response (str)\\n</SchemaInfo>\\n<InputSchema>\\nPlanExecute\\n</InputSchema>\\n<OutputSchema>\\nPlanExecute\\n</OutputSchema>\\n<Description>\\nPlan step generates a plan based on the input using llm structured output functionality, stores it to the plan field\\n</Description>\\n<FunctionName>\\nplan_step\\n</FunctionName>\\n\\nBelow is the skeleton of the function that you need to implement:\\ndef plan_step(state:PlanExecute) -> PlanExecute:\\n    \"\"\"Plan step generates a plan based on the input using llm structured output functionality, stores it to the plan field\"\"\"\\n    # Implement the function to meet the description.\\n\\nthe state is of type PlanExecute and the function is of type PlanExecute\\nThe general idea is that the implementation would involve extracting the input from the state, and updating the state with the output. Description contains the logic for this blackbox\\n', additional_kwargs={}, response_metadata={}, id='2fa485ff-18c1-4e38-b787-9ad741b75275')], 'node_info': '\\nYou are provided with the following information about the node:\\n<SchemaInfo>\\nPlanExecute: TypedDict with fields input (str), plan (List[str]), past_steps (Annotated[List[Tuple], operator.add]), response (str)\\n</SchemaInfo>\\n<InputSchema>\\nPlanExecute\\n</InputSchema>\\n<OutputSchema>\\nPlanExecute\\n</OutputSchema>\\n<Description>\\nPlan step generates a plan based on the input using llm structured output functionality, stores it to the plan field\\n</Description>\\n<FunctionName>\\nplan_step\\n</FunctionName>\\n\\nBelow is the skeleton of the function that you need to implement:\\ndef plan_step(state:PlanExecute) -> PlanExecute:\\n    \"\"\"Plan step generates a plan based on the input using llm structured output functionality, stores it to the plan field\"\"\"\\n    # Implement the function to meet the description.\\n\\nthe state is of type PlanExecute and the function is of type PlanExecute\\nThe general idea is that the implementation would involve extracting the input from the state, and updating the state with the output. Description contains the logic for this blackbox\\n'}}\n",
      "{'node_process': {'node_reports': [NodeEvaluationReport(node_name='__END__', node_code_stub='no implementation needed', edge_code='# No edges to implement as the edge information list is empty.\\n# No code is needed for edges since there are no source or target nodes specified.\\n# There are no edges to implement since the edge information list is empty.\\n# Hence, no code for edges is required.')]}}\n",
      "{'planner': {'plan': ['Generate a prompt that is designed to handle stock-related queries using a large language model (LLM).', 'Bind the generated prompt with appropriate stock-related tools using the tool binding capability to ensure accurate and factual responses.']}}\n",
      "{'planner': {'plan': [\"Formulate a prompt that includes the 'input', the 'past_steps', and the current 'plan' to evaluate progress and decide whether to revise the plan or generate a final response.\", 'Use the structured output strategy to ensure the LLM returns a well-defined output that either contains a revised plan or a final response.']}}\n",
      "{'node_process': {'node_reports': [NodeEvaluationReport(node_name='__START__', node_code_stub='no implementation needed', edge_code='```python\\n# Since the edge is a non-conditional edge from \\'__START__\\' to \\'planner\\', \\n# we add it using the add_edge method as per NonConditionalEdges instructions.\\n\\n# Example assuming you have a graph object named \\'graph\\':\\ngraph.add_edge(\"__START__\", \"planner\")\\n```')]}}\n",
      "{'planner': {'plan': [\"Use the LLM's structured output functionality to ensure the plan is returned as a list of steps, adhering to a predefined schema for consistency and reliability.\"]}}\n",
      "{'ai_node_gen_supervisor': {'next': 'prompt_generation', 'task': 'Generate a prompt that is designed to handle stock-related queries using a large language model (LLM).', 'messages': [AIMessage(content=\"The current step requires generating a prompt specifically designed for stock-related queries using a large language model (LLM). The 'prompt_generation' worker is suited to handle this task as it specializes in creating effective prompts. Once the prompt is generated, the process can move on to binding the tools.\", additional_kwargs={}, response_metadata={}, id='7a8010b6-5428-4061-a668-b68cbc6311d4')]}}\n",
      "{'ai_node_gen_supervisor': {'next': 'prompt_generation', 'task': \"Formulate a prompt that includes the 'input', the 'past_steps', and the current 'plan' to evaluate progress and decide whether to revise the plan or generate a final response.\", 'messages': [AIMessage(content=\"The current step requires formulating a prompt that integrates the 'input', 'past_steps', and current 'plan' to evaluate progress and decide the next actions. The 'prompt_generation' worker specializes in generating such prompts, making it the appropriate choice for this step.\", additional_kwargs={}, response_metadata={}, id='d72254cb-d969-443a-84fa-59caecb3e504')]}}\n",
      "{'prompt_generation': {'past_steps': [('Generate a prompt that is designed to handle stock-related queries using a large language model (LLM).', \"You are an expert system for handling stock-related queries using a large language model (LLM) with tool integrations. Given the user's input query about stocks, generate a clear and detailed plan breaking down the steps needed to address the query effectively. Then produce an output response that reflects the execution of the current step in the plan, incorporating any relevant tool results if available, and including context from past steps taken. Provide your output in a structured format with fields: input (the original user query), plan (a list of planning steps to solve the query), past_steps (a list of tuples documenting previous plan steps and their outcomes), and response (the generated answer or intermediate response for the current step). Make sure your reasoning caters specifically to stock market data, financial instruments, and relevant analytic tools.\")], 'messages': [HumanMessage(content=\"You are an expert system for handling stock-related queries using a large language model (LLM) with tool integrations. Given the user's input query about stocks, generate a clear and detailed plan breaking down the steps needed to address the query effectively. Then produce an output response that reflects the execution of the current step in the plan, incorporating any relevant tool results if available, and including context from past steps taken. Provide your output in a structured format with fields: input (the original user query), plan (a list of planning steps to solve the query), past_steps (a list of tuples documenting previous plan steps and their outcomes), and response (the generated answer or intermediate response for the current step). Make sure your reasoning caters specifically to stock market data, financial instruments, and relevant analytic tools.\", additional_kwargs={}, response_metadata={}, name='prompt_generator', id='d8eb50a5-b21b-41d6-8cf3-c6eea64cd13d')]}}\n",
      "{'ai_node_gen_supervisor': {'next': 'structured_output_generation', 'task': \"Use the LLM's structured output functionality to ensure the plan is returned as a list of steps, adhering to a predefined schema for consistency and reliability.\", 'messages': [AIMessage(content=\"Step 1 requires using the LLM's structured output functionality to ensure the plan is generated as a list of steps adhering to a predefined schema. The 'structured_output_generation' worker specializes in generating outputs in structured formats as per schemas, making it the most appropriate worker to execute this step.\", additional_kwargs={}, response_metadata={}, id='6c881fab-4818-4455-a423-55548dea002e')]}}\n",
      "{'prompt_generation': {'past_steps': [(\"Formulate a prompt that includes the 'input', the 'past_steps', and the current 'plan' to evaluate progress and decide whether to revise the plan or generate a final response.\", 'You are an intelligent assistant helping to evaluate the progress of a multi-step plan and decide the next action. You are given the original input query, the current plan as a list of steps, and the list of past completed steps with their outputs. Your task is to:\\n\\n1. Assess whether the current plan sufficiently addresses the input or if the plan needs to be revised.\\n2. If the plan needs revision, generate a clear, updated plan as a list of step descriptions.\\n3. If the plan is complete, generate a final response that satisfies the input.\\n\\nUse the given information carefully:\\n- Input (task to solve): {input}\\n- Past steps and their outcomes: {past_steps}\\n- Current plan steps: {plan}\\n\\nBased on this information, reply with either:\\n- A revised plan as a list of steps to perform next, or\\n- A final response answering the input query.\\n\\nEnsure your output matches exactly one of these two JSON structures:\\n\\nTo revise plan:\\n{{\"plan\": [step1, step2, ...]}}\\n\\nTo finalize response:\\n{{\"response\": \"final answer text\"}}\\n\\nDo not include anything else outside the JSON object.')], 'messages': [HumanMessage(content='You are an intelligent assistant helping to evaluate the progress of a multi-step plan and decide the next action. You are given the original input query, the current plan as a list of steps, and the list of past completed steps with their outputs. Your task is to:\\n\\n1. Assess whether the current plan sufficiently addresses the input or if the plan needs to be revised.\\n2. If the plan needs revision, generate a clear, updated plan as a list of step descriptions.\\n3. If the plan is complete, generate a final response that satisfies the input.\\n\\nUse the given information carefully:\\n- Input (task to solve): {input}\\n- Past steps and their outcomes: {past_steps}\\n- Current plan steps: {plan}\\n\\nBased on this information, reply with either:\\n- A revised plan as a list of steps to perform next, or\\n- A final response answering the input query.\\n\\nEnsure your output matches exactly one of these two JSON structures:\\n\\nTo revise plan:\\n{{\"plan\": [step1, step2, ...]}}\\n\\nTo finalize response:\\n{{\"response\": \"final answer text\"}}\\n\\nDo not include anything else outside the JSON object.', additional_kwargs={}, response_metadata={}, name='prompt_generator', id='5c5ea919-f7b9-424a-83ee-aa1c6dfeff98')]}}\n",
      "{'replan': {'plan': ['Bind the generated prompt with appropriate stock-related tools using the tool binding capability to ensure accurate and factual responses.']}}\n",
      "{'structured_output_generation': {'past_steps': [(\"Use the LLM's structured output functionality to ensure the plan is returned as a list of steps, adhering to a predefined schema for consistency and reliability.\", 'Yes, this node requires using the LLM\\'s with_structured_output() function because:\\n\\n- The node generates a \"plan\" (a list of strings) based on the input text.\\n- The output field \"plan\" is a List[str], which benefits from enforcing a consistent output format.\\n- Using structured output ensures type safety, reduces hallucinations, and guarantees the model returns a valid plan list.\\n- This aligns with \"Reduced Hallucinations,\" \"Reliable Type-Safety,\" and \"Consistent and Verifiable Output\" points from the AboutStructuredOutput section.\\n\\nBelow is the code implementing the structured output functionality using a Pydantic class for the plan output:\\n\\n```python\\nfrom typing import List\\nfrom pydantic import BaseModel, Field\\nfrom some_llm_module import llm  # placeholder for actual llm import\\n\\nclass PlanOutput(BaseModel):\\n    plan: List[str] = Field(description=\"Generated list of plan steps\")\\n\\ndef plan_step(state: PlanExecute) -> PlanExecute:\\n    \"\"\"Plan step generates a plan based on the input using llm structured output functionality, stores it to the plan field\"\"\"\\n    structured_llm = llm.with_structured_output(PlanOutput)\\n    # Invoke the LLM to generate the plan based on the input\\n    result: PlanOutput = structured_llm.invoke(state[\\'input\\'])\\n    # Update the state with the generated plan\\n    state[\\'plan\\'] = result.plan\\n    return state\\n```')], 'messages': [HumanMessage(content='Yes, this node requires using the LLM\\'s with_structured_output() function because:\\n\\n- The node generates a \"plan\" (a list of strings) based on the input text.\\n- The output field \"plan\" is a List[str], which benefits from enforcing a consistent output format.\\n- Using structured output ensures type safety, reduces hallucinations, and guarantees the model returns a valid plan list.\\n- This aligns with \"Reduced Hallucinations,\" \"Reliable Type-Safety,\" and \"Consistent and Verifiable Output\" points from the AboutStructuredOutput section.\\n\\nBelow is the code implementing the structured output functionality using a Pydantic class for the plan output:\\n\\n```python\\nfrom typing import List\\nfrom pydantic import BaseModel, Field\\nfrom some_llm_module import llm  # placeholder for actual llm import\\n\\nclass PlanOutput(BaseModel):\\n    plan: List[str] = Field(description=\"Generated list of plan steps\")\\n\\ndef plan_step(state: PlanExecute) -> PlanExecute:\\n    \"\"\"Plan step generates a plan based on the input using llm structured output functionality, stores it to the plan field\"\"\"\\n    structured_llm = llm.with_structured_output(PlanOutput)\\n    # Invoke the LLM to generate the plan based on the input\\n    result: PlanOutput = structured_llm.invoke(state[\\'input\\'])\\n    # Update the state with the generated plan\\n    state[\\'plan\\'] = result.plan\\n    return state\\n```', additional_kwargs={}, response_metadata={}, name='struct_output_generator', id='fd65a44c-f201-4c07-bda1-bc6a3cdc88c3')]}}\n",
      "{'ai_node_gen_supervisor': {'next': 'toolset_generation', 'task': 'Bind the generated prompt with appropriate stock-related tools using the tool binding capability to ensure accurate and factual responses.', 'messages': [AIMessage(content=\"Step 1 requires binding the generated prompt with appropriate stock-related tools to ensure accurate and factual responses. The 'toolset_generation' worker is best suited to perform tool binding based on the given prompt and related context.\", additional_kwargs={}, response_metadata={}, id='50efa46f-87ad-4b66-b8cf-05e27139231b')]}}\n",
      "{'replan': {'response': 'The OriginalPlan has been fully executed as the prompt formulation and structured output strategy have been completed. Therefore, I am providing the final response as per the plan.'}}\n",
      "{'replan': {'response': \"The plan to use the LLM's structured output functionality to return the plan as a list of steps adhering to a predefined schema has been fully executed. The implementation and justification are complete.\"}}\n",
      "{'code_compiler': {'final_code': '```python\\nfrom typing import List\\nfrom pydantic import BaseModel, Field\\nfrom some_llm_module import llm  # placeholder for actual llm import\\n\\nclass PlanOutput(BaseModel):\\n    plan: List[str] = Field(description=\"Generated list of plan steps\")\\n\\ndef plan_step(state: PlanExecute) -> PlanExecute:\\n    \"\"\"Plan step generates a plan based on the input using llm structured output functionality, stores it to the plan field\"\"\"\\n    structured_llm = llm.with_structured_output(PlanOutput)\\n    # Invoke the LLM to generate the plan based on the input\\n    result: PlanOutput = structured_llm.invoke(state[\\'input\\'])\\n    # Update the state with the generated plan\\n    state[\\'plan\\'] = result.plan\\n    return state\\n```'}}\n",
      "{'toolset_generation': {'past_steps': [('Bind the generated prompt with appropriate stock-related tools using the tool binding capability to ensure accurate and factual responses.', '```python\\nfrom typing import List, Tuple, TypedDict, Annotated\\nfrom langchain_core.tools import tool\\nfrom langchain_core.messages import SystemMessage\\nfrom langgraph.prebuilt import ToolNode\\nfrom langchain.chat_models import ChatAnthropic\\nfrom langgraph.graph import StateGraph, MessagesState, START, END\\n\\nclass PlanExecute(TypedDict):\\n    input: str\\n    plan: List[str]\\n    past_steps: Annotated[List[Tuple], \"operator.add\"]\\n    response: str\\n\\n# Define stock-related tools\\n\\n@tool\\ndef get_stock_price(ticker: str):\\n    \"\"\"Tool to get current stock price.\"\"\"\\n    # In practice, this would call an API\\n    stock_data = {\\n        \"AAPL\": \"Apple stock is $150\",\\n        \"GOOG\": \"Google stock is $2800\",\\n        \"TSLA\": \"Tesla stock is $700\"\\n    }\\n    return stock_data.get(ticker.upper(), \"Ticker not found\")\\n\\n@tool\\ndef get_stock_news(ticker: str):\\n    \"\"\"Tool to get recent news about a stock.\"\"\"\\n    news_data = {\\n        \"AAPL\": \"Apple releases new iPhone 15.\",\\n        \"GOOG\": \"Google introduces new AI features.\",\\n        \"TSLA\": \"Tesla announces new battery tech.\"\\n    }\\n    return news_data.get(ticker.upper(), \"No recent news available\")\\n\\ntools = [get_stock_price, get_stock_news]\\n\\n# Bind model with tools\\nmodel_with_tools = ChatAnthropic(\\n    model=\"claude-3-haiku-20240307\", temperature=0\\n).bind_tools(tools)\\n\\n# Tool node for execution in graph scenarios if needed\\ntool_node = ToolNode(tools)\\n\\ndef execute_step(state: PlanExecute) -> PlanExecute:\\n    \"\"\"Uses llm with tool binding for the stock related queries\"\"\"\\n    user_query = state[\"input\"]\\n    past_steps = state.get(\"past_steps\", [])\\n    plan = state.get(\"plan\", [])\\n\\n    # Compose system prompt to guide model on stock queries and tool usage\\n    system_prompt = SystemMessage(\\n        content=(\\n            \"You are a stock market expert assistant. Use available tools like get_stock_price \"\\n            \"and get_stock_news to answer queries about stock prices, financial data, \"\\n            \"and related news. Incorporate past steps context when appropriate.\"\\n        )\\n    )\\n\\n    # Prepare messages for model invocation\\n    from langchain_core.messages import HumanMessage\\n    messages = [system_prompt, HumanMessage(content=user_query)]\\n\\n    # Invoke model with tools enabled\\n    response = model_with_tools.invoke(messages)\\n\\n    # Extract the text response from the model\\'s output\\n    reply = response.content if hasattr(response, \"content\") else str(response)\\n\\n    # Update and return the updated state\\n    return {\\n        \"input\": user_query,\\n        \"plan\": plan,\\n        \"past_steps\": past_steps,\\n        \"response\": reply\\n    }\\n```')], 'messages': [HumanMessage(content='```python\\nfrom typing import List, Tuple, TypedDict, Annotated\\nfrom langchain_core.tools import tool\\nfrom langchain_core.messages import SystemMessage\\nfrom langgraph.prebuilt import ToolNode\\nfrom langchain.chat_models import ChatAnthropic\\nfrom langgraph.graph import StateGraph, MessagesState, START, END\\n\\nclass PlanExecute(TypedDict):\\n    input: str\\n    plan: List[str]\\n    past_steps: Annotated[List[Tuple], \"operator.add\"]\\n    response: str\\n\\n# Define stock-related tools\\n\\n@tool\\ndef get_stock_price(ticker: str):\\n    \"\"\"Tool to get current stock price.\"\"\"\\n    # In practice, this would call an API\\n    stock_data = {\\n        \"AAPL\": \"Apple stock is $150\",\\n        \"GOOG\": \"Google stock is $2800\",\\n        \"TSLA\": \"Tesla stock is $700\"\\n    }\\n    return stock_data.get(ticker.upper(), \"Ticker not found\")\\n\\n@tool\\ndef get_stock_news(ticker: str):\\n    \"\"\"Tool to get recent news about a stock.\"\"\"\\n    news_data = {\\n        \"AAPL\": \"Apple releases new iPhone 15.\",\\n        \"GOOG\": \"Google introduces new AI features.\",\\n        \"TSLA\": \"Tesla announces new battery tech.\"\\n    }\\n    return news_data.get(ticker.upper(), \"No recent news available\")\\n\\ntools = [get_stock_price, get_stock_news]\\n\\n# Bind model with tools\\nmodel_with_tools = ChatAnthropic(\\n    model=\"claude-3-haiku-20240307\", temperature=0\\n).bind_tools(tools)\\n\\n# Tool node for execution in graph scenarios if needed\\ntool_node = ToolNode(tools)\\n\\ndef execute_step(state: PlanExecute) -> PlanExecute:\\n    \"\"\"Uses llm with tool binding for the stock related queries\"\"\"\\n    user_query = state[\"input\"]\\n    past_steps = state.get(\"past_steps\", [])\\n    plan = state.get(\"plan\", [])\\n\\n    # Compose system prompt to guide model on stock queries and tool usage\\n    system_prompt = SystemMessage(\\n        content=(\\n            \"You are a stock market expert assistant. Use available tools like get_stock_price \"\\n            \"and get_stock_news to answer queries about stock prices, financial data, \"\\n            \"and related news. Incorporate past steps context when appropriate.\"\\n        )\\n    )\\n\\n    # Prepare messages for model invocation\\n    from langchain_core.messages import HumanMessage\\n    messages = [system_prompt, HumanMessage(content=user_query)]\\n\\n    # Invoke model with tools enabled\\n    response = model_with_tools.invoke(messages)\\n\\n    # Extract the text response from the model\\'s output\\n    reply = response.content if hasattr(response, \"content\") else str(response)\\n\\n    # Update and return the updated state\\n    return {\\n        \"input\": user_query,\\n        \"plan\": plan,\\n        \"past_steps\": past_steps,\\n        \"response\": reply\\n    }\\n```', additional_kwargs={}, response_metadata={}, name='toolset_generator', id='32b89b5d-464e-4c0c-bce1-9b7cf89b6445')]}}\n",
      "{'node_process': {'node_reports': [NodeEvaluationReport(node_name='planner', node_code_stub='```python\\nfrom typing import List\\nfrom pydantic import BaseModel, Field\\nfrom some_llm_module import llm  # placeholder for actual llm import\\n\\nclass PlanOutput(BaseModel):\\n    plan: List[str] = Field(description=\"Generated list of plan steps\")\\n\\ndef plan_step(state: PlanExecute) -> PlanExecute:\\n    \"\"\"Plan step generates a plan based on the input using llm structured output functionality, stores it to the plan field\"\"\"\\n    structured_llm = llm.with_structured_output(PlanOutput)\\n    # Invoke the LLM to generate the plan based on the input\\n    result: PlanOutput = structured_llm.invoke(state[\\'input\\'])\\n    # Update the state with the generated plan\\n    state[\\'plan\\'] = result.plan\\n    return state\\n```', edge_code='```python\\n# Since this is a direct routing from planner to agent without any conditional logic,\\n# use the non-conditional edge implementation.\\ngraph.add_edge(\"planner\", \"agent\")\\n```')]}}\n",
      "{'replan': {'response': \"The plan to bind the generated prompt with appropriate stock-related tools using the tool binding capability has been fully executed. The implementation includes defining stock-related tools, binding them with the language model, and preparing the system prompt to guide the model's responses. This ensures accurate and factual responses to stock-related queries. If you have any further questions or need assistance with the implementation, feel free to ask.\"}}\n",
      "{'code_compiler': {'final_code': '```python\\nfrom typing import List, Tuple, TypedDict, Union, Annotated\\nimport operator\\nimport json\\n\\n# Define the types as per the provided schema info\\nclass PlanExecute(TypedDict):\\n    input: str\\n    plan: List[str]\\n    past_steps: Annotated[List[Tuple], operator.add]\\n    response: str\\n\\n# Define the output schemas responses\\nclass Response(TypedDict):\\n    response: str\\n\\nclass Plan(TypedDict):\\n    plan: List[str]\\n\\ndef replan_step(state: PlanExecute) -> Union[Response, Plan]:\\n    \"\"\"Evaluates progress and uses an LLM to either revise the plan or generate a final response.\"\"\"\\n    \\n    # Extract relevant information from state\\n    input_task = state[\\'input\\']\\n    past_steps = state[\\'past_steps\\']\\n    current_plan = state[\\'plan\\']\\n\\n    # Format past_steps for prompt insertion, converting each tuple to a descriptive string\\n    # Assuming each past step tuple is like (step_description, step_output)\\n    past_steps_str = []\\n    for step in past_steps:\\n        # Defensive: convert each element in the tuple to string to ensure json serializable\\n        step_desc = str(step[0])\\n        step_out = str(step[1])\\n        past_steps_str.append(f\"Step: {step_desc}, Outcome: {step_out}\")\\n\\n    # Compose the prompt for the LLM as per the prompt_generator instructions\\n    prompt = (\\n        \"You are an intelligent assistant helping to evaluate the progress of a multi-step plan and decide the next action. \"\\n        \"You are given the original input query, the current plan as a list of steps, and the list of past completed steps with their outputs. Your task is to:\\\\n\\\\n\"\\n        \"1. Assess whether the current plan sufficiently addresses the input or if the plan needs to be revised.\\\\n\"\\n        \"2. If the plan needs revision, generate a clear, updated plan as a list of step descriptions.\\\\n\"\\n        \"3. If the plan is complete, generate a final response that satisfies the input.\\\\n\\\\n\"\\n        \"Use the given information carefully:\\\\n\"\\n        f\"- Input (task to solve): {input_task}\\\\n\"\\n        f\"- Past steps and their outcomes: {past_steps_str}\\\\n\"\\n        f\"- Current plan steps: {current_plan}\\\\n\\\\n\"\\n        \"Based on this information, reply with either:\\\\n\"\\n        \"- A revised plan as a list of steps to perform next, or\\\\n\"\\n        \"- A final response answering the input query.\\\\n\\\\n\"\\n        \"Ensure your output matches exactly one of these two JSON structures:\\\\n\\\\n\"\\n        \"To revise plan:\\\\n\"\\n        \\'{{\"plan\": [step1, step2, ...]}}\\\\n\\\\n\\'\\n        \"To finalize response:\\\\n\"\\n        \\'{{\"response\": \"final answer text\"}}\\\\n\\\\n\\'\\n        \"Do not include anything else outside the JSON object.\"\\n    )\\n\\n    # Here we simulate calling the LLM with the prompt and getting a response.\\n    # In practice, this would be a call to some LLM API.\\n    # For the purpose of this code, we will mock this with a placeholder function.\\n\\n    def call_llm(prompt: str) -> str:\\n        \"\"\"\\n        This is a placeholder function simulating an LLM call.\\n        It should return a json string that is either like:\\n        {\"plan\": [...]} or {\"response\": \"...\"}\\n        \"\"\"\\n        # In actual implementation, replace with LLM call\\n        # For safety, return a not implemented response here\\n        raise NotImplementedError(\"LLM call must be implemented with actual LLM API\")\\n\\n    llm_response_str = call_llm(prompt)\\n\\n    # Parse LLM response as JSON\\n    try:\\n        llm_response = json.loads(llm_response_str)\\n    except json.JSONDecodeError:\\n        # If response is malformed, respond with final response indicating error\\n        return {\"response\": \"Error: LLM response was not a valid JSON.\"}\\n\\n    # Detect which output type it is (plan or response)\\n    if \"plan\" in llm_response:\\n        # Validate that plan is a list of strings\\n        plan_list = llm_response[\"plan\"]\\n        if not isinstance(plan_list, list) or not all(isinstance(step, str) for step in plan_list):\\n            return {\"response\": \"Error: Updated plan must be a list of strings.\"}\\n        # Return as Plan type\\n        return {\"plan\": plan_list}\\n\\n    elif \"response\" in llm_response:\\n        # Validate response is a string\\n        response_text = llm_response[\"response\"]\\n        if not isinstance(response_text, str):\\n            return {\"response\": \"Error: Final response must be a string.\"}\\n        # Return as Response type\\n        return {\"response\": response_text}\\n\\n    else:\\n        # If neither key present, error response\\n        return {\"response\": \"Error: LLM response must contain either \\'plan\\' or \\'response\\' key.\"}\\n```'}}\n",
      "{'code_compiler': {'final_code': '```python\\nfrom typing import List, Tuple, TypedDict, Annotated\\nfrom langchain_core.tools import tool\\nfrom langchain_core.messages import SystemMessage, HumanMessage\\nfrom langchain.chat_models import ChatAnthropic\\n\\nclass PlanExecute(TypedDict):\\n    input: str\\n    plan: List[str]\\n    past_steps: Annotated[List[Tuple], \"operator.add\"]\\n    response: str\\n\\n# Define stock-related tools\\n\\n@tool\\ndef get_stock_price(ticker: str) -> str:\\n    \"\"\"Tool to get current stock price.\"\"\"\\n    # In practice, this would call an API\\n    stock_data = {\\n        \"AAPL\": \"Apple stock is $150\",\\n        \"GOOG\": \"Google stock is $2800\",\\n        \"TSLA\": \"Tesla stock is $700\"\\n    }\\n    return stock_data.get(ticker.upper(), \"Ticker not found\")\\n\\n@tool\\ndef get_stock_news(ticker: str) -> str:\\n    \"\"\"Tool to get recent news about a stock.\"\"\"\\n    news_data = {\\n        \"AAPL\": \"Apple releases new iPhone 15.\",\\n        \"GOOG\": \"Google introduces new AI features.\",\\n        \"TSLA\": \"Tesla announces new battery tech.\"\\n    }\\n    return news_data.get(ticker.upper(), \"No recent news available\")\\n\\n# List of available tools\\ntools = [get_stock_price, get_stock_news]\\n\\n# Bind model with tools\\nmodel_with_tools = ChatAnthropic(\\n    model=\"claude-3-haiku-20240307\", temperature=0\\n).bind_tools(tools)\\n\\ndef execute_step(state: PlanExecute) -> PlanExecute:\\n    \"\"\"Uses llm with tool binding for the stock related queries\"\"\"\\n    user_query = state[\"input\"]\\n    past_steps = state.get(\"past_steps\", [])\\n    plan = state.get(\"plan\", [])\\n\\n    # Compose system prompt guiding model on stock queries and tool usage\\n    system_prompt = SystemMessage(\\n        content=(\\n            \"You are a stock market expert assistant. Use available tools like get_stock_price \"\\n            \"and get_stock_news to answer queries about stock prices, financial data, \"\\n            \"and related news. Incorporate past steps context when appropriate.\"\\n        )\\n    )\\n\\n    # Prepare messages for model invocation including past steps context\\n    # We add previous step responses for context if any\\n    context = \"\"\\n    if past_steps:\\n        for step, outcome in past_steps:\\n            context += f\"Step: {step}\\\\nOutcome: {outcome}\\\\n\"\\n\\n    # Final user query message augmented by past context if available\\n    final_query = f\"{context}\\\\nUser query: {user_query}\" if context else user_query\\n\\n    messages = [system_prompt, HumanMessage(content=final_query)]\\n\\n    # Invoke model with tools enabled\\n    response = model_with_tools.invoke(messages)\\n\\n    # Extract the text response from the model\\'s output\\n    reply = response.content if hasattr(response, \"content\") else str(response)\\n\\n    # Return updated state with new response, preserving input, plan, and past steps\\n    return {\\n        \"input\": user_query,\\n        \"plan\": plan,\\n        \"past_steps\": past_steps,\\n        \"response\": reply\\n    }\\n```'}}\n",
      "{'node_process': {'node_reports': [NodeEvaluationReport(node_name='agent', node_code_stub='```python\\nfrom typing import List, Tuple, TypedDict, Annotated\\nfrom langchain_core.tools import tool\\nfrom langchain_core.messages import SystemMessage, HumanMessage\\nfrom langchain.chat_models import ChatAnthropic\\n\\nclass PlanExecute(TypedDict):\\n    input: str\\n    plan: List[str]\\n    past_steps: Annotated[List[Tuple], \"operator.add\"]\\n    response: str\\n\\n# Define stock-related tools\\n\\n@tool\\ndef get_stock_price(ticker: str) -> str:\\n    \"\"\"Tool to get current stock price.\"\"\"\\n    # In practice, this would call an API\\n    stock_data = {\\n        \"AAPL\": \"Apple stock is $150\",\\n        \"GOOG\": \"Google stock is $2800\",\\n        \"TSLA\": \"Tesla stock is $700\"\\n    }\\n    return stock_data.get(ticker.upper(), \"Ticker not found\")\\n\\n@tool\\ndef get_stock_news(ticker: str) -> str:\\n    \"\"\"Tool to get recent news about a stock.\"\"\"\\n    news_data = {\\n        \"AAPL\": \"Apple releases new iPhone 15.\",\\n        \"GOOG\": \"Google introduces new AI features.\",\\n        \"TSLA\": \"Tesla announces new battery tech.\"\\n    }\\n    return news_data.get(ticker.upper(), \"No recent news available\")\\n\\n# List of available tools\\ntools = [get_stock_price, get_stock_news]\\n\\n# Bind model with tools\\nmodel_with_tools = ChatAnthropic(\\n    model=\"claude-3-haiku-20240307\", temperature=0\\n).bind_tools(tools)\\n\\ndef execute_step(state: PlanExecute) -> PlanExecute:\\n    \"\"\"Uses llm with tool binding for the stock related queries\"\"\"\\n    user_query = state[\"input\"]\\n    past_steps = state.get(\"past_steps\", [])\\n    plan = state.get(\"plan\", [])\\n\\n    # Compose system prompt guiding model on stock queries and tool usage\\n    system_prompt = SystemMessage(\\n        content=(\\n            \"You are a stock market expert assistant. Use available tools like get_stock_price \"\\n            \"and get_stock_news to answer queries about stock prices, financial data, \"\\n            \"and related news. Incorporate past steps context when appropriate.\"\\n        )\\n    )\\n\\n    # Prepare messages for model invocation including past steps context\\n    # We add previous step responses for context if any\\n    context = \"\"\\n    if past_steps:\\n        for step, outcome in past_steps:\\n            context += f\"Step: {step}\\\\nOutcome: {outcome}\\\\n\"\\n\\n    # Final user query message augmented by past context if available\\n    final_query = f\"{context}\\\\nUser query: {user_query}\" if context else user_query\\n\\n    messages = [system_prompt, HumanMessage(content=final_query)]\\n\\n    # Invoke model with tools enabled\\n    response = model_with_tools.invoke(messages)\\n\\n    # Extract the text response from the model\\'s output\\n    reply = response.content if hasattr(response, \"content\") else str(response)\\n\\n    # Return updated state with new response, preserving input, plan, and past steps\\n    return {\\n        \"input\": user_query,\\n        \"plan\": plan,\\n        \"past_steps\": past_steps,\\n        \"response\": reply\\n    }\\n```', edge_code='```python\\n# This is a non-conditional edge from \"agent\" node to \"replan\" node.\\n# According to NonConditionalEdges implementation:\\n# Simply add an edge from \"agent\" to \"replan\" without any condition or routing function.\\n\\ngraph.add_edge(\"agent\", \"replan\")\\n```\\nThis code snippet should be added where the graph is constructed to link the \"agent\" node to the \"replan\" node unconditionally, representing the logic \"After executing a step, check if replanning is needed.\"')]}}\n",
      "{'node_process': {'node_reports': [NodeEvaluationReport(node_name='replan', node_code_stub='```python\\nfrom typing import List, Tuple, TypedDict, Union, Annotated\\nimport operator\\nimport json\\n\\n# Define the types as per the provided schema info\\nclass PlanExecute(TypedDict):\\n    input: str\\n    plan: List[str]\\n    past_steps: Annotated[List[Tuple], operator.add]\\n    response: str\\n\\n# Define the output schemas responses\\nclass Response(TypedDict):\\n    response: str\\n\\nclass Plan(TypedDict):\\n    plan: List[str]\\n\\ndef replan_step(state: PlanExecute) -> Union[Response, Plan]:\\n    \"\"\"Evaluates progress and uses an LLM to either revise the plan or generate a final response.\"\"\"\\n    \\n    # Extract relevant information from state\\n    input_task = state[\\'input\\']\\n    past_steps = state[\\'past_steps\\']\\n    current_plan = state[\\'plan\\']\\n\\n    # Format past_steps for prompt insertion, converting each tuple to a descriptive string\\n    # Assuming each past step tuple is like (step_description, step_output)\\n    past_steps_str = []\\n    for step in past_steps:\\n        # Defensive: convert each element in the tuple to string to ensure json serializable\\n        step_desc = str(step[0])\\n        step_out = str(step[1])\\n        past_steps_str.append(f\"Step: {step_desc}, Outcome: {step_out}\")\\n\\n    # Compose the prompt for the LLM as per the prompt_generator instructions\\n    prompt = (\\n        \"You are an intelligent assistant helping to evaluate the progress of a multi-step plan and decide the next action. \"\\n        \"You are given the original input query, the current plan as a list of steps, and the list of past completed steps with their outputs. Your task is to:\\\\n\\\\n\"\\n        \"1. Assess whether the current plan sufficiently addresses the input or if the plan needs to be revised.\\\\n\"\\n        \"2. If the plan needs revision, generate a clear, updated plan as a list of step descriptions.\\\\n\"\\n        \"3. If the plan is complete, generate a final response that satisfies the input.\\\\n\\\\n\"\\n        \"Use the given information carefully:\\\\n\"\\n        f\"- Input (task to solve): {input_task}\\\\n\"\\n        f\"- Past steps and their outcomes: {past_steps_str}\\\\n\"\\n        f\"- Current plan steps: {current_plan}\\\\n\\\\n\"\\n        \"Based on this information, reply with either:\\\\n\"\\n        \"- A revised plan as a list of steps to perform next, or\\\\n\"\\n        \"- A final response answering the input query.\\\\n\\\\n\"\\n        \"Ensure your output matches exactly one of these two JSON structures:\\\\n\\\\n\"\\n        \"To revise plan:\\\\n\"\\n        \\'{{\"plan\": [step1, step2, ...]}}\\\\n\\\\n\\'\\n        \"To finalize response:\\\\n\"\\n        \\'{{\"response\": \"final answer text\"}}\\\\n\\\\n\\'\\n        \"Do not include anything else outside the JSON object.\"\\n    )\\n\\n    # Here we simulate calling the LLM with the prompt and getting a response.\\n    # In practice, this would be a call to some LLM API.\\n    # For the purpose of this code, we will mock this with a placeholder function.\\n\\n    def call_llm(prompt: str) -> str:\\n        \"\"\"\\n        This is a placeholder function simulating an LLM call.\\n        It should return a json string that is either like:\\n        {\"plan\": [...]} or {\"response\": \"...\"}\\n        \"\"\"\\n        # In actual implementation, replace with LLM call\\n        # For safety, return a not implemented response here\\n        raise NotImplementedError(\"LLM call must be implemented with actual LLM API\")\\n\\n    llm_response_str = call_llm(prompt)\\n\\n    # Parse LLM response as JSON\\n    try:\\n        llm_response = json.loads(llm_response_str)\\n    except json.JSONDecodeError:\\n        # If response is malformed, respond with final response indicating error\\n        return {\"response\": \"Error: LLM response was not a valid JSON.\"}\\n\\n    # Detect which output type it is (plan or response)\\n    if \"plan\" in llm_response:\\n        # Validate that plan is a list of strings\\n        plan_list = llm_response[\"plan\"]\\n        if not isinstance(plan_list, list) or not all(isinstance(step, str) for step in plan_list):\\n            return {\"response\": \"Error: Updated plan must be a list of strings.\"}\\n        # Return as Plan type\\n        return {\"plan\": plan_list}\\n\\n    elif \"response\" in llm_response:\\n        # Validate response is a string\\n        response_text = llm_response[\"response\"]\\n        if not isinstance(response_text, str):\\n            return {\"response\": \"Error: Final response must be a string.\"}\\n        # Return as Response type\\n        return {\"response\": response_text}\\n\\n    else:\\n        # If neither key present, error response\\n        return {\"response\": \"Error: LLM response must contain either \\'plan\\' or \\'response\\' key.\"}\\n```', edge_code='```python\\nfrom typing import TypedDict, Union, Literal\\nfrom langgraph.commands import Command\\nfrom langgraph.constants import END\\n\\n# Define the state typing to match the node input structure\\nclass PlanExecute(TypedDict):\\n    input: str\\n    plan: list[str]\\n    past_steps: list[tuple]\\n    response: str\\n\\n# The node function provided in the user\\'s input is named \\'replan\\'\\n# We will implement conditional edges from \\'replan\\' node to either \\'agent\\' or \\'__END__\\'\\n# depending on whether a response is generated by the node.\\n# According to the description:\\n# - If a response is generated (i.e., state[\\'response\\'] is non-empty), then end the process (__END__). \\n# - If no response is generated, continue to \\'agent\\' for further execution.\\n\\n# Here is the routing function to determine which node to route to next from \\'replan\\'\\n\\ndef replan_routing(state: PlanExecute) -> Union[Literal[\"agent\"], Literal[END]]:\\n    \"\"\"\\n    Routing function for conditional edges from \\'replan\\' node.\\n    If \\'response\\' field in state is non-empty, route to END node.\\n    Otherwise route to \\'agent\\' node.\\n    \"\"\"\\n    # Check if a final response has been generated\\n    if state.get(\"response\") and state[\"response\"].strip():\\n        return END\\n    else:\\n        return \"agent\"\\n\\n# Example of adding conditional edges to a LangGraph instance graph:\\n# graph.add_conditional_edges(\"replan\", replan_routing)\\n\\n# The user requested only code snippet implementing edges given the source \\'replan\\' and potential targets \\'agent\\' and \\'__END__\\'\\n# So here is the minimal example with inline comments\\n\\n# Example usage (assuming graph object exists)\\n# graph.add_conditional_edges(\"replan\", replan_routing)\\n```\\nThis code snippet defines the conditional routing logic for the \\'replan\\' node based on whether a response was produced. It sends the flow to \\'agent\\' if no final response exists to keep processing, or to the END node if the response is ready to conclude. This matches the edge information conditions you provided.')]}}\n",
      "{'graph_compile': {'messages': [AIMessage(content='```python\\n# === Node Implementations ===\\n\\n# 1. planner node implementation\\nfrom typing import List\\nfrom pydantic import BaseModel, Field\\nfrom some_llm_module import llm  # placeholder for actual llm import\\n\\nclass PlanOutput(BaseModel):\\n    plan: List[str] = Field(description=\"Generated list of plan steps\")\\n\\ndef plan_step(state: dict) -> dict:\\n    \"\"\"Plan step generates a plan based on the input using llm structured output functionality, stores it to the plan field\"\"\"\\n    structured_llm = llm.with_structured_output(PlanOutput)\\n    # Invoke the LLM to generate the plan based on the input\\n    result: PlanOutput = structured_llm.invoke(state[\\'input\\'])\\n    # Update the state with the generated plan\\n    state[\\'plan\\'] = result.plan\\n    return state\\n\\n\\n# 2. agent node implementation\\nfrom typing import List, Tuple, TypedDict, Annotated\\nfrom langchain_core.tools import tool\\nfrom langchain_core.messages import SystemMessage, HumanMessage\\nfrom langchain.chat_models import ChatAnthropic\\n\\nclass PlanExecute(TypedDict):\\n    input: str\\n    plan: List[str]\\n    past_steps: Annotated[List[Tuple], \"operator.add\"]\\n    response: str\\n\\n# Define stock-related tools\\n\\n@tool\\ndef get_stock_price(ticker: str) -> str:\\n    \"\"\"Tool to get current stock price.\"\"\"\\n    # In practice, this would call an API\\n    stock_data = {\\n        \"AAPL\": \"Apple stock is $150\",\\n        \"GOOG\": \"Google stock is $2800\",\\n        \"TSLA\": \"Tesla stock is $700\"\\n    }\\n    return stock_data.get(ticker.upper(), \"Ticker not found\")\\n\\n@tool\\ndef get_stock_news(ticker: str) -> str:\\n    \"\"\"Tool to get recent news about a stock.\"\"\"\\n    news_data = {\\n        \"AAPL\": \"Apple releases new iPhone 15.\",\\n        \"GOOG\": \"Google introduces new AI features.\",\\n        \"TSLA\": \"Tesla announces new battery tech.\"\\n    }\\n    return news_data.get(ticker.upper(), \"No recent news available\")\\n\\n# List of available tools\\ntools = [get_stock_price, get_stock_news]\\n\\n# Bind model with tools\\nmodel_with_tools = ChatAnthropic(\\n    model=\"claude-3-haiku-20240307\", temperature=0\\n).bind_tools(tools)\\n\\ndef execute_step(state: PlanExecute) -> PlanExecute:\\n    \"\"\"Uses llm with tool binding for the stock related queries\"\"\"\\n    user_query = state[\"input\"]\\n    past_steps = state.get(\"past_steps\", [])\\n    plan = state.get(\"plan\", [])\\n\\n    # Compose system prompt guiding model on stock queries and tool usage\\n    system_prompt = SystemMessage(\\n        content=(\\n            \"You are a stock market expert assistant. Use available tools like get_stock_price \"\\n            \"and get_stock_news to answer queries about stock prices, financial data, \"\\n            \"and related news. Incorporate past steps context when appropriate.\"\\n        )\\n    )\\n\\n    # Prepare messages for model invocation including past steps context\\n    # We add previous step responses for context if any\\n    context = \"\"\\n    if past_steps:\\n        for step, outcome in past_steps:\\n            context += f\"Step: {step}\\\\nOutcome: {outcome}\\\\n\"\\n\\n    # Final user query message augmented by past context if available\\n    final_query = f\"{context}\\\\nUser query: {user_query}\" if context else user_query\\n\\n    messages = [system_prompt, HumanMessage(content=final_query)]\\n\\n    # Invoke model with tools enabled\\n    response = model_with_tools.invoke(messages)\\n\\n    # Extract the text response from the model\\'s output\\n    reply = response.content if hasattr(response, \"content\") else str(response)\\n\\n    # Return updated state with new response, preserving input, plan, and past steps\\n    return {\\n        \"input\": user_query,\\n        \"plan\": plan,\\n        \"past_steps\": past_steps,\\n        \"response\": reply\\n    }\\n\\n\\n# 3. replan node implementation\\nfrom typing import List, Tuple, TypedDict, Union, Annotated\\nimport operator\\nimport json\\n\\nclass PlanExecute(TypedDict):\\n    input: str\\n    plan: List[str]\\n    past_steps: Annotated[List[Tuple], operator.add]\\n    response: str\\n\\nclass Response(TypedDict):\\n    response: str\\n\\nclass Plan(TypedDict):\\n    plan: List[str]\\n\\ndef replan_step(state: PlanExecute) -> Union[Response, Plan]:\\n    \"\"\"Evaluates progress and uses an LLM to either revise the plan or generate a final response.\"\"\"\\n\\n    input_task = state[\\'input\\']\\n    past_steps = state[\\'past_steps\\']\\n    current_plan = state[\\'plan\\']\\n\\n    past_steps_str = []\\n    for step in past_steps:\\n        step_desc = str(step[0])\\n        step_out = str(step[1])\\n        past_steps_str.append(f\"Step: {step_desc}, Outcome: {step_out}\")\\n\\n    prompt = (\\n        \"You are an intelligent assistant helping to evaluate the progress of a multi-step plan and decide the next action. \"\\n        \"You are given the original input query, the current plan as a list of steps, and the list of past completed steps with their outputs. Your task is to:\\\\n\\\\n\"\\n        \"1. Assess whether the current plan sufficiently addresses the input or if the plan needs to be revised.\\\\n\"\\n        \"2. If the plan needs revision, generate a clear, updated plan as a list of step descriptions.\\\\n\"\\n        \"3. If the plan is complete, generate a final response that satisfies the input.\\\\n\\\\n\"\\n        \"Use the given information carefully:\\\\n\"\\n        f\"- Input (task to solve): {input_task}\\\\n\"\\n        f\"- Past steps and their outcomes: {past_steps_str}\\\\n\"\\n        f\"- Current plan steps: {current_plan}\\\\n\\\\n\"\\n        \"Based on this information, reply with either:\\\\n\"\\n        \"- A revised plan as a list of steps to perform next, or\\\\n\"\\n        \"- A final response answering the input query.\\\\n\\\\n\"\\n        \"Ensure your output matches exactly one of these two JSON structures:\\\\n\\\\n\"\\n        \"To revise plan:\\\\n\"\\n        \\'{\"plan\": [step1, step2, ...]}\\\\n\\\\n\\'\\n        \"To finalize response:\\\\n\"\\n        \\'{\"response\": \"final answer text\"}\\\\n\\\\n\\'\\n        \"Do not include anything else outside the JSON object.\"\\n    )\\n\\n    def call_llm(prompt: str) -> str:\\n        \"\"\"\\n        Placeholder for actual LLM call.\\n        Should return a JSON string with either {\"plan\": [...]} or {\"response\": \"...\"}.\\n        \"\"\"\\n        raise NotImplementedError(\"LLM call must be implemented with actual LLM API\")\\n\\n    llm_response_str = call_llm(prompt)\\n\\n    try:\\n        llm_response = json.loads(llm_response_str)\\n    except json.JSONDecodeError:\\n        return {\"response\": \"Error: LLM response was not a valid JSON.\"}\\n\\n    if \"plan\" in llm_response:\\n        plan_list = llm_response[\"plan\"]\\n        if not isinstance(plan_list, list) or not all(isinstance(step, str) for step in plan_list):\\n            return {\"response\": \"Error: Updated plan must be a list of strings.\"}\\n        return {\"plan\": plan_list}\\n\\n    elif \"response\" in llm_response:\\n        response_text = llm_response[\"response\"]\\n        if not isinstance(response_text, str):\\n            return {\"response\": \"Error: Final response must be a string.\"}\\n        return {\"response\": response_text}\\n\\n    else:\\n        return {\"response\": \"Error: LLM response must contain either \\'plan\\' or \\'response\\' key.\"}\\n\\n\\n# === Edge Implementations ===\\n\\n# Edge 1: __START__ -> planner (non-conditional)\\ngraph.add_edge(\"__START__\", \"planner\")\\n\\n# Edge 2: planner -> agent (non-conditional)\\ngraph.add_edge(\"planner\", \"agent\")\\n\\n# Edge 3: agent -> replan (non-conditional)\\ngraph.add_edge(\"agent\", \"replan\")\\n\\n# Edges 4 & 5: replan -> agent or __END__ (conditional)\\nfrom typing import TypedDict, Union, Literal\\nfrom langgraph.constants import END\\n\\ndef replan_routing(state: PlanExecute) -> Union[Literal[\"agent\"], Literal[END]]:\\n    \"\"\"\\n    Routing function for conditional edges from \\'replan\\' node.\\n    If \\'response\\' field in state is non-empty, route to END node.\\n    Otherwise route to \\'agent\\' node.\\n    \"\"\"\\n    if state.get(\"response\") and state[\"response\"].strip():\\n        return END\\n    else:\\n        return \"agent\"\\n\\ngraph.add_conditional_edges(\"replan\", replan_routing)\\n```', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 1835, 'prompt_tokens': 3108, 'total_tokens': 4943, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4.1-mini-2025-04-14', 'system_fingerprint': 'fp_38647f5e19', 'id': 'chatcmpl-BRjZqlrcyzdZY3hjzzArYlyb59fUM', 'finish_reason': 'stop', 'logprobs': None}, id='run-959b4f73-efcb-4727-8f55-908e539132dd-0', usage_metadata={'input_tokens': 3108, 'output_tokens': 1835, 'total_tokens': 4943, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}}\n"
     ]
    }
   ],
   "source": [
    "import uuid\n",
    "uuid_str1= uuid.uuid4()\n",
    "config = {\"configurable\": {\"thread_id\": str(uuid_str1)}}\n",
    "\n",
    "for output in compiler_graph.stream({\"json_objects\": json_objects}, config, stream_mode=\"updates\"\n",
    "    ):\n",
    "        print(output)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1656998b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "llm = ChatOpenAI(model=\"gpt-4.1-mini\", temperature=0)\n",
    "\n",
    "result = llm.invoke(\"write langgraph code for plan-and-execute, use structured output functionality\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bcaf441d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Certainly! Below is an example of how you can write LangGraph code for a **plan-and-execute** pattern using the **structured output** functionality. This example demonstrates a simple agent that first **plans** a sequence of steps to achieve a goal, then **executes** each step in order.\n",
      "\n",
      "```langgraph\n",
      "agent PlanAndExecuteAgent {\n",
      "  input: string goal\n",
      "\n",
      "  // Step 1: Plan - generate a structured plan as a list of steps\n",
      "  step plan {\n",
      "    prompt: \"\"\"\n",
      "    You are an AI assistant. Given the goal below, create a step-by-step plan to achieve it.\n",
      "    Output the plan as a JSON array of steps, each with a \"step_number\" and \"instruction\".\n",
      "\n",
      "    Goal: {goal}\n",
      "\n",
      "    Example output:\n",
      "    [\n",
      "      {\"step_number\": 1, \"instruction\": \"Do this first\"},\n",
      "      {\"step_number\": 2, \"instruction\": \"Then do this\"}\n",
      "    ]\n",
      "\n",
      "    Your output:\n",
      "    \"\"\"\n",
      "    output: json array of {\n",
      "      step_number: int,\n",
      "      instruction: string\n",
      "    }\n",
      "  }\n",
      "\n",
      "  // Step 2: Execute - iterate over the plan steps and execute each instruction\n",
      "  step execute {\n",
      "    input: plan_steps = plan.output\n",
      "    prompt: \"\"\"\n",
      "    You are an AI assistant executing instructions one by one.\n",
      "\n",
      "    Execute the following instruction:\n",
      "    {current_instruction}\n",
      "\n",
      "    Provide the result or output of this step.\n",
      "    \"\"\"\n",
      "    // This step will be called multiple times, once per instruction\n",
      "    // We'll define a loop in the graph to iterate over plan steps\n",
      "    output: string\n",
      "  }\n",
      "\n",
      "  // Control flow: iterate over plan steps and execute them\n",
      "  flow {\n",
      "    plan_output = plan(goal)\n",
      "    results = []\n",
      "    for step in plan_output {\n",
      "      current_instruction = step.instruction\n",
      "      result = execute(current_instruction)\n",
      "      results.append({\n",
      "        step_number: step.step_number,\n",
      "        instruction: current_instruction,\n",
      "        result: result\n",
      "      })\n",
      "    }\n",
      "    return results\n",
      "  }\n",
      "}\n",
      "```\n",
      "\n",
      "### Explanation:\n",
      "\n",
      "- **plan step**: Takes the goal as input and outputs a structured JSON array of steps.\n",
      "- **execute step**: Takes a single instruction and outputs the result of executing that instruction.\n",
      "- **flow**: Runs the plan step, then loops over each planned step, executing it and collecting results.\n",
      "\n",
      "You can adapt the prompts and execution logic to your specific use case. Let me know if you want a runnable example or integration with a specific LLM!\n"
     ]
    }
   ],
   "source": [
    "result.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b5ae7e32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "# LangGraph Code for Plan-and-Execute with Structured Output\n",
      "\n",
      "Below is a complete implementation of a plan-and-execute pattern using LangGraph with structured output functionality. This implementation creates a graph that:\n",
      "\n",
      "1. Takes a user query\n",
      "2. Plans steps to address the query\n",
      "3. Executes each step in sequence\n",
      "4. Provides a final answer\n",
      "\n",
      "```python\n",
      "import os\n",
      "from typing import List, Tuple, TypedDict, Annotated, Literal\n",
      "from datetime import datetime\n",
      "\n",
      "from langchain_core.prompts import ChatPromptTemplate, PromptTemplate\n",
      "from langchain_openai import ChatOpenAI\n",
      "from langchain_core.output_parsers import JsonOutputParser\n",
      "from langchain_core.pydantic_v1 import BaseModel, Field\n",
      "from langgraph.graph import StateGraph, END\n",
      "\n",
      "# Define structured output schemas\n",
      "class PlanStep(BaseModel):\n",
      "    step_number: int = Field(description=\"The number of this step in the plan\")\n",
      "    step_description: str = Field(description=\"Description of what this step will do\")\n",
      "\n",
      "class Plan(BaseModel):\n",
      "    steps: List[PlanStep] = Field(description=\"The sequential steps to solve the problem\")\n",
      "    reasoning: str = Field(description=\"The reasoning behind the plan\")\n",
      "\n",
      "class ExecutionResult(BaseModel):\n",
      "    output: str = Field(description=\"The result of executing the step\")\n",
      "    success: bool = Field(description=\"Whether the step execution was successful\")\n",
      "    notes: str = Field(description=\"Any notes about the execution process or outcome\")\n",
      "\n",
      "class FinalAnswer(BaseModel):\n",
      "    answer: str = Field(description=\"The final answer to the user's query\")\n",
      "    summary: str = Field(description=\"A summary of the steps taken to arrive at the answer\")\n",
      "\n",
      "# Define the state of our graph\n",
      "class AgentState(TypedDict):\n",
      "    query: str\n",
      "    plan: Plan\n",
      "    current_step_index: int\n",
      "    execution_results: List[ExecutionResult]\n",
      "    final_answer: FinalAnswer\n",
      "\n",
      "# Initialize LLM\n",
      "llm = ChatOpenAI(model=\"gpt-4\", temperature=0)\n",
      "\n",
      "# Create prompts with structured output\n",
      "planner_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
      "You are a planning assistant that breaks down complex tasks into specific, concrete steps.\n",
      "\n",
      "Based on the user query, create a detailed plan with sequential steps.\n",
      "\n",
      "USER QUERY:\n",
      "{query}\n",
      "\n",
      "Create a plan with clear, executable steps. Be specific and thorough.\n",
      "\"\"\")\n",
      "\n",
      "executor_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
      "You are an execution assistant that carries out specific steps in a plan.\n",
      "\n",
      "PLAN: {plan}\n",
      "\n",
      "You need to execute the following step:\n",
      "Step {step_number}: {step_description}\n",
      "\n",
      "Previous execution results: {previous_results}\n",
      "\n",
      "Execute just this step and provide the result. Be thorough and detailed in your execution.\n",
      "\"\"\")\n",
      "\n",
      "final_answer_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
      "You are a synthesis assistant that provides final answers based on a plan and its execution.\n",
      "\n",
      "ORIGINAL QUERY: {query}\n",
      "\n",
      "PLAN: {plan}\n",
      "\n",
      "EXECUTION RESULTS:\n",
      "{execution_results}\n",
      "\n",
      "Synthesize the information above to provide a comprehensive final answer to the original query.\n",
      "\"\"\")\n",
      "\n",
      "# Create chains with structured output parsing\n",
      "planner_chain = (\n",
      "    planner_prompt \n",
      "    | llm.with_structured_output(Plan)\n",
      ")\n",
      "\n",
      "executor_chain = (\n",
      "    executor_prompt \n",
      "    | llm.with_structured_output(ExecutionResult)\n",
      ")\n",
      "\n",
      "final_answer_chain = (\n",
      "    final_answer_prompt \n",
      "    | llm.with_structured_output(FinalAnswer)\n",
      ")\n",
      "\n",
      "# Define the graph nodes\n",
      "def create_plan(state: AgentState) -> AgentState:\n",
      "    \"\"\"Create a plan based on the user query.\"\"\"\n",
      "    plan = planner_chain.invoke({\"query\": state[\"query\"]})\n",
      "    return {\"query\": state[\"query\"], \"plan\": plan, \"current_step_index\": 0, \"execution_results\": []}\n",
      "\n",
      "def execute_step(state: AgentState) -> AgentState:\n",
      "    \"\"\"Execute the current step in the plan.\"\"\"\n",
      "    current_index = state[\"current_step_index\"]\n",
      "    current_step = state[\"plan\"].steps[current_index]\n",
      "    \n",
      "    # Format previous results for context\n",
      "    previous_results = \"\\n\".join([\n",
      "        f\"Step {i+1}: {res.output}\"\n"
     ]
    }
   ],
   "source": [
    "from langchain_anthropic import ChatAnthropic\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "model = ChatAnthropic(model='claude-3-7-sonnet-20250219')\n",
    "result = model.invoke(\"write langgraph code for plan-and-execute, use structured output functionality\")\n",
    "result.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7e367551",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Certainly! Below is an example of how you might write LangGraph code to implement a **Plan-and-Execute** pattern. This pattern typically involves two main steps:\n",
      "\n",
      "1. **Plan**: Generate a plan or a sequence of steps to solve a problem.\n",
      "2. **Execute**: Carry out the plan step-by-step, possibly with feedback or verification after each step.\n",
      "\n",
      "---\n",
      "\n",
      "### LangGraph Code for Plan-and-Execute\n",
      "\n",
      "```langgraph\n",
      "graph PlanAndExecute {\n",
      "  \n",
      "  node Input {\n",
      "    type: \"text\"\n",
      "    description: \"User input or problem statement\"\n",
      "  }\n",
      "\n",
      "  node Planner {\n",
      "    type: \"llm\"\n",
      "    description: \"Generate a plan based on the input\"\n",
      "    model: \"gpt-4\"\n",
      "    prompt: \"\"\"\n",
      "    You are a planner. Given the following problem, generate a step-by-step plan to solve it.\n",
      "\n",
      "    Problem:\n",
      "    {Input}\n",
      "\n",
      "    Plan:\n",
      "    1.\n",
      "    \"\"\"\n",
      "  }\n",
      "\n",
      "  node Executor {\n",
      "    type: \"llm\"\n",
      "    description: \"Execute each step of the plan\"\n",
      "    model: \"gpt-4\"\n",
      "    prompt: \"\"\"\n",
      "    You are an executor. Given the following step from a plan, perform the step and provide the result.\n",
      "\n",
      "    Step:\n",
      "    {Planner.step}\n",
      "\n",
      "    Result:\n",
      "    \"\"\"\n",
      "  }\n",
      "\n",
      "  node StepSplitter {\n",
      "    type: \"function\"\n",
      "    description: \"Split the plan into individual steps\"\n",
      "    function: (planText) => {\n",
      "      // Simple split by lines starting with numbers\n",
      "      return planText.split(/\\n\\d+\\.\\s/).filter(s => s.trim().length > 0);\n",
      "    }\n",
      "  }\n",
      "\n",
      "  node Loop {\n",
      "    type: \"loop\"\n",
      "    description: \"Iterate over each step in the plan\"\n",
      "    input: StepSplitter.output\n",
      "    body: Executor\n",
      "  }\n",
      "\n",
      "  edge Input -> Planner\n",
      "  edge Planner.output -> StepSplitter.input\n",
      "  edge StepSplitter.output -> Loop.input\n",
      "  edge Loop.output -> Output\n",
      "\n",
      "  node Output {\n",
      "    type: \"collector\"\n",
      "    description: \"Collect results from executing all steps\"\n",
      "  }\n",
      "}\n",
      "```\n",
      "\n",
      "---\n",
      "\n",
      "### Explanation:\n",
      "\n",
      "- **Input**: The initial problem or user query.\n",
      "- **Planner**: Uses an LLM to generate a step-by-step plan.\n",
      "- **StepSplitter**: A function node that splits the plan text into individual steps.\n",
      "- **Loop**: Iterates over each step, feeding it into the **Executor** node.\n",
      "- **Executor**: Executes each step and returns the result.\n",
      "- **Output**: Collects all execution results.\n",
      "\n",
      "---\n",
      "\n",
      "If you want me to tailor this to a specific use case or add more details (like error handling, verification, or multi-turn interaction), just let me know!\n"
     ]
    }
   ],
   "source": [
    "result.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01384714",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
