{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62b92789",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_objects = {\n",
    "  \"nodes\": {\n",
    "    \"__START__\": {\n",
    "      \"schema_info\": \"\",\n",
    "      \"input_schema\": \"\",\n",
    "      \"output_schema\": \"\",\n",
    "      \"description\": \"Entry point of the graph.\",\n",
    "      \"function_name\": \"\"\n",
    "    },\n",
    "    \"planner\": {\n",
    "      \"schema_info\": \"PlanExecute: TypedDict with fields input (str), plan (List[str]), past_steps (Annotated[List[Tuple], operator.add]), response (str)\",\n",
    "      \"input_schema\": \"PlanExecute\",\n",
    "      \"output_schema\": \"PlanExecute\",\n",
    "      \"description\": \"Plan step generates a plan based on the input using llm structured output functionality, stores it to the plan field\",\n",
    "      \"function_name\": \"plan_step\"\n",
    "    },\n",
    "    \"agent\": {\n",
    "      \"schema_info\": \"PlanExecute: TypedDict with fields input (str), plan (List[str]), past_steps (Annotated[List[Tuple], operator.add]), response (str)\",\n",
    "      \"input_schema\": \"PlanExecute\",\n",
    "      \"output_schema\": \"PlanExecute\",\n",
    "      \"description\": \"Uses llm with tool binding for the stock related queries\",\n",
    "      \"function_name\": \"execute_step\"\n",
    "    },\n",
    "    \"replan\": {\n",
    "      \"schema_info\": \"PlanExecute: TypedDict with fields input (str), plan (List[str]), past_steps (Annotated[List[Tuple], operator.add]), response (str)\",\n",
    "      \"input_schema\": \"PlanExecute\",\n",
    "      \"output_schema\": \"Union[Response, Plan]\",\n",
    "      \"description\": \"Evaluates progress and uses an LLM to either revise the plan or generate a final response.\",\n",
    "      \"function_name\": \"replan_step\"\n",
    "    },\n",
    "    \"__END__\": {\n",
    "      \"schema_info\": \"\",\n",
    "      \"input_schema\": \"\",\n",
    "      \"output_schema\": \"\",\n",
    "      \"description\": \"End point of the graph.\",\n",
    "      \"function_name\": \"\"\n",
    "    }\n",
    "  },\n",
    "  \"edges\": {\n",
    "    \"edge_1\": {\n",
    "      \"source\": \"__START__\",\n",
    "      \"target\": \"planner\",\n",
    "      \"routing_conditions\": \"Start the planning process.\",\n",
    "      \"conditional\": False\n",
    "    },\n",
    "    \"edge_2\": {\n",
    "      \"source\": \"planner\",\n",
    "      \"target\": \"agent\",\n",
    "      \"routing_conditions\": \"After planning, execute the first step.\",\n",
    "      \"conditional\": False\n",
    "    },\n",
    "    \"edge_3\": {\n",
    "      \"source\": \"agent\",\n",
    "      \"target\": \"replan\",\n",
    "      \"routing_conditions\": \"After executing a step, check if replanning is needed.\",\n",
    "      \"conditional\": False\n",
    "    },\n",
    "    \"edge_4\": {\n",
    "      \"source\": \"replan\",\n",
    "      \"target\": \"agent\",\n",
    "      \"routing_conditions\": \"If no response is generated, continue to agent for further execution.\",\n",
    "      \"conditional\": True\n",
    "    },\n",
    "    \"edge_5\": {\n",
    "      \"source\": \"replan\",\n",
    "      \"target\": \"__END__\",\n",
    "      \"routing_conditions\": \"If a response is generated, end the process.\",\n",
    "      \"conditional\": True\n",
    "    }\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb05768b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\agentagent\\AgentAgent\\myenv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import import_ipynb\n",
    "from phase1_edge_handling import edge_builder_agent\n",
    "from phase1_node_to_code import node_to_code_app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cdd16f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# node name to corresponding code stubs\n",
    "node_to_code = {}\n",
    "\n",
    "# edge_to_code \n",
    "# key will be src#dest\n",
    "# contains code snippets generated.\n",
    "edge_to_code = {}\n",
    "\n",
    "skipList = [\"__START__\", \"__END__\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b05b5e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'identify_node': {'node_type': 'planner', 'messages': [HumanMessage(content='\\nYou are provided with the following information about the node:\\n<SchemaInfo>\\nPlanExecute: TypedDict with fields input (str), plan (List[str]), past_steps (Annotated[List[Tuple], operator.add]), response (str)\\n</SchemaInfo>\\n<InputSchema>\\nPlanExecute\\n</InputSchema>\\n<OutputSchema>\\nPlanExecute\\n</OutputSchema>\\n<Description>\\nUses llm with tool binding for the stock related queries\\n</Description>\\n<FunctionName>\\nexecute_step\\n</FunctionName>\\n\\nBelow is the skeleton of the function that you need to implement:\\ndef execute_step(state:PlanExecute) -> PlanExecute:\\n    \"\"\"Uses llm with tool binding for the stock related queries\"\"\"\\n    # Implement the function to meet the description.\\n\\nthe state is of type PlanExecute and the function is of type PlanExecute\\nThe general idea is that the implementation would involve extracting the input from the state, and updating the state with the output. Description contains the logic for this blackbox\\n', additional_kwargs={}, response_metadata={}, id='fd597635-ab91-4673-b8d9-7bd24d801c8a')], 'node_info': '\\nYou are provided with the following information about the node:\\n<SchemaInfo>\\nPlanExecute: TypedDict with fields input (str), plan (List[str]), past_steps (Annotated[List[Tuple], operator.add]), response (str)\\n</SchemaInfo>\\n<InputSchema>\\nPlanExecute\\n</InputSchema>\\n<OutputSchema>\\nPlanExecute\\n</OutputSchema>\\n<Description>\\nUses llm with tool binding for the stock related queries\\n</Description>\\n<FunctionName>\\nexecute_step\\n</FunctionName>\\n\\nBelow is the skeleton of the function that you need to implement:\\ndef execute_step(state:PlanExecute) -> PlanExecute:\\n    \"\"\"Uses llm with tool binding for the stock related queries\"\"\"\\n    # Implement the function to meet the description.\\n\\nthe state is of type PlanExecute and the function is of type PlanExecute\\nThe general idea is that the implementation would involve extracting the input from the state, and updating the state with the output. Description contains the logic for this blackbox\\n'}}\n",
      "{'planner': {'plan': ['Generate a prompt based on the extracted input to handle stock-related queries.', 'Identify and bind the appropriate tools that can be used to answer the stock-related query based on the generated prompt.']}}\n",
      "{'ai_node_gen_supervisor': {'next': 'prompt_generation', 'task': 'Generate a prompt based on the extracted input to handle stock-related queries.', 'messages': [AIMessage(content=\"The first step in the plan is to generate a prompt for handling stock-related queries, which is the responsibility of the 'prompt_generation' worker. This worker will create the necessary prompt based on the provided input.\", additional_kwargs={}, response_metadata={}, id='5ee9b965-d5c6-4226-9f19-2451b5f397e2')]}}\n",
      "{'prompt_generation': {'past_steps': [('Generate a prompt based on the extracted input to handle stock-related queries.', 'To handle stock-related queries effectively with an LLM, I\\'ll generate a prompt that involves active reasoning and relevant examples. The prompt will ask the LLM to analyze stock data and provide insights.\\n\\nHere’s the prompt:\\n\\n\"Please analyze the following stock data and answer the questions based on the given information. \\n\\n1. What is the current trend of the stock?\\n2. How has the stock performed over the last month?\\n3. Are there any significant changes in volume or price?\\n\\nGiven stock data: \\n- Company A: Current Price: $100, Last Month: $95 to $105, Volume: 1M shares.\\n- Company B: Current Price: $50, Last Month: $48 to $52, Volume: 500K shares.\\n\\nPlease provide a detailed analysis based on the data above.\"')], 'messages': [HumanMessage(content='To handle stock-related queries effectively with an LLM, I\\'ll generate a prompt that involves active reasoning and relevant examples. The prompt will ask the LLM to analyze stock data and provide insights.\\n\\nHere’s the prompt:\\n\\n\"Please analyze the following stock data and answer the questions based on the given information. \\n\\n1. What is the current trend of the stock?\\n2. How has the stock performed over the last month?\\n3. Are there any significant changes in volume or price?\\n\\nGiven stock data: \\n- Company A: Current Price: $100, Last Month: $95 to $105, Volume: 1M shares.\\n- Company B: Current Price: $50, Last Month: $48 to $52, Volume: 500K shares.\\n\\nPlease provide a detailed analysis based on the data above.\"', additional_kwargs={}, response_metadata={}, name='prompt_generator', id='94e095e9-14ac-4a4e-9d6c-ed61ddc4b2e5')]}}\n",
      "{'replan': {'plan': ['Identify and bind the appropriate tools that can be used to answer the stock-related query based on the generated prompt.']}}\n",
      "{'ai_node_gen_supervisor': {'next': 'toolset_generation', 'task': 'Identify and bind the appropriate tools that can be used to answer the stock-related query based on the generated prompt.', 'messages': [AIMessage(content=\"The next step requires identifying and binding appropriate tools for stock-related queries, which falls under the responsibilities of the 'toolset_generation' worker.\", additional_kwargs={}, response_metadata={}, id='bb627530-92ec-4b46-a388-39089533b4f3')]}}\n",
      "{'toolset_generation': {'past_steps': [('Identify and bind the appropriate tools that can be used to answer the stock-related query based on the generated prompt.', '```python\\nfrom typing import List, Tuple\\nfrom langgraph.graph import StateGraph, MessagesState, START, END\\nfrom langchain_core.messages import SystemMessage\\nfrom langchain_core.tools import tool\\nfrom langgraph.prebuilt import ToolNode\\n\\n# Define the tools needed by the LLM for stock-related queries\\n\\n@tool\\ndef analyze_stock_data(stock_data: str):\\n    \"\"\"Analyze the provided stock data and return insights.\"\"\"\\n    # Placeholder for actual analysis\\n    # A more sophisticated implementation would analyze the stock data.\\n    return f\"Based on the data provided: {stock_data}, trend analysis is needed.\"\\n\\ntools = [analyze_stock_data]\\n\\n# Bind the model(llm) with tools\\nmodel_with_tools = ChatAnthropic(\\n    model=\"claude-3-haiku-20240307\", temperature=0\\n).bind_tools(tools)\\n\\ndef execute_step(state: PlanExecute) -> PlanExecute:\\n    \"\"\"Uses llm with tool binding for the stock related queries\"\"\"\\n    # Extract the input from state\\n    input_query = state[\\'input\\']\\n    \\n    # Create a prompt using the stock data provided\\n    prompt = f\"\"\"\\n    Please analyze the following stock data and answer the questions based on the given information.\\n\\n    Given stock data: {input_query}\\n    \\n    Please provide a detailed analysis based on the data above.\\n    \"\"\"\\n    \\n    # Prepare the messages for the model\\n    response = model_with_tools.invoke([SystemMessage(content=prompt)])\\n    \\n    # Update the state with the response\\n    state[\\'response\\'] = response.content\\n    state[\\'past_steps\\'].append((\\'analyze_stock_data\\', prompt))\\n    \\n    return state\\n\\n# Stategraph compilation\\nworkflow = StateGraph(MessagesState)\\n\\n# Generate a tool node.\\ntool_node = ToolNode(tools)\\n\\n# conditional edge\\ndef should_continue(state: MessagesState):\\n    messages = state[\"messages\"]\\n    last_message = messages[-1]\\n    if last_message.tool_calls:\\n        return \"tools\"\\n    return END\\n\\n# Define the two nodes we will cycle between\\nworkflow.add_node(\"agent\", execute_step)\\nworkflow.add_node(\"tools\", tool_node)\\n\\nworkflow.add_edge(START, \"agent\")\\nworkflow.add_conditional_edges(\"agent\", should_continue, [\"tools\", END])\\nworkflow.add_edge(\"tools\", \"agent\")\\napp = workflow.compile()\\n```')], 'messages': [HumanMessage(content='```python\\nfrom typing import List, Tuple\\nfrom langgraph.graph import StateGraph, MessagesState, START, END\\nfrom langchain_core.messages import SystemMessage\\nfrom langchain_core.tools import tool\\nfrom langgraph.prebuilt import ToolNode\\n\\n# Define the tools needed by the LLM for stock-related queries\\n\\n@tool\\ndef analyze_stock_data(stock_data: str):\\n    \"\"\"Analyze the provided stock data and return insights.\"\"\"\\n    # Placeholder for actual analysis\\n    # A more sophisticated implementation would analyze the stock data.\\n    return f\"Based on the data provided: {stock_data}, trend analysis is needed.\"\\n\\ntools = [analyze_stock_data]\\n\\n# Bind the model(llm) with tools\\nmodel_with_tools = ChatAnthropic(\\n    model=\"claude-3-haiku-20240307\", temperature=0\\n).bind_tools(tools)\\n\\ndef execute_step(state: PlanExecute) -> PlanExecute:\\n    \"\"\"Uses llm with tool binding for the stock related queries\"\"\"\\n    # Extract the input from state\\n    input_query = state[\\'input\\']\\n    \\n    # Create a prompt using the stock data provided\\n    prompt = f\"\"\"\\n    Please analyze the following stock data and answer the questions based on the given information.\\n\\n    Given stock data: {input_query}\\n    \\n    Please provide a detailed analysis based on the data above.\\n    \"\"\"\\n    \\n    # Prepare the messages for the model\\n    response = model_with_tools.invoke([SystemMessage(content=prompt)])\\n    \\n    # Update the state with the response\\n    state[\\'response\\'] = response.content\\n    state[\\'past_steps\\'].append((\\'analyze_stock_data\\', prompt))\\n    \\n    return state\\n\\n# Stategraph compilation\\nworkflow = StateGraph(MessagesState)\\n\\n# Generate a tool node.\\ntool_node = ToolNode(tools)\\n\\n# conditional edge\\ndef should_continue(state: MessagesState):\\n    messages = state[\"messages\"]\\n    last_message = messages[-1]\\n    if last_message.tool_calls:\\n        return \"tools\"\\n    return END\\n\\n# Define the two nodes we will cycle between\\nworkflow.add_node(\"agent\", execute_step)\\nworkflow.add_node(\"tools\", tool_node)\\n\\nworkflow.add_edge(START, \"agent\")\\nworkflow.add_conditional_edges(\"agent\", should_continue, [\"tools\", END])\\nworkflow.add_edge(\"tools\", \"agent\")\\napp = workflow.compile()\\n```', additional_kwargs={}, response_metadata={}, name='toolset_generator', id='b9a2c5ea-588e-4115-a586-e44e7e390af3')]}}\n",
      "{'replan': {'response': 'All steps in the OriginalPlan have been successfully executed.'}}\n",
      "{'code_compiler': {'final_code': '```python\\nfrom typing import List, Tuple, Annotated, TypedDict\\nfrom langchain_core.messages import SystemMessage\\nfrom langchain_core.tools import tool\\nfrom langgraph.graph import StateGraph, MessagesState, START, END\\nfrom langgraph.prebuilt import ToolNode\\n\\n# Defining the PlanExecute TypedDict according to the provided schema\\nclass PlanExecute(TypedDict):\\n    input: str\\n    plan: List[str]\\n    past_steps: Annotated[List[Tuple], operator.add]\\n    response: str\\n\\n# Define the tools needed by the LLM for stock-related queries\\n@tool\\ndef analyze_stock_data(stock_data: str):\\n    \"\"\"Analyze the provided stock data and return insights.\"\"\"\\n    # Placeholder for actual analysis\\n    return f\"Based on the data provided: {stock_data}, trend analysis is needed.\"\\n\\n# Bind the model(llm) with tools\\nmodel_with_tools = ChatAnthropic(\\n    model=\"claude-3-haiku-20240307\", temperature=0\\n).bind_tools([analyze_stock_data])\\n\\ndef execute_step(state: PlanExecute) -> PlanExecute:\\n    \"\"\"Uses llm with tool binding for the stock related queries\"\"\"\\n    # Extract the input from state\\n    input_query = state[\\'input\\']\\n    \\n    # Create a prompt using the stock data provided\\n    prompt = f\"\"\"\\n    Please analyze the following stock data and answer the questions based on the given information.\\n\\n    Given stock data: {input_query}\\n\\n    Please provide a detailed analysis based on the data above.\\n    \"\"\"\\n    \\n    # Prepare the messages for the model\\n    response = model_with_tools.invoke([SystemMessage(content=prompt)])\\n    \\n    # Update the state with the response\\n    state[\\'response\\'] = response.content\\n    state[\\'past_steps\\'].append((\\'analyze_stock_data\\', prompt))\\n    \\n    return state\\n\\n# Stategraph compilation\\nworkflow = StateGraph(MessagesState)\\n\\n# Generate a tool node.\\ntool_node = ToolNode([analyze_stock_data])\\n\\n# Conditional check to determine edge direction\\ndef should_continue(state: MessagesState):\\n    messages = state[\"messages\"]\\n    last_message = messages[-1]\\n    if last_message.tool_calls:\\n        return \"tools\"\\n    return END\\n\\n# Define the two nodes we will cycle between\\nworkflow.add_node(\"agent\", execute_step)\\nworkflow.add_node(\"tools\", tool_node)\\n\\n# Set up the workflow edges\\nworkflow.add_edge(START, \"agent\")\\nworkflow.add_conditional_edges(\"agent\", should_continue, [\"tools\", END])\\nworkflow.add_edge(\"tools\", \"agent\")\\napp = workflow.compile()\\n```'}}\n"
     ]
    }
   ],
   "source": [
    "import uuid\n",
    "\n",
    "uuid_str = uuid.uuid4()\n",
    "config = {\"configurable\": {\"thread_id\": str(uuid_str)}}\n",
    "\n",
    "for output in node_to_code_app.stream(json_objects[\"nodes\"][\"agent\"], config, stream_mode=\"updates\"):\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a5a83516",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.messages import HumanMessage\n",
    "edge_info_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "<GraphNodeImplementation>\n",
    "{node_code}\n",
    "</GraphNodeImplementation>\n",
    "<EdgeInformation>\n",
    "{edge_json}\n",
    "</Edgeinformation>\"\"\")\n",
    "\n",
    "def dfs_traverse(json_objects, current_node, visited=None):\n",
    "    if visited is None:\n",
    "        visited = set()  # Initialize the visited set\n",
    "\n",
    "    # If we encounter a node we've already visited, we skip to avoid infinite loops\n",
    "    if current_node in visited:\n",
    "        return\n",
    "\n",
    "    visited.add(current_node)  # Mark the current node as visited\n",
    "    print(f\"Visiting node: {current_node}\")\n",
    "\n",
    "    # Get edges originating from the current node\n",
    "    edges = json_objects[\"edges\"]\n",
    "    nodes = json_objects[\"nodes\"]\n",
    "    outgoing_edges = list(filter(lambda edge: edge[\"source\"] == current_node, edges.values()))\n",
    "    \n",
    "    if current_node not in skipList:\n",
    "        uuid_str = uuid.uuid4()\n",
    "        config = {\"configurable\": {\"thread_id\": str(uuid_str)}}\n",
    "\n",
    "        for output in node_to_code_app.stream(nodes[current_node], config, stream_mode=\"updates\"):\n",
    "            print(output)\n",
    "        code= node_to_code_app.get_state(config).values[\"final_code\"]\n",
    "        result = edge_builder_agent.invoke({\"messages\": [HumanMessage(content = edge_info_prompt.format(node_code=code,edge_json=outgoing_edges))]},config)\n",
    "        node_to_code[current_node] = code\n",
    "        edge_to_code[current_node] = result[\"messages\"][-1].content\n",
    "    for edge_key, edge_value in edges.items():\n",
    "        if edge_value[\"source\"] == current_node:  # Check if the edge originates from the current node\n",
    "            next_node = edge_value[\"target\"]\n",
    "            print(f\"Following edge from {current_node} to {next_node}\")\n",
    "            dfs_traverse(json_objects, next_node, visited)  # Recursively call DFS on the next node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d3ff4d35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visiting node: __START__\n",
      "Following edge from __START__ to planner\n",
      "Visiting node: planner\n",
      "{'identify_node': {'node_type': 'planner', 'messages': [HumanMessage(content='\\nYou are provided with the following information about the node:\\n<SchemaInfo>\\nPlanExecute: TypedDict with fields input (str), plan (List[str]), past_steps (Annotated[List[Tuple], operator.add]), response (str)\\n</SchemaInfo>\\n<InputSchema>\\nPlanExecute\\n</InputSchema>\\n<OutputSchema>\\nPlanExecute\\n</OutputSchema>\\n<Description>\\nPlan step generates a plan based on the input using llm structured output functionality, stores it to the plan field\\n</Description>\\n<FunctionName>\\nplan_step\\n</FunctionName>\\n\\nBelow is the skeleton of the function that you need to implement:\\ndef plan_step(state:PlanExecute) -> PlanExecute:\\n    \"\"\"Plan step generates a plan based on the input using llm structured output functionality, stores it to the plan field\"\"\"\\n    # Implement the function to meet the description.\\n\\nthe state is of type PlanExecute and the function is of type PlanExecute\\nThe general idea is that the implementation would involve extracting the input from the state, and updating the state with the output. Description contains the logic for this blackbox\\n', additional_kwargs={}, response_metadata={}, id='42acc735-c61c-4c97-82b5-6e9b5cb36dbf')], 'node_info': '\\nYou are provided with the following information about the node:\\n<SchemaInfo>\\nPlanExecute: TypedDict with fields input (str), plan (List[str]), past_steps (Annotated[List[Tuple], operator.add]), response (str)\\n</SchemaInfo>\\n<InputSchema>\\nPlanExecute\\n</InputSchema>\\n<OutputSchema>\\nPlanExecute\\n</OutputSchema>\\n<Description>\\nPlan step generates a plan based on the input using llm structured output functionality, stores it to the plan field\\n</Description>\\n<FunctionName>\\nplan_step\\n</FunctionName>\\n\\nBelow is the skeleton of the function that you need to implement:\\ndef plan_step(state:PlanExecute) -> PlanExecute:\\n    \"\"\"Plan step generates a plan based on the input using llm structured output functionality, stores it to the plan field\"\"\"\\n    # Implement the function to meet the description.\\n\\nthe state is of type PlanExecute and the function is of type PlanExecute\\nThe general idea is that the implementation would involve extracting the input from the state, and updating the state with the output. Description contains the logic for this blackbox\\n'}}\n",
      "{'planner': {'plan': ['Generate a prompt based on the extracted input.', 'Use structured output functionality to create a structured plan based on the generated prompt.']}}\n",
      "{'ai_node_gen_supervisor': {'next': 'prompt_generation', 'task': 'Generate a prompt based on the extracted input.', 'messages': [AIMessage(content=\"The next step requires generating a prompt based on the extracted input, which is the responsibility of the 'prompt_generation' worker.\", additional_kwargs={}, response_metadata={}, id='a4f42522-242f-498c-9dff-b02738db72b6')]}}\n",
      "{'prompt_generation': {'past_steps': [('Generate a prompt based on the extracted input.', 'Generate a detailed plan based on the following input, formatted clearly in structured output. The plan should outline specific steps to accomplish the task described. Consider including the objectives, key actions, responsible parties, and a timeline for execution.\\n\\n**Input:**\\n\"Please create a strategic plan for organizing a community event aimed at promoting local businesses.\"\\n\\n**Output Format:**\\n- Objective: [Main goal of the event]\\n- Key Actions: [List of critical actions to undertake]\\n- Responsible Parties: [Who is responsible for each action]\\n- Timeline: [When each action should be completed]\\n\\nMake sure to include metrics for success and any potential challenges that might arise, along with proposed solutions.')], 'messages': [HumanMessage(content='Generate a detailed plan based on the following input, formatted clearly in structured output. The plan should outline specific steps to accomplish the task described. Consider including the objectives, key actions, responsible parties, and a timeline for execution.\\n\\n**Input:**\\n\"Please create a strategic plan for organizing a community event aimed at promoting local businesses.\"\\n\\n**Output Format:**\\n- Objective: [Main goal of the event]\\n- Key Actions: [List of critical actions to undertake]\\n- Responsible Parties: [Who is responsible for each action]\\n- Timeline: [When each action should be completed]\\n\\nMake sure to include metrics for success and any potential challenges that might arise, along with proposed solutions.', additional_kwargs={}, response_metadata={}, name='prompt_generator', id='a6e902e9-b7f9-4fae-b924-01aaf9fd5518')]}}\n",
      "{'replan': {'plan': ['Use structured output functionality to create a structured plan based on the generated prompt.']}}\n",
      "{'ai_node_gen_supervisor': {'next': 'structured_output_generation', 'task': 'Use structured output functionality to create a structured plan based on the generated prompt.', 'messages': [AIMessage(content=\"The next logical step is to call the 'structured_output_generation' worker, as this worker is responsible for creating structured outputs based on the generated prompt, which aligns directly with the need to generate a structured plan as per the description.\", additional_kwargs={}, response_metadata={}, id='708980d2-3173-4a06-ba39-bc8ac29a407a')]}}\n",
      "{'structured_output_generation': {'past_steps': [('Use structured output functionality to create a structured plan based on the generated prompt.', 'Given the provided information, it is determined that the node requires the LLM\\'s structured output functionality. The LLM will generate a detailed plan based on the input while adhering to the defined structure. Below is the implementation for the `plan_step` function using structured output:\\n\\n```python\\nfrom typing import List, Tuple, Optional\\nfrom pydantic import BaseModel, Field\\n\\n# Pydantic class for structured output\\nclass PlanOutput(BaseModel):\\n    objective: str = Field(description=\"Main goal of the event\")\\n    key_actions: List[str] = Field(description=\"Critical actions to undertake\")\\n    responsible_parties: List[str] = Field(description=\"Who is responsible for each action\")\\n    timeline: List[str] = Field(description=\"When each action should be completed\")\\n    success_metrics: Optional[str] = Field(default=None, description=\"Metrics for success\")\\n    challenges: Optional[List[str]] = Field(default=None, description=\"Potential challenges\")\\n    proposed_solutions: Optional[List[str]] = Field(default=None, description=\"Proposed solutions for challenges\")\\n\\nclass PlanExecute(BaseModel):\\n    input: str\\n    plan: List[str]\\n    past_steps: List[Tuple]\\n    response: str\\n\\ndef plan_step(state: PlanExecute) -> PlanExecute:\\n    \"\"\"Plan step generates a plan based on the input using llm structured output functionality, stores it to the plan field\"\"\"\\n    structured_llm = llm.with_structured_output(PlanOutput)\\n    plan_output: PlanOutput = structured_llm.invoke(state.input)\\n    \\n    # Update the state with the generated plan\\n    state.plan = [\\n        plan_output.objective,\\n        plan_output.key_actions,\\n        plan_output.responsible_parties,\\n        plan_output.timeline,\\n        plan_output.success_metrics,\\n        plan_output.challenges,\\n        plan_output.proposed_solutions\\n    ]\\n    state.response = \"Plan generated successfully.\"\\n    \\n    return state\\n```\\n\\nThis implementation ensures that the LLM generates a detailed structured plan based on the input, following the required format. The input is taken from the state, and the output is stored back into the state, fulfilling all requirements of the task.')], 'messages': [HumanMessage(content='Given the provided information, it is determined that the node requires the LLM\\'s structured output functionality. The LLM will generate a detailed plan based on the input while adhering to the defined structure. Below is the implementation for the `plan_step` function using structured output:\\n\\n```python\\nfrom typing import List, Tuple, Optional\\nfrom pydantic import BaseModel, Field\\n\\n# Pydantic class for structured output\\nclass PlanOutput(BaseModel):\\n    objective: str = Field(description=\"Main goal of the event\")\\n    key_actions: List[str] = Field(description=\"Critical actions to undertake\")\\n    responsible_parties: List[str] = Field(description=\"Who is responsible for each action\")\\n    timeline: List[str] = Field(description=\"When each action should be completed\")\\n    success_metrics: Optional[str] = Field(default=None, description=\"Metrics for success\")\\n    challenges: Optional[List[str]] = Field(default=None, description=\"Potential challenges\")\\n    proposed_solutions: Optional[List[str]] = Field(default=None, description=\"Proposed solutions for challenges\")\\n\\nclass PlanExecute(BaseModel):\\n    input: str\\n    plan: List[str]\\n    past_steps: List[Tuple]\\n    response: str\\n\\ndef plan_step(state: PlanExecute) -> PlanExecute:\\n    \"\"\"Plan step generates a plan based on the input using llm structured output functionality, stores it to the plan field\"\"\"\\n    structured_llm = llm.with_structured_output(PlanOutput)\\n    plan_output: PlanOutput = structured_llm.invoke(state.input)\\n    \\n    # Update the state with the generated plan\\n    state.plan = [\\n        plan_output.objective,\\n        plan_output.key_actions,\\n        plan_output.responsible_parties,\\n        plan_output.timeline,\\n        plan_output.success_metrics,\\n        plan_output.challenges,\\n        plan_output.proposed_solutions\\n    ]\\n    state.response = \"Plan generated successfully.\"\\n    \\n    return state\\n```\\n\\nThis implementation ensures that the LLM generates a detailed structured plan based on the input, following the required format. The input is taken from the state, and the output is stored back into the state, fulfilling all requirements of the task.', additional_kwargs={}, response_metadata={}, name='struct_output_generator', id='06c35504-7cb9-49ec-bbe8-c4af843f24e0')]}}\n",
      "{'replan': {'response': 'All steps in the OriginalPlan have been successfully executed. The structured plan has been created based on the generated prompt.'}}\n",
      "{'code_compiler': {'final_code': '```python\\nfrom typing import List, Tuple, Optional\\nfrom pydantic import BaseModel, Field\\n\\n# Pydantic class for structured output\\nclass PlanOutput(BaseModel):\\n    objective: str = Field(description=\"Main goal of the event\")\\n    key_actions: List[str] = Field(description=\"Critical actions to undertake\")\\n    responsible_parties: List[str] = Field(description=\"Who is responsible for each action\")\\n    timeline: List[str] = Field(description=\"When each action should be completed\")\\n    success_metrics: Optional[str] = Field(default=None, description=\"Metrics for success\")\\n    challenges: Optional[List[str]] = Field(default=None, description=\"Potential challenges\")\\n    proposed_solutions: Optional[List[str]] = Field(default=None, description=\"Proposed solutions for challenges\")\\n\\nclass PlanExecute(BaseModel):\\n    input: str\\n    plan: List[str]\\n    past_steps: List[Tuple]\\n    response: str\\n\\ndef plan_step(state: PlanExecute) -> PlanExecute:\\n    \"\"\"Plan step generates a plan based on the input using llm structured output functionality, stores it to the plan field\"\"\"\\n    structured_llm = llm.with_structured_output(PlanOutput)\\n    plan_output: PlanOutput = structured_llm.invoke(state.input)\\n    \\n    # Update the state with the generated plan\\n    state.plan = [\\n        plan_output.objective,\\n        plan_output.key_actions,\\n        plan_output.responsible_parties,\\n        plan_output.timeline,\\n        plan_output.success_metrics,\\n        plan_output.challenges,\\n        plan_output.proposed_solutions\\n    ]\\n    state.response = \"Plan generated successfully.\"\\n    \\n    return state\\n```'}}\n",
      "Following edge from planner to agent\n",
      "Visiting node: agent\n",
      "{'identify_node': {'node_type': 'planner', 'messages': [HumanMessage(content='\\nYou are provided with the following information about the node:\\n<SchemaInfo>\\nPlanExecute: TypedDict with fields input (str), plan (List[str]), past_steps (Annotated[List[Tuple], operator.add]), response (str)\\n</SchemaInfo>\\n<InputSchema>\\nPlanExecute\\n</InputSchema>\\n<OutputSchema>\\nPlanExecute\\n</OutputSchema>\\n<Description>\\nUses llm with tool binding for the stock related queries\\n</Description>\\n<FunctionName>\\nexecute_step\\n</FunctionName>\\n\\nBelow is the skeleton of the function that you need to implement:\\ndef execute_step(state:PlanExecute) -> PlanExecute:\\n    \"\"\"Uses llm with tool binding for the stock related queries\"\"\"\\n    # Implement the function to meet the description.\\n\\nthe state is of type PlanExecute and the function is of type PlanExecute\\nThe general idea is that the implementation would involve extracting the input from the state, and updating the state with the output. Description contains the logic for this blackbox\\n', additional_kwargs={}, response_metadata={}, id='1f5198f9-9bcf-4570-b215-c542ef8ab36f')], 'node_info': '\\nYou are provided with the following information about the node:\\n<SchemaInfo>\\nPlanExecute: TypedDict with fields input (str), plan (List[str]), past_steps (Annotated[List[Tuple], operator.add]), response (str)\\n</SchemaInfo>\\n<InputSchema>\\nPlanExecute\\n</InputSchema>\\n<OutputSchema>\\nPlanExecute\\n</OutputSchema>\\n<Description>\\nUses llm with tool binding for the stock related queries\\n</Description>\\n<FunctionName>\\nexecute_step\\n</FunctionName>\\n\\nBelow is the skeleton of the function that you need to implement:\\ndef execute_step(state:PlanExecute) -> PlanExecute:\\n    \"\"\"Uses llm with tool binding for the stock related queries\"\"\"\\n    # Implement the function to meet the description.\\n\\nthe state is of type PlanExecute and the function is of type PlanExecute\\nThe general idea is that the implementation would involve extracting the input from the state, and updating the state with the output. Description contains the logic for this blackbox\\n'}}\n",
      "{'planner': {'plan': ['Generate a prompt based on the extracted input to handle stock-related queries.', 'Bind the generated prompt with appropriate tools for stock information retrieval.']}}\n",
      "{'ai_node_gen_supervisor': {'next': 'prompt_generation', 'task': 'Generate a prompt based on the extracted input to handle stock-related queries.', 'messages': [AIMessage(content=\"The first step in the plan is to generate a prompt based on the extracted input for stock-related queries. Therefore, the next worker to act should be 'prompt_generation' to fulfill this requirement.\", additional_kwargs={}, response_metadata={}, id='bfaf7ec8-4c67-4678-bb00-ac64e92b467a')]}}\n",
      "{'prompt_generation': {'past_steps': [('Generate a prompt based on the extracted input to handle stock-related queries.', 'Based on the retrieved information about prompting techniques relevant to stock-related queries, I will craft a prompt that utilizes these insights. The goal is to create a structured input for the LLM to generate a response about stocks effectively. \\n\\nHere’s the prompt:\\n\\n---\\n\\n\"Given the following stock-related query, please provide a detailed analysis and response: \\n\\nInput: [INSERT USER\\'S STOCK QUERY HERE]\\n\\nPlease consider the current market trends, historical data, and any relevant financial metrics while formulating your response. Include insights on potential impacts on the stock market and any recommendations based on your analysis.\" \\n\\n--- \\n\\nMake sure to replace `[INSERT USER\\'S STOCK QUERY HERE]` with the actual user input from the `PlanExecute` state.')], 'messages': [HumanMessage(content='Based on the retrieved information about prompting techniques relevant to stock-related queries, I will craft a prompt that utilizes these insights. The goal is to create a structured input for the LLM to generate a response about stocks effectively. \\n\\nHere’s the prompt:\\n\\n---\\n\\n\"Given the following stock-related query, please provide a detailed analysis and response: \\n\\nInput: [INSERT USER\\'S STOCK QUERY HERE]\\n\\nPlease consider the current market trends, historical data, and any relevant financial metrics while formulating your response. Include insights on potential impacts on the stock market and any recommendations based on your analysis.\" \\n\\n--- \\n\\nMake sure to replace `[INSERT USER\\'S STOCK QUERY HERE]` with the actual user input from the `PlanExecute` state.', additional_kwargs={}, response_metadata={}, name='prompt_generator', id='5fa2863c-219c-4648-a035-3b4968561b12')]}}\n",
      "{'replan': {'plan': ['Bind the generated prompt with appropriate tools for stock information retrieval.']}}\n",
      "{'ai_node_gen_supervisor': {'next': 'toolset_generation', 'task': 'Bind the generated prompt with appropriate tools for stock information retrieval.', 'messages': [AIMessage(content='To bind the generated prompt with appropriate tools for stock information retrieval, we first need to generate the necessary tools. The toolset_generation worker is responsible for providing the specific tools required for stock queries.', additional_kwargs={}, response_metadata={}, id='2a2d8291-19e5-4728-9a05-0d1d95cabd75')]}}\n",
      "{'toolset_generation': {'past_steps': [('Bind the generated prompt with appropriate tools for stock information retrieval.', '```python\\nfrom typing import List, Tuple\\nfrom langgraph.graph import StateGraph, MessagesState, START, END\\nfrom langchain_core.messages import SystemMessage\\nfrom langchain_core.tools import tool\\nfrom langgraph.prebuilt import ToolNode\\n\\n# Define the tools needed by the LLM for stock-related queries\\n\\n@tool\\ndef get_stock_price(symbol: str):\\n    \"\"\"Call to get the current stock price of a given symbol.\"\"\"\\n    # Here you would have code that fetches the stock price from an API\\n    return f\"The current price of {symbol} is $XYZ.\"  # Replace XYZ with actual data\\n\\n@tool\\ndef get_market_trends():\\n    \"\"\"Get current market trends\"\"\"\\n    return \"Bullish trends observed in technology and consumer goods sectors.\"\\n\\ntools = [get_stock_price, get_market_trends]\\n\\n# Bind the model(llm) with tools\\nmodel_with_tools = ChatAnthropic(\\n    model=\"claude-3-haiku-20240307\", temperature=0\\n).bind_tools(tools)\\n\\n# Generate a tool node.\\ntool_node = ToolNode(tools)\\n\\n# Node implementation for executing stock-related queries\\ndef execute_step(state: PlanExecute) -> PlanExecute:\\n    \"\"\"Uses llm with tool binding for the stock related queries\"\"\"\\n    user_query = state[\"input\"]\\n    response = model_with_tools.invoke([\\n        SystemMessage(content=f\"Given the following stock-related query, please provide a detailed analysis and response: Input: {user_query}. Please consider the current market trends, historical data, and any relevant financial metrics while formulating your response.\")\\n    ])\\n    \\n    # Update the state with the response generated by the LLM\\n    state[\"response\"] = response\\n    return state\\n\\n# Stategraph compilation\\nworkflow = StateGraph(MessagesState)\\n\\n# Define the nodes we will cycle between\\nworkflow.add_node(\"execute\", execute_step)\\nworkflow.add_node(\"tools\", tool_node)\\n\\n# Conditions for transitioning between nodes\\ndef should_continue(state: MessagesState):\\n    messages = state[\"messages\"]\\n    last_message = messages[-1]\\n    if last_message.tool_calls:\\n        return \"tools\"\\n    return END\\n\\nworkflow.add_edge(START, \"execute\")\\nworkflow.add_conditional_edges(\"execute\", should_continue, [\"tools\", END])\\nworkflow.add_edge(\"tools\", \"execute\")\\napp = workflow.compile()\\n```')], 'messages': [HumanMessage(content='```python\\nfrom typing import List, Tuple\\nfrom langgraph.graph import StateGraph, MessagesState, START, END\\nfrom langchain_core.messages import SystemMessage\\nfrom langchain_core.tools import tool\\nfrom langgraph.prebuilt import ToolNode\\n\\n# Define the tools needed by the LLM for stock-related queries\\n\\n@tool\\ndef get_stock_price(symbol: str):\\n    \"\"\"Call to get the current stock price of a given symbol.\"\"\"\\n    # Here you would have code that fetches the stock price from an API\\n    return f\"The current price of {symbol} is $XYZ.\"  # Replace XYZ with actual data\\n\\n@tool\\ndef get_market_trends():\\n    \"\"\"Get current market trends\"\"\"\\n    return \"Bullish trends observed in technology and consumer goods sectors.\"\\n\\ntools = [get_stock_price, get_market_trends]\\n\\n# Bind the model(llm) with tools\\nmodel_with_tools = ChatAnthropic(\\n    model=\"claude-3-haiku-20240307\", temperature=0\\n).bind_tools(tools)\\n\\n# Generate a tool node.\\ntool_node = ToolNode(tools)\\n\\n# Node implementation for executing stock-related queries\\ndef execute_step(state: PlanExecute) -> PlanExecute:\\n    \"\"\"Uses llm with tool binding for the stock related queries\"\"\"\\n    user_query = state[\"input\"]\\n    response = model_with_tools.invoke([\\n        SystemMessage(content=f\"Given the following stock-related query, please provide a detailed analysis and response: Input: {user_query}. Please consider the current market trends, historical data, and any relevant financial metrics while formulating your response.\")\\n    ])\\n    \\n    # Update the state with the response generated by the LLM\\n    state[\"response\"] = response\\n    return state\\n\\n# Stategraph compilation\\nworkflow = StateGraph(MessagesState)\\n\\n# Define the nodes we will cycle between\\nworkflow.add_node(\"execute\", execute_step)\\nworkflow.add_node(\"tools\", tool_node)\\n\\n# Conditions for transitioning between nodes\\ndef should_continue(state: MessagesState):\\n    messages = state[\"messages\"]\\n    last_message = messages[-1]\\n    if last_message.tool_calls:\\n        return \"tools\"\\n    return END\\n\\nworkflow.add_edge(START, \"execute\")\\nworkflow.add_conditional_edges(\"execute\", should_continue, [\"tools\", END])\\nworkflow.add_edge(\"tools\", \"execute\")\\napp = workflow.compile()\\n```', additional_kwargs={}, response_metadata={}, name='toolset_generator', id='6223ddb3-8532-43b5-b12c-47130d62039a')]}}\n",
      "{'replan': {'response': 'The original plan has been successfully completed. The prompt for handling stock-related queries has been generated and bound with the appropriate tools for stock information retrieval.'}}\n",
      "{'code_compiler': {'final_code': '```python\\nfrom typing import List, Tuple, TypedDict, Annotated\\nfrom langchain_core.messages import SystemMessage\\nfrom langchain_core.tools import tool\\nfrom langgraph.graph import StateGraph, MessagesState, START, END\\nfrom langgraph.prebuilt import ToolNode\\n\\n# Define the PlanExecute schema using TypedDict\\nclass PlanExecute(TypedDict):\\n    input: str\\n    plan: List[str]\\n    past_steps: Annotated[List[Tuple], operator.add]\\n    response: str\\n\\n# Define the tools needed by the LLM for stock-related queries\\n@tool\\ndef get_stock_price(symbol: str):\\n    \"\"\"Call to get the current stock price of a given symbol.\"\"\"\\n    return f\"The current price of {symbol} is $XYZ.\"  # Replace XYZ with actual data\\n\\n@tool\\ndef get_market_trends():\\n    \"\"\"Get current market trends\"\"\"\\n    return \"Bullish trends observed in technology and consumer goods sectors.\"\\n\\n# Bind the model (LLM) with tools\\nmodel_with_tools = ChatAnthropic(\\n    model=\"claude-3-haiku-20240307\", temperature=0\\n).bind_tools([get_stock_price, get_market_trends])\\n\\n# Node implementation for executing stock-related queries\\ndef execute_step(state: PlanExecute) -> PlanExecute:\\n    \"\"\"Uses llm with tool binding for the stock related queries\"\"\"\\n    # Extract user query from the state\\n    user_query = state[\"input\"]\\n    \\n    # Generate the prompt\\n    prompt = f\"Given the following stock-related query, please provide a detailed analysis and response: Input: {user_query}. Please consider the current market trends, historical data, and any relevant financial metrics while formulating your response.\"\\n    \\n    # Invoke the model with the generated prompt\\n    response = model_with_tools.invoke([SystemMessage(content=prompt)])\\n    \\n    # Update the state with the response generated by the LLM\\n    state[\"response\"] = response\\n    return state\\n\\n# Stategraph compilation\\nworkflow = StateGraph(MessagesState)\\n\\n# Define the nodes we will cycle between\\ntool_node = ToolNode([get_stock_price, get_market_trends])\\nworkflow.add_node(\"execute\", execute_step)\\nworkflow.add_node(\"tools\", tool_node)\\n\\n# Conditions for transitioning between nodes\\ndef should_continue(state: MessagesState):\\n    messages = state[\"messages\"]\\n    last_message = messages[-1]\\n    if last_message.tool_calls:\\n        return \"tools\"\\n    return END\\n\\n# Define the edges in the workflow\\nworkflow.add_edge(START, \"execute\")\\nworkflow.add_conditional_edges(\"execute\", should_continue, [\"tools\", END])\\nworkflow.add_edge(\"tools\", \"execute\")\\n\\n# Compile and prepare the app for execution\\napp = workflow.compile()\\n```'}}\n",
      "Following edge from agent to replan\n",
      "Visiting node: replan\n",
      "{'identify_node': {'node_type': 'planner', 'messages': [HumanMessage(content='\\nYou are provided with the following information about the node:\\n<SchemaInfo>\\nPlanExecute: TypedDict with fields input (str), plan (List[str]), past_steps (Annotated[List[Tuple], operator.add]), response (str)\\n</SchemaInfo>\\n<InputSchema>\\nPlanExecute\\n</InputSchema>\\n<OutputSchema>\\nUnion[Response, Plan]\\n</OutputSchema>\\n<Description>\\nEvaluates progress and uses an LLM to either revise the plan or generate a final response.\\n</Description>\\n<FunctionName>\\nreplan_step\\n</FunctionName>\\n\\nBelow is the skeleton of the function that you need to implement:\\ndef replan_step(state:PlanExecute) -> Union[Response, Plan]:\\n    \"\"\"Evaluates progress and uses an LLM to either revise the plan or generate a final response.\"\"\"\\n    # Implement the function to meet the description.\\n\\nthe state is of type PlanExecute and the function is of type Union[Response, Plan]\\nThe general idea is that the implementation would involve extracting the input from the state, and updating the state with the output. Description contains the logic for this blackbox\\n', additional_kwargs={}, response_metadata={}, id='798f8852-6277-42ad-a3f5-6a437f644c1b')], 'node_info': '\\nYou are provided with the following information about the node:\\n<SchemaInfo>\\nPlanExecute: TypedDict with fields input (str), plan (List[str]), past_steps (Annotated[List[Tuple], operator.add]), response (str)\\n</SchemaInfo>\\n<InputSchema>\\nPlanExecute\\n</InputSchema>\\n<OutputSchema>\\nUnion[Response, Plan]\\n</OutputSchema>\\n<Description>\\nEvaluates progress and uses an LLM to either revise the plan or generate a final response.\\n</Description>\\n<FunctionName>\\nreplan_step\\n</FunctionName>\\n\\nBelow is the skeleton of the function that you need to implement:\\ndef replan_step(state:PlanExecute) -> Union[Response, Plan]:\\n    \"\"\"Evaluates progress and uses an LLM to either revise the plan or generate a final response.\"\"\"\\n    # Implement the function to meet the description.\\n\\nthe state is of type PlanExecute and the function is of type Union[Response, Plan]\\nThe general idea is that the implementation would involve extracting the input from the state, and updating the state with the output. Description contains the logic for this blackbox\\n'}}\n",
      "{'planner': {'plan': [\"Generate a prompt using the extracted 'input' to evaluate progress and determine if the plan needs revision or if a final response can be generated.\", 'Use the LLM to process the generated prompt and obtain a response or a revised plan.']}}\n",
      "{'ai_node_gen_supervisor': {'next': 'prompt_generation', 'task': \"Generate a prompt using the extracted 'input' to evaluate progress and determine if the plan needs revision or if a final response can be generated.\", 'messages': [AIMessage(content=\"The first step is to generate a prompt using the extracted 'input' from the PlanExecute state. This is necessary to evaluate progress and determine if the plan needs revision or if a final response can be generated.\", additional_kwargs={}, response_metadata={}, id='bfefbedc-37de-4a59-b139-7ab5ccb0b4a4')]}}\n",
      "{'prompt_generation': {'past_steps': [(\"Generate a prompt using the extracted 'input' to evaluate progress and determine if the plan needs revision or if a final response can be generated.\", 'Based on the information gathered, I will create a prompt that effectively utilizes the LLM to evaluate progress, revise the plan if necessary, or generate a final response. \\n\\nHere is the prompt:\\n\\n---\\n\\n\"Evaluate the current state based on the following input: {input}. Consider the past steps taken: {past_steps}. Analyze the effectiveness of these steps in achieving the target goal. If the current plan appears insufficient based on your analysis, propose a revised plan of action. Otherwise, generate a final response that summarizes the progress made and the outcome achieved.\"')], 'messages': [HumanMessage(content='Based on the information gathered, I will create a prompt that effectively utilizes the LLM to evaluate progress, revise the plan if necessary, or generate a final response. \\n\\nHere is the prompt:\\n\\n---\\n\\n\"Evaluate the current state based on the following input: {input}. Consider the past steps taken: {past_steps}. Analyze the effectiveness of these steps in achieving the target goal. If the current plan appears insufficient based on your analysis, propose a revised plan of action. Otherwise, generate a final response that summarizes the progress made and the outcome achieved.\"', additional_kwargs={}, response_metadata={}, name='prompt_generator', id='faa11d14-1448-49c3-9419-79da43241fd7')]}}\n",
      "{'replan': {'plan': ['Use the LLM to process the generated prompt and obtain a response or a revised plan.']}}\n",
      "{'ai_node_gen_supervisor': {'next': 'prompt_generation', 'task': 'Use the LLM to process the generated prompt and obtain a response or a revised plan.', 'messages': [AIMessage(content=\"The task involves processing a generated prompt and obtaining a response. The 'prompt_generation' worker is responsible for generating or revising prompts, which aligns with the first step in using the LLM to process the generated input.\", additional_kwargs={}, response_metadata={}, id='45cc7d74-2b55-4310-80e1-a483aead6a30')]}}\n",
      "{'prompt_generation': {'past_steps': [('Use the LLM to process the generated prompt and obtain a response or a revised plan.', '\"Evaluate the current state based on the following input: {input}. Consider the past steps taken: {past_steps}. Analyze the effectiveness of these steps in achieving the target goal. If the current plan appears insufficient based on your analysis, propose a revised plan of action. Otherwise, generate a final response that summarizes the progress made and the outcome achieved.\"')], 'messages': [HumanMessage(content='\"Evaluate the current state based on the following input: {input}. Consider the past steps taken: {past_steps}. Analyze the effectiveness of these steps in achieving the target goal. If the current plan appears insufficient based on your analysis, propose a revised plan of action. Otherwise, generate a final response that summarizes the progress made and the outcome achieved.\"', additional_kwargs={}, response_metadata={}, name='prompt_generator', id='1362e9ee-1573-4f1d-92a5-71be45292d12')]}}\n",
      "{'replan': {'response': 'All steps in the OriginalPlan have been successfully executed. The prompt has been generated and processed using the LLM, leading to the evaluation of the current state and the generation of a response or revised plan.'}}\n",
      "{'code_compiler': {'final_code': '```python\\nfrom typing import List, Tuple, Union, TypedDict\\n\\nclass PlanExecute(TypedDict):\\n    input: str\\n    plan: List[str]\\n    past_steps: List[Tuple]\\n    response: str\\n\\nResponse = str  # Define Response as a string type for simplicity\\nPlan = List[str]  # Define Plan as a list of strings type\\n\\ndef replan_step(state: PlanExecute) -> Union[Response, Plan]:\\n    \"\"\"Evaluates progress and uses an LLM to either revise the plan or generate a final response.\"\"\"\\n    \\n    # Extract necessary information from the state\\n    input_data = state[\\'input\\']\\n    past_steps = state[\\'past_steps\\']\\n    \\n    # Generate the prompt for the LLM\\n    prompt = (\\n        f\"Evaluate the current state based on the following input: {input_data}. \"\\n        f\"Consider the past steps taken: {past_steps}. \"\\n        \"Analyze the effectiveness of these steps in achieving the target goal. \"\\n        \"If the current plan appears insufficient based on your analysis, propose a revised plan of action. \"\\n        \"Otherwise, generate a final response that summarizes the progress made and the outcome achieved.\"\\n    )\\n    \\n    # Here we would interact with the LLM to get a response based on the prompt\\n    # response = call_to_llm(prompt)  # This line is a placeholder for the actual LLM call\\n    \\n    # For the purpose of this implementation, we will simulate an LLM response\\n    response = \"Simulated response based on input analysis.\"  # Placeholder for LLM response\\n    updated_plan = [\"Next step based on LLM response\"]  # Placeholder for the revised plan if needed\\n\\n    # Update the state with the response\\n    state[\\'response\\'] = response\\n    \\n    # Decide whether to return a final response or a revised plan\\n    if \"revise\" in response.lower():  # Simulated condition for revision\\n        return updated_plan  # Return revised plan\\n    else:\\n        return response  # Return final response\\n```'}}\n",
      "Following edge from replan to agent\n",
      "Following edge from replan to __END__\n",
      "Visiting node: __END__\n"
     ]
    }
   ],
   "source": [
    "dfs_traverse(json_objects, \"__START__\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ed2d40f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "################planner##########\n",
      "The provided edge information indicates that this is a non-conditional edge that directly routes from the 'planner' node to the 'agent' node after the planning step is completed. Here’s the Python code snippet that implements the edge:\n",
      "\n",
      "```python\n",
      "# Assuming we have a 'graph' object already defined\n",
      "\n",
      "# Adding a non-conditional edge from the 'planner' node to the 'agent' node\n",
      "graph.add_edge(\"planner\", \"agent\")\n",
      "\n",
      "# This edge defines a direct path from 'planner' to 'agent', which implies \n",
      "# that once the planning step is complete, the execution will immediately \n",
      "# move to the 'agent' node for further action.\n",
      "```\n",
      "\n",
      "This snippet demonstrates the implementation of a non-conditional edge as specified, ensuring that after executing the 'planner' node, the flow continues to the 'agent' node without any conditions.\n",
      "####################################################\n",
      "################agent##########\n",
      "```python\n",
      "# Implementing a non-conditional edge from \"agent\" to \"replan\" in the workflow defined above.\n",
      "\n",
      "# Adding a non-conditional edge that connects the \"agent\" node to the \"replan\" node.\n",
      "# This will create a direct flow from \"agent\" to \"replan\" after the agent node has completed its processing.\n",
      "workflow.add_edge(\"agent\", \"replan\")\n",
      "```\n",
      "This code snippet adds a non-conditional edge between the `agent` node and a new node named `replan`, allowing for direct routing to the `replan` node after the execution of the `agent` node without any conditions.\n",
      "####################################################\n",
      "################replan##########\n",
      "Here's the Python code implementing the specified logic to create conditional edges based on whether a response is generated or not, from the `replan` node to either continue to the `agent` node or end the process. The code uses the `add_conditional_edges` method to route the flow based on the conditions provided. \n",
      "\n",
      "```python\n",
      "from langgraph import StateGraph, Command, START, END\n",
      "\n",
      "# Assuming we have already defined the PlanExecute TypedDict and the replan_step function\n",
      "# based on the previous user input.\n",
      "\n",
      "# Initialize the graph\n",
      "graph = StateGraph(PlanExecute)\n",
      "\n",
      "# Add the 'replan' node to the graph\n",
      "graph.add_node(\"replan\", replan_step)\n",
      "\n",
      "# Define the routing function for conditional edges\n",
      "def routing_function(state: PlanExecute) -> str:\n",
      "    # Check if a response was generated in the state\n",
      "    if state['response']:  # If response is generated, move to __END__\n",
      "        return \"__END__\"\n",
      "    else:  # If no response is generated, continue to agent\n",
      "        return \"agent\"\n",
      "\n",
      "# Add conditional edges based on the routing function\n",
      "graph.add_conditional_edges(\"replan\", routing_function)\n",
      "\n",
      "# Add terminal node indicating the end of the process\n",
      "graph.add_edge(\"__END__\", END)\n",
      "\n",
      "# Finally, define the other relevant nodes, e.g., 'agent'\n",
      "# graph.add_node(\"agent\", agent_function) # Uncomment this line when agent function is defined.\n",
      "\n",
      "# Connect the start node to the 'replan' node to begin the process\n",
      "graph.add_edge(START, \"replan\")\n",
      "\n",
      "# You could potentially visualize the graph here or execute the graph based on your needs.\n",
      "```\n",
      "\n",
      "### Explanation:\n",
      "- **Graph Initialization:** The graph is created using `StateGraph` where the state is defined by `PlanExecute`.\n",
      "- **Node Addition:** The `replan` node is added to the graph, utilizing the `replan_step` function.\n",
      "- **Routing Function:** The `routing_function` checks if a response has been generated. If so, it routes to `__END__`, otherwise, it continues to the `agent` node.\n",
      "- **Adding Conditional Edges:** The conditional edges are added using `add_conditional_edges`, allowing for dynamic routing based on the graph's state.\n",
      "- **Ending the Process:** An edge is added to denote the endpoint of the process.\n",
      "- **Start Node Connection:** The graph starts at the `replan` node when executed.\n",
      "\n",
      "This structure helps manage the process flow effectively in your LangGraph implementation.\n",
      "####################################################\n"
     ]
    }
   ],
   "source": [
    "for edge_key, edge_val in edge_to_code.items():\n",
    "    print(f\"################{edge_key}##########\")\n",
    "    print(edge_val)\n",
    "    print(\"####################################################\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0d99e1b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "################planner##########\n",
      "```python\n",
      "from typing import List, Tuple, TypedDict\n",
      "import operator\n",
      "\n",
      "# Define the TypedDict as per the schema information\n",
      "class PlanExecute(TypedDict):\n",
      "    input: str\n",
      "    plan: List[str]\n",
      "    past_steps: List[Tuple]\n",
      "    response: str\n",
      "\n",
      "def plan_step(state: PlanExecute) -> PlanExecute:\n",
      "    \"\"\"Plan step generates a plan based on the input using llm structured output functionality, stores it to the plan field.\"\"\"\n",
      "    \n",
      "    # Extracting input from the state\n",
      "    input_data = state['input']\n",
      "    \n",
      "    # Generating the prompt for the LLM\n",
      "    prompt = (\n",
      "        \"Hello, ChatGPT! I hope you are doing well. I am reaching out to you for assistance with generating a structured plan based on specific input I will provide. \\n\\n\"\n",
      "        \"Please find the details below:\\n\\n\"\n",
      "        \"**function_name:** [generate_plan]  \\n\"\n",
      "        f\"**input:** {input_data}  \\n\"\n",
      "        \"**rule:** [I want you to act as a planning assistant. Based on the input, please create a detailed, step-by-step plan that outlines the actions necessary to achieve the objectives outlined in the input. Please format the plan as a list and ensure that it is clear and logical.]  \\n\\n\"\n",
      "        \"Your assistance is greatly appreciated. Thank you! Please provide the plan directly with no additional explanations.\"\n",
      "    )\n",
      "    \n",
      "    # Mocking LLM response (replace this block with actual call to LLM)\n",
      "    # response = call_to_llm(prompt)\n",
      "    \n",
      "    # Here we assume the LLM has returned the following response as a mock example\n",
      "    response = \"1. Research the topic. 2. Create an outline. 3. Write the first draft.\"\n",
      "    \n",
      "    # Storing the output in the state\n",
      "    state['plan'] = response.split(\". \")  # Convert string response to list\n",
      "    state['response'] = response\n",
      "    \n",
      "    return state\n",
      "```\n",
      "####################################################\n",
      "################agent##########\n",
      "```python\n",
      "from typing import List, Tuple, Dict, Any, TypedDict\n",
      "from langchain_core.messages import SystemMessage\n",
      "from langchain_core.tools import tool\n",
      "from langgraph.graph import StateGraph, MessagesState, START, END\n",
      "from langgraph.prebuilt import ToolNode\n",
      "from langchain import ChatAnthropic\n",
      "import operator\n",
      "\n",
      "# Define TypedDict structure for PlanExecute\n",
      "class PlanExecute(TypedDict):\n",
      "    input: str\n",
      "    plan: List[str]\n",
      "    past_steps: List[Tuple]\n",
      "    response: str\n",
      "\n",
      "# Define tools needed by the LLM\n",
      "@tool\n",
      "def get_stock_price(stock_symbol: str) -> str:\n",
      "    \"\"\"Call to get the current stock price.\"\"\"\n",
      "    # In a real implementation, this would query a financial API\n",
      "    return f\"The current price for {stock_symbol} is $100.00.\"\n",
      "\n",
      "@tool\n",
      "def analyze_stock(stock_symbol: str) -> str:\n",
      "    \"\"\"Get an analysis of the stock.\"\"\"\n",
      "    # In a real implementation, this would analyze stock data\n",
      "    return f\"{stock_symbol} has shown a steady increase over the last month.\"\n",
      "\n",
      "# List of tools\n",
      "tools = [get_stock_price, analyze_stock]\n",
      "\n",
      "# Bind the model (LLM) with tools\n",
      "model_with_tools = ChatAnthropic(\n",
      "    model=\"claude-3-haiku-20240307\", temperature=0\n",
      ").bind_tools(tools)\n",
      "\n",
      "# Generate a tool node\n",
      "tool_node = ToolNode(tools)\n",
      "\n",
      "# Conditional edge to determine next node\n",
      "def should_continue(state: MessagesState) -> str:\n",
      "    messages = state[\"messages\"]\n",
      "    last_message = messages[-1]\n",
      "    if last_message.tool_calls:\n",
      "        return \"tools\"\n",
      "    return END\n",
      "\n",
      "# Implement the execute_step function\n",
      "def execute_step(state: PlanExecute) -> PlanExecute:\n",
      "    \"\"\"Uses llm with tool binding for the stock related queries\"\"\"\n",
      "    input_query = state[\"input\"]\n",
      "    stock_symbol = extract_stock_symbol(input_query)  # function to extract stock symbol from input\n",
      "\n",
      "    # Create prompt message for LLM\n",
      "    messages = [\n",
      "        SystemMessage(content=f\"Generate a detailed response to the following stock-related query: {input_query}. \"\n",
      "                             \"Please consider the latest trends in the stock market, relevant financial data, \"\n",
      "                             \"and any insights on potential stock performance.\")\n",
      "    ]\n",
      "    \n",
      "    # Invoke the LLM with the messages\n",
      "    response = model_with_tools.invoke(messages)\n",
      "    \n",
      "    # Update the state with the response\n",
      "    state[\"response\"] = response\n",
      "    return state\n",
      "\n",
      "# StateGraph compilation\n",
      "workflow = StateGraph(MessagesState)\n",
      "\n",
      "# Define the two nodes we will cycle between\n",
      "workflow.add_node(\"agent\", execute_step)\n",
      "workflow.add_node(\"tools\", tool_node)\n",
      "\n",
      "# Add edges for the workflow cycle\n",
      "workflow.add_edge(START, \"agent\")\n",
      "workflow.add_conditional_edges(\"agent\", should_continue, [\"tools\", END])\n",
      "workflow.add_edge(\"tools\", \"agent\")\n",
      "app = workflow.compile()\n",
      "```\n",
      "####################################################\n",
      "################replan##########\n",
      "```python\n",
      "from typing import TypedDict, List, Tuple, Union\n",
      "import operator\n",
      "\n",
      "# Define the TypedDict as per the SchemaInfo\n",
      "class PlanExecute(TypedDict):\n",
      "    input: str\n",
      "    plan: List[str]\n",
      "    past_steps: List[Tuple]\n",
      "    response: str\n",
      "\n",
      "# Define the expected types for response and plan\n",
      "Response = str  # Assuming Response is a string\n",
      "Plan = List[str]  # Assuming Plan is a list of strings\n",
      "\n",
      "def replan_step(state: PlanExecute) -> Union[Response, Plan]:\n",
      "    \"\"\"Evaluates progress and uses an LLM to either revise the plan or generate a final response.\"\"\"\n",
      "    \n",
      "    # Extract input from the state\n",
      "    input_data = state['input']\n",
      "    current_plan = state['plan']\n",
      "    past_steps = state['past_steps']\n",
      "    \n",
      "    # Generate the prompt using the provided prompt template\n",
      "    prompt = (\n",
      "        f\"Given the current input: '{input_data}', \"\n",
      "        f\"evaluate the existing plan: {current_plan} \"\n",
      "        f\"and assess its effectiveness in achieving the desired outcome. \"\n",
      "        \"Based on this evaluation, either revise the plan with specific improvements \"\n",
      "        \"or generate a final response if the current plan suffices. \"\n",
      "        \"Please provide a detailed explanation of your reasoning.\"\n",
      "    )\n",
      "    \n",
      "    # Here we would normally call the LLM with the prompt and get a response.\n",
      "    # For this task, let's assume we have a function `call_llm(prompt: str) -> Union[Response, Plan]`\n",
      "    # response = call_llm(prompt)  # Placeholder for LLM call\n",
      "    \n",
      "    # For demonstration, we will return a mock response\n",
      "    response = \"This is a mock response from the LLM indicating the current plan is sufficient.\"\n",
      "    \n",
      "    # Update the state with the new response, we could also include updated plan if needed.\n",
      "    state['response'] = response\n",
      "    \n",
      "    # For the sake of this example, we return the mock response.\n",
      "    return response  # or return current_plan if a plan was updated\n",
      "```\n",
      "####################################################\n"
     ]
    }
   ],
   "source": [
    "for node_key, node_val in node_to_code.items():\n",
    "    print(f\"################{node_key}##########\")\n",
    "    print(node_val)\n",
    "    print(\"####################################################\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "94152e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import  MessagesState\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.messages import SystemMessage\n",
    "code_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "You are responsible for merging and compiling the code  for the given graph\n",
    "<Graph>\n",
    "{graphEdgeDict}\n",
    "</Graph>\n",
    "\n",
    "<GraphNodeImplementation>\n",
    "{graphImplementations}\n",
    "</GraphNodeImplementation>\n",
    "\n",
    "<EdgeImplementation>\n",
    "{edgeImplementations}\n",
    "</EdgeImplementation>\n",
    "\"\"\")\n",
    "class CodeCompilerState(MessagesState):\n",
    "    graphEdgeDict: dict\n",
    "    graphImplementations: dict\n",
    "    edgeImplementations: dict\n",
    "    \n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "def all_code_compiler(state: CodeCompilerState):\n",
    "    response = llm.invoke([SystemMessage(content= code_prompt.format(\n",
    "        graphEdgeDict=state[\"graphEdgeDict\"],\n",
    "        graphImplementations=state[\"graphImplementations\"],\n",
    "        edgeImplementations=state[\"edgeImplementations\"]))])\n",
    "    return {\n",
    "        \"messages\": [response]\n",
    "    }\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4e65fb79",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "\n",
    "workflow = StateGraph(CodeCompilerState)\n",
    "\n",
    "workflow.add_node(\"code_compiler\", all_code_compiler)\n",
    "workflow.add_edge(START, \"code_compiler\")\n",
    "workflow.add_edge(\"code_compiler\", END)\n",
    "\n",
    "checkpointer = InMemorySaver()\n",
    "app = workflow.compile(checkpointer= checkpointer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "39f76505",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'code_compiler': {'messages': [AIMessage(content='To merge and compile the code for the given graph, we will integrate the node implementations and edge definitions into a cohesive structure. Below is the complete code that combines the planner, agent, and replan nodes along with their respective edges.\\n\\n```python\\nfrom typing import List, Tuple, Optional, Union, TypedDict, Annotated\\nfrom pydantic import BaseModel, Field\\nfrom langchain_core.messages import SystemMessage\\nfrom langchain_core.tools import tool\\nfrom langgraph.graph import StateGraph, MessagesState, START, END\\nfrom langgraph.prebuilt import ToolNode\\n\\n# Pydantic class for structured output\\nclass PlanOutput(BaseModel):\\n    objective: str = Field(description=\"Main goal of the event\")\\n    key_actions: List[str] = Field(description=\"Critical actions to undertake\")\\n    responsible_parties: List[str] = Field(description=\"Who is responsible for each action\")\\n    timeline: List[str] = Field(description=\"When each action should be completed\")\\n    success_metrics: Optional[str] = Field(default=None, description=\"Metrics for success\")\\n    challenges: Optional[List[str]] = Field(default=None, description=\"Potential challenges\")\\n    proposed_solutions: Optional[List[str]] = Field(default=None, description=\"Proposed solutions for challenges\")\\n\\nclass PlanExecute(TypedDict):\\n    input: str\\n    plan: List[str]\\n    past_steps: Annotated[List[Tuple], operator.add]\\n    response: str\\n\\ndef plan_step(state: PlanExecute) -> PlanExecute:\\n    \"\"\"Plan step generates a plan based on the input using llm structured output functionality, stores it to the plan field\"\"\"\\n    structured_llm = llm.with_structured_output(PlanOutput)\\n    plan_output: PlanOutput = structured_llm.invoke(state.input)\\n    \\n    # Update the state with the generated plan\\n    state.plan = [\\n        plan_output.objective,\\n        plan_output.key_actions,\\n        plan_output.responsible_parties,\\n        plan_output.timeline,\\n        plan_output.success_metrics,\\n        plan_output.challenges,\\n        plan_output.proposed_solutions\\n    ]\\n    state.response = \"Plan generated successfully.\"\\n    \\n    return state\\n\\n# Define the tools needed by the LLM for stock-related queries\\n@tool\\ndef get_stock_price(symbol: str):\\n    \"\"\"Call to get the current stock price of a given symbol.\"\"\"\\n    return f\"The current price of {symbol} is $XYZ.\"  # Replace XYZ with actual data\\n\\n@tool\\ndef get_market_trends():\\n    \"\"\"Get current market trends\"\"\"\\n    return \"Bullish trends observed in technology and consumer goods sectors.\"\\n\\n# Bind the model (LLM) with tools\\nmodel_with_tools = ChatAnthropic(\\n    model=\"claude-3-haiku-20240307\", temperature=0\\n).bind_tools([get_stock_price, get_market_trends])\\n\\ndef execute_step(state: PlanExecute) -> PlanExecute:\\n    \"\"\"Uses llm with tool binding for the stock related queries\"\"\"\\n    # Extract user query from the state\\n    user_query = state[\"input\"]\\n    \\n    # Generate the prompt\\n    prompt = f\"Given the following stock-related query, please provide a detailed analysis and response: Input: {user_query}. Please consider the current market trends, historical data, and any relevant financial metrics while formulating your response.\"\\n    \\n    # Invoke the model with the generated prompt\\n    response = model_with_tools.invoke([SystemMessage(content=prompt)])\\n    \\n    # Update the state with the response generated by the LLM\\n    state[\"response\"] = response\\n    return state\\n\\nResponse = str  # Define Response as a string type for simplicity\\nPlan = List[str]  # Define Plan as a list of strings type\\n\\ndef replan_step(state: PlanExecute) -> Union[Response, Plan]:\\n    \"\"\"Evaluates progress and uses an LLM to either revise the plan or generate a final response.\"\"\"\\n    \\n    # Extract necessary information from the state\\n    input_data = state[\\'input\\']\\n    past_steps = state[\\'past_steps\\']\\n    \\n    # Generate the prompt for the LLM\\n    prompt = (\\n        f\"Evaluate the current state based on the following input: {input_data}. \"\\n        f\"Consider the past steps taken: {past_steps}. \"\\n        \"Analyze the effectiveness of these steps in achieving the target goal. \"\\n        \"If the current plan appears insufficient based on your analysis, propose a revised plan of action. \"\\n        \"Otherwise, generate a final response that summarizes the progress made and the outcome achieved.\"\\n    )\\n    \\n    # Simulate the LLM response\\n    response = \"Simulated response based on input analysis.\"  # Placeholder for LLM response\\n    updated_plan = [\"Next step based on LLM response\"]  # Placeholder for the revised plan if needed\\n\\n    # Update the state with the response\\n    state[\\'response\\'] = response\\n    \\n    # Decide whether to return a final response or a revised plan\\n    if \"revise\" in response.lower():  # Simulated condition for revision\\n        return updated_plan  # Return revised plan\\n    else:\\n        return response  # Return final response\\n\\n# Stategraph compilation\\nworkflow = StateGraph(MessagesState)\\n\\n# Add nodes to the workflow\\nworkflow.add_node(\"planner\", plan_step)\\nworkflow.add_node(\"agent\", execute_step)\\nworkflow.add_node(\"replan\", replan_step)\\n\\n# Define the edges in the workflow\\nworkflow.add_edge(START, \"planner\")  # Edge from START to planner\\nworkflow.add_edge(\"planner\", \"agent\")  # Edge from planner to agent\\nworkflow.add_edge(\"agent\", \"replan\")  # Edge from agent to replan\\n\\n# Define conditional edges for replan\\ndef should_continue(state: MessagesState):\\n    return state[\"response\"] == \"\"  # Check if no response was generated\\n\\nworkflow.add_conditional_edges(\"replan\", should_continue, [\"agent\", \"__END__\"])  # Conditional edges based on response\\n\\n# Compile and prepare the app for execution\\napp = workflow.compile()\\n```\\n\\n### Explanation:\\n1. **Node Implementations**: Each node (`planner`, `agent`, `replan`) is implemented as a function that processes the state and returns an updated state.\\n2. **Edge Definitions**: The edges are defined to connect the nodes in the specified order, with conditional logic for the `replan` node to determine the next step based on whether a response was generated.\\n3. **StateGraph**: The `StateGraph` is created, nodes are added, and edges are defined to create a complete workflow.\\n4. **Compilation**: Finally, the graph is compiled to prepare it for execution.\\n\\nThis code structure allows for a clear flow of data and control through the graph, enabling the execution of planning, execution, and replanning steps as defined in the original graph structure.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 1397, 'prompt_tokens': 3100, 'total_tokens': 4497, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_0392822090', 'id': 'chatcmpl-BRPCxzMZnQrEYkrzgQN8eZ4wA2nNE', 'finish_reason': 'stop', 'logprobs': None}, id='run-8972effc-e02f-431d-8704-95bbea905b73-0', usage_metadata={'input_tokens': 3100, 'output_tokens': 1397, 'total_tokens': 4497, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}}\n"
     ]
    }
   ],
   "source": [
    "import uuid\n",
    "uuid = uuid.uuid4()\n",
    "config = {\"configurable\": {\"thread_id\": str(uuid)}}\n",
    "\n",
    "for output in app.stream({\"graphEdgeDict\":json_objects , \"graphImplementations\": node_to_code, \"edgeImplementations\": edge_to_code}, config, stream_mode=\"updates\"\n",
    "    ):\n",
    "        print(output)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d0edec5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
