{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "62b92789",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_objects = {\n",
    "  \"nodes\": {\n",
    "    \"__START__\": {\n",
    "      \"schema_info\": \"\",\n",
    "      \"input_schema\": \"\",\n",
    "      \"output_schema\": \"\",\n",
    "      \"description\": \"Entry point of the graph.\",\n",
    "      \"function_name\": \"\"\n",
    "    },\n",
    "    \"planner\": {\n",
    "      \"schema_info\": \"PlanExecute: TypedDict with fields input (str), plan (List[str]), past_steps (Annotated[List[Tuple], operator.add]), response (str)\",\n",
    "      \"input_schema\": \"PlanExecute\",\n",
    "      \"output_schema\": \"PlanExecute\",\n",
    "      \"description\": \"Plan step generates a plan based on the input using llm structured output functionality, stores it to the plan field\",\n",
    "      \"function_name\": \"plan_step\"\n",
    "    },\n",
    "    \"agent\": {\n",
    "      \"schema_info\": \"PlanExecute: TypedDict with fields input (str), plan (List[str]), past_steps (Annotated[List[Tuple], operator.add]), response (str)\",\n",
    "      \"input_schema\": \"PlanExecute\",\n",
    "      \"output_schema\": \"PlanExecute\",\n",
    "      \"description\": \"Uses llm with tool binding for the stock related queries\",\n",
    "      \"function_name\": \"execute_step\"\n",
    "    },\n",
    "    \"replan\": {\n",
    "      \"schema_info\": \"PlanExecute: TypedDict with fields input (str), plan (List[str]), past_steps (Annotated[List[Tuple], operator.add]), response (str)\",\n",
    "      \"input_schema\": \"PlanExecute\",\n",
    "      \"output_schema\": \"Union[Response, Plan]\",\n",
    "      \"description\": \"Evaluates progress and uses an LLM to either revise the plan or generate a final response.\",\n",
    "      \"function_name\": \"replan_step\"\n",
    "    },\n",
    "    \"__END__\": {\n",
    "      \"schema_info\": \"\",\n",
    "      \"input_schema\": \"\",\n",
    "      \"output_schema\": \"\",\n",
    "      \"description\": \"End point of the graph.\",\n",
    "      \"function_name\": \"\"\n",
    "    }\n",
    "  },\n",
    "  \"edges\": {\n",
    "    \"edge_1\": {\n",
    "      \"source\": \"__START__\",\n",
    "      \"target\": \"planner\",\n",
    "      \"routing_conditions\": \"Start the planning process.\",\n",
    "      \"conditional\": False\n",
    "    },\n",
    "    \"edge_2\": {\n",
    "      \"source\": \"planner\",\n",
    "      \"target\": \"agent\",\n",
    "      \"routing_conditions\": \"After planning, execute the first step.\",\n",
    "      \"conditional\": False\n",
    "    },\n",
    "    \"edge_3\": {\n",
    "      \"source\": \"agent\",\n",
    "      \"target\": \"replan\",\n",
    "      \"routing_conditions\": \"After executing a step, check if replanning is needed.\",\n",
    "      \"conditional\": False\n",
    "    },\n",
    "    \"edge_4\": {\n",
    "      \"source\": \"replan\",\n",
    "      \"target\": \"agent\",\n",
    "      \"routing_conditions\": \"If no response is generated, continue to agent for further execution.\",\n",
    "      \"conditional\": True\n",
    "    },\n",
    "    \"edge_5\": {\n",
    "      \"source\": \"replan\",\n",
    "      \"target\": \"__END__\",\n",
    "      \"routing_conditions\": \"If a response is generated, end the process.\",\n",
    "      \"conditional\": True\n",
    "    }\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bb05768b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import import_ipynb\n",
    "from phase1_edge_handling import edge_builder_agent\n",
    "from phase1_node_to_code import node_to_code_app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cdd16f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# node name to corresponding code stubs\n",
    "node_to_code = {}\n",
    "\n",
    "# edge_to_code \n",
    "# key will be src#dest\n",
    "# contains code snippets generated.\n",
    "edge_to_code = {}\n",
    "\n",
    "skipList = [\"__START__\", \"__END__\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a5a83516",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.messages import HumanMessage\n",
    "edge_info_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "<GraphNodeImplementation>\n",
    "{node_code}\n",
    "</GraphNodeImplementation>\n",
    "<EdgeInformation>\n",
    "{edge_json}\n",
    "</Edgeinformation>\"\"\")\n",
    "\n",
    "def dfs_traverse(json_objects, current_node, visited=None):\n",
    "    if visited is None:\n",
    "        visited = set()  # Initialize the visited set\n",
    "\n",
    "    # If we encounter a node we've already visited, we skip to avoid infinite loops\n",
    "    if current_node in visited:\n",
    "        return\n",
    "\n",
    "    visited.add(current_node)  # Mark the current node as visited\n",
    "    print(f\"Visiting node: {current_node}\")\n",
    "\n",
    "    # Get edges originating from the current node\n",
    "    edges = json_objects[\"edges\"]\n",
    "    nodes = json_objects[\"nodes\"]\n",
    "    outgoing_edges = list(filter(lambda edge: edge[\"source\"] == current_node, edges.values()))\n",
    "    \n",
    "    if current_node not in skipList:\n",
    "        uuid_str = uuid.uuid4()\n",
    "        config = {\"configurable\": {\"thread_id\": str(uuid_str)}}\n",
    "\n",
    "        for output in node_to_code_app.stream(nodes[current_node], config, stream_mode=\"updates\"):\n",
    "            print(output)\n",
    "        code= node_to_code_app.get_state(config).values[\"final_code\"]\n",
    "        result = edge_builder_agent.invoke({\"messages\": [HumanMessage(content = edge_info_prompt.format(node_code=code,edge_json=outgoing_edges))]},config)\n",
    "        node_to_code[current_node] = code\n",
    "        edge_to_code[current_node] = result[\"messages\"][-1].content\n",
    "    for edge_key, edge_value in edges.items():\n",
    "        if edge_value[\"source\"] == current_node:  # Check if the edge originates from the current node\n",
    "            next_node = edge_value[\"target\"]\n",
    "            print(f\"Following edge from {current_node} to {next_node}\")\n",
    "            dfs_traverse(json_objects, next_node, visited)  # Recursively call DFS on the next node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d3ff4d35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visiting node: __START__\n",
      "Following edge from __START__ to planner\n",
      "Visiting node: planner\n",
      "{'identify_node': {'node_type': 'planner', 'messages': [HumanMessage(content='\\nYou are provided with the following information about the node:\\n<SchemaInfo>\\nPlanExecute: TypedDict with fields input (str), plan (List[str]), past_steps (Annotated[List[Tuple], operator.add]), response (str)\\n</SchemaInfo>\\n<InputSchema>\\nPlanExecute\\n</InputSchema>\\n<OutputSchema>\\nPlanExecute\\n</OutputSchema>\\n<Description>\\nPlan step generates a plan based on the input using llm structured output functionality, stores it to the plan field\\n</Description>\\n<FunctionName>\\nplan_step\\n</FunctionName>\\n\\nBelow is the skeleton of the function that you need to implement:\\ndef plan_step(state:PlanExecute) -> PlanExecute:\\n    \"\"\"Plan step generates a plan based on the input using llm structured output functionality, stores it to the plan field\"\"\"\\n    # Implement the function to meet the description.\\n\\nthe state is of type PlanExecute and the function is of type PlanExecute\\nThe general idea is that the implementation would involve extracting the input from the state, and updating the state with the output. Description contains the logic for this blackbox\\n', additional_kwargs={}, response_metadata={}, id='439284f3-6cfe-4bd9-87b1-af29bd72404d')], 'node_info': '\\nYou are provided with the following information about the node:\\n<SchemaInfo>\\nPlanExecute: TypedDict with fields input (str), plan (List[str]), past_steps (Annotated[List[Tuple], operator.add]), response (str)\\n</SchemaInfo>\\n<InputSchema>\\nPlanExecute\\n</InputSchema>\\n<OutputSchema>\\nPlanExecute\\n</OutputSchema>\\n<Description>\\nPlan step generates a plan based on the input using llm structured output functionality, stores it to the plan field\\n</Description>\\n<FunctionName>\\nplan_step\\n</FunctionName>\\n\\nBelow is the skeleton of the function that you need to implement:\\ndef plan_step(state:PlanExecute) -> PlanExecute:\\n    \"\"\"Plan step generates a plan based on the input using llm structured output functionality, stores it to the plan field\"\"\"\\n    # Implement the function to meet the description.\\n\\nthe state is of type PlanExecute and the function is of type PlanExecute\\nThe general idea is that the implementation would involve extracting the input from the state, and updating the state with the output. Description contains the logic for this blackbox\\n'}}\n",
      "{'planner': {'plan': ['Generate a prompt based on the extracted input to create a plan.', 'Use structured output functionality to format the generated plan into a list of steps.']}}\n",
      "{'ai_node_gen_supervisor': {'next': 'prompt_generation', 'task': 'Generate a prompt based on the extracted input to create a plan.', 'messages': [AIMessage(content='The next step involves generating a prompt based on the extracted input to create a plan, which is the responsibility of the prompt_generation worker. This worker will create a suitable prompt leveraging the provided input schema.', additional_kwargs={}, response_metadata={}, id='61ebed63-3717-4d0c-8c77-b167523c4a5a')]}}\n",
      "{'prompt_generation': {'past_steps': [('Generate a prompt based on the extracted input to create a plan.', 'Based on the retrieved information, here is a prompt that can be used to generate a detailed plan using LLM structured output:\\n\\n---\\n\\n\"Hello, ChatGPT! I hope you are doing well. I am reaching out to you for assistance with generating a structured plan based on specific input I will provide. \\n\\nPlease find the details below:\\n\\n**function_name:** [generate_plan]  \\n**input:** [State input here including any specific requirements or goals]  \\n**rule:** [I want you to act as a planning assistant. Based on the input, please create a detailed, step-by-step plan that outlines the actions necessary to achieve the objectives outlined in the input. Please format the plan as a list and ensure that it is clear and logical.]  \\n\\nYour assistance is greatly appreciated. Thank you! Please provide the plan directly with no additional explanations.\" \\n\\n--- \\n\\nThis prompt is designed to instruct the LLM to generate a thorough plan while utilizing structured output functionality.')], 'messages': [HumanMessage(content='Based on the retrieved information, here is a prompt that can be used to generate a detailed plan using LLM structured output:\\n\\n---\\n\\n\"Hello, ChatGPT! I hope you are doing well. I am reaching out to you for assistance with generating a structured plan based on specific input I will provide. \\n\\nPlease find the details below:\\n\\n**function_name:** [generate_plan]  \\n**input:** [State input here including any specific requirements or goals]  \\n**rule:** [I want you to act as a planning assistant. Based on the input, please create a detailed, step-by-step plan that outlines the actions necessary to achieve the objectives outlined in the input. Please format the plan as a list and ensure that it is clear and logical.]  \\n\\nYour assistance is greatly appreciated. Thank you! Please provide the plan directly with no additional explanations.\" \\n\\n--- \\n\\nThis prompt is designed to instruct the LLM to generate a thorough plan while utilizing structured output functionality.', additional_kwargs={}, response_metadata={}, name='prompt_generator', id='6e2cb417-cd58-45ea-9dd9-cf5bb347f821')]}}\n",
      "{'replan': {'response': 'The prompt has been successfully generated based on the extracted input to create a plan. You can now proceed with using the structured output functionality to format the generated plan into a list of steps.'}}\n",
      "{'code_compiler': {'final_code': '```python\\nfrom typing import List, Tuple, TypedDict\\nimport operator\\n\\n# Define the TypedDict as per the schema information\\nclass PlanExecute(TypedDict):\\n    input: str\\n    plan: List[str]\\n    past_steps: List[Tuple]\\n    response: str\\n\\ndef plan_step(state: PlanExecute) -> PlanExecute:\\n    \"\"\"Plan step generates a plan based on the input using llm structured output functionality, stores it to the plan field.\"\"\"\\n    \\n    # Extracting input from the state\\n    input_data = state[\\'input\\']\\n    \\n    # Generating the prompt for the LLM\\n    prompt = (\\n        \"Hello, ChatGPT! I hope you are doing well. I am reaching out to you for assistance with generating a structured plan based on specific input I will provide. \\\\n\\\\n\"\\n        \"Please find the details below:\\\\n\\\\n\"\\n        \"**function_name:** [generate_plan]  \\\\n\"\\n        f\"**input:** {input_data}  \\\\n\"\\n        \"**rule:** [I want you to act as a planning assistant. Based on the input, please create a detailed, step-by-step plan that outlines the actions necessary to achieve the objectives outlined in the input. Please format the plan as a list and ensure that it is clear and logical.]  \\\\n\\\\n\"\\n        \"Your assistance is greatly appreciated. Thank you! Please provide the plan directly with no additional explanations.\"\\n    )\\n    \\n    # Mocking LLM response (replace this block with actual call to LLM)\\n    # response = call_to_llm(prompt)\\n    \\n    # Here we assume the LLM has returned the following response as a mock example\\n    response = \"1. Research the topic. 2. Create an outline. 3. Write the first draft.\"\\n    \\n    # Storing the output in the state\\n    state[\\'plan\\'] = response.split(\". \")  # Convert string response to list\\n    state[\\'response\\'] = response\\n    \\n    return state\\n```'}}\n",
      "Following edge from planner to agent\n",
      "Visiting node: agent\n",
      "{'identify_node': {'node_type': 'planner', 'messages': [HumanMessage(content='\\nYou are provided with the following information about the node:\\n<SchemaInfo>\\nPlanExecute: TypedDict with fields input (str), plan (List[str]), past_steps (Annotated[List[Tuple], operator.add]), response (str)\\n</SchemaInfo>\\n<InputSchema>\\nPlanExecute\\n</InputSchema>\\n<OutputSchema>\\nPlanExecute\\n</OutputSchema>\\n<Description>\\nUses llm with tool binding for the stock related queries\\n</Description>\\n<FunctionName>\\nexecute_step\\n</FunctionName>\\n\\nBelow is the skeleton of the function that you need to implement:\\ndef execute_step(state:PlanExecute) -> PlanExecute:\\n    \"\"\"Uses llm with tool binding for the stock related queries\"\"\"\\n    # Implement the function to meet the description.\\n\\nthe state is of type PlanExecute and the function is of type PlanExecute\\nThe general idea is that the implementation would involve extracting the input from the state, and updating the state with the output. Description contains the logic for this blackbox\\n', additional_kwargs={}, response_metadata={}, id='afb81c77-9b25-41a2-bab9-12899b2ea731')], 'node_info': '\\nYou are provided with the following information about the node:\\n<SchemaInfo>\\nPlanExecute: TypedDict with fields input (str), plan (List[str]), past_steps (Annotated[List[Tuple], operator.add]), response (str)\\n</SchemaInfo>\\n<InputSchema>\\nPlanExecute\\n</InputSchema>\\n<OutputSchema>\\nPlanExecute\\n</OutputSchema>\\n<Description>\\nUses llm with tool binding for the stock related queries\\n</Description>\\n<FunctionName>\\nexecute_step\\n</FunctionName>\\n\\nBelow is the skeleton of the function that you need to implement:\\ndef execute_step(state:PlanExecute) -> PlanExecute:\\n    \"\"\"Uses llm with tool binding for the stock related queries\"\"\"\\n    # Implement the function to meet the description.\\n\\nthe state is of type PlanExecute and the function is of type PlanExecute\\nThe general idea is that the implementation would involve extracting the input from the state, and updating the state with the output. Description contains the logic for this blackbox\\n'}}\n",
      "{'planner': {'plan': ['Write a prompt that addresses the stock-related query based on the extracted input.', 'Identify and pair the appropriate tools that can be used to answer the stock-related query.']}}\n",
      "{'ai_node_gen_supervisor': {'next': 'prompt_generation', 'task': 'Write a prompt that addresses the stock-related query based on the extracted input.', 'messages': [AIMessage(content='The next step is to generate a prompt that directly addresses the stock-related query using the input provided. Prompt generation is critical for delineating the query context needed for subsequent tool utilization.', additional_kwargs={}, response_metadata={}, id='1c200038-9a68-47e0-8c3e-d59209a315ad')]}}\n",
      "{'prompt_generation': {'past_steps': [('Write a prompt that addresses the stock-related query based on the extracted input.', 'Based on the retrieved prompting techniques relevant to stock-related queries, I will create a prompt to guide the LLM to handle such queries effectively.\\n\\nHere is the prompt:\\n\\n\"Generate a detailed response to the following stock-related query: [INSERT_USER_QUERY_HERE]. Please consider the latest trends in the stock market, relevant financial data, and any insights on potential stock performance. If applicable, include analysis or predictions to help the user understand the stock\\'s current position and future potential.\"')], 'messages': [HumanMessage(content='Based on the retrieved prompting techniques relevant to stock-related queries, I will create a prompt to guide the LLM to handle such queries effectively.\\n\\nHere is the prompt:\\n\\n\"Generate a detailed response to the following stock-related query: [INSERT_USER_QUERY_HERE]. Please consider the latest trends in the stock market, relevant financial data, and any insights on potential stock performance. If applicable, include analysis or predictions to help the user understand the stock\\'s current position and future potential.\"', additional_kwargs={}, response_metadata={}, name='prompt_generator', id='f4fbb7e4-b6ac-44a7-8939-75a84a32379f')]}}\n",
      "{'replan': {'plan': ['Identify and pair the appropriate tools that can be used to answer the stock-related query.']}}\n",
      "{'ai_node_gen_supervisor': {'next': 'toolset_generation', 'task': 'Identify and pair the appropriate tools that can be used to answer the stock-related query.', 'messages': [AIMessage(content=\"The next logical step is to identify the appropriate tools for answering stock-related queries. This task requires specialized knowledge about available tools, making 'toolset_generation' the right choice.\", additional_kwargs={}, response_metadata={}, id='9a0ee1ef-efbe-4324-8e0d-9da577e30e1b')]}}\n",
      "{'toolset_generation': {'past_steps': [('Identify and pair the appropriate tools that can be used to answer the stock-related query.', '```python\\nfrom typing import List, Tuple\\nfrom langchain_core.messages import SystemMessage\\nfrom langchain_core.tools import tool\\nfrom langgraph.graph import StateGraph, MessagesState, START, END\\nfrom langgraph.prebuilt import ToolNode\\nfrom langchain import ChatAnthropic\\n\\n# Define the tools needed by the LLM\\n\\n@tool\\ndef get_stock_price(stock_symbol: str):\\n    \"\"\"Call to get the current stock price.\"\"\"\\n    # In a real implementation, this would query a financial API\\n    return f\"The current price for {stock_symbol} is $100.00.\"\\n\\n@tool\\ndef analyze_stock(stock_symbol: str):\\n    \"\"\"Get an analysis of the stock.\"\"\"\\n    # In a real implementation, this would analyze stock data\\n    return f\"{stock_symbol} has shown a steady increase over the last month.\"\\n\\ntools = [get_stock_price, analyze_stock]\\n\\n# Bind the model(llm) with tools\\nmodel_with_tools = ChatAnthropic(\\n    model=\"claude-3-haiku-20240307\", temperature=0\\n).bind_tools(tools)\\n\\n# Generate a tool node.\\ntool_node = ToolNode(tools)\\n\\n# conditional edge\\ndef should_continue(state: MessagesState):\\n    messages = state[\"messages\"]\\n    last_message = messages[-1]\\n    if last_message.tool_calls:\\n        return \"tools\"\\n    return END\\n\\n# node implementation\\ndef execute_step(state: dict) -> dict:\\n    \"\"\"Uses llm with tool binding for the stock related queries\"\"\"\\n    input_query = state[\"input\"]\\n    stock_symbol = extract_stock_symbol(input_query)  # function to extract stock symbol from input\\n\\n    messages = [SystemMessage(content=f\"Generate a detailed response to the following stock-related query: {input_query}. Please consider the latest trends in the stock market, relevant financial data, and any insights on potential stock performance.\")]\\n    \\n    response = model_with_tools.invoke(messages)\\n    \\n    # Assuming we get the tool calls from the response\\n    state[\"response\"] = response\\n    return state\\n\\n# Stategraph compilation\\nworkflow = StateGraph(MessagesState)\\n\\n# Define the two nodes we will cycle between\\nworkflow.add_node(\"agent\", execute_step)\\nworkflow.add_node(\"tools\", tool_node)\\n\\nworkflow.add_edge(START, \"agent\")\\nworkflow.add_conditional_edges(\"agent\", should_continue, [\"tools\", END])\\nworkflow.add_edge(\"tools\", \"agent\")\\napp = workflow.compile()\\n```')], 'messages': [HumanMessage(content='```python\\nfrom typing import List, Tuple\\nfrom langchain_core.messages import SystemMessage\\nfrom langchain_core.tools import tool\\nfrom langgraph.graph import StateGraph, MessagesState, START, END\\nfrom langgraph.prebuilt import ToolNode\\nfrom langchain import ChatAnthropic\\n\\n# Define the tools needed by the LLM\\n\\n@tool\\ndef get_stock_price(stock_symbol: str):\\n    \"\"\"Call to get the current stock price.\"\"\"\\n    # In a real implementation, this would query a financial API\\n    return f\"The current price for {stock_symbol} is $100.00.\"\\n\\n@tool\\ndef analyze_stock(stock_symbol: str):\\n    \"\"\"Get an analysis of the stock.\"\"\"\\n    # In a real implementation, this would analyze stock data\\n    return f\"{stock_symbol} has shown a steady increase over the last month.\"\\n\\ntools = [get_stock_price, analyze_stock]\\n\\n# Bind the model(llm) with tools\\nmodel_with_tools = ChatAnthropic(\\n    model=\"claude-3-haiku-20240307\", temperature=0\\n).bind_tools(tools)\\n\\n# Generate a tool node.\\ntool_node = ToolNode(tools)\\n\\n# conditional edge\\ndef should_continue(state: MessagesState):\\n    messages = state[\"messages\"]\\n    last_message = messages[-1]\\n    if last_message.tool_calls:\\n        return \"tools\"\\n    return END\\n\\n# node implementation\\ndef execute_step(state: dict) -> dict:\\n    \"\"\"Uses llm with tool binding for the stock related queries\"\"\"\\n    input_query = state[\"input\"]\\n    stock_symbol = extract_stock_symbol(input_query)  # function to extract stock symbol from input\\n\\n    messages = [SystemMessage(content=f\"Generate a detailed response to the following stock-related query: {input_query}. Please consider the latest trends in the stock market, relevant financial data, and any insights on potential stock performance.\")]\\n    \\n    response = model_with_tools.invoke(messages)\\n    \\n    # Assuming we get the tool calls from the response\\n    state[\"response\"] = response\\n    return state\\n\\n# Stategraph compilation\\nworkflow = StateGraph(MessagesState)\\n\\n# Define the two nodes we will cycle between\\nworkflow.add_node(\"agent\", execute_step)\\nworkflow.add_node(\"tools\", tool_node)\\n\\nworkflow.add_edge(START, \"agent\")\\nworkflow.add_conditional_edges(\"agent\", should_continue, [\"tools\", END])\\nworkflow.add_edge(\"tools\", \"agent\")\\napp = workflow.compile()\\n```', additional_kwargs={}, response_metadata={}, name='toolset_generator', id='8cf3cee3-29dc-4206-899e-01911aa1a087')]}}\n",
      "{'replan': {'response': 'All components of the OriginalPlan have been successfully executed. The necessary tools have been identified and paired to address stock-related queries. If you have any further questions or need assistance, feel free to ask!'}}\n",
      "{'code_compiler': {'final_code': '```python\\nfrom typing import List, Tuple, Dict, Any, TypedDict\\nfrom langchain_core.messages import SystemMessage\\nfrom langchain_core.tools import tool\\nfrom langgraph.graph import StateGraph, MessagesState, START, END\\nfrom langgraph.prebuilt import ToolNode\\nfrom langchain import ChatAnthropic\\nimport operator\\n\\n# Define TypedDict structure for PlanExecute\\nclass PlanExecute(TypedDict):\\n    input: str\\n    plan: List[str]\\n    past_steps: List[Tuple]\\n    response: str\\n\\n# Define tools needed by the LLM\\n@tool\\ndef get_stock_price(stock_symbol: str) -> str:\\n    \"\"\"Call to get the current stock price.\"\"\"\\n    # In a real implementation, this would query a financial API\\n    return f\"The current price for {stock_symbol} is $100.00.\"\\n\\n@tool\\ndef analyze_stock(stock_symbol: str) -> str:\\n    \"\"\"Get an analysis of the stock.\"\"\"\\n    # In a real implementation, this would analyze stock data\\n    return f\"{stock_symbol} has shown a steady increase over the last month.\"\\n\\n# List of tools\\ntools = [get_stock_price, analyze_stock]\\n\\n# Bind the model (LLM) with tools\\nmodel_with_tools = ChatAnthropic(\\n    model=\"claude-3-haiku-20240307\", temperature=0\\n).bind_tools(tools)\\n\\n# Generate a tool node\\ntool_node = ToolNode(tools)\\n\\n# Conditional edge to determine next node\\ndef should_continue(state: MessagesState) -> str:\\n    messages = state[\"messages\"]\\n    last_message = messages[-1]\\n    if last_message.tool_calls:\\n        return \"tools\"\\n    return END\\n\\n# Implement the execute_step function\\ndef execute_step(state: PlanExecute) -> PlanExecute:\\n    \"\"\"Uses llm with tool binding for the stock related queries\"\"\"\\n    input_query = state[\"input\"]\\n    stock_symbol = extract_stock_symbol(input_query)  # function to extract stock symbol from input\\n\\n    # Create prompt message for LLM\\n    messages = [\\n        SystemMessage(content=f\"Generate a detailed response to the following stock-related query: {input_query}. \"\\n                             \"Please consider the latest trends in the stock market, relevant financial data, \"\\n                             \"and any insights on potential stock performance.\")\\n    ]\\n    \\n    # Invoke the LLM with the messages\\n    response = model_with_tools.invoke(messages)\\n    \\n    # Update the state with the response\\n    state[\"response\"] = response\\n    return state\\n\\n# StateGraph compilation\\nworkflow = StateGraph(MessagesState)\\n\\n# Define the two nodes we will cycle between\\nworkflow.add_node(\"agent\", execute_step)\\nworkflow.add_node(\"tools\", tool_node)\\n\\n# Add edges for the workflow cycle\\nworkflow.add_edge(START, \"agent\")\\nworkflow.add_conditional_edges(\"agent\", should_continue, [\"tools\", END])\\nworkflow.add_edge(\"tools\", \"agent\")\\napp = workflow.compile()\\n```'}}\n",
      "Following edge from agent to replan\n",
      "Visiting node: replan\n",
      "{'identify_node': {'node_type': 'planner', 'messages': [HumanMessage(content='\\nYou are provided with the following information about the node:\\n<SchemaInfo>\\nPlanExecute: TypedDict with fields input (str), plan (List[str]), past_steps (Annotated[List[Tuple], operator.add]), response (str)\\n</SchemaInfo>\\n<InputSchema>\\nPlanExecute\\n</InputSchema>\\n<OutputSchema>\\nUnion[Response, Plan]\\n</OutputSchema>\\n<Description>\\nEvaluates progress and uses an LLM to either revise the plan or generate a final response.\\n</Description>\\n<FunctionName>\\nreplan_step\\n</FunctionName>\\n\\nBelow is the skeleton of the function that you need to implement:\\ndef replan_step(state:PlanExecute) -> Union[Response, Plan]:\\n    \"\"\"Evaluates progress and uses an LLM to either revise the plan or generate a final response.\"\"\"\\n    # Implement the function to meet the description.\\n\\nthe state is of type PlanExecute and the function is of type Union[Response, Plan]\\nThe general idea is that the implementation would involve extracting the input from the state, and updating the state with the output. Description contains the logic for this blackbox\\n', additional_kwargs={}, response_metadata={}, id='f40bba66-c66b-47ad-ab63-62c0c47edf14')], 'node_info': '\\nYou are provided with the following information about the node:\\n<SchemaInfo>\\nPlanExecute: TypedDict with fields input (str), plan (List[str]), past_steps (Annotated[List[Tuple], operator.add]), response (str)\\n</SchemaInfo>\\n<InputSchema>\\nPlanExecute\\n</InputSchema>\\n<OutputSchema>\\nUnion[Response, Plan]\\n</OutputSchema>\\n<Description>\\nEvaluates progress and uses an LLM to either revise the plan or generate a final response.\\n</Description>\\n<FunctionName>\\nreplan_step\\n</FunctionName>\\n\\nBelow is the skeleton of the function that you need to implement:\\ndef replan_step(state:PlanExecute) -> Union[Response, Plan]:\\n    \"\"\"Evaluates progress and uses an LLM to either revise the plan or generate a final response.\"\"\"\\n    # Implement the function to meet the description.\\n\\nthe state is of type PlanExecute and the function is of type Union[Response, Plan]\\nThe general idea is that the implementation would involve extracting the input from the state, and updating the state with the output. Description contains the logic for this blackbox\\n'}}\n",
      "{'planner': {'plan': ['Generate a prompt to evaluate the current progress and determine if a revision of the plan is needed or if a final response can be generated.', 'Use the LLM to process the generated prompt and obtain a response or a revised plan.']}}\n",
      "{'ai_node_gen_supervisor': {'next': 'prompt_generation', 'task': 'Generate a prompt to evaluate the current progress and determine if a revision of the plan is needed or if a final response can be generated.', 'messages': [AIMessage(content='To evaluate current progress, the next step is to generate a prompt that will assess whether the existing plan needs revision or if a final response can be created.', additional_kwargs={}, response_metadata={}, id='fbec1bc9-d6c3-4e2d-804d-8c45e63ac7da')]}}\n",
      "{'prompt_generation': {'past_steps': [('Generate a prompt to evaluate the current progress and determine if a revision of the plan is needed or if a final response can be generated.', 'To effectively evaluate progress and generate a relevant next step, the following prompt can be used:\\n\\n\"Given the current input, evaluate the existing plan and assess its effectiveness in achieving the desired outcome. Based on this evaluation, either revise the plan with specific improvements or generate a final response if the current plan suffices. Please provide a detailed explanation of your reasoning.\"')], 'messages': [HumanMessage(content='To effectively evaluate progress and generate a relevant next step, the following prompt can be used:\\n\\n\"Given the current input, evaluate the existing plan and assess its effectiveness in achieving the desired outcome. Based on this evaluation, either revise the plan with specific improvements or generate a final response if the current plan suffices. Please provide a detailed explanation of your reasoning.\"', additional_kwargs={}, response_metadata={}, name='prompt_generator', id='717f39de-c6b8-48e8-a8ff-7dcaffcb8e25')]}}\n",
      "{'replan': {'response': 'The prompt has been successfully generated to evaluate the current progress and determine if a revision of the plan is needed or if a final response can be generated. Please proceed with processing this prompt to obtain the necessary response or revised plan.'}}\n",
      "{'code_compiler': {'final_code': '```python\\nfrom typing import TypedDict, List, Tuple, Union\\nimport operator\\n\\n# Define the TypedDict as per the SchemaInfo\\nclass PlanExecute(TypedDict):\\n    input: str\\n    plan: List[str]\\n    past_steps: List[Tuple]\\n    response: str\\n\\n# Define the expected types for response and plan\\nResponse = str  # Assuming Response is a string\\nPlan = List[str]  # Assuming Plan is a list of strings\\n\\ndef replan_step(state: PlanExecute) -> Union[Response, Plan]:\\n    \"\"\"Evaluates progress and uses an LLM to either revise the plan or generate a final response.\"\"\"\\n    \\n    # Extract input from the state\\n    input_data = state[\\'input\\']\\n    current_plan = state[\\'plan\\']\\n    past_steps = state[\\'past_steps\\']\\n    \\n    # Generate the prompt using the provided prompt template\\n    prompt = (\\n        f\"Given the current input: \\'{input_data}\\', \"\\n        f\"evaluate the existing plan: {current_plan} \"\\n        f\"and assess its effectiveness in achieving the desired outcome. \"\\n        \"Based on this evaluation, either revise the plan with specific improvements \"\\n        \"or generate a final response if the current plan suffices. \"\\n        \"Please provide a detailed explanation of your reasoning.\"\\n    )\\n    \\n    # Here we would normally call the LLM with the prompt and get a response.\\n    # For this task, let\\'s assume we have a function `call_llm(prompt: str) -> Union[Response, Plan]`\\n    # response = call_llm(prompt)  # Placeholder for LLM call\\n    \\n    # For demonstration, we will return a mock response\\n    response = \"This is a mock response from the LLM indicating the current plan is sufficient.\"\\n    \\n    # Update the state with the new response, we could also include updated plan if needed.\\n    state[\\'response\\'] = response\\n    \\n    # For the sake of this example, we return the mock response.\\n    return response  # or return current_plan if a plan was updated\\n```'}}\n",
      "Following edge from replan to agent\n",
      "Following edge from replan to __END__\n",
      "Visiting node: __END__\n"
     ]
    }
   ],
   "source": [
    "dfs_traverse(json_objects, \"__START__\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ed2d40f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "################planner##########\n",
      "The provided edge information indicates that this is a non-conditional edge that directly routes from the 'planner' node to the 'agent' node after the planning step is completed. Hereâ€™s the Python code snippet that implements the edge:\n",
      "\n",
      "```python\n",
      "# Assuming we have a 'graph' object already defined\n",
      "\n",
      "# Adding a non-conditional edge from the 'planner' node to the 'agent' node\n",
      "graph.add_edge(\"planner\", \"agent\")\n",
      "\n",
      "# This edge defines a direct path from 'planner' to 'agent', which implies \n",
      "# that once the planning step is complete, the execution will immediately \n",
      "# move to the 'agent' node for further action.\n",
      "```\n",
      "\n",
      "This snippet demonstrates the implementation of a non-conditional edge as specified, ensuring that after executing the 'planner' node, the flow continues to the 'agent' node without any conditions.\n",
      "####################################################\n",
      "################agent##########\n",
      "```python\n",
      "# Implementing a non-conditional edge from \"agent\" to \"replan\" in the workflow defined above.\n",
      "\n",
      "# Adding a non-conditional edge that connects the \"agent\" node to the \"replan\" node.\n",
      "# This will create a direct flow from \"agent\" to \"replan\" after the agent node has completed its processing.\n",
      "workflow.add_edge(\"agent\", \"replan\")\n",
      "```\n",
      "This code snippet adds a non-conditional edge between the `agent` node and a new node named `replan`, allowing for direct routing to the `replan` node after the execution of the `agent` node without any conditions.\n",
      "####################################################\n",
      "################replan##########\n",
      "Here's the Python code implementing the specified logic to create conditional edges based on whether a response is generated or not, from the `replan` node to either continue to the `agent` node or end the process. The code uses the `add_conditional_edges` method to route the flow based on the conditions provided. \n",
      "\n",
      "```python\n",
      "from langgraph import StateGraph, Command, START, END\n",
      "\n",
      "# Assuming we have already defined the PlanExecute TypedDict and the replan_step function\n",
      "# based on the previous user input.\n",
      "\n",
      "# Initialize the graph\n",
      "graph = StateGraph(PlanExecute)\n",
      "\n",
      "# Add the 'replan' node to the graph\n",
      "graph.add_node(\"replan\", replan_step)\n",
      "\n",
      "# Define the routing function for conditional edges\n",
      "def routing_function(state: PlanExecute) -> str:\n",
      "    # Check if a response was generated in the state\n",
      "    if state['response']:  # If response is generated, move to __END__\n",
      "        return \"__END__\"\n",
      "    else:  # If no response is generated, continue to agent\n",
      "        return \"agent\"\n",
      "\n",
      "# Add conditional edges based on the routing function\n",
      "graph.add_conditional_edges(\"replan\", routing_function)\n",
      "\n",
      "# Add terminal node indicating the end of the process\n",
      "graph.add_edge(\"__END__\", END)\n",
      "\n",
      "# Finally, define the other relevant nodes, e.g., 'agent'\n",
      "# graph.add_node(\"agent\", agent_function) # Uncomment this line when agent function is defined.\n",
      "\n",
      "# Connect the start node to the 'replan' node to begin the process\n",
      "graph.add_edge(START, \"replan\")\n",
      "\n",
      "# You could potentially visualize the graph here or execute the graph based on your needs.\n",
      "```\n",
      "\n",
      "### Explanation:\n",
      "- **Graph Initialization:** The graph is created using `StateGraph` where the state is defined by `PlanExecute`.\n",
      "- **Node Addition:** The `replan` node is added to the graph, utilizing the `replan_step` function.\n",
      "- **Routing Function:** The `routing_function` checks if a response has been generated. If so, it routes to `__END__`, otherwise, it continues to the `agent` node.\n",
      "- **Adding Conditional Edges:** The conditional edges are added using `add_conditional_edges`, allowing for dynamic routing based on the graph's state.\n",
      "- **Ending the Process:** An edge is added to denote the endpoint of the process.\n",
      "- **Start Node Connection:** The graph starts at the `replan` node when executed.\n",
      "\n",
      "This structure helps manage the process flow effectively in your LangGraph implementation.\n",
      "####################################################\n"
     ]
    }
   ],
   "source": [
    "for edge_key, edge_val in edge_to_code.items():\n",
    "    print(f\"################{edge_key}##########\")\n",
    "    print(edge_val)\n",
    "    print(\"####################################################\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0d99e1b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "################planner##########\n",
      "```python\n",
      "from typing import List, Tuple, TypedDict\n",
      "import operator\n",
      "\n",
      "# Define the TypedDict as per the schema information\n",
      "class PlanExecute(TypedDict):\n",
      "    input: str\n",
      "    plan: List[str]\n",
      "    past_steps: List[Tuple]\n",
      "    response: str\n",
      "\n",
      "def plan_step(state: PlanExecute) -> PlanExecute:\n",
      "    \"\"\"Plan step generates a plan based on the input using llm structured output functionality, stores it to the plan field.\"\"\"\n",
      "    \n",
      "    # Extracting input from the state\n",
      "    input_data = state['input']\n",
      "    \n",
      "    # Generating the prompt for the LLM\n",
      "    prompt = (\n",
      "        \"Hello, ChatGPT! I hope you are doing well. I am reaching out to you for assistance with generating a structured plan based on specific input I will provide. \\n\\n\"\n",
      "        \"Please find the details below:\\n\\n\"\n",
      "        \"**function_name:** [generate_plan]  \\n\"\n",
      "        f\"**input:** {input_data}  \\n\"\n",
      "        \"**rule:** [I want you to act as a planning assistant. Based on the input, please create a detailed, step-by-step plan that outlines the actions necessary to achieve the objectives outlined in the input. Please format the plan as a list and ensure that it is clear and logical.]  \\n\\n\"\n",
      "        \"Your assistance is greatly appreciated. Thank you! Please provide the plan directly with no additional explanations.\"\n",
      "    )\n",
      "    \n",
      "    # Mocking LLM response (replace this block with actual call to LLM)\n",
      "    # response = call_to_llm(prompt)\n",
      "    \n",
      "    # Here we assume the LLM has returned the following response as a mock example\n",
      "    response = \"1. Research the topic. 2. Create an outline. 3. Write the first draft.\"\n",
      "    \n",
      "    # Storing the output in the state\n",
      "    state['plan'] = response.split(\". \")  # Convert string response to list\n",
      "    state['response'] = response\n",
      "    \n",
      "    return state\n",
      "```\n",
      "####################################################\n",
      "################agent##########\n",
      "```python\n",
      "from typing import List, Tuple, Dict, Any, TypedDict\n",
      "from langchain_core.messages import SystemMessage\n",
      "from langchain_core.tools import tool\n",
      "from langgraph.graph import StateGraph, MessagesState, START, END\n",
      "from langgraph.prebuilt import ToolNode\n",
      "from langchain import ChatAnthropic\n",
      "import operator\n",
      "\n",
      "# Define TypedDict structure for PlanExecute\n",
      "class PlanExecute(TypedDict):\n",
      "    input: str\n",
      "    plan: List[str]\n",
      "    past_steps: List[Tuple]\n",
      "    response: str\n",
      "\n",
      "# Define tools needed by the LLM\n",
      "@tool\n",
      "def get_stock_price(stock_symbol: str) -> str:\n",
      "    \"\"\"Call to get the current stock price.\"\"\"\n",
      "    # In a real implementation, this would query a financial API\n",
      "    return f\"The current price for {stock_symbol} is $100.00.\"\n",
      "\n",
      "@tool\n",
      "def analyze_stock(stock_symbol: str) -> str:\n",
      "    \"\"\"Get an analysis of the stock.\"\"\"\n",
      "    # In a real implementation, this would analyze stock data\n",
      "    return f\"{stock_symbol} has shown a steady increase over the last month.\"\n",
      "\n",
      "# List of tools\n",
      "tools = [get_stock_price, analyze_stock]\n",
      "\n",
      "# Bind the model (LLM) with tools\n",
      "model_with_tools = ChatAnthropic(\n",
      "    model=\"claude-3-haiku-20240307\", temperature=0\n",
      ").bind_tools(tools)\n",
      "\n",
      "# Generate a tool node\n",
      "tool_node = ToolNode(tools)\n",
      "\n",
      "# Conditional edge to determine next node\n",
      "def should_continue(state: MessagesState) -> str:\n",
      "    messages = state[\"messages\"]\n",
      "    last_message = messages[-1]\n",
      "    if last_message.tool_calls:\n",
      "        return \"tools\"\n",
      "    return END\n",
      "\n",
      "# Implement the execute_step function\n",
      "def execute_step(state: PlanExecute) -> PlanExecute:\n",
      "    \"\"\"Uses llm with tool binding for the stock related queries\"\"\"\n",
      "    input_query = state[\"input\"]\n",
      "    stock_symbol = extract_stock_symbol(input_query)  # function to extract stock symbol from input\n",
      "\n",
      "    # Create prompt message for LLM\n",
      "    messages = [\n",
      "        SystemMessage(content=f\"Generate a detailed response to the following stock-related query: {input_query}. \"\n",
      "                             \"Please consider the latest trends in the stock market, relevant financial data, \"\n",
      "                             \"and any insights on potential stock performance.\")\n",
      "    ]\n",
      "    \n",
      "    # Invoke the LLM with the messages\n",
      "    response = model_with_tools.invoke(messages)\n",
      "    \n",
      "    # Update the state with the response\n",
      "    state[\"response\"] = response\n",
      "    return state\n",
      "\n",
      "# StateGraph compilation\n",
      "workflow = StateGraph(MessagesState)\n",
      "\n",
      "# Define the two nodes we will cycle between\n",
      "workflow.add_node(\"agent\", execute_step)\n",
      "workflow.add_node(\"tools\", tool_node)\n",
      "\n",
      "# Add edges for the workflow cycle\n",
      "workflow.add_edge(START, \"agent\")\n",
      "workflow.add_conditional_edges(\"agent\", should_continue, [\"tools\", END])\n",
      "workflow.add_edge(\"tools\", \"agent\")\n",
      "app = workflow.compile()\n",
      "```\n",
      "####################################################\n",
      "################replan##########\n",
      "```python\n",
      "from typing import TypedDict, List, Tuple, Union\n",
      "import operator\n",
      "\n",
      "# Define the TypedDict as per the SchemaInfo\n",
      "class PlanExecute(TypedDict):\n",
      "    input: str\n",
      "    plan: List[str]\n",
      "    past_steps: List[Tuple]\n",
      "    response: str\n",
      "\n",
      "# Define the expected types for response and plan\n",
      "Response = str  # Assuming Response is a string\n",
      "Plan = List[str]  # Assuming Plan is a list of strings\n",
      "\n",
      "def replan_step(state: PlanExecute) -> Union[Response, Plan]:\n",
      "    \"\"\"Evaluates progress and uses an LLM to either revise the plan or generate a final response.\"\"\"\n",
      "    \n",
      "    # Extract input from the state\n",
      "    input_data = state['input']\n",
      "    current_plan = state['plan']\n",
      "    past_steps = state['past_steps']\n",
      "    \n",
      "    # Generate the prompt using the provided prompt template\n",
      "    prompt = (\n",
      "        f\"Given the current input: '{input_data}', \"\n",
      "        f\"evaluate the existing plan: {current_plan} \"\n",
      "        f\"and assess its effectiveness in achieving the desired outcome. \"\n",
      "        \"Based on this evaluation, either revise the plan with specific improvements \"\n",
      "        \"or generate a final response if the current plan suffices. \"\n",
      "        \"Please provide a detailed explanation of your reasoning.\"\n",
      "    )\n",
      "    \n",
      "    # Here we would normally call the LLM with the prompt and get a response.\n",
      "    # For this task, let's assume we have a function `call_llm(prompt: str) -> Union[Response, Plan]`\n",
      "    # response = call_llm(prompt)  # Placeholder for LLM call\n",
      "    \n",
      "    # For demonstration, we will return a mock response\n",
      "    response = \"This is a mock response from the LLM indicating the current plan is sufficient.\"\n",
      "    \n",
      "    # Update the state with the new response, we could also include updated plan if needed.\n",
      "    state['response'] = response\n",
      "    \n",
      "    # For the sake of this example, we return the mock response.\n",
      "    return response  # or return current_plan if a plan was updated\n",
      "```\n",
      "####################################################\n"
     ]
    }
   ],
   "source": [
    "for node_key, node_val in node_to_code.items():\n",
    "    print(f\"################{node_key}##########\")\n",
    "    print(node_val)\n",
    "    print(\"####################################################\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
