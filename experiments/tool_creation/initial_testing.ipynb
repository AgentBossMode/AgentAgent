{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "38a14c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "import logging\n",
    "from bs4 import BeautifulSoup\n",
    "import html2text\n",
    "import httpx\n",
    "import yaml\n",
    "import json\n",
    "from pydantic import Field, BaseModel\n",
    "from langgraph.graph import MessagesState, StateGraph, START, END\n",
    "from typing import List\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage, ToolMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.types import Send\n",
    "import operator\n",
    "from typing import Annotated\n",
    "from typing import NamedTuple\n",
    "import composio_langchain\n",
    "\n",
    "# Set up the logger\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,  # Set to DEBUG for detailed logs\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    "    handlers=[\n",
    "        # logging.FileHandler(\"scraper.log\"),  # Log to a file\n",
    "        logging.StreamHandler()  # Log to console\n",
    "    ]\n",
    ")\n",
    "\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5df1b6cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Tools\n",
      "\n",
      "[Tools](/docs/concepts/tools/) are utilities designed to be called by a model: their inputs are designed to be generated by models, and their outputs are designed to be passed back to models.\n",
      "\n",
      "A [toolkit](/docs/concepts/tools/#toolkits) is a collection of tools meant to be used together.\n",
      "\n",
      "info\n",
      "\n",
      "If you'd like to write your own tool, see [this how-to](/docs/how_to/custom_tools/). If you'd like to contribute an integration, see [Contributing integrations](/docs/contributing/how_to/integrations/).\n",
      "\n",
      "## Search​\n",
      "\n",
      "The following table shows tools that execute online searches in some shape or form:\n",
      "\n",
      "Tool/Toolkit| Free/Paid| Return Data  \n",
      "---|---|---  \n",
      "[Bing Search](/docs/integrations/tools/bing_search/)| Paid| URL, Snippet, Title  \n",
      "[Brave Search](/docs/integrations/tools/brave_search/)| Free| URL, Snippet, Title  \n",
      "[DuckDuckgoSearch](/docs/integrations/tools/ddg/)| Free| URL, Snippet, Title  \n",
      "[Exa Search](/docs/integrations/tools/exa_search/)| 1000 free searches/month| URL, Author, Title, Published Date  \n",
      "[Google Search](/docs/integrations/tools/google_search/)| Paid| URL, Snippet, Title  \n",
      "[Google Serper](/docs/integrations/tools/google_serper/)| Free| URL, Snippet, Title, Search Rank, Site Links  \n",
      "[Jina Search](/docs/integrations/tools/jina_search/)| 1M Response Tokens Free| URL, Snippet, Title, Page Content  \n",
      "[Mojeek Search](/docs/integrations/tools/mojeek_search/)| Paid| URL, Snippet, Title  \n",
      "[SearchApi](/docs/integrations/tools/searchapi/)| 100 Free Searches on Sign Up| URL, Snippet, Title, Search Rank, Site Links, Authors  \n",
      "[SearxNG Search](/docs/integrations/tools/searx_search/)| Free| URL, Snippet, Title, Category  \n",
      "[SerpAPI](/docs/integrations/tools/serpapi/)| 100 Free Searches/Month| Answer  \n",
      "[Tavily Search](/docs/integrations/tools/tavily_search/)| 1000 free searches/month| URL, Content, Title, Images, Answer  \n",
      "[You.com Search](/docs/integrations/tools/you/)| Free for 60 days| URL, Title, Page Content  \n",
      "  \n",
      "## Code Interpreter​\n",
      "\n",
      "The following table shows tools that can be used as code interpreters:\n",
      "\n",
      "Tool/Toolkit| Supported Languages| Sandbox Lifetime| Supports File Uploads| Return Types| Supports Self-Hosting  \n",
      "---|---|---|---|---|---  \n",
      "[Azure Container Apps dynamic sessions](/docs/integrations/tools/azure_dynamic_sessions/)| Python| 1 Hour| ✅| Text, Images| ❌  \n",
      "[Bearly Code Interpreter](/docs/integrations/tools/bearly/)| Python| Resets on Execution| ✅| Text| ❌  \n",
      "[Riza Code Interpreter](/docs/integrations/tools/riza/)| Python, JavaScript, PHP, Ruby| Resets on Execution| ✅| Text| ✅  \n",
      "  \n",
      "## Productivity​\n",
      "\n",
      "The following table shows tools that can be used to automate tasks in productivity tools:\n",
      "\n",
      "Tool/Toolkit| Pricing  \n",
      "---|---  \n",
      "[Github Toolkit](/docs/integrations/tools/github/)| Free  \n",
      "[Gitlab Toolkit](/docs/integrations/tools/gitlab/)| Free for personal project  \n",
      "[Gmail Toolkit](/docs/integrations/tools/gmail/)| Free, with limit of 250 quota units per user per second  \n",
      "[Infobip Tool](/docs/integrations/tools/infobip/)| Free trial, with variable pricing after  \n",
      "[Jira Toolkit](/docs/integrations/tools/jira/)| Free, with [rate limits](https://developer.atlassian.com/cloud/jira/platform/rate-limiting/)  \n",
      "[Office365 Toolkit](/docs/integrations/tools/office365/)| Free with Office365, includes [rate limits](https://learn.microsoft.com/en-us/graph/throttling-limits)  \n",
      "[Slack Toolkit](/docs/integrations/tools/slack/)| Free  \n",
      "[Twilio Tool](/docs/integrations/tools/twilio/)| Free trial, with [pay-as-you-go pricing](https://www.twilio.com/en-us/pricing) after  \n",
      "  \n",
      "## Web Browsing​\n",
      "\n",
      "The following table shows tools that can be used to automate tasks in web browsers:\n",
      "\n",
      "Tool/Toolkit| Pricing| Supports Interacting with the Browser  \n",
      "---|---|---  \n",
      "[AgentQL Toolkit](/docs/integrations/tools/agentql/)| Free trial, with pay-as-you-go and flat rate plans after| ✅  \n",
      "[Hyperbrowser Browser Agent Tools](/docs/integrations/tools/hyperbrowser_browser_agent_tools/)| Free trial, with flat rate plans and pre-paid credits after| ✅  \n",
      "[Hyperbrowser Web Scraping Tools](/docs/integrations/tools/hyperbrowser_web_scraping_tools/)| Free trial, with flat rate plans and pre-paid credits after| ❌  \n",
      "[MultiOn Toolkit](/docs/integrations/tools/multion/)| 40 free requests/day| ✅  \n",
      "[PlayWright Browser Toolkit](/docs/integrations/tools/playwright/)| Free| ✅  \n",
      "[Requests Toolkit](/docs/integrations/tools/requests/)| Free| ❌  \n",
      "  \n",
      "## Database​\n",
      "\n",
      "The following table shows tools that can be used to automate tasks in databases:\n",
      "\n",
      "Tool/Toolkit| Allowed Operations  \n",
      "---|---  \n",
      "[Cassandra Database Toolkit](/docs/integrations/tools/cassandra_database/)| SELECT and schema introspection  \n",
      "[SQLDatabase Toolkit](/docs/integrations/tools/sql_database/)| Any SQL operation  \n",
      "[Spark SQL Toolkit](/docs/integrations/tools/spark_sql/)| Any SQL operation  \n",
      "  \n",
      "## Finance​\n",
      "\n",
      "The following table shows tools that can be used to execute financial transactions such as payments, purchases, and more:\n",
      "\n",
      "Tool/Toolkit| Pricing| Capabilities  \n",
      "---|---|---  \n",
      "[GOAT](/docs/integrations/tools/goat/)| Free| Create and receive payments, purchase physical goods, make investments, and more.  \n",
      "  \n",
      "## All tools​\n",
      "\n",
      "Name| Description  \n",
      "---|---  \n",
      "[ADS4GPTs](/docs/integrations/tools/ads4gpts)| Integrate AI native advertising into your Agentic application.  \n",
      "[AgentQL](/docs/integrations/tools/agentql)| AgentQL tools provides web interaction and structured data extraction...  \n",
      "[AINetwork Toolkit](/docs/integrations/tools/ainetwork)| AI Network is a layer 1 blockchain designed to accommodate large-scal...  \n",
      "[Alpha Vantage](/docs/integrations/tools/alpha_vantage)| Alpha Vantage Alpha Vantage provides realtime and historical financia...  \n",
      "[Amadeus Toolkit](/docs/integrations/tools/amadeus)| This notebook walks you through connecting LangChain to the Amadeus t...  \n",
      "[Apify Actor](/docs/integrations/tools/apify_actors)| Apify Actors are cloud programs designed for a wide range of web scra...  \n",
      "[ArXiv](/docs/integrations/tools/arxiv)| This notebook goes over how to use the arxiv tool with an agent.  \n",
      "[AskNews](/docs/integrations/tools/asknews)| AskNews infuses any LLM with the latest global news (or historical ne...  \n",
      "[AWS Lambda](/docs/integrations/tools/awslambda)| Amazon AWS Lambda is a serverless computing service provided by Amazo...  \n",
      "[Azure AI Services Toolkit](/docs/integrations/tools/azure_ai_services)| This toolkit is used to interact with the Azure AI Services API to ac...  \n",
      "[Azure Cognitive Services Toolkit](/docs/integrations/tools/azure_cognitive_services)| This toolkit is used to interact with the Azure Cognitive Services AP...  \n",
      "[Azure Container Apps dynamic sessions](/docs/integrations/tools/azure_dynamic_sessions)| Azure Container Apps dynamic sessions provides a secure and scalable ...  \n",
      "[Shell (bash)](/docs/integrations/tools/bash)| Giving agents access to the shell is powerful (though risky outside a...  \n",
      "[Bearly Code Interpreter](/docs/integrations/tools/bearly)| Bearly Code Interpreter allows for remote execution of code. This mak...  \n",
      "[Bing Search](/docs/integrations/tools/bing_search)| Bing Search is an Azure service and enables safe, ad-free, location-a...  \n",
      "[Brave Search](/docs/integrations/tools/brave_search)| This notebook goes over how to use the Brave Search tool.  \n",
      "[Cassandra Database Toolkit](/docs/integrations/tools/cassandra_database)| Apache Cassandra® is a widely used database for storing transactional...  \n",
      "[CDP](/docs/integrations/tools/cdp_agentkit)| The CDP Agentkit toolkit contains tools that enable an LLM agent to i...  \n",
      "[ChatGPT Plugins](/docs/integrations/tools/chatgpt_plugins)| OpenAI has deprecated plugins.  \n",
      "[ClickUp Toolkit](/docs/integrations/tools/clickup)| ClickUp is an all-in-one productivity platform that provides small an...  \n",
      "[Cogniswitch Toolkit](/docs/integrations/tools/cogniswitch)| CogniSwitch is used to build production ready applications that can c...  \n",
      "[Connery Toolkit and Tools](/docs/integrations/tools/connery)| Using the Connery toolkit and tools, you can integrate Connery Action...  \n",
      "[Dall-E Image Generator](/docs/integrations/tools/dalle_image_generator)| OpenAI Dall-E are text-to-image models developed by OpenAI using deep...  \n",
      "[Dappier](/docs/integrations/tools/dappier)| Dappier connects any LLM or your Agentic AI to real-time, rights-clea...  \n",
      "[Databricks Unity Catalog (UC)](/docs/integrations/tools/databricks)| This notebook shows how to use UC functions as LangChain tools, with ...  \n",
      "[DataForSEO](/docs/integrations/tools/dataforseo)| DataForSeo provides comprehensive SEO and digital marketing data solu...  \n",
      "[Dataherald](/docs/integrations/tools/dataherald)| This notebook goes over how to use the dataherald component.  \n",
      "[DuckDuckGo Search](/docs/integrations/tools/ddg)| This guide shows over how to use the DuckDuckGo search component.  \n",
      "[Discord](/docs/integrations/tools/discord)| This notebook provides a quick overview for getting started with Disc...  \n",
      "[E2B Data Analysis](/docs/integrations/tools/e2b_data_analysis)| E2B's cloud environments are great runtime sandboxes for LLMs.  \n",
      "[Eden AI](/docs/integrations/tools/edenai_tools)| This Jupyter Notebook demonstrates how to use Eden AI tools with an A...  \n",
      "[ElevenLabs Text2Speech](/docs/integrations/tools/eleven_labs_tts)| This notebook shows how to interact with the ElevenLabs API to achiev...  \n",
      "[Exa Search](/docs/integrations/tools/exa_search)| Exa is a search engine fully designed for use by LLMs. Search for doc...  \n",
      "[File System](/docs/integrations/tools/filesystem)| LangChain provides tools for interacting with a local file system out...  \n",
      "[FinancialDatasets Toolkit](/docs/integrations/tools/financial_datasets)| The financial datasets stock market API provides REST endpoints that ...  \n",
      "[FMP Data](/docs/integrations/tools/fmp-data)| Access financial market data through natural language queries.  \n",
      "[Github Toolkit](/docs/integrations/tools/github)| The Github toolkit contains tools that enable an LLM agent to interac...  \n",
      "[Gitlab Toolkit](/docs/integrations/tools/gitlab)| The Gitlab toolkit contains tools that enable an LLM agent to interac...  \n",
      "[Gmail Toolkit](/docs/integrations/tools/gmail)| This will help you getting started with the GMail toolkit. This toolk...  \n",
      "[GOAT](/docs/integrations/tools/goat)| GOAT is the finance toolkit for AI agents.  \n",
      "[Golden Query](/docs/integrations/tools/golden_query)| Golden provides a set of natural language APIs for querying and enric...  \n",
      "[Google Books](/docs/integrations/tools/google_books)| Overview  \n",
      "[Google Calendar Toolkit](/docs/integrations/tools/google_calendar)| Google Calendar is a product of Google Workspace that allows users to...  \n",
      "[Google Cloud Text-to-Speech](/docs/integrations/tools/google_cloud_texttospeech)| Google Cloud Text-to-Speech enables developers to synthesize natural-...  \n",
      "[Google Drive](/docs/integrations/tools/google_drive)| This notebook walks through connecting a LangChain to the Google Driv...  \n",
      "[Google Finance](/docs/integrations/tools/google_finance)| This notebook goes over how to use the Google Finance Tool to get inf...  \n",
      "[Google Imagen](/docs/integrations/tools/google_imagen)| Imagen on Vertex AI brings Google's state of the art image generative...  \n",
      "[Google Jobs](/docs/integrations/tools/google_jobs)| This notebook goes over how to use the Google Jobs Tool to fetch curr...  \n",
      "[Google Lens](/docs/integrations/tools/google_lens)| This notebook goes over how to use the Google Lens Tool to fetch info...  \n",
      "[Google Places](/docs/integrations/tools/google_places)| This notebook goes through how to use Google Places API  \n",
      "[Google Scholar](/docs/integrations/tools/google_scholar)| This notebook goes through how to use Google Scholar Tool  \n",
      "[Google Search](/docs/integrations/tools/google_search)| This notebook goes over how to use the google search component.  \n",
      "[Google Serper](/docs/integrations/tools/google_serper)| This notebook goes over how to use the Google Serper component to sea...  \n",
      "[Google Trends](/docs/integrations/tools/google_trends)| This notebook goes over how to use the Google Trends Tool to fetch tr...  \n",
      "[Gradio](/docs/integrations/tools/gradio_tools)| There are many 1000s of Gradio apps on Hugging Face Spaces. This libr...  \n",
      "[GraphQL](/docs/integrations/tools/graphql)| GraphQL is a query language for APIs and a runtime for executing thos...  \n",
      "[HuggingFace Hub Tools](/docs/integrations/tools/huggingface_tools)| Huggingface Tools that supporting text I/O can be  \n",
      "[Human as a tool](/docs/integrations/tools/human_tools)| Human are AGI so they can certainly be used as a tool to help out AI ...  \n",
      "[Hyperbrowser Browser Agent Tools](/docs/integrations/tools/hyperbrowser_browser_agent_tools)| Hyperbrowser is a platform for running, running browser agents, and s...  \n",
      "[Hyperbrowser Web Scraping Tools](/docs/integrations/tools/hyperbrowser_web_scraping_tools)| Hyperbrowser is a platform for running and scaling headless browsers....  \n",
      "[IBM watsonx.ai](/docs/integrations/tools/ibm_watsonx)| WatsonxToolkit is a wrapper for IBM watsonx.ai Toolkit.  \n",
      "[IFTTT WebHooks](/docs/integrations/tools/ifttt)| This notebook shows how to use IFTTT Webhooks.  \n",
      "[Infobip](/docs/integrations/tools/infobip)| This notebook that shows how to use Infobip API wrapper to send SMS m...  \n",
      "[Ionic Shopping Tool](/docs/integrations/tools/ionic_shopping)| Ionic is a plug and play ecommerce marketplace for AI Assistants. By ...  \n",
      "[Jenkins](/docs/integrations/tools/jenkins)| Tools for interacting with Jenkins.  \n",
      "[Jina Search](/docs/integrations/tools/jina_search)| This notebook provides a quick overview for getting started with Jina...  \n",
      "[Jira Toolkit](/docs/integrations/tools/jira)| This notebook goes over how to use the Jira toolkit.  \n",
      "[JSON Toolkit](/docs/integrations/tools/json)| This notebook showcases an agent interacting with large JSON/dict obj...  \n",
      "[Lemon Agent](/docs/integrations/tools/lemonai)| Lemon Agent helps you build powerful AI assistants in minutes and aut...  \n",
      "[LinkupSearchTool](/docs/integrations/tools/linkup_search)| Linkup provides an API to connect LLMs to the web and the Linkup Prem...  \n",
      "[Memgraph](/docs/integrations/tools/memgraph)| Overview  \n",
      "[Memorize](/docs/integrations/tools/memorize)| Fine-tuning LLM itself to memorize information using unsupervised lea...  \n",
      "[Mojeek Search](/docs/integrations/tools/mojeek_search)| The following notebook will explain how to get results using Mojeek S...  \n",
      "[MultiOn Toolkit](/docs/integrations/tools/multion)| MultiON has built an AI Agent that can interact with a broad array of...  \n",
      "[NASA Toolkit](/docs/integrations/tools/nasa)| This notebook shows how to use agents to interact with the NASA toolk...  \n",
      "[Naver Search](/docs/integrations/tools/naver_search)| Overview  \n",
      "[Nuclia Understanding](/docs/integrations/tools/nuclia)| Nuclia automatically indexes your unstructured data from any internal...  \n",
      "[NVIDIA Riva: ASR and TTS](/docs/integrations/tools/nvidia_riva)| NVIDIA Riva  \n",
      "[Office365 Toolkit](/docs/integrations/tools/office365)| Microsoft 365 is a product family of productivity software, collabora...  \n",
      "[OpenAPI Toolkit](/docs/integrations/tools/openapi)| We can construct agents to consume arbitrary APIs, here APIs conforma...  \n",
      "[Natural Language API Toolkits](/docs/integrations/tools/openapi_nla)| Natural Language API Toolkits (NLAToolkits) permit LangChain Agents t...  \n",
      "[OpenGradient](/docs/integrations/tools/opengradient_toolkit)| This notebook shows how to build tools using the OpenGradient toolkit...  \n",
      "[OpenWeatherMap](/docs/integrations/tools/openweathermap)| This notebook goes over how to use the OpenWeatherMap component to fe...  \n",
      "[Oracle AI Vector Search: Generate Summary](/docs/integrations/tools/oracleai)| Oracle AI Vector Search is designed for Artificial Intelligence (AI) ...  \n",
      "[Oxylabs](/docs/integrations/tools/oxylabs)| Oxylabs is a market-leading web intelligence collection platform, dri...  \n",
      "[Pandas Dataframe](/docs/integrations/tools/pandas)| This notebook shows how to use agents to interact with a Pandas DataF...  \n",
      "[Passio NutritionAI](/docs/integrations/tools/passio_nutrition_ai)| To best understand how NutritionAI can give your agents super food-nu...  \n",
      "[PaymanAI](/docs/integrations/tools/payman-tool)| PaymanAI provides functionality to send and receive payments (fiat an...  \n",
      "[Permit](/docs/integrations/tools/permit)| Permit is an access control platform that provides fine-grained, real...  \n",
      "[PlayWright Browser Toolkit](/docs/integrations/tools/playwright)| Playwright is an open-source automation tool developed by Microsoft t...  \n",
      "[Polygon IO Toolkit and Tools](/docs/integrations/tools/polygon)| This notebook shows how to use agents to interact with the Polygon IO...  \n",
      "[PowerBI Toolkit](/docs/integrations/tools/powerbi)| This notebook showcases an agent interacting with a Power BI Dataset....  \n",
      "[Prolog](/docs/integrations/tools/prolog_tool)| LangChain tools that use Prolog rules to generate answers.  \n",
      "[PubMed](/docs/integrations/tools/pubmed)| PubMed® comprises more than 35 million citations for biomedical liter...  \n",
      "[Python REPL](/docs/integrations/tools/python)| Sometimes, for complex calculations, rather than have an LLM generate...  \n",
      "[Reddit Search](/docs/integrations/tools/reddit_search)| In this notebook, we learn how the Reddit search tool works.  \n",
      "[Requests Toolkit](/docs/integrations/tools/requests)| We can use the Requests toolkit to construct agents that generate HTT...  \n",
      "[Riza Code Interpreter](/docs/integrations/tools/riza)| The Riza Code Interpreter is a WASM-based isolated environment for ru...  \n",
      "[Robocorp Toolkit](/docs/integrations/tools/robocorp)| This notebook covers how to get started with Robocorp Action Server a...  \n",
      "[Salesforce](/docs/integrations/tools/salesforce)| Tools for interacting with Salesforce.  \n",
      "[SceneXplain](/docs/integrations/tools/sceneXplain)| SceneXplain is an ImageCaptioning service accessible through the Scen...  \n",
      "[ScrapeGraph](/docs/integrations/tools/scrapegraph)| This notebook provides a quick overview for getting started with Scra...  \n",
      "[SearchApi](/docs/integrations/tools/searchapi)| This notebook shows examples of how to use SearchApi to search the we...  \n",
      "[SearxNG Search](/docs/integrations/tools/searx_search)| This notebook goes over how to use a self hosted SearxNG search API t...  \n",
      "[Semantic Scholar API Tool](/docs/integrations/tools/semanticscholar)| This notebook demos how to use the semantic scholar tool with an agen...  \n",
      "[SerpAPI](/docs/integrations/tools/serpapi)| This notebook goes over how to use the SerpAPI component to search th...  \n",
      "[Slack Toolkit](/docs/integrations/tools/slack)| This will help you getting started with the Slack toolkit. For detail...  \n",
      "[Spark SQL Toolkit](/docs/integrations/tools/spark_sql)| This notebook shows how to use agents to interact with Spark SQL. Sim...  \n",
      "[SQLDatabase Toolkit](/docs/integrations/tools/sql_database)| This will help you getting started with the SQL Database toolkit. For...  \n",
      "[StackExchange](/docs/integrations/tools/stackexchange)| Stack Exchange is a network of question-and-answer (Q&A) websites on ...  \n",
      "[Steam Toolkit](/docs/integrations/tools/steam)| Steam (Wikipedia)) is a video game digital distribution service and s...  \n",
      "[Stripe](/docs/integrations/tools/stripe)| This notebook provides a quick overview for getting started with Stri...  \n",
      "[Tableau](/docs/integrations/tools/tableau)| This notebook provides a quick overview for getting started with Tabl...  \n",
      "[Taiga](/docs/integrations/tools/taiga)| This notebook provides a quick overview for getting started with Taig...  \n",
      "[Tavily Extract](/docs/integrations/tools/tavily_extract)| Tavily is a search engine built specifically for AI agents (LLMs), de...  \n",
      "[Tavily Search](/docs/integrations/tools/tavily_search)| Tavily's Search API is a search engine built specifically for AI agen...  \n",
      "[Tilores](/docs/integrations/tools/tilores)| This notebook covers how to get started with the Tilores tools.  \n",
      "[Twilio](/docs/integrations/tools/twilio)| This notebook goes over how to use the Twilio API wrapper to send a m...  \n",
      "[Upstage](/docs/integrations/tools/upstage_groundedness_check)| This notebook covers how to get started with Upstage groundedness che...  \n",
      "[Valthera](/docs/integrations/tools/valthera)| Enable AI agents to engage users when they're most likely to respond.  \n",
      "[ValyuContext](/docs/integrations/tools/valyu_context)| Valyu allows AI applications and agents to search the internet and pr...  \n",
      "[Wikidata](/docs/integrations/tools/wikidata)| Wikidata is a free and open knowledge base that can be read and edite...  \n",
      "[Wikipedia](/docs/integrations/tools/wikipedia)| Wikipedia is a multilingual free online encyclopedia written and main...  \n",
      "[Wolfram Alpha](/docs/integrations/tools/wolfram_alpha)| This notebook goes over how to use the wolfram alpha component.  \n",
      "[Writer Tools](/docs/integrations/tools/writer)| This notebook provides a quick overview for getting started with Writ...  \n",
      "[Yahoo Finance News](/docs/integrations/tools/yahoo_finance_news)| This notebook goes over how to use the yahoofinancenews tool with an ...  \n",
      "[You.com Search](/docs/integrations/tools/you)| The you.com API is a suite of tools designed to help developers groun...  \n",
      "[YouTube](/docs/integrations/tools/youtube)| YouTube Search package searches YouTube videos avoiding using their h...  \n",
      "[Zapier Natural Language Actions](/docs/integrations/tools/zapier)| Deprecated This API will be sunset on 2023-11-17//nla.zapier.com/star...  \n",
      "[ZenGuard AI](/docs/integrations/tools/zenguard)| This tool lets you quickly set up ZenGuard AI in your Langchain-power...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def fetch_documents_with_links_html(url: str) -> tuple[str, list[tuple[str, str]]]:\n",
    "    \"\"\"Fetch a document from a URL, return the markdownified text with links as markdown, and extract links with their titles.\n",
    "\n",
    "    Args:\n",
    "        url (str): The URL of the document to fetch.\n",
    "\n",
    "    Returns:\n",
    "        tuple[str, list[tuple[str, str]]]: A tuple containing the markdownified text of the document with links, and a list of (link, title) tuples.\n",
    "    \"\"\"\n",
    "    httpx_client = httpx.Client(follow_redirects=True, timeout=10)\n",
    "    links = []\n",
    "\n",
    "    try:\n",
    "        response = httpx_client.get(url, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        html_content = response.text\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "        target_div = soup.find('div', class_= \"theme-doc-markdown markdown\") #langchain\n",
    "\n",
    "        if not target_div:\n",
    "            target_div = soup.find('article') #langraph\n",
    "\n",
    "        if not target_div:\n",
    "            return \"\", links # Return empty text but still the links\n",
    "\n",
    "        # Extract links *before* converting to markdown\n",
    "        a_tags = target_div.find_all('a')\n",
    "        for a_tag in a_tags:\n",
    "            link = a_tag.get('href')\n",
    "            title = a_tag.get_text(strip=True)\n",
    "            if link:\n",
    "                links.append((link, title))\n",
    "\n",
    "        markdown_converter = html2text.HTML2Text()\n",
    "        markdown_converter.body_width = 0  # Disable line wrapping for links to stay on one line\n",
    "        markdown_text = markdown_converter.handle(str(target_div))\n",
    "\n",
    "        return markdown_text, links\n",
    "    except (httpx.HTTPStatusError, httpx.RequestError) as e:\n",
    "        return f\"Encountered an HTTP error: {str(e)}\", [] # Return error message and empty links\n",
    "\n",
    "doc, link = fetch_documents_with_links_html(\"https://python.langchain.com/docs/integrations/tools\")\n",
    "print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e9e421",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ads4gpts', 'agentql', 'ainetwork', 'alpha_vantage', 'amadeus', 'apify_actors', 'arxiv', 'asknews', 'awslambda', 'azure_ai_services', 'azure_cognitive_services', 'azure_dynamic_sessions', 'bash', 'bearly', 'bing_search', 'brave_search', 'cassandra_database', 'cdp_agentkit', 'chatgpt_plugins', 'clickup', 'cogniswitch', 'connery', 'dalle_image_generator', 'dappier', 'databricks', 'dataforseo', 'dataherald', 'ddg', 'discord', 'e2b_data_analysis', 'edenai_tools', 'eleven_labs_tts', 'exa_search', 'filesystem', 'financial_datasets', 'fmp-data', 'github', 'gitlab', 'gmail', 'goat', 'golden_query', 'google_books', 'google_calendar', 'google_cloud_texttospeech', 'google_drive', 'google_finance', 'google_imagen', 'google_jobs', 'google_lens', 'google_places', 'google_scholar', 'google_search', 'google_serper', 'google_trends', 'gradio_tools', 'graphql', 'huggingface_tools', 'human_tools', 'hyperbrowser_browser_agent_tools', 'hyperbrowser_web_scraping_tools', 'ibm_watsonx', 'ifttt', 'infobip', 'ionic_shopping', 'jenkins', 'jina_search', 'jira', 'json', 'lemonai', 'linkup_search', 'memgraph', 'memorize', 'mojeek_search', 'multion', 'nasa', 'naver_search', 'nuclia', 'nvidia_riva', 'office365', 'openapi', 'openapi_nla', 'opengradient_toolkit', 'openweathermap', 'oracleai', 'oxylabs', 'pandas', 'passio_nutrition_ai', 'payman-tool', 'permit', 'playwright', 'polygon', 'powerbi', 'prolog_tool', 'pubmed', 'python', 'reddit_search', 'requests', 'riza', 'robocorp', 'salesforce', 'sceneXplain', 'scrapegraph', 'searchapi', 'searx_search', 'semanticscholar', 'serpapi', 'slack', 'spark_sql', 'sql_database', 'stackexchange', 'steam', 'stripe', 'tableau', 'taiga', 'tavily_extract', 'tavily_search', 'tilores', 'twilio', 'upstage_groundedness_check', 'valthera', 'valyu_context', 'wikidata', 'wikipedia', 'wolfram_alpha', 'writer', 'yahoo_finance_news', 'you', 'youtube', 'zapier', 'zenguard']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Your input text\n",
    "text = \"\"\"\n",
    "[ADS4GPTs](/docs/integrations/tools/ads4gpts)| Integrate AI native advertising into your Agentic application.  \n",
    "[AgentQL](/docs/integrations/tools/agentql)| AgentQL tools provides web interaction and structured data extraction...  \n",
    "[AINetwork Toolkit](/docs/integrations/tools/ainetwork)| AI Network is a layer 1 blockchain designed to accommodate large-scal...  \n",
    "[Alpha Vantage](/docs/integrations/tools/alpha_vantage)| Alpha Vantage Alpha Vantage provides realtime and historical financia...  \n",
    "[Amadeus Toolkit](/docs/integrations/tools/amadeus)| This notebook walks you through connecting LangChain to the Amadeus t...  \n",
    "[Apify Actor](/docs/integrations/tools/apify_actors)| Apify Actors are cloud programs designed for a wide range of web scra...  \n",
    "[ArXiv](/docs/integrations/tools/arxiv)| This notebook goes over how to use the arxiv tool with an agent.  \n",
    "[AskNews](/docs/integrations/tools/asknews)| AskNews infuses any LLM with the latest global news (or historical ne...  \n",
    "[AWS Lambda](/docs/integrations/tools/awslambda)| Amazon AWS Lambda is a serverless computing service provided by Amazo...  \n",
    "[Azure AI Services Toolkit](/docs/integrations/tools/azure_ai_services)| This toolkit is used to interact with the Azure AI Services API to ac...  \n",
    "[Azure Cognitive Services Toolkit](/docs/integrations/tools/azure_cognitive_services)| This toolkit is used to interact with the Azure Cognitive Services AP...  \n",
    "[Azure Container Apps dynamic sessions](/docs/integrations/tools/azure_dynamic_sessions)| Azure Container Apps dynamic sessions provides a secure and scalable ...  \n",
    "[Shell (bash)](/docs/integrations/tools/bash)| Giving agents access to the shell is powerful (though risky outside a...  \n",
    "[Bearly Code Interpreter](/docs/integrations/tools/bearly)| Bearly Code Interpreter allows for remote execution of code. This mak...  \n",
    "[Bing Search](/docs/integrations/tools/bing_search)| Bing Search is an Azure service and enables safe, ad-free, location-a...  \n",
    "[Brave Search](/docs/integrations/tools/brave_search)| This notebook goes over how to use the Brave Search tool.  \n",
    "[Cassandra Database Toolkit](/docs/integrations/tools/cassandra_database)| Apache Cassandra® is a widely used database for storing transactional...  \n",
    "[CDP](/docs/integrations/tools/cdp_agentkit)| The CDP Agentkit toolkit contains tools that enable an LLM agent to i...  \n",
    "[ChatGPT Plugins](/docs/integrations/tools/chatgpt_plugins)| OpenAI has deprecated plugins.  \n",
    "[ClickUp Toolkit](/docs/integrations/tools/clickup)| ClickUp is an all-in-one productivity platform that provides small an...  \n",
    "[Cogniswitch Toolkit](/docs/integrations/tools/cogniswitch)| CogniSwitch is used to build production ready applications that can c...  \n",
    "[Connery Toolkit and Tools](/docs/integrations/tools/connery)| Using the Connery toolkit and tools, you can integrate Connery Action...  \n",
    "[Dall-E Image Generator](/docs/integrations/tools/dalle_image_generator)| OpenAI Dall-E are text-to-image models developed by OpenAI using deep...  \n",
    "[Dappier](/docs/integrations/tools/dappier)| Dappier connects any LLM or your Agentic AI to real-time, rights-clea...  \n",
    "[Databricks Unity Catalog (UC)](/docs/integrations/tools/databricks)| This notebook shows how to use UC functions as LangChain tools, with ...  \n",
    "[DataForSEO](/docs/integrations/tools/dataforseo)| DataForSeo provides comprehensive SEO and digital marketing data solu...  \n",
    "[Dataherald](/docs/integrations/tools/dataherald)| This notebook goes over how to use the dataherald component.  \n",
    "[DuckDuckGo Search](/docs/integrations/tools/ddg)| This guide shows over how to use the DuckDuckGo search component.  \n",
    "[Discord](/docs/integrations/tools/discord)| This notebook provides a quick overview for getting started with Disc...  \n",
    "[E2B Data Analysis](/docs/integrations/tools/e2b_data_analysis)| E2B's cloud environments are great runtime sandboxes for LLMs.  \n",
    "[Eden AI](/docs/integrations/tools/edenai_tools)| This Jupyter Notebook demonstrates how to use Eden AI tools with an A...  \n",
    "[ElevenLabs Text2Speech](/docs/integrations/tools/eleven_labs_tts)| This notebook shows how to interact with the ElevenLabs API to achiev...  \n",
    "[Exa Search](/docs/integrations/tools/exa_search)| Exa is a search engine fully designed for use by LLMs. Search for doc...  \n",
    "[File System](/docs/integrations/tools/filesystem)| LangChain provides tools for interacting with a local file system out...  \n",
    "[FinancialDatasets Toolkit](/docs/integrations/tools/financial_datasets)| The financial datasets stock market API provides REST endpoints that ...  \n",
    "[FMP Data](/docs/integrations/tools/fmp-data)| Access financial market data through natural language queries.  \n",
    "[Github Toolkit](/docs/integrations/tools/github)| The Github toolkit contains tools that enable an LLM agent to interac...  \n",
    "[Gitlab Toolkit](/docs/integrations/tools/gitlab)| The Gitlab toolkit contains tools that enable an LLM agent to interac...  \n",
    "[Gmail Toolkit](/docs/integrations/tools/gmail)| This will help you getting started with the GMail toolkit. This toolk...  \n",
    "[GOAT](/docs/integrations/tools/goat)| GOAT is the finance toolkit for AI agents.  \n",
    "[Golden Query](/docs/integrations/tools/golden_query)| Golden provides a set of natural language APIs for querying and enric...  \n",
    "[Google Books](/docs/integrations/tools/google_books)| Overview  \n",
    "[Google Calendar Toolkit](/docs/integrations/tools/google_calendar)| Google Calendar is a product of Google Workspace that allows users to...  \n",
    "[Google Cloud Text-to-Speech](/docs/integrations/tools/google_cloud_texttospeech)| Google Cloud Text-to-Speech enables developers to synthesize natural-...  \n",
    "[Google Drive](/docs/integrations/tools/google_drive)| This notebook walks through connecting a LangChain to the Google Driv...  \n",
    "[Google Finance](/docs/integrations/tools/google_finance)| This notebook goes over how to use the Google Finance Tool to get inf...  \n",
    "[Google Imagen](/docs/integrations/tools/google_imagen)| Imagen on Vertex AI brings Google's state of the art image generative...  \n",
    "[Google Jobs](/docs/integrations/tools/google_jobs)| This notebook goes over how to use the Google Jobs Tool to fetch curr...  \n",
    "[Google Lens](/docs/integrations/tools/google_lens)| This notebook goes over how to use the Google Lens Tool to fetch info...  \n",
    "[Google Places](/docs/integrations/tools/google_places)| This notebook goes through how to use Google Places API  \n",
    "[Google Scholar](/docs/integrations/tools/google_scholar)| This notebook goes through how to use Google Scholar Tool  \n",
    "[Google Search](/docs/integrations/tools/google_search)| This notebook goes over how to use the google search component.  \n",
    "[Google Serper](/docs/integrations/tools/google_serper)| This notebook goes over how to use the Google Serper component to sea...  \n",
    "[Google Trends](/docs/integrations/tools/google_trends)| This notebook goes over how to use the Google Trends Tool to fetch tr...  \n",
    "[Gradio](/docs/integrations/tools/gradio_tools)| There are many 1000s of Gradio apps on Hugging Face Spaces. This libr...  \n",
    "[GraphQL](/docs/integrations/tools/graphql)| GraphQL is a query language for APIs and a runtime for executing thos...  \n",
    "[HuggingFace Hub Tools](/docs/integrations/tools/huggingface_tools)| Huggingface Tools that supporting text I/O can be  \n",
    "[Human as a tool](/docs/integrations/tools/human_tools)| Human are AGI so they can certainly be used as a tool to help out AI ...  \n",
    "[Hyperbrowser Browser Agent Tools](/docs/integrations/tools/hyperbrowser_browser_agent_tools)| Hyperbrowser is a platform for running, running browser agents, and s...  \n",
    "[Hyperbrowser Web Scraping Tools](/docs/integrations/tools/hyperbrowser_web_scraping_tools)| Hyperbrowser is a platform for running and scaling headless browsers....  \n",
    "[IBM watsonx.ai](/docs/integrations/tools/ibm_watsonx)| WatsonxToolkit is a wrapper for IBM watsonx.ai Toolkit.  \n",
    "[IFTTT WebHooks](/docs/integrations/tools/ifttt)| This notebook shows how to use IFTTT Webhooks.  \n",
    "[Infobip](/docs/integrations/tools/infobip)| This notebook that shows how to use Infobip API wrapper to send SMS m...  \n",
    "[Ionic Shopping Tool](/docs/integrations/tools/ionic_shopping)| Ionic is a plug and play ecommerce marketplace for AI Assistants. By ...  \n",
    "[Jenkins](/docs/integrations/tools/jenkins)| Tools for interacting with Jenkins.  \n",
    "[Jina Search](/docs/integrations/tools/jina_search)| This notebook provides a quick overview for getting started with Jina...  \n",
    "[Jira Toolkit](/docs/integrations/tools/jira)| This notebook goes over how to use the Jira toolkit.  \n",
    "[JSON Toolkit](/docs/integrations/tools/json)| This notebook showcases an agent interacting with large JSON/dict obj...  \n",
    "[Lemon Agent](/docs/integrations/tools/lemonai)| Lemon Agent helps you build powerful AI assistants in minutes and aut...  \n",
    "[LinkupSearchTool](/docs/integrations/tools/linkup_search)| Linkup provides an API to connect LLMs to the web and the Linkup Prem...  \n",
    "[Memgraph](/docs/integrations/tools/memgraph)| Overview  \n",
    "[Memorize](/docs/integrations/tools/memorize)| Fine-tuning LLM itself to memorize information using unsupervised lea...  \n",
    "[Mojeek Search](/docs/integrations/tools/mojeek_search)| The following notebook will explain how to get results using Mojeek S...  \n",
    "[MultiOn Toolkit](/docs/integrations/tools/multion)| MultiON has built an AI Agent that can interact with a broad array of...  \n",
    "[NASA Toolkit](/docs/integrations/tools/nasa)| This notebook shows how to use agents to interact with the NASA toolk...  \n",
    "[Naver Search](/docs/integrations/tools/naver_search)| Overview  \n",
    "[Nuclia Understanding](/docs/integrations/tools/nuclia)| Nuclia automatically indexes your unstructured data from any internal...  \n",
    "[NVIDIA Riva: ASR and TTS](/docs/integrations/tools/nvidia_riva)| NVIDIA Riva  \n",
    "[Office365 Toolkit](/docs/integrations/tools/office365)| Microsoft 365 is a product family of productivity software, collabora...  \n",
    "[OpenAPI Toolkit](/docs/integrations/tools/openapi)| We can construct agents to consume arbitrary APIs, here APIs conforma...  \n",
    "[Natural Language API Toolkits](/docs/integrations/tools/openapi_nla)| Natural Language API Toolkits (NLAToolkits) permit LangChain Agents t...  \n",
    "[OpenGradient](/docs/integrations/tools/opengradient_toolkit)| This notebook shows how to build tools using the OpenGradient toolkit...  \n",
    "[OpenWeatherMap](/docs/integrations/tools/openweathermap)| This notebook goes over how to use the OpenWeatherMap component to fe...  \n",
    "[Oracle AI Vector Search: Generate Summary](/docs/integrations/tools/oracleai)| Oracle AI Vector Search is designed for Artificial Intelligence (AI) ...  \n",
    "[Oxylabs](/docs/integrations/tools/oxylabs)| Oxylabs is a market-leading web intelligence collection platform, dri...  \n",
    "[Pandas Dataframe](/docs/integrations/tools/pandas)| This notebook shows how to use agents to interact with a Pandas DataF...  \n",
    "[Passio NutritionAI](/docs/integrations/tools/passio_nutrition_ai)| To best understand how NutritionAI can give your agents super food-nu...  \n",
    "[PaymanAI](/docs/integrations/tools/payman-tool)| PaymanAI provides functionality to send and receive payments (fiat an...  \n",
    "[Permit](/docs/integrations/tools/permit)| Permit is an access control platform that provides fine-grained, real...  \n",
    "[PlayWright Browser Toolkit](/docs/integrations/tools/playwright)| Playwright is an open-source automation tool developed by Microsoft t...  \n",
    "[Polygon IO Toolkit and Tools](/docs/integrations/tools/polygon)| This notebook shows how to use agents to interact with the Polygon IO...  \n",
    "[PowerBI Toolkit](/docs/integrations/tools/powerbi)| This notebook showcases an agent interacting with a Power BI Dataset....  \n",
    "[Prolog](/docs/integrations/tools/prolog_tool)| LangChain tools that use Prolog rules to generate answers.  \n",
    "[PubMed](/docs/integrations/tools/pubmed)| PubMed® comprises more than 35 million citations for biomedical liter...  \n",
    "[Python REPL](/docs/integrations/tools/python)| Sometimes, for complex calculations, rather than have an LLM generate...  \n",
    "[Reddit Search](/docs/integrations/tools/reddit_search)| In this notebook, we learn how the Reddit search tool works.  \n",
    "[Requests Toolkit](/docs/integrations/tools/requests)| We can use the Requests toolkit to construct agents that generate HTT...  \n",
    "[Riza Code Interpreter](/docs/integrations/tools/riza)| The Riza Code Interpreter is a WASM-based isolated environment for ru...  \n",
    "[Robocorp Toolkit](/docs/integrations/tools/robocorp)| This notebook covers how to get started with Robocorp Action Server a...  \n",
    "[Salesforce](/docs/integrations/tools/salesforce)| Tools for interacting with Salesforce.  \n",
    "[SceneXplain](/docs/integrations/tools/sceneXplain)| SceneXplain is an ImageCaptioning service accessible through the Scen...  \n",
    "[ScrapeGraph](/docs/integrations/tools/scrapegraph)| This notebook provides a quick overview for getting started with Scra...  \n",
    "[SearchApi](/docs/integrations/tools/searchapi)| This notebook shows examples of how to use SearchApi to search the we...  \n",
    "[SearxNG Search](/docs/integrations/tools/searx_search)| This notebook goes over how to use a self hosted SearxNG search API t...  \n",
    "[Semantic Scholar API Tool](/docs/integrations/tools/semanticscholar)| This notebook demos how to use the semantic scholar tool with an agen...  \n",
    "[SerpAPI](/docs/integrations/tools/serpapi)| This notebook goes over how to use the SerpAPI component to search th...  \n",
    "[Slack Toolkit](/docs/integrations/tools/slack)| This will help you getting started with the Slack toolkit. For detail...  \n",
    "[Spark SQL Toolkit](/docs/integrations/tools/spark_sql)| This notebook shows how to use agents to interact with Spark SQL. Sim...  \n",
    "[SQLDatabase Toolkit](/docs/integrations/tools/sql_database)| This will help you getting started with the SQL Database toolkit. For...  \n",
    "[StackExchange](/docs/integrations/tools/stackexchange)| Stack Exchange is a network of question-and-answer (Q&A) websites on ...  \n",
    "[Steam Toolkit](/docs/integrations/tools/steam)| Steam (Wikipedia)) is a video game digital distribution service and s...  \n",
    "[Stripe](/docs/integrations/tools/stripe)| This notebook provides a quick overview for getting started with Stri...  \n",
    "[Tableau](/docs/integrations/tools/tableau)| This notebook provides a quick overview for getting started with Tabl...  \n",
    "[Taiga](/docs/integrations/tools/taiga)| This notebook provides a quick overview for getting started with Taig...  \n",
    "[Tavily Extract](/docs/integrations/tools/tavily_extract)| Tavily is a search engine built specifically for AI agents (LLMs), de...  \n",
    "[Tavily Search](/docs/integrations/tools/tavily_search)| Tavily's Search API is a search engine built specifically for AI agen...  \n",
    "[Tilores](/docs/integrations/tools/tilores)| This notebook covers how to get started with the Tilores tools.  \n",
    "[Twilio](/docs/integrations/tools/twilio)| This notebook goes over how to use the Twilio API wrapper to send a m...  \n",
    "[Upstage](/docs/integrations/tools/upstage_groundedness_check)| This notebook covers how to get started with Upstage groundedness che...  \n",
    "[Valthera](/docs/integrations/tools/valthera)| Enable AI agents to engage users when they're most likely to respond.  \n",
    "[ValyuContext](/docs/integrations/tools/valyu_context)| Valyu allows AI applications and agents to search the internet and pr...  \n",
    "[Wikidata](/docs/integrations/tools/wikidata)| Wikidata is a free and open knowledge base that can be read and edite...  \n",
    "[Wikipedia](/docs/integrations/tools/wikipedia)| Wikipedia is a multilingual free online encyclopedia written and main...  \n",
    "[Wolfram Alpha](/docs/integrations/tools/wolfram_alpha)| This notebook goes over how to use the wolfram alpha component.  \n",
    "[Writer Tools](/docs/integrations/tools/writer)| This notebook provides a quick overview for getting started with Writ...  \n",
    "[Yahoo Finance News](/docs/integrations/tools/yahoo_finance_news)| This notebook goes over how to use the yahoofinancenews tool with an ...  \n",
    "[You.com Search](/docs/integrations/tools/you)| The you.com API is a suite of tools designed to help developers groun...  \n",
    "[YouTube](/docs/integrations/tools/youtube)| YouTube Search package searches YouTube videos avoiding using their h...  \n",
    "[Zapier Natural Language Actions](/docs/integrations/tools/zapier)| Deprecated This API will be sunset on 2023-11-17//nla.zapier.com/star...  \n",
    "[ZenGuard AI](/docs/integrations/tools/zenguard)| This tool lets you quickly set up ZenGuard AI in your Langchain-power...\n",
    "\"\"\"\n",
    "\n",
    "# Base URL for formatting\n",
    "base_url = \"https://python.langchain.com/docs/integrations/tools/\"\n",
    "\n",
    "# Regex to find and extract text after \"/docs/integrations/tools/\" stopping at \")\"\n",
    "pattern = r\"/docs/integrations/tools/([^)\\s]+)\"\n",
    "matches = re.findall(pattern, text)\n",
    "\n",
    "\n",
    "# Format matches into full links\n",
    "links_tools = [base_url + match for match in matches]\n",
    "name_tools = [match for match in matches]\n",
    "\n",
    "# Print the extracted links\n",
    "print(name_tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a87aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import html2text\n",
    "import httpx\n",
    "\n",
    "def fetch_documents(url: str) -> str:\n",
    "    \"\"\"Fetch a document from a URL and return the markdownified text.\n",
    "\n",
    "    Args:\n",
    "        url (str): The URL of the document to fetch.\n",
    "\n",
    "    Returns:\n",
    "        str: The markdownified text of the document.\n",
    "    \"\"\"\n",
    "    httpx_client = httpx.Client(follow_redirects=True, timeout=10)\n",
    "\n",
    "    try:\n",
    "        response = httpx_client.get(url, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        html_content = response\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    \n",
    "        img_tags = soup.find_all('img')\n",
    "        for img_tag in img_tags:\n",
    "            img_tag.decompose()\n",
    "\n",
    "        target_div = soup.find('div', class_= \"theme-doc-markdown markdown\") #langchain\n",
    "        \n",
    "        if not target_div:\n",
    "            target_div = soup.find('article') #langraph\n",
    "\n",
    "        if not target_div:\n",
    "            target_div = soup.find('html') #langraph\n",
    "\n",
    "        if not target_div:\n",
    "            return html2text.html2text(str(soup))\n",
    "        \n",
    "        return html2text.html2text(str(target_div))\n",
    "    except (httpx.HTTPStatusError, httpx.RequestError) as e:\n",
    "        return f\"Encountered an HTTP error: {str(e)}\"\n",
    "\n",
    "\n",
    "def extract_text_between_headings(text):\n",
    "    \"\"\"\n",
    "    Extracts the text between the first and second headings in the given text.\n",
    "\n",
    "    Parameters:\n",
    "        text (str): The input document containing headings and content.\n",
    "\n",
    "    Returns:\n",
    "        str: Extracted text between the first and second headings, or None if not found.\n",
    "    \"\"\"\n",
    "    # Regex to match the first heading and the second heading\n",
    "    pattern = r\"#\\s*[^\\n]+\\s*(.*?)\\s*##\\s*[^\\n]+\"\n",
    "    match = re.search(pattern, text, re.DOTALL)\n",
    "\n",
    "    # Extract the text between the headings\n",
    "    return match.group(1).strip() if match else None\n",
    "dict_tool_link = {}\n",
    "dict_tool_doc = {}\n",
    "for i in range(len(name_tools)):\n",
    "    dict_tool_link[name_tools[i]] = links_tools[i]\n",
    "    # dict_tool_doc[name_tools[i]] = extract_text_between_headings(fetch_documents(links_tools[i]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b200eaaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Serialize data into file:\n",
    "# json.dump( dict_tool_doc, open( \"tools_doc_json.json\", 'w' ) )\n",
    "# json.dump( dict_tool_link, open( \"tools_link_json.json\", 'w' ) )\n",
    "# Read data from file:\n",
    "# data = json.load( open( \"tools_json.json\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a16c35c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_tool_link = json.load( open( \"tools_link_json.json\") )\n",
    "dict_tool_doc = json.load( open( \"tools_doc_json.json\") )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ba03e620",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "190250"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(str(dict_tool_doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "15bfaf08",
   "metadata": {},
   "outputs": [],
   "source": [
    "Initial_prompt = \"\"\"You are an expert python developer. You will be given a description of a python function. \n",
    "\n",
    "You job is to estimate and extract the following information:\n",
    "\n",
    "- What exactly does this python do. What is the detailed objective of the function. Please write 1-5 lines\n",
    "- Suggest or extract the name of the the function\n",
    "- What would be the inputs/arguements required into this function to make it work. Please all mentioned the type of each input\n",
    "- WHat would be output produced by this input. Please mention the output type \n",
    "\"\"\"\n",
    "\n",
    "human_prompt = \"\"\"\n",
    "Create a function to fetch ticket from jira given a user\"\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "5b542135",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=0, model=\"gpt-4o-mini\", streaming=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "e7cfc715",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class FunctionInstructions(BaseModel):\n",
    "    \"\"\"Instructions for defining a python function\"\"\"\n",
    "    objective: str = Field(description= \"what does this pythion function do\")\n",
    "    name: str = Field(description=\"name of the python function\")\n",
    "    input : List[str] = Field(description= \"what would be the input arguements to this function along with the types\")\n",
    "    output: List[str] = Field(description=\"what would be the output/return attributes for the function along with the types\")\n",
    "    name_toolkit: str = Field(description=\"what would be the toolkit/ code SDK that will be used\")\n",
    "    code: str = Field(description=\"the final python code\")\n",
    "\n",
    "class CodebuilderState(BaseModel):\n",
    "    \"\"\"Instructions for defining a python function\"\"\"\n",
    "    code: str = Field(description= \"tailored code for the python function\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "0ae12f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def functional_analysis_node(state: FunctionInstructions):\n",
    "  llm_with_structured_output = llm.with_structured_output(FunctionInstructions)\n",
    "  functionalReport: FunctionInstructions = llm_with_structured_output.invoke(\n",
    "      [SystemMessage(content=Initial_prompt)]+ [HumanMessage(content=human_prompt)])\n",
    "  return {  \"messages\": [AIMessage(content=\"Generated JSON code!\")],\n",
    "           \"objective\": functionalReport.objective,\n",
    "           \"name\": functionalReport.name,\n",
    "           \"input\": functionalReport.input,\n",
    "           \"output\": functionalReport.output}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "4fe0565c",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_code_prompt = \"\"\"You are an expert Python developer tasked with creating Python functions (tools) based on user requests.\n",
    "\n",
    "            Your process is as follows:\n",
    "            1. Understand the user's request for a tool (e.g., \"tool to send a discord message\").\n",
    "            2. Find relevant Python SDKs for the core task.\n",
    "            3. See if Composio offers an integration for the relevant service (e.g., 'discord').\n",
    "            4. Analyze the results:\n",
    "                - If Composio has an integration, prioritize generating code that utilizes Composio (assume this involves calling a hypothetical 'composio.run_action()' function). Include a comment explaining this choice.\n",
    "                - If Composio does not have a clear integration, choose the most promising Python SDK found\n",
    "                - If no suitable SDK is found, state that you cannot create the function.\n",
    "            5. Generate *only* the complete, runnable Python function code based on your decision.\n",
    "                - The function should have clear arguments based on the user's likely intent (e.g., for discord, `channel_id` and `message_text`).\n",
    "                - Include a comprehensive docstring explaining the function, its arguments, and what it returns.\n",
    "                - Use type hints for all arguments and the return type.\n",
    "                - If using a standard SDK, add a comment indicating which SDK is intended (e.g., `# Uses discord.py`).\n",
    "                - If using Composio, structure the function to call `composio.run_action('service_name', 'action_name', params={{...}})` (you'll need to infer 'service_name' and 'action_name' and necessary params). Add comments explaining this structure.\n",
    "            6. Do not include any explanatory text before or after the code block. Output only the Python code for the function.\n",
    "\n",
    "    Here are some details about the python function you will be creating:\n",
    "    <objective>\n",
    "    {objective}\n",
    "    </objective>\n",
    "\n",
    "    <input schema>\n",
    "    {inputs}\n",
    "    </input schema>\n",
    "\n",
    "    <output schema>\n",
    "    {output}\n",
    "    </output schema>\n",
    "\n",
    "    <name of function>\n",
    "    {name}\n",
    "    </name of function>\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "77bee104",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_code_prompt = \"\"\"\n",
    "You are a skilled code generation assistant. Your task is to create executable code using the following information:\n",
    "- SDK Documentation: The provided documentation outlines the functionalities and usage details of the SDK. Use this as the reference for constructing your code.\n",
    "- Objective: A clear description of what the code is intended to achieve.\n",
    "- Input: The expected input for the code (e.g., variables, parameters, data types).\n",
    "- Output: The desired result or outcome of the code (e.g., format, type, or structure).\n",
    "- SDK Name: The name of the SDK that must be used in the code.\n",
    "\n",
    "Your goal is to generate executable code that:\n",
    "- Adheres to the requirements outlined above.\n",
    "- Follows standard coding practices and is optimized for readability and efficiency.\n",
    "- Utilizes the specified SDK appropriately based on the documentation provided.\n",
    "- Only return a self contained function\n",
    "- Your output should only contain a code block containing the required function and nothing else. Please do no include any explainantions\n",
    "- Write your code in python\n",
    "\n",
    "Here are some details about the python function you will be creating:\n",
    "<objective>\n",
    "{objective}\n",
    "</objective>\n",
    "\n",
    "<input schema>\n",
    "{inputs}\n",
    "</input schema>\n",
    "\n",
    "<output schema>\n",
    "{output}\n",
    "</output schema>\n",
    "\n",
    "<name of function>\n",
    "{name}\n",
    "</name of function>\n",
    "\n",
    "Documentation for SDK that might be helpful:\n",
    "<documentation>\n",
    "{docs}\n",
    "</documentation>\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "5a2a51bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "Best_sdk_prompt = \"\"\"\n",
    "You are a highly specialized language model designed to assist in selecting the most suitable SDK for a given use case. You are provided with the following:\n",
    "- A dictionary containing pairs of SDK names and their respective descriptions.\n",
    "- Requirements for a piece of code, including the objective, input, and output.\n",
    "\n",
    "Your task is to:\n",
    "- Identify the SDK from the provided dictionary whose description best matches the given use case described in the code requirements.\n",
    "- Return only the name of the matching SDK without any additional text or formatting.\n",
    "\n",
    "Input Example:\n",
    "Dictionary:\n",
    "{{\n",
    "\"SDK_A\": \"Provides tools for web scraping and data extraction.\",\n",
    "\"SDK_B\": \"Enables natural language processing for unstructured text.\",\n",
    "\"SDK_C\": \"Facilitates the integration of payment gateways in applications.\"\n",
    "}}\n",
    "Code Requirements:\n",
    "Objective: Extract data from multiple web pages.\n",
    "Input: URLs of the web pages.\n",
    "Output: Structured data in JSON format.\n",
    "\n",
    "Expected Output:\n",
    "SDK_A\n",
    "\n",
    "\n",
    "Input :\n",
    "<dictionary>\n",
    "{dictionary}\n",
    "</dictionary>\n",
    "\n",
    "<objective>\n",
    "{objective}\n",
    "</objective>\n",
    "\n",
    "<input schema>\n",
    "{inputs}\n",
    "</input schema>\n",
    "\n",
    "<output schema>\n",
    "{output}\n",
    "</output schema>\n",
    "\n",
    "<name of function>\n",
    "{name}\n",
    "</name of function>\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "caa0cd7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def sdk_production_node(state: FunctionInstructions):\n",
    "    objective_agent: str = state.objective\n",
    "    name: str = state.name\n",
    "    input_args : List[str] = state.input\n",
    "    output_args: List[str] = state.output\n",
    "    response = llm.invoke([SystemMessage(content=Best_sdk_prompt.format(\n",
    "          objective=objective_agent,\n",
    "          inputs=input_args,\n",
    "          output=output_args,\n",
    "          name=name,\n",
    "          dictionary = dict_tool_doc\n",
    "    ))])\n",
    "    code_snips = response\n",
    "    return {\n",
    "            \"name_toolkit\": response.content}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "deb96806",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def code_production_node(state: FunctionInstructions):\n",
    "    objective_agent: str = state.objective\n",
    "    name: str = state.name\n",
    "    input_args : List[str] = state.input\n",
    "    output_args: List[str] = state.output\n",
    "    toolkit: str = state.name_toolkit\n",
    "    docs = fetch_documents(dict_tool_link[toolkit])\n",
    "    response = llm.invoke([SystemMessage(content=write_code_prompt.format(\n",
    "          objective=objective_agent,\n",
    "          inputs=input_args,\n",
    "          output=output_args,\n",
    "          name=name,\n",
    "          docs = docs,\n",
    "    ))])\n",
    "    code_snips = response\n",
    "    return {\n",
    "            \"code\": response.content}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "4bf611d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anupa\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\langgraph\\graph\\state.py:92: UserWarning: Invalid state_schema: <function functional_analysis_node at 0x00000163D0B26A20>. Expected a type or Annotated[type, reducer]. Please provide a valid schema to ensure correct updates.\n",
      " See: https://langchain-ai.github.io/langgraph/reference/graphs/#stategraph\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "workflow = StateGraph(functional_analysis_node)\n",
    "workflow.add_node(\"func_analysis\", functional_analysis_node)\n",
    "# workflow.add_node(\"code_write\", code_production_node)\n",
    "\n",
    "workflow.add_edge(\"func_analysis\", END)\n",
    "# workflow.add_edge(\"func_analysis\",\"code_write\")\n",
    "workflow.add_edge(START, \"func_analysis\")\n",
    "infograph = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "a698194b",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = StateGraph(functional_analysis_node)\n",
    "workflow.add_node(\"func_analysis\", functional_analysis_node)\n",
    "workflow.add_node(\"sdk_write\", sdk_production_node)\n",
    "workflow.add_node(\"code_write\", code_production_node)\n",
    "\n",
    "workflow.add_edge(\"code_write\", END)\n",
    "workflow.add_edge(\"sdk_write\",\"code_write\")\n",
    "workflow.add_edge(\"func_analysis\",\"sdk_write\")\n",
    "workflow.add_edge(START, \"func_analysis\")\n",
    "infograph = workflow.compile()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "450809e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Output from node 'func_analysis':\"\n",
      "'---'\n",
      "{ 'input': ['user_id: str', 'jira_url: str', 'api_token: str'],\n",
      "  'name': 'fetch_jira_ticket',\n",
      "  'objective': 'This Python function is designed to fetch a ticket from Jira '\n",
      "               'based on a specified user. It connects to the Jira API, '\n",
      "               'retrieves the ticket information associated with the user, and '\n",
      "               'returns the relevant ticket details.',\n",
      "  'output': ['ticket_details: dict']}\n",
      "'\\n---\\n'\n",
      "\"Output from node 'sdk_write':\"\n",
      "'---'\n",
      "{'name_toolkit': 'jira'}\n",
      "'\\n---\\n'\n",
      "\"Output from node 'code_write':\"\n",
      "'---'\n",
      "{ 'code': '```python\\n'\n",
      "          'import os\\n'\n",
      "          'from langchain_community.utilities.jira import JiraAPIWrapper\\n'\n",
      "          '\\n'\n",
      "          'def fetch_jira_ticket(user_id: str, jira_url: str, api_token: str) '\n",
      "          '-> dict:\\n'\n",
      "          '    os.environ[\"JIRA_API_TOKEN\"] = api_token\\n'\n",
      "          '    os.environ[\"JIRA_USERNAME\"] = user_id\\n'\n",
      "          '    os.environ[\"JIRA_INSTANCE_URL\"] = jira_url\\n'\n",
      "          '    os.environ[\"JIRA_CLOUD\"] = \"True\"\\n'\n",
      "          '    \\n'\n",
      "          '    jira = JiraAPIWrapper()\\n'\n",
      "          '    jql_query = f\\'assignee = \"{user_id}\"\\'\\n'\n",
      "          '    ticket_details = jira.jql(jql_query)\\n'\n",
      "          '    \\n'\n",
      "          '    return ticket_details\\n'\n",
      "          '```'}\n",
      "'\\n---\\n'\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "from langgraph.graph import END, StateGraph, START, MessagesState\n",
    "from langgraph.prebuilt import ToolNode\n",
    "\n",
    "from pydantic import BaseModel\n",
    "\n",
    "class Maintain(BaseModel):\n",
    "    class Config:\n",
    "            arbitrary_types_allowed = True\n",
    "inputs = {\n",
    "    \"messages\": [\n",
    "        (\"user\", \"Create a python function that gets all my emails from gmail and filter them based on senders\"),\n",
    "    ]\n",
    "}\n",
    "\n",
    "code_snip = dict()\n",
    "for output in infograph.stream(inputs):\n",
    "    for key, value in output.items():\n",
    "        pprint.pprint(f\"Output from node '{key}':\")\n",
    "        pprint.pprint(\"---\")\n",
    "        pprint.pprint(value, indent=2, width=80, depth=None)\n",
    "        code_snip = value\n",
    "    pprint.pprint(\"\\n---\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be54077",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3033422e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import END, StateGraph, START, MessagesState\n",
    "from langgraph.prebuilt import ToolNode\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca3bae3c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd07a386",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pydantic import BaseModel\n",
    "\n",
    "class Maintain(BaseModel):\n",
    "    class Config:\n",
    "            arbitrary_types_allowed = True\n",
    "inputs = {\n",
    "    \"messages\": [\n",
    "        (\"user\", \"Create a python function that gets all my emails from gmail and filter them based on senders\"),\n",
    "    ]\n",
    "}\n",
    "\n",
    "code_snip = dict()\n",
    "for output in infograph.stream(inputs):\n",
    "    for key, value in output.items():\n",
    "        pprint.pprint(f\"Output from node '{key}':\")\n",
    "        pprint.pprint(\"---\")\n",
    "        pprint.pprint(value, indent=2, width=80, depth=None)\n",
    "        code_snip = value\n",
    "    pprint.pprint(\"\\n---\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2dc8ed6d",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'name'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[54]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[43mcode_snip\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mname\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m.content)\n",
      "\u001b[31mKeyError\u001b[39m: 'name'"
     ]
    }
   ],
   "source": [
    "print(code_snip['name'].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25aa0d73",
   "metadata": {},
   "source": [
    "from composio_langchain import ComposioToolSet, App\n",
    "\n",
    "composio_toolset = ComposioToolSet(api_key='zg6uxxn5rlc3aployj5ddo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "098b1b3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anupa\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\composio\\tools\\toolset.py:2042: UserWarning: Running without a Composio API key\n",
      "  self.workspace.check_for_missing_dependencies(\n",
      "C:\\Users\\anupa\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\composio\\client\\collections.py:1183: UserWarning: Using all actions of an app is not recommended for production.Learn more: https://docs.composio.dev/patterns/tools/use-tools/use-specific-actions\n",
      "\n",
      "Give Feedback / Get Help:\n",
      "    On GitHub: https://github.com/ComposioHQ/composio/issues/new\n",
      "    On Discord: https://dub.composio.dev/discord\n",
      "    On Email: tech@composio.dev\n",
      "    Talk to us on Intercom: https://composio.dev\n",
      "    Book a call with us: https://composio.dev/redirect?url=https://calendly.com/composiohq/support?utm_source=py-sdk-logs&utm_campaign=calendly\n",
      "If you need to debug this error, set `COMPOSIO_LOGGING_LEVEL=debug`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ApiKeyNotProvidedError",
     "evalue": "API Key not provided, either provide API key or export it as `COMPOSIO_API_KEY` or run `composio login`",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mApiKeyNotProvidedError\u001b[39m                    Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[26]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# Initialize ToolSet (assuming API key is in env)\u001b[39;00m\n\u001b[32m      3\u001b[39m toolset = ComposioToolSet()\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m github_tools = \u001b[43mtoolset\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_tools\u001b[49m\u001b[43m(\u001b[49m\u001b[43mapps\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mApp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mGITHUB\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFetched \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(github_tools)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m tools for GitHub.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\composio_openai\\toolset.py:142\u001b[39m, in \u001b[36mComposioToolSet.get_tools\u001b[39m\u001b[34m(self, actions, apps, tags, processors, check_connected_accounts)\u001b[39m\n\u001b[32m    128\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m processors \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    129\u001b[39m     \u001b[38;5;28mself\u001b[39m._processor_helpers.merge_processors(processors)\n\u001b[32m    131\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[32m    132\u001b[39m     ChatCompletionToolParam(  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[32m    133\u001b[39m         **t.cast(\n\u001b[32m    134\u001b[39m             OpenAISchema,\n\u001b[32m    135\u001b[39m             \u001b[38;5;28mself\u001b[39m.schema.format(\n\u001b[32m    136\u001b[39m                 schema.model_dump(\n\u001b[32m    137\u001b[39m                     exclude_none=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    138\u001b[39m                 )\n\u001b[32m    139\u001b[39m             ),\n\u001b[32m    140\u001b[39m         ).model_dump()\n\u001b[32m    141\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m142\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m schema \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_action_schemas\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    143\u001b[39m \u001b[43m        \u001b[49m\u001b[43mactions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mactions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    144\u001b[39m \u001b[43m        \u001b[49m\u001b[43mapps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mapps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    145\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtags\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtags\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    146\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcheck_connected_accounts\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcheck_connected_accounts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    147\u001b[39m \u001b[43m        \u001b[49m\u001b[43m_populate_requested\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    148\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    149\u001b[39m ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\composio\\tools\\toolset.py:2067\u001b[39m, in \u001b[36mComposioToolSet.get_action_schemas\u001b[39m\u001b[34m(self, apps, actions, tags, check_connected_accounts, _populate_requested)\u001b[39m\n\u001b[32m   2059\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._version_lock \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   2060\u001b[39m     actions = \u001b[38;5;28mself\u001b[39m._version_lock.apply(actions=actions)\n\u001b[32m   2062\u001b[39m items: t.List[ActionModel] = [\n\u001b[32m   2063\u001b[39m     *\u001b[38;5;28mself\u001b[39m._schema_helper.get_runtime_action_schemas(actions=actions),\n\u001b[32m   2064\u001b[39m     *\u001b[38;5;28mself\u001b[39m._schema_helper.get_local_action_schemas(\n\u001b[32m   2065\u001b[39m         apps=apps, actions=actions, tags=tags\n\u001b[32m   2066\u001b[39m     ),\n\u001b[32m-> \u001b[39m\u001b[32m2067\u001b[39m     *\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_schema_helper\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_remote_actions_schemas\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2068\u001b[39m \u001b[43m        \u001b[49m\u001b[43mapps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mapps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2069\u001b[39m \u001b[43m        \u001b[49m\u001b[43mactions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mactions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2070\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtags\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtags\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2071\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcheck_connected_account\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2072\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcheck_connected_account\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcheck_connected_accounts\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[32m   2073\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2074\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m   2075\u001b[39m ]\n\u001b[32m   2077\u001b[39m items = \u001b[38;5;28mlist\u001b[39m(\n\u001b[32m   2078\u001b[39m     \u001b[38;5;28mmap\u001b[39m(\n\u001b[32m   2079\u001b[39m         \u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;28mself\u001b[39m._schema_helper.process_schema(\n\u001b[32m   (...)\u001b[39m\u001b[32m   2086\u001b[39m     )\n\u001b[32m   2087\u001b[39m )\n\u001b[32m   2089\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _populate_requested:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\composio\\tools\\toolset.py:535\u001b[39m, in \u001b[36mSchemaHelper.get_remote_actions_schemas\u001b[39m\u001b[34m(self, apps, actions, tags, check_connected_account)\u001b[39m\n\u001b[32m    533\u001b[39m items = [\u001b[38;5;28mself\u001b[39m.client.actions.get(a) \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m versioned_actions]\n\u001b[32m    534\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(none_versioned_actions) > \u001b[32m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(apps) > \u001b[32m0\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m535\u001b[39m     items += \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mactions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    536\u001b[39m \u001b[43m        \u001b[49m\u001b[43mapps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mapps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    537\u001b[39m \u001b[43m        \u001b[49m\u001b[43mactions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnone_versioned_actions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    538\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtags\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtags\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    539\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    541\u001b[39m \u001b[38;5;28mself\u001b[39m._schema_cache.update({item.name: item.model_copy() \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m items})\n\u001b[32m    542\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m check_connected_account \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\composio\\client\\collections.py:1301\u001b[39m, in \u001b[36mActions.get\u001b[39m\u001b[34m(self, action, actions, apps, tags, limit, use_case, allow_all)\u001b[39m\n\u001b[32m   1298\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1299\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._get_action(action=action)\n\u001b[32m-> \u001b[39m\u001b[32m1301\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_actions\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1302\u001b[39m \u001b[43m    \u001b[49m\u001b[43mactions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mactions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1303\u001b[39m \u001b[43m    \u001b[49m\u001b[43mapps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mapps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1304\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtags\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtags\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1305\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlimit\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlimit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1306\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_case\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_case\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1307\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_all\u001b[49m\u001b[43m=\u001b[49m\u001b[43mallow_all\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1308\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\composio\\client\\collections.py:1222\u001b[39m, in \u001b[36mActions._get_actions\u001b[39m\u001b[34m(self, actions, apps, tags, limit, use_case, allow_all)\u001b[39m\n\u001b[32m   1218\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m limit \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1219\u001b[39m     queries[\u001b[33m\"\u001b[39m\u001b[33mlimit\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28mstr\u001b[39m(limit)\n\u001b[32m   1221\u001b[39m response = \u001b[38;5;28mself\u001b[39m._raise_if_required(\n\u001b[32m-> \u001b[39m\u001b[32m1222\u001b[39m     response=\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhttp\u001b[49m.get(\n\u001b[32m   1223\u001b[39m         url=\u001b[38;5;28mstr\u001b[39m(\n\u001b[32m   1224\u001b[39m             \u001b[38;5;28mself\u001b[39m.endpoint(\n\u001b[32m   1225\u001b[39m                 queries=queries,\n\u001b[32m   1226\u001b[39m             )\n\u001b[32m   1227\u001b[39m         )\n\u001b[32m   1228\u001b[39m     )\n\u001b[32m   1229\u001b[39m )\n\u001b[32m   1231\u001b[39m response_json = response.json()\n\u001b[32m   1232\u001b[39m items = [\u001b[38;5;28mself\u001b[39m.model(**action) \u001b[38;5;28;01mfor\u001b[39;00m action \u001b[38;5;129;01min\u001b[39;00m response_json.get(\u001b[33m\"\u001b[39m\u001b[33mitems\u001b[39m\u001b[33m\"\u001b[39m)]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\composio\\client\\__init__.py:132\u001b[39m, in \u001b[36mComposio.http\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    127\u001b[39m \u001b[38;5;129m@property\u001b[39m\n\u001b[32m    128\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mhttp\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> HttpClient:\n\u001b[32m    129\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._http:\n\u001b[32m    130\u001b[39m         \u001b[38;5;28mself\u001b[39m._http = HttpClient(\n\u001b[32m    131\u001b[39m             base_url=\u001b[38;5;28mself\u001b[39m.base_url,\n\u001b[32m--> \u001b[39m\u001b[32m132\u001b[39m             api_key=\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mapi_key\u001b[49m,\n\u001b[32m    133\u001b[39m             runtime=\u001b[38;5;28mself\u001b[39m.runtime,\n\u001b[32m    134\u001b[39m         )\n\u001b[32m    135\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._http\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\composio\\client\\__init__.py:114\u001b[39m, in \u001b[36mComposio.api_key\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    111\u001b[39m         \u001b[38;5;28mself\u001b[39m._api_key = env_api_key\n\u001b[32m    113\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._api_key \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m ApiKeyNotProvidedError\n\u001b[32m    116\u001b[39m \u001b[38;5;28mself\u001b[39m._api_key = \u001b[38;5;28mself\u001b[39m.validate_api_key(\n\u001b[32m    117\u001b[39m     key=t.cast(\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mself\u001b[39m._api_key),\n\u001b[32m    118\u001b[39m     base_url=\u001b[38;5;28mself\u001b[39m.base_url,\n\u001b[32m    119\u001b[39m )\n\u001b[32m    121\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._api_key\n",
      "\u001b[31mApiKeyNotProvidedError\u001b[39m: API Key not provided, either provide API key or export it as `COMPOSIO_API_KEY` or run `composio login`"
     ]
    }
   ],
   "source": [
    "from composio_openai import ComposioToolSet, Action\n",
    "# Initialize ToolSet (assuming API key is in env)\n",
    "toolset = ComposioToolSet()\n",
    "github_tools = toolset.get_tools(apps=[App.GITHUB])\n",
    "print(f\"Fetched {len(github_tools)} tools for GitHub.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a4334495",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain, SimpleSequentialChain, TransformChain\n",
    "from langchain_community.tools.zapier.tool import ZapierNLARunAction\n",
    "from langchain_community.utilities.zapier import ZapierNLAWrapper\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3de84ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import html2text\n",
    "import httpx\n",
    "\n",
    "def fetch_documents(url: str) -> str:\n",
    "    \"\"\"Fetch a document from a URL and return the markdownified text.\n",
    "\n",
    "    Args:\n",
    "        url (str): The URL of the document to fetch.\n",
    "\n",
    "    Returns:\n",
    "        str: The markdownified text of the document.\n",
    "    \"\"\"\n",
    "    httpx_client = httpx.Client(follow_redirects=True, timeout=10)\n",
    "\n",
    "    try:\n",
    "        response = httpx_client.get(url, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        html_content = response\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    \n",
    "        img_tags = soup.find_all('img')\n",
    "        for img_tag in img_tags:\n",
    "            img_tag.decompose()\n",
    "\n",
    "        target_div = soup.find('div', class_= \"theme-doc-markdown markdown\") #langchain\n",
    "        \n",
    "        if not target_div:\n",
    "            target_div = soup.find('article') #langraph\n",
    "\n",
    "        if not target_div:\n",
    "            target_div = soup.find('html') #langraph\n",
    "\n",
    "        if not target_div:\n",
    "            return html2text.html2text(str(soup))\n",
    "        \n",
    "        return html2text.html2text(str(target_div))\n",
    "    except (httpx.HTTPStatusError, httpx.RequestError) as e:\n",
    "        return f\"Encountered an HTTP error: {str(e)}\"\n",
    "    \n",
    "def fetch_documents_with_links_html(url: str) -> tuple[str, list[tuple[str, str]]]:\n",
    "    \"\"\"Fetch a document from a URL, return the markdownified text with links as markdown, and extract links with their titles.\n",
    "\n",
    "    Args:\n",
    "        url (str): The URL of the document to fetch.\n",
    "\n",
    "    Returns:\n",
    "        tuple[str, list[tuple[str, str]]]: A tuple containing the markdownified text of the document with links, and a list of (link, title) tuples.\n",
    "    \"\"\"\n",
    "    httpx_client = httpx.Client(follow_redirects=True, timeout=10)\n",
    "    links = []\n",
    "\n",
    "    try:\n",
    "        response = httpx_client.get(url, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        html_content = response.text\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "        target_div = soup.find('div', class_= \"theme-doc-markdown markdown\") #langchain\n",
    "\n",
    "        if not target_div:\n",
    "            target_div = soup.find('article') #langraph\n",
    "\n",
    "        if not target_div:\n",
    "            return \"\", links # Return empty text but still the links\n",
    "\n",
    "        # Extract links *before* converting to markdown\n",
    "        a_tags = target_div.find_all('a')\n",
    "        for a_tag in a_tags:\n",
    "            link = a_tag.get('href')\n",
    "            title = a_tag.get_text(strip=True)\n",
    "            if link:\n",
    "                links.append((link, title))\n",
    "\n",
    "        markdown_converter = html2text.HTML2Text()\n",
    "        markdown_converter.body_width = 0  # Disable line wrapping for links to stay on one line\n",
    "        markdown_text = markdown_converter.handle(str(target_div))\n",
    "\n",
    "        return markdown_text, links\n",
    "    except (httpx.HTTPStatusError, httpx.RequestError) as e:\n",
    "        return f\"Encountered an HTTP error: {str(e)}\", [] # Return error message and empty links\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2cdd200c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# ArXiv\n",
      "\n",
      "This notebook goes over how to use the `arxiv` tool with an agent.\n",
      "\n",
      "First, you need to install the `arxiv` python package.\n",
      "\n",
      "    \n",
      "    \n",
      "    %pip install --upgrade --quiet  langchain-community arxiv  \n",
      "    \n",
      "    \n",
      "    \n",
      "    from langchain import hub  \n",
      "    from langchain.agents import AgentExecutor, create_react_agent, load_tools  \n",
      "    from langchain_openai import ChatOpenAI  \n",
      "      \n",
      "    llm = ChatOpenAI(temperature=0.0)  \n",
      "    tools = load_tools(  \n",
      "        [\"arxiv\"],  \n",
      "    )  \n",
      "    prompt = hub.pull(\"hwchase17/react\")  \n",
      "      \n",
      "    agent = create_react_agent(llm, tools, prompt)  \n",
      "    agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)  \n",
      "    \n",
      "\n",
      "**API Reference:**[hub](https://python.langchain.com/api_reference/langchain/hub/langchain.hub.hub.html) | [AgentExecutor](https://python.langchain.com/api_reference/langchain/agents/langchain.agents.agent.AgentExecutor.html) | [create_react_agent](https://python.langchain.com/api_reference/langchain/agents/langchain.agents.react.agent.create_react_agent.html) | [load_tools](https://python.langchain.com/api_reference/community/agent_toolkits/langchain_community.agent_toolkits.load_tools.load_tools.html) | [ChatOpenAI](https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html)\n",
      "    \n",
      "    \n",
      "    agent_executor.invoke(  \n",
      "        {  \n",
      "            \"input\": \"What's the paper 1605.08386 about?\",  \n",
      "        }  \n",
      "    )  \n",
      "    \n",
      "    \n",
      "    \n",
      "      \n",
      "      \n",
      "    \u001b[1m> Entering new AgentExecutor chain...\u001b[0m  \n",
      "    \u001b[32;1m\u001b[1;3mI should use the arxiv tool to search for the paper with the given identifier.  \n",
      "    Action: arxiv  \n",
      "    Action Input: 1605.08386\u001b[0m\u001b[36;1m\u001b[1;3mPublished: 2016-05-26  \n",
      "    Title: Heat-bath random walks with Markov bases  \n",
      "    Authors: Caprice Stanley, Tobias Windisch  \n",
      "    Summary: Graphs on lattice points are studied whose edges come from a finite set of  \n",
      "    allowed moves of arbitrary length. We show that the diameter of these graphs on  \n",
      "    fibers of a fixed integer matrix can be bounded from above by a constant. We  \n",
      "    then study the mixing behaviour of heat-bath random walks on these graphs. We  \n",
      "    also state explicit conditions on the set of moves so that the heat-bath random  \n",
      "    walk, a generalization of the Glauber dynamics, is an expander in fixed  \n",
      "    dimension.\u001b[0m\u001b[32;1m\u001b[1;3mThe paper \"1605.08386\" is titled \"Heat-bath random walks with Markov bases\" and is authored by Caprice Stanley and Tobias Windisch. It was published on May 26, 2016. The paper discusses the study of graphs on lattice points with edges coming from a finite set of allowed moves. It explores the diameter of these graphs and the mixing behavior of heat-bath random walks on them. The paper also discusses conditions for the heat-bath random walk to be an expander in fixed dimension.  \n",
      "    Final Answer: The paper \"1605.08386\" is about heat-bath random walks with Markov bases.\u001b[0m  \n",
      "      \n",
      "    \u001b[1m> Finished chain.\u001b[0m  \n",
      "    \n",
      "    \n",
      "    \n",
      "    {'input': \"What's the paper 1605.08386 about?\",  \n",
      "     'output': 'The paper \"1605.08386\" is about heat-bath random walks with Markov bases.'}  \n",
      "    \n",
      "\n",
      "## The ArXiv API Wrapper​\n",
      "\n",
      "The tool uses the `API Wrapper`. Below, we explore some of the features it\n",
      "provides.\n",
      "\n",
      "    \n",
      "    \n",
      "    from langchain_community.utilities import ArxivAPIWrapper  \n",
      "    \n",
      "\n",
      "**API\n",
      "Reference:**[ArxivAPIWrapper](https://python.langchain.com/api_reference/community/utilities/langchain_community.utilities.arxiv.ArxivAPIWrapper.html)\n",
      "\n",
      "You can use the ArxivAPIWrapper to get information about a scientific article\n",
      "or articles. The query text is limited to 300 characters.\n",
      "\n",
      "The ArxivAPIWrapper returns these article fields:\n",
      "\n",
      "  * Publishing date\n",
      "  * Title\n",
      "  * Authors\n",
      "  * Summary\n",
      "\n",
      "The following query returns information about one article with the arxiv ID\n",
      "\"1605.08386\".\n",
      "\n",
      "    \n",
      "    \n",
      "    arxiv = ArxivAPIWrapper()  \n",
      "    docs = arxiv.run(\"1605.08386\")  \n",
      "    docs  \n",
      "    \n",
      "    \n",
      "    \n",
      "    'Published: 2016-05-26\\nTitle: Heat-bath random walks with Markov bases\\nAuthors: Caprice Stanley, Tobias Windisch\\nSummary: Graphs on lattice points are studied whose edges come from a finite set of\\nallowed moves of arbitrary length. We show that the diameter of these graphs on\\nfibers of a fixed integer matrix can be bounded from above by a constant. We\\nthen study the mixing behaviour of heat-bath random walks on these graphs. We\\nalso state explicit conditions on the set of moves so that the heat-bath random\\nwalk, a generalization of the Glauber dynamics, is an expander in fixed\\ndimension.'  \n",
      "    \n",
      "\n",
      "Now, we want to get information about one author, `Caprice Stanley`.\n",
      "\n",
      "This query returns information about three articles. By default, the query\n",
      "returns information only about three top articles.\n",
      "\n",
      "    \n",
      "    \n",
      "    docs = arxiv.run(\"Caprice Stanley\")  \n",
      "    docs  \n",
      "    \n",
      "    \n",
      "    \n",
      "    'Published: 2017-10-10\\nTitle: On Mixing Behavior of a Family of Random Walks Determined by a Linear Recurrence\\nAuthors: Caprice Stanley, Seth Sullivant\\nSummary: We study random walks on the integers mod $G_n$ that are determined by an\\ninteger sequence $\\\\{ G_n \\\\}_{n \\\\geq 1}$ generated by a linear recurrence\\nrelation. Fourier analysis provides explicit formulas to compute the\\neigenvalues of the transition matrices and we use this to bound the mixing time\\nof the random walks.\\n\\nPublished: 2016-05-26\\nTitle: Heat-bath random walks with Markov bases\\nAuthors: Caprice Stanley, Tobias Windisch\\nSummary: Graphs on lattice points are studied whose edges come from a finite set of\\nallowed moves of arbitrary length. We show that the diameter of these graphs on\\nfibers of a fixed integer matrix can be bounded from above by a constant. We\\nthen study the mixing behaviour of heat-bath random walks on these graphs. We\\nalso state explicit conditions on the set of moves so that the heat-bath random\\nwalk, a generalization of the Glauber dynamics, is an expander in fixed\\ndimension.\\n\\nPublished: 2003-03-18\\nTitle: Calculation of fluxes of charged particles and neutrinos from atmospheric showers\\nAuthors: V. Plyaskin\\nSummary: The results on the fluxes of charged particles and neutrinos from a\\n3-dimensional (3D) simulation of atmospheric showers are presented. An\\nagreement of calculated fluxes with data on charged particles from the AMS and\\nCAPRICE detectors is demonstrated. Predictions on neutrino fluxes at different\\nexperimental sites are compared with results from other calculations.'  \n",
      "    \n",
      "\n",
      "Now, we are trying to find information about non-existing article. In this\n",
      "case, the response is \"No good Arxiv Result was found\"\n",
      "\n",
      "    \n",
      "    \n",
      "    docs = arxiv.run(\"1605.08386WWW\")  \n",
      "    docs  \n",
      "    \n",
      "    \n",
      "    \n",
      "    'No good Arxiv Result was found'  \n",
      "    \n",
      "\n",
      "## Related​\n",
      "\n",
      "  * Tool [conceptual guide](/docs/concepts/tools/)\n",
      "  * Tool [how-to guides](/docs/how_to/#tools)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "doc = fetch_documents(\"https://python.langchain.com/docs/integrations/tools/arxiv/\")\n",
    "print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b3473828",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "from langchain.agents import AgentExecutor, create_react_agent, load_tools\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.0)\n",
    "tools = load_tools(\n",
    "    [\"arxiv\"],\n",
    ")\n",
    "prompt = hub.pull(\"hwchase17/react\")\n",
    "\n",
    "agent = create_react_agent(llm, tools, prompt)\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "415a4c39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Code:\n",
      "```python\n",
      "def calculate_area(length: float, width: float) -> float:\n",
      "    \"\"\"\n",
      "    Calculates the area of a rectangle.\n",
      "\n",
      "    Parameters:\n",
      "    length (float): The length of the rectangle.\n",
      "    width (float): The width of the rectangle.\n",
      "\n",
      "    Returns:\n",
      "    float: The area of the rectangle, calculated as length * width.\n",
      "\n",
      "    Example:\n",
      "    >>> calculate_area(5, 3)\n",
      "    15.0\n",
      "    \"\"\"\n",
      "    return length * width\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.agents import AgentType, initialize_agent\n",
    "from langchain.tools import BaseTool\n",
    "from langchain.prompts import PromptTemplate\n",
    "import json\n",
    "\n",
    "# 1. Define Tool Specification (within a Python string, for simplicity)\n",
    "tool_spec = \"\"\"\n",
    "{\n",
    "  \"name\": \"calculate_area\",\n",
    "  \"description\": \"Calculates the area of a rectangle.\",\n",
    "  \"input_parameters\": [\n",
    "    {\n",
    "      \"name\": \"length\",\n",
    "      \"type\": \"number\",\n",
    "      \"description\": \"The length of the rectangle.\",\n",
    "      \"required\": true\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"width\",\n",
    "      \"type\": \"number\",\n",
    "      \"description\": \"The width of the rectangle.\",\n",
    "      \"required\": true\n",
    "    }\n",
    "  ],\n",
    "  \"output_specification\": {\n",
    "    \"type\": \"number\",\n",
    "    \"description\": \"The area of the rectangle.\"\n",
    "  },\n",
    "  \"functionality\": {\n",
    "    \"type\": \"python_function\"\n",
    "  }\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "# 2. Prepare the LLM Agent\n",
    "llm = ChatOpenAI(temperature=0, model=\"gpt-4o-mini\", streaming=True)  # Use a powerful LLM\n",
    "# (In a full application, you might have other tools)\n",
    "tools = []\n",
    "\n",
    "\n",
    "# 3. Create Code Generation Prompt\n",
    "prompt_template = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "You are a Python programmer. You need to write a Python function\n",
    "that implements the following tool:\n",
    "\n",
    "Tool Specification:\n",
    "{tool_specification}\n",
    "\n",
    "-   Write a complete, runnable Python function.\n",
    "-   Include a docstring that explains what the function does and how to use it.\n",
    "-   Use type hints.\n",
    "-   Do not include any code outside of the function definition.\n",
    "\"\"\"\n",
    ")\n",
    "prompt = prompt_template.format(tool_specification=tool_spec)\n",
    "\n",
    "# 4. Agent Generates Code\n",
    "s = llm.invoke([SystemMessage(content=prompt)])\n",
    "print(\"Generated Code:\")\n",
    "print(s.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e425957",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You are an expert Python developer tasked with creating Python functions (tools) based on user requests.\\n\\n            Your process is as follows:\\n            1. Understand the user\\'s request for a tool (e.g., \"tool to send a discord message\").\\n            2. Use the \\'sdk_search_tool\\' to find relevant Python SDKs for the core task.\\n            3. Use the \\'composio_check_tool\\' to see if Composio offers an integration for the relevant service (e.g., \\'discord\\').\\n            4. Analyze the results:\\n                - If Composio has an integration, prioritize generating code that utilizes Composio (assume this involves calling a hypothetical \\'composio.run_action()\\' function). Include a comment explaining this choice.\\n                - If Composio does not have a clear integration, choose the most promising Python SDK found in the search results.\\n                - If no suitable SDK is found, state that you cannot create the function.\\n            5. Generate *only* the complete, runnable Python function code based on your decision.\\n                - The function should have clear arguments based on the user\\'s likely intent (e.g., for discord, `channel_id` and `message_text`).\\n                - Include a comprehensive docstring explaining the function, its arguments, and what it returns.\\n                - Use type hints for all arguments and the return type.\\n                - If using a standard SDK, add a comment indicating which SDK is intended (e.g., `# Uses discord.py`).\\n                - If using Composio, structure the function to call `composio.run_action(\\'service_name\\', \\'action_name\\', params={{...}})` (you\\'ll need to infer \\'service_name\\' and \\'action_name\\' and necessary params). Add comments explaining this structure.\\n            6. Do not include any explanatory text before or after the code block. Output only the Python code for the function.\\n            '"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"You are an expert Python developer tasked with creating Python functions (tools) based on user requests.\n",
    "\n",
    "            Your process is as follows:\n",
    "            1. Understand the user's request for a tool (e.g., \"tool to send a discord message\").\n",
    "            2. Find relevant Python SDKs for the core task.\n",
    "            3. See if Composio offers an integration for the relevant service (e.g., 'discord').\n",
    "            4. Analyze the results:\n",
    "                - If Composio has an integration, prioritize generating code that utilizes Composio (assume this involves calling a hypothetical 'composio.run_action()' function). Include a comment explaining this choice.\n",
    "                - If Composio does not have a clear integration, choose the most promising Python SDK found\n",
    "                - If no suitable SDK is found, state that you cannot create the function.\n",
    "            5. Generate *only* the complete, runnable Python function code based on your decision.\n",
    "                - The function should have clear arguments based on the user's likely intent (e.g., for discord, `channel_id` and `message_text`).\n",
    "                - Include a comprehensive docstring explaining the function, its arguments, and what it returns.\n",
    "                - Use type hints for all arguments and the return type.\n",
    "                - If using a standard SDK, add a comment indicating which SDK is intended (e.g., `# Uses discord.py`).\n",
    "                - If using Composio, structure the function to call `composio.run_action('service_name', 'action_name', params={{...}})` (you'll need to infer 'service_name' and 'action_name' and necessary params). Add comments explaining this structure.\n",
    "            6. Do not include any explanatory text before or after the code block. Output only the Python code for the function.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33bbe063",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9003dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "desc->check if any specific tool is good-> fetch doc link-> execute\n",
    "\n",
    "https://langchain-ai.github.io/langgraph/how-tos/update-state-from-tools/ \n",
    "tool command handling \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
