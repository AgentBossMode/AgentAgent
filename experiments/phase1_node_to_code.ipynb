{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 394,
   "id": "103bf42a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools.retriever import create_retriever_tool\n",
    "from utils.dependency_setup import get_embeddings, get_vector_store\n",
    "\n",
    "embeddings = get_embeddings()\n",
    "vector_store = get_vector_store(embeddings, \"langstuffindex\")\n",
    "\n",
    "retriever = vector_store.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "id": "18ad2bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Choose the LLM that will drive the agent\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e17c7264",
   "metadata": {},
   "source": [
    "# AGENT state creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "id": "f06d29f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import List, Annotated, Tuple\n",
    "from langgraph.graph import  MessagesState\n",
    "import operator\n",
    "class NodeBuilderState(MessagesState):\n",
    "    \"\"\"State for the node builder.\"\"\"\n",
    "    input: str\n",
    "    plan: List[str]\n",
    "    past_steps: Annotated[List[Tuple], operator.add]\n",
    "    response: str\n",
    "    next: str = Field(description=\"Next node to go to\")\n",
    "    schema_info: str = Field(description=\"Schema information about the node\")\n",
    "    input_schema: str = Field(description=\"Input schema of the node\")\n",
    "    output_schema: str = Field(description=\"Output schema of the node\")\n",
    "    description: str = Field(description=\"Description of the node\")\n",
    "    function_name: str = Field(description=\"Function name of the node\")\n",
    "    code: str = Field(description=\"Code for the node\")\n",
    "    toolset: list[str] = Field(description=\"List of tools to be used in the node by the llm\")\n",
    "    node_type : str = Field(description=\"Type of node, deterministic if the function is deterministic and requires simple python code generation, ai if the function is not deterministic and requires a llm usage for meeting the requirements\")\n",
    "    node_info: str = Field(description=\"node information\")\n",
    "    task: str = Field(description=\"current task\")\n",
    "    final_code: str = Field(description=\"the final code to output\")\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e29399",
   "metadata": {},
   "source": [
    "# NODE type identification and deterministic code gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "id": "5eddc5d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from typing import Literal\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "\n",
    "class NodeType(BaseModel):\n",
    "    node_type: Literal[\"deterministic\", \"planner\"] = Field(description=\n",
    "                                                      \"\"\"Type of node, \n",
    "                                                      : deterministic if the function is deterministic and requires simple python code generation,\n",
    "                                                      : planner if the function is not deterministic, like analysis of input, plan generation or any other thing which is fuzzy logic requires Artificial intelligence for meeting the requirements\"\"\")\n",
    "\n",
    "\n",
    "def determine_node_type(state: NodeBuilderState):\n",
    "    \"\"\"Determine the type of node.\"\"\"\n",
    "    node_type: str = state[\"node_type\"]\n",
    "    if node_type == \"deterministic\":\n",
    "        return \"deterministic_code\"\n",
    "    elif node_type == \"planner\":\n",
    "        return \"planner\"\n",
    "\n",
    "node_info_prompt= \"\"\"\n",
    "You are provided with the following information about the node:\n",
    "<SchemaInfo>\n",
    "{schema_info}\n",
    "</SchemaInfo>\n",
    "<InputSchema>\n",
    "{input_schema}\n",
    "</InputSchema>\n",
    "<OutputSchema>\n",
    "{output_schema}\n",
    "</OutputSchema>\n",
    "<Description>\n",
    "{description}\n",
    "</Description>\n",
    "<FunctionName>\n",
    "{function_name}\n",
    "</FunctionName>\n",
    "\n",
    "Below is the skeleton of the function that you need to implement:\n",
    "def {function_name}(state:{input_schema}) -> {output_schema}:\n",
    "    \\\"\\\"\\\"{description}\\\"\\\"\\\"\n",
    "    # Implement the function to meet the description.\n",
    "    \n",
    "the state is of type {input_schema} and the function is of type {output_schema}\n",
    "The general idea is that the implementation would involve extracting the input from the state, and updating the state with the output. Description contains the logic for this blackbox\n",
    "\"\"\"\n",
    "    \n",
    "def identify_node(state: NodeBuilderState):\n",
    "    \"\"\"Identify the node and return the information.\"\"\"\n",
    "    # Extract the information from the state\n",
    "    llm_with_structured_output = llm.with_structured_output(NodeType)\n",
    "    node_type_identifier_prompt = \"\"\"\n",
    "        You are an expert in identifying whether a function definition could be handled via simple algorithmic logic, or requires the use of large language model aka LLMs \n",
    "        \n",
    "        Examples:\n",
    "        Q: The node's job is to generate a plan given user query.\n",
    "        A: planner\n",
    "        \n",
    "        Q: Node adds two numbers\n",
    "        A: determinisitc\n",
    "        \n",
    "        Q: Node needs to analyze user query and classify sentiment\n",
    "        A: planner\n",
    "        \n",
    "        Q: Node performs function that needs critical thinking or fuzzy logic\n",
    "        A: planner\n",
    "        \"\"\"\n",
    "    type_of_node = llm_with_structured_output.invoke([SystemMessage(content=node_type_identifier_prompt)]+ [HumanMessage(content = node_info_prompt.format(\n",
    "        schema_info=state[\"schema_info\"],\n",
    "        input_schema=state[\"input_schema\"],\n",
    "        output_schema=state[\"output_schema\"],\n",
    "        description=state[\"description\"],\n",
    "        function_name=state[\"function_name\"]))])\n",
    "    \n",
    "    return {\"node_type\": type_of_node.node_type, \"messages\": [HumanMessage(content=node_info_prompt.format(\n",
    "        schema_info=state[\"schema_info\"],\n",
    "        input_schema=state[\"input_schema\"],\n",
    "        output_schema=state[\"output_schema\"],\n",
    "        description=state[\"description\"],\n",
    "        function_name=state[\"function_name\"])) ], \n",
    "            \"node_info\": node_info_prompt.format(\n",
    "        schema_info=state[\"schema_info\"],\n",
    "        input_schema=state[\"input_schema\"],\n",
    "        output_schema=state[\"output_schema\"],\n",
    "        description=state[\"description\"],\n",
    "        function_name=state[\"function_name\"])}\n",
    "\n",
    "def deterministic_code_gen(state: NodeBuilderState):\n",
    "    \"\"\"Generate the code for the node.\"\"\"\n",
    "    prompt = ChatPromptTemplate.from_template(\n",
    "        \"\"\"Generate the python code for the function {function_name}.\n",
    "You are provided with the following information about the node:\n",
    "The schema information is as follows: {schema_info}\n",
    "The input schema is: {input_schema}\n",
    "The output schema is: {output_schema}\n",
    "The description of the function is: {description}\n",
    "\n",
    "Implement the function to meet the requirements.\n",
    "\"\"\")\n",
    "    code = llm.invoke(prompt.format(\n",
    "        schema_info=state[\"schema_info\"],\n",
    "        input_schema=state[\"input_schema\"],\n",
    "        output_schema=state[\"output_schema\"],\n",
    "        description=state[\"description\"],\n",
    "        function_name=state[\"function_name\"]))\n",
    "    return {\"final_code\": code.content}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "301471a1",
   "metadata": {},
   "source": [
    "# Planner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0867fe33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "from typing import Annotated, List, Tuple\n",
    "from typing_extensions import TypedDict\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain import hub\n",
    "from langchain_openai import ChatOpenAI\n",
    "from typing import Union\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "# Choose the LLM that will drive the agent\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "\n",
    "class Plan(BaseModel):\n",
    "    \"\"\"Plan to follow in future\"\"\"\n",
    "\n",
    "    steps: List[str] = Field(\n",
    "        description=\"different steps to follow, should be in sorted order\"\n",
    "    )\n",
    "    justification: str = Field(description = \"justification for the proposed plan\")\n",
    "\n",
    "planner_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"For the given node information by user, come up with a simple step by step plan to implement the node.\n",
    "This plan should involve individual tasks, that if executed correctly will yield the correct answer. Do not add any superfluous steps.\n",
    "\n",
    "You need to closely adhere to one of the following node building strategies\n",
    "<NodeBuildingStrategies>\n",
    "\n",
    "\n",
    "*** prompting + tool binding ***\n",
    "\n",
    "When to use:\n",
    "a. if the node's function is to answer a user_query which needs factual information to answer\n",
    "b. personal assistant which may need to respond with which tool should be used if any to meet the requirements \n",
    "c. Whenever the node involves a LLM(model) needing sensors or actuators, for information retrieval, and allowing llm(model) to 'DO' stuff\n",
    "d. One of the best use cases of this technique is when you know that there would be 'n' different tools which could be used in different scenarios to handle a query in a particular domain.\n",
    "\n",
    "Algorithm:\n",
    "1. Write a prompt to handle the user query\n",
    "2. Use the toolset_generation capbility to pair the prompts with appropriate tools. \n",
    "\n",
    "IMPORTANT: this node is not responsible for execution, that will be handled separately, PLAN SHOULD NOT INCLUDE TOOL CALLING/EXECUTION\n",
    "**************************************\n",
    "\n",
    "\n",
    "***prompting + structured_output***\n",
    "When not to use: \n",
    "DONOT USE if the user query needs to be answered based on factual data, refer to 'prompting + tool_binding' for the same.\n",
    "do not rely on LLM's ability to answer factual data, always refer to tool_binding strategy\n",
    "When to use:\n",
    "a. Reduced Hallucinations: By enforcing adherence to a JSON Schema, structured outputs minimize the chance of models generating incorrect or irrelevant data. \n",
    "b. Simplified Prompting: You don't need overly complex or specific prompts to get consistently formatted output, as the schema provides the structure. \n",
    "c. Reliable Type-Safety: Structured outputs ensure that the model always generates data that fits the defined schema, eliminating the need for validation or retries due to format errors. \n",
    "d. Building Complex Workflows: Structured outputs are particularly useful for building multi-step workflows where the output of one step serves as input for the next. \n",
    "e. Integration with Databases and APIs: When integrating with systems that require structured data formats (like databases or APIs), structured outputs ensure consistency and avoid integration problems. \n",
    "f. Function Calling and Data Extraction: Structured outputs are recommended for function calling and extracting structured data from various sources. \n",
    "g. Consistent and Verifiable Output: The predictable format of structured outputs makes it easier to test, debug, and evaluate applications that rely on them. \n",
    "h. Explicit Refusals: You can now detect model refusals programmatically when using structured outputs, as the model will explicitly return a structured error message instead of a text-based one. \n",
    "\n",
    "\n",
    "Steps to follow:\n",
    "1. Generate a prompt for the given ask\n",
    "2. Use structured output functionality \n",
    "\n",
    "***************************************\n",
    "\n",
    "*** Interrupt ***\n",
    "Use case: \n",
    "a. Reviewing tool calls: Humans can review, edit, or approve tool calls requested by the LLM before tool execution.\n",
    "b. Validating LLM outputs: Humans can review, edit, or approve content generated by the LLM.\n",
    "c. Providing context: Enable the LLM to explicitly request human input for clarification or additional details or to support multi-turn conversations.\n",
    "\n",
    "Algorithm:\n",
    "1. follow the interrupt pattern\n",
    "***************************************\n",
    "\n",
    "</NodeBuildingStrategies>\n",
    "\n",
    "\n",
    "Read the complete 'NodeBuildingStrategies' section to select the best strategy and create a tailored plan accordingly\n",
    "\n",
    "Filters on plan: \n",
    "1. The result of the final step should be the final answer\n",
    "2. Do not add steps like extract _user_query , tool_execution, or update state\n",
    "\"\"\",\n",
    "\n",
    "\n",
    "        ),\n",
    "        (\"placeholder\", \"{messages}\"),\n",
    "    ]\n",
    ")\n",
    "planner = planner_prompt | ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\", temperature=0\n",
    ").with_structured_output(Plan) \n",
    "\n",
    "plan_filter_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "You have been provided with a plan\n",
    "<Plan>\n",
    "{original_plan}\n",
    "</Plan>\n",
    "\n",
    "You are tasked with filtering this prompt\n",
    "1. Do not focus on input manipulation - any plan steps like extracting input/decomposing etc.\n",
    "2. Do not focus on updating output to state - store ouput to final state\n",
    "\n",
    "Check each step of the plan, and filter them to give final plan\n",
    "\"\"\")\n",
    "\n",
    "def plan_step(state: NodeBuilderState):\n",
    "    plan: Plan = planner.invoke({\"messages\": state[\"messages\"]})\n",
    "    llm_plan = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0).with_structured_output(Plan)\n",
    "    filtered_plan = llm_plan.invoke(plan_filter_prompt.format(original_plan=\"\\n\".join(plan_step for plan_step in plan.steps)))\n",
    "    return {\"plan\": filtered_plan.steps}\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "class Response(BaseModel):\n",
    "    \"\"\"Response to user.\"\"\"\n",
    "\n",
    "    response: str\n",
    "\n",
    "\n",
    "class Act(BaseModel):\n",
    "    \"\"\"Action to perform.\"\"\"\n",
    "\n",
    "    action: Union[Response, Plan] = Field(\n",
    "        description=\"Action to perform. If you want to respond to user, use Response. \"\n",
    "        \"If you need to further use tools to get the answer, use Plan.\"\n",
    "    )\n",
    "    justification: str = Field(description= \"Justificsation for choosing this action\")\n",
    "\n",
    "\n",
    "replanner_prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    \n",
    "You are tasked with taking next action based on the current state of the workflow.\n",
    "You have two actions to choose from: 'Response' or 'Plan'\n",
    "You will be provided an 'OriginalPlan' and 'PastSteps' of the plan that have been executed below:\n",
    "<OriginalPlan>\n",
    "{plan}\n",
    "</OriginalPlan>\n",
    "\n",
    "<PastSteps>\n",
    "{past_steps}\n",
    "</PastSteps>\n",
    "\n",
    "\n",
    "If the 'PastSteps' section covers all the components in the 'OriginalPlan', then no more steps are needed, respond back to the user using Response action.\n",
    "\n",
    "Otherwise, fill out the plan with the 'Plan' action. Only add steps to the plan that still NEED to be done. Do not return previously done steps as part of the plan.\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "\n",
    "replanner = replanner_prompt | ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\", temperature=0\n",
    ").with_structured_output(Act)\n",
    "\n",
    "def replan_step(state: NodeBuilderState):\n",
    "    output = replanner.invoke(state)\n",
    "    if isinstance(output.action, Response):\n",
    "        return {\"response\": output.action.response}\n",
    "    else:\n",
    "        return {\"plan\": output.action.steps}\n",
    "\n",
    "\n",
    "def should_end(state: NodeBuilderState\n",
    "):\n",
    "    if \"response\" in state and state[\"response\"]:\n",
    "        return \"code_compiler\"\n",
    "    else:\n",
    "        return \"ai_node_gen_supervisor\"\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14074b44",
   "metadata": {},
   "source": [
    "# Supervisor creation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "id": "e31de38d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.types import Command\n",
    "from typing import  Literal, TypedDict\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.language_models.chat_models import BaseChatModel\n",
    "from langchain_core.messages import SystemMessage, AIMessage\n",
    "\n",
    "def make_supervisor_node(llm: BaseChatModel, members: list[str]) -> str:\n",
    "    options = [\"FINISH\"] + members\n",
    "\n",
    "\n",
    "    class Router(BaseModel):\n",
    "        \"\"\"Worker to route to next. If no workers needed, route to FINISH.\"\"\"\n",
    "        next: Literal[*options] = Field(description=\"worker to act next\") # type: ignore\n",
    "        reason: str = Field(description=\"justification for selecting the particular next worker and expectations from it.\")\n",
    "\n",
    "\n",
    "    def supervisor_node(state: NodeBuilderState) -> Command[Literal[*members, \"__end__\"]]:\n",
    "        \"\"\"An LLM-based router.\"\"\"\n",
    "        plan = state[\"plan\"]\n",
    "        plan_str = \"\\n\".join(f\"{i+1}. {step}\" for i, step in enumerate(plan))\n",
    "        task = plan[0]\n",
    "        task_formatted = f\"\"\"For the following plan:\n",
    "{plan_str}\\n\\nYou are tasked with executing step {1}, {task}.\n",
    "\n",
    "You are a supervisor tasked with managing a conversation between the\n",
    "        following workers: {members}. Given the step,\n",
    "        respond with the worker to act next. The worker will perform a\n",
    "        task and respond with their results and status. When finished,\n",
    "        respond with FINISH.\n",
    "        \n",
    "        As a supervisor you need to identify which worker to call to execute the step along with justification.\n",
    "\"\"\"\n",
    "\n",
    "        messages =  [(\"user\", task_formatted)]+[HumanMessage(content=state[\"node_info\"])]\n",
    "        response = llm.with_structured_output(Router).invoke(messages)\n",
    "        goto = response.next\n",
    "        if goto == \"FINISH\":\n",
    "            return Command(goto=\"replan\", update={\"next\": \"replan\",\n",
    "                                          \"past_steps\": [(task, response.reason)],\n",
    "                                          \"messages\": [AIMessage(content= response.reason)]})\n",
    "               \n",
    "        return Command(goto=goto, update={\"next\": goto,\n",
    "                                          \"task\": task,\n",
    "                                          \"messages\": [AIMessage(content= response.reason)]})\n",
    "\n",
    "    return supervisor_node\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "902e38e9",
   "metadata": {},
   "source": [
    "# TOOLSETGEN agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "id": "62bd2ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "\n",
    "toolset_retrieval_prompt = \"\"\"\n",
    "    User will provide information about a node, Your task is to see if meeting the requirements of the node needs llm(aka model)'s tool-binding capability.\n",
    "    \n",
    "    Information about tool calling capabilities of LLMs, how to create tools and how to bind them with a LLM is provided below:\n",
    "\n",
    "<NEED_FOR_TOOL_BINDING>\n",
    "a. if the node's function is to answer a user_query which needs factual information to answer\n",
    "b. personal assistant which may need to respond with which tool should be used if any to meet the requirements \n",
    "c. Whenever the node involves a LLM(model) needing sensors or actuators, for information retrieval, and allowing llm(model) to 'DO' stuff\n",
    "d. One of the best use cases of this technique is when you know that there would be 'n' different tools which could be used in different scenarios to handle a query in a particular domain.\n",
    "</NEED_FOR_TOOL_BINDING>\n",
    "\n",
    "<TOOL_BINDING_EXAMPLE>\n",
    "Example 1: \n",
    "``` python\n",
    "from typing import Literal\n",
    "from langgraph.graph import StateGraph, MessagesState, START, END\n",
    "from langchain_core.messages import AIMessage\n",
    "from langchain_core.tools import tool\n",
    "from langgraph.prebuilt import ToolNode\n",
    "\n",
    "# Define the tools needed by the LLM\n",
    "\n",
    "@tool\n",
    "def get_weather(location: str):\n",
    "    \\\"\\\"\\\"Call to get the current weather.\\\"\\\"\\\"\n",
    "    if location.lower() in [\"sf\", \"san francisco\"]:\n",
    "        return \"It's 60 degrees and foggy.\"\n",
    "    else:\n",
    "        return \"It's 90 degrees and sunny.\"\n",
    "\n",
    "\n",
    "@tool\n",
    "def get_coolest_cities():\n",
    "    \\\"\\\"\\\"Get a list of coolest cities\\\"\\\"\\\"\n",
    "    return \"nyc, sf\"\n",
    "\n",
    "tools = [get_weather, get_coolest_cities]\n",
    "\n",
    "# Bind the model(llm) with tools\n",
    "model_with_tools = ChatAnthropic(\n",
    "    model=\"claude-3-haiku-20240307\", temperature=0\n",
    ").bind_tools(tools)\n",
    "\n",
    "# Generate a tool node.\n",
    "tool_node = ToolNode(tools)\n",
    "\n",
    "# conditional edge\n",
    "def should_continue(state: MessagesState):\n",
    "    messages = state[\"messages\"]\n",
    "    last_message = messages[-1]\n",
    "    if last_message.tool_calls:\n",
    "        return \"tools\"\n",
    "    return END\n",
    "\n",
    "# node implementation\n",
    "def call_model(state: MessagesState):\n",
    "    \\\"\\\"\\\"This node answers questions related to weather using a varied weather related toolset\\\"\\\"\\\"\n",
    "    messages = state[\"messages\"]\n",
    "    response = model_with_tools.invoke([SystemMessage(content= \"Please analyze the following weather-related query and provide a detailed response with relevant information: You have tools like get_weather and get_coolest_cities to help you answer the queries\" )] + messages)\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "\n",
    "# Stategraph compilation\n",
    "workflow = StateGraph(MessagesState)\n",
    "\n",
    "# Define the two nodes we will cycle between\n",
    "workflow.add_node(\"agent\", call_model)\n",
    "workflow.add_node(\"tools\", tool_node)\n",
    "\n",
    "workflow.add_edge(START, \"agent\")\n",
    "workflow.add_conditional_edges(\"agent\", should_continue, [\"tools\", END])\n",
    "workflow.add_edge(\"tools\", \"agent\")\n",
    "app = workflow.compile()\n",
    "```\n",
    "</TOOL_BINDING_EXAMPLE>\n",
    "\n",
    "<Output> \n",
    "Identify if tool_binding is relevant based on information 'NEED_FOR_TOOL_BINDING'\n",
    "if yes: identify what kind of tools may be needed to be used by a LLM, and then write code for the llm binding functionality, use 'TOOL_BINDING_EXAMPLE'\n",
    "if no: just respond no tool_binding needed.\n",
    "\n",
    "Only write python code as output\n",
    "</Output>\n",
    "    \"\"\"\n",
    "\n",
    "def toolset_generation(state: NodeBuilderState) -> Command[Literal[\"ai_node_gen_supervisor\"]]:\n",
    "    \"\"\"Generate the code for the node.\"\"\"\n",
    "    result = llm.invoke([SystemMessage(content=toolset_retrieval_prompt)] + state[\"messages\"])\n",
    "          \n",
    "    return Command(\n",
    "        update={\n",
    "            \"past_steps\": [(state[\"task\"], result.content)],\n",
    "            \"messages\": [\n",
    "                HumanMessage(content=result.content, name=\"toolset_generator\")\n",
    "            ]\n",
    "        },\n",
    "        goto=\"replan\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e1a9651",
   "metadata": {},
   "source": [
    "# PROMPTGEN agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "id": "8d1fcb24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "\n",
    "prompt_vector_store = get_vector_store(embeddings, \"promptguide\")\n",
    "prompt_retriever = prompt_vector_store.as_retriever()\n",
    "prompt_gen_retriever = create_retriever_tool(prompt_retriever, \"Retrieve_info_on_prompting\", \"Search information about what are different prompting techniques relevant to the user requirements.\")\n",
    "\n",
    "promptgen_prompt = ChatPromptTemplate.from_messages([\n",
    "         (\"system\", \"\"\"\n",
    "    You are a ReAct (Reasoning and Act) agent.\n",
    "    You are tasked with generating a prompt to meet the objectives of the langgraph node.\n",
    "    The langgraph node information is provided. \n",
    "    \n",
    "    For example: \n",
    "    User query: the node is supposed to generate a plan using llms\n",
    "    Thought: I need to generate a prompt that will make the LLM generate a plan for the given task.\n",
    "    Action: Use the  'Retrieve_info_on_prompting' tool to search for plan-and-execute prompting techniques.\n",
    "    Observation: I found a plan-and-execute prompting technique that can generate a plan for the given task.\n",
    "    Action: I will customize the observed prompt to meet the requirements of the node.\n",
    "    \n",
    "    IMPORTANT: Your final output will be only a prompt, no code\n",
    "    \"\"\"),\n",
    "    (\"placeholder\", \"{messages}\"),\n",
    "    ])\n",
    "prompt_gen_agent = create_react_agent(llm, tools=[prompt_gen_retriever], prompt = promptgen_prompt)\n",
    "\n",
    "def prompt_generation(state: NodeBuilderState) -> Command[Literal[\"ai_node_gen_supervisor\"]]:\n",
    "    \"\"\"Generate the code for the node.\"\"\"\n",
    "    response = prompt_gen_agent.invoke({\"messages\": state[\"messages\"]})\n",
    "    return Command(\n",
    "        update={\n",
    "            \"past_steps\": [(state[\"task\"], response[\"messages\"][-1].content)],\n",
    "            \"messages\": [\n",
    "                HumanMessage(content=response[\"messages\"][-1].content, name=\"prompt_generator\")\n",
    "            ]\n",
    "        },\n",
    "        goto=\"replan\",\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "226799c9",
   "metadata": {},
   "source": [
    "# STRUCTUTRED OUTPUT GEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "id": "6fdc4480",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "struc_output_prompt = \"\"\"\n",
    "    User will provide you with the information about the node,  you are supposed to analyze the information and see if it requires LLM (aka model)'s 'with_structured_output()' function.\n",
    "\n",
    "Here is information about structured outputs, why and when to use structured outputs:\n",
    "<AboutStructuredOutput>\n",
    "Structured output is beneficial in situations where consistent and verifiable data formats are needed, especially when integrating with databases, APIs, or building complex workflows. It helps reduce hallucinations, simplifies prompting, and enables reliable type-safety, making applications more predictable and easier to evaluate. \n",
    "\n",
    "Here's a more detailed breakdown:\n",
    "1. Reduced Hallucinations: By enforcing adherence to a JSON Schema, structured outputs minimize the chance of models generating incorrect or irrelevant data. \n",
    "2. Simplified Prompting: You don't need overly complex or specific prompts to get consistently formatted output, as the schema provides the structure. \n",
    "3. Reliable Type-Safety: Structured outputs ensure that the model always generates data that fits the defined schema, eliminating the need for validation or retries due to format errors. \n",
    "4. Building Complex Workflows: Structured outputs are particularly useful for building multi-step workflows where the output of one step serves as input for the next. \n",
    "5. Integration with Databases and APIs: When integrating with systems that require structured data formats (like databases or APIs), structured outputs ensure consistency and avoid integration problems. \n",
    "6. Function Calling and Data Extraction: Structured outputs are recommended for function calling and extracting structured data from various sources. \n",
    "7. Consistent and Verifiable Output: The predictable format of structured outputs makes it easier to test, debug, and evaluate applications that rely on them. \n",
    "8. Explicit Refusals: You can now detect model refusals programmatically when using structured outputs, as the model will explicitly return a structured error message instead of a text-based one. \n",
    "\n",
    "In essence, structured output provides a robust and predictable way to manage the output of language models, making them more reliable and easier to integrate into various applications. \n",
    "</AboutStructuredOutput>\n",
    "\n",
    "Here is an example showing how to use 'with_structured_output' method is below:\n",
    "<StructuredOutputExample>\n",
    "Here is an example of how to use structured output. In this example, we want the LLM to generate and fill up the pydantic class Joke, based on user query. \n",
    "``` python\n",
    "from typing import Optional\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "# Pydantic class for structured output\n",
    "class Joke(BaseModel):\n",
    "    setup: str = Field(description=\"The setup of the joke\")\n",
    "    punchline: str = Field(description=\"The punchline to the joke\")\n",
    "    rating: Optional[int] = Field(\n",
    "        default=None, description=\"How funny the joke is, from 1 to 10\"\n",
    "    )\n",
    "\n",
    "class JokeBuilderState(MessagesState):\n",
    "    joke: Joke = Field(description= \"joke generated by the GenerateJoke node.\")\n",
    "\n",
    "def GenerateJoke(state: JokeBuilderState):\n",
    "    structured_llm = llm.with_structured_output(Joke)\n",
    "    joke: Joke = structured_llm.invoke(\"Tell me a joke about cats\")\n",
    "    return { \"joke\": joke }\n",
    "```\n",
    "\n",
    "Output:\n",
    "Joke(setup='Why was the cat sitting on the computer?', punchline='Because it wanted to keep an eye on the mouse!', rating=7)\n",
    "</StructuredOutputExample>\n",
    "    \n",
    "<Notes> \n",
    "1. The structured output pydantic class is not to be the same as the input_schema or output_schema, need to be specific\n",
    "2. Do not hallucinate or write your own code to implement structured output, refer to 'StructuredOutputExample' section\n",
    "</Notes>\n",
    "    \n",
    "<Output>\n",
    "Check if the node needs llm's structured output functionality based on 'AboutStructuredOutput' section.\n",
    "If yes, you are supposed to generate the code for the structured output functionality for the llm to use. \n",
    "If not needed, just mention there is no need for structured output functionality.\n",
    "</Output>\n",
    "    \"\"\"\n",
    "\n",
    "def structured_output_generation(state: NodeBuilderState) -> Command[Literal[\"ai_node_gen_supervisor\"]]:\n",
    "    \"\"\"Generate the code for the node.\"\"\"\n",
    "    # fetch_documents(\"https://python.langchain.com/docs/how_to/structured_output/\")\n",
    "    result = llm.invoke( [SystemMessage(content=struc_output_prompt)]  + state[\"messages\"])\n",
    "\n",
    "    return Command(\n",
    "        update={\n",
    "            \"past_steps\": [(state[\"task\"], result.content)],\n",
    "            \"messages\": [\n",
    "                HumanMessage(content=result.content, name=\"struct_output_generator\")\n",
    "            ]\n",
    "            \n",
    "        },\n",
    "        goto=\"replan\",\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "209b2f8f",
   "metadata": {},
   "source": [
    "# Interrupt gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc504e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "interrupt_gen_prompt: str = \"\"\"\n",
    "    User will provide you with the information about the node, you are supposed to analyze the information and see if it requires interrupt functionality.\n",
    "    \n",
    "    Following is the information about the interrupt functionality, what is the purpose of interrupt, design patterns, how to implement them:\n",
    "    <InterruptInfo>\n",
    "    The interrupt function in LangGraph enables human-in-the-loop workflows by pausing the graph at a specific node, presenting information to a human, and resuming the graph with their input. This function is useful for tasks like approvals, edits, or collecting additional input. The interrupt function is used in conjunction with the Command object to resume the graph with a value provided by the human.\n",
    "\n",
    "\n",
    "``` python\n",
    "from langgraph.types import interrupt\n",
    "\n",
    "def human_node(state: State):\n",
    "    value = interrupt(\n",
    "        # Any JSON serializable value to surface to the human.\n",
    "        # For example, a question or a piece of text or a set of keys in the state\n",
    "       {\n",
    "          \"text_to_revise\": state[\"some_text\"]\n",
    "       }\n",
    "    )\n",
    "    # Update the state with the human's input or route the graph based on the input.\n",
    "    return {\n",
    "        \"some_text\": value\n",
    "    }\n",
    "\n",
    "graph = graph_builder.compile(\n",
    "    checkpointer=checkpointer # Required for `interrupt` to work\n",
    ")\n",
    "\n",
    "# Run the graph until the interrupt\n",
    "thread_config = {\"configurable\": {\"thread_id\": \"some_id\"}}\n",
    "graph.invoke(some_input, config=thread_config)\n",
    "\n",
    "# Resume the graph with the human's input\n",
    "graph.invoke(Command(resume=value_from_human), config=thread_config)\n",
    "```\n",
    "    </InterruptInfo>\n",
    "    \n",
    "    <Output>\n",
    "    Unless explicitly mentioned in node requirements, human-in-loop aka interrupt is not needed in the scenario.\n",
    "    If needed: implement the code with interrupt functionality tailored to the use case\n",
    "    If not needed: just respond that interrupt functionality is not needed\n",
    "    </Output>\n",
    "    \"\"\"\n",
    "\n",
    "def interrupt_generation(state: NodeBuilderState) -> Command[Literal[\"ai_node_gen_supervisor\"]]:\n",
    "    \"\"\"Generate the code for the node.\"\"\"\n",
    "    # fetch_docs=fetch_documents(\"https://langchain-ai.github.io/langgraph/concepts/human_in_the_loop/\")\n",
    "    response = llm.invoke( [SystemMessage(content=interrupt_gen_prompt)]  + state[\"messages\"])\n",
    "    return Command(\n",
    "        update={\n",
    "            \"past_steps\": [(state[\"task\"], response.content)],\n",
    "            \"messages\": [\n",
    "                HumanMessage(content=response.content, name=\"interrupt_generator\")\n",
    "            ]\n",
    "        },\n",
    "        goto=\"replan\",\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0deeedc8",
   "metadata": {},
   "source": [
    "# Code compiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "id": "dbec43be",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "code_compiler_prompt = \"\"\"\n",
    "    You are tasked with furnishing a final output code for the user provided node information. \n",
    "    \n",
    "    Analyze the message history, you will find multiple code pieces\n",
    "    You need to carefully merge all the generate code together to furnish a final output.\n",
    "    \n",
    "    Only write python code with comments as output, no other explanations\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "def code_compiler(state: NodeBuilderState):\n",
    "    \"\"\"Generate the code for the node.\"\"\"\n",
    "    response = llm.invoke([SystemMessage(content=code_compiler_prompt)]  + state[\"messages\"])\n",
    "    return {\"final_code\": response.content}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "id": "862c59c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ai_node_gen_supervisor = make_supervisor_node(llm, [\"prompt_generation\", \"toolset_generation\", \"structured_output_generation\", \"interrupt_generation\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da4e52b1",
   "metadata": {},
   "source": [
    "# Graph creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "id": "d05ec844",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph.state import CompiledStateGraph\n",
    "from langgraph.graph import StateGraph, START\n",
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "\n",
    "\n",
    "workflow = StateGraph(NodeBuilderState)\n",
    "workflow.add_node(\"identify_node\", identify_node)\n",
    "workflow.add_node(\"deterministic_code\", deterministic_code_gen)\n",
    "workflow.add_node(\"replan\", replan_step)\n",
    "workflow.add_node(\"planner\", plan_step)\n",
    "workflow.add_node(\"ai_node_gen_supervisor\", ai_node_gen_supervisor)\n",
    "workflow.add_node(\"prompt_generation\", prompt_generation)\n",
    "workflow.add_node(\"toolset_generation\", toolset_generation)\n",
    "workflow.add_node(\"structured_output_generation\", structured_output_generation)\n",
    "workflow.add_node(\"interrupt_generation\", interrupt_generation)\n",
    "workflow.add_node(\"code_compiler\", code_compiler)\n",
    "\n",
    "workflow.add_edge(START, \"identify_node\")\n",
    "workflow.add_conditional_edges(\"identify_node\", determine_node_type, [\"deterministic_code\", \"planner\"])\n",
    "workflow.add_edge(\"deterministic_code\", END)\n",
    "workflow.add_edge(\"planner\", \"ai_node_gen_supervisor\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"replan\",\n",
    "    # Next, we pass in the function that will determine which node is called next.\n",
    "    should_end,\n",
    "    [\"ai_node_gen_supervisor\", \"code_compiler\"],\n",
    ")\n",
    "workflow.add_edge(\"code_compiler\", END)\n",
    "\n",
    "checkpointer = InMemorySaver()\n",
    "node_to_code_app : CompiledStateGraph = workflow.compile(checkpointer=checkpointer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
